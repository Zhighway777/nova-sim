{
  "bossa_nova.__init__._link_alias": {
    "id": "bossa_nova.__init__._link_alias",
    "name": "_link_alias",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/bossa_nova/__init__.py",
    "relative_path": "bossa_nova/__init__.py",
    "depends_on": [],
    "source_code": "def _link_alias(alias: str, target: str) -> None:\n    module = importlib.import_module(target)\n    sys.modules[f\"{__name__}.{alias}\"] = module",
    "start_line": 25,
    "end_line": 27,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "alias",
      "target"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _link_alias",
    "component_id": "bossa_nova.__init__._link_alias"
  },
  "cache_model.arch.AbstractGCU": {
    "id": "cache_model.arch.AbstractGCU",
    "name": "AbstractGCU",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/arch.py",
    "relative_path": "cache_model/arch.py",
    "depends_on": [],
    "source_code": "class AbstractGCU:\n    \"\"\"Base class emulating the external cache model API.\"\"\"\n\n    def __init__(self, hardware):\n        self.hardware = hardware\n\n    def get_last_memory_manager(self):\n        raise NotImplementedError\n\n    def process(self, request, timestamp):\n        raise NotImplementedError\n\n    def post_process(self, timestamp):\n        pass\n\n    def stat_dict(self):\n        return {}\n\n    def histogram_dict(self):\n        return {}",
    "start_line": 1,
    "end_line": 20,
    "has_docstring": true,
    "docstring": "Base class emulating the external cache model API.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AbstractGCU",
    "component_id": "cache_model.arch.AbstractGCU"
  },
  "cache_model.entity.model.No_MSHR": {
    "id": "cache_model.entity.model.No_MSHR",
    "name": "No_MSHR",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class No_MSHR(RuntimeError):\n    \"\"\"Raised when the simulated cache runs out of MSHR slots.\"\"\"",
    "start_line": 7,
    "end_line": 8,
    "has_docstring": true,
    "docstring": "Raised when the simulated cache runs out of MSHR slots.",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "RuntimeError"
    ],
    "class_name": null,
    "display_name": "class No_MSHR",
    "component_id": "cache_model.entity.model.No_MSHR"
  },
  "cache_model.entity.model.L1C_Config": {
    "id": "cache_model.entity.model.L1C_Config",
    "name": "L1C_Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class L1C_Config:\n    CACHE_LINE_SIZE: int\n    CACHE_WAYS: int\n    CACHE_SIZE: int\n    MEM_LATENCY: int\n    NON_MEM_LATENCY: int\n    NUM_MSHR: int\n    NUM_OF_SIP: int\n    CACHE_SIZE_PER_SIP: int",
    "start_line": 12,
    "end_line": 20,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class L1C_Config",
    "component_id": "cache_model.entity.model.L1C_Config"
  },
  "cache_model.entity.model.LLC_Config": {
    "id": "cache_model.entity.model.LLC_Config",
    "name": "LLC_Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class LLC_Config:\n    CACHE_LINE_SIZE: int\n    CACHE_WAYS: int\n    CACHE_SIZE: int\n    MEM_LATENCY: int\n    NON_MEM_LATENCY: int\n    NUM_MSHR: int\n    NUM_OF_PARTITIONS: int = 1\n    NUM_OF_SLICES_PER_PARTITION: int = 1",
    "start_line": 24,
    "end_line": 32,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class LLC_Config",
    "component_id": "cache_model.entity.model.LLC_Config"
  },
  "cache_model.entity.model.L3_Config": {
    "id": "cache_model.entity.model.L3_Config",
    "name": "L3_Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class L3_Config:\n    START_ADDR: int\n    SIZE_PER_HBM: int\n    NUM_OF_HBM: int",
    "start_line": 36,
    "end_line": 39,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class L3_Config",
    "component_id": "cache_model.entity.model.L3_Config"
  },
  "cache_model.entity.model.Memory": {
    "id": "cache_model.entity.model.Memory",
    "name": "Memory",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class Memory:\n    LLC: LLC_Config\n    L3: Optional[L3_Config] = None\n    L1C: Optional[L1C_Config] = None",
    "start_line": 43,
    "end_line": 46,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Memory",
    "component_id": "cache_model.entity.model.Memory"
  },
  "cache_model.entity.model.HardwareConfig": {
    "id": "cache_model.entity.model.HardwareConfig",
    "name": "HardwareConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class HardwareConfig:\n    MEMORY: Memory\n\n    def __getattr__(self, item: str):\n        \"\"\"Expose LLC fields directly for the legacy SimpleStrategy helper.\"\"\"\n        llc = getattr(self.MEMORY, \"LLC\", None)\n        if llc and hasattr(llc, item):\n            return getattr(llc, item)\n        raise AttributeError(item)",
    "start_line": 50,
    "end_line": 58,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class HardwareConfig",
    "component_id": "cache_model.entity.model.HardwareConfig"
  },
  "cache_model.entity.model.ModelContext": {
    "id": "cache_model.entity.model.ModelContext",
    "name": "ModelContext",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class ModelContext:\n    hardware: HardwareConfig\n    timestamp: int = 0",
    "start_line": 62,
    "end_line": 64,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ModelContext",
    "component_id": "cache_model.entity.model.ModelContext"
  },
  "cache_model.entity.model.Access": {
    "id": "cache_model.entity.model.Access",
    "name": "Access",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class Access:\n    direction: int\n    address: int\n    line_addr: int\n    width: int\n    end_address: int\n    thread: Any = None",
    "start_line": 68,
    "end_line": 74,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Access",
    "component_id": "cache_model.entity.model.Access"
  },
  "cache_model.entity.model.Request": {
    "id": "cache_model.entity.model.Request",
    "name": "Request",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/model.py",
    "relative_path": "cache_model/entity/model.py",
    "depends_on": [],
    "source_code": "class Request:\n    direction: int\n    address: int\n    line_addr: int\n    thread: Any = None",
    "start_line": 78,
    "end_line": 82,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Request",
    "component_id": "cache_model.entity.model.Request"
  },
  "cache_model.entity.report.DistanceCount": {
    "id": "cache_model.entity.report.DistanceCount",
    "name": "DistanceCount",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/entity/report.py",
    "relative_path": "cache_model/entity/report.py",
    "depends_on": [],
    "source_code": "class DistanceCount:\n    distance: int\n    count: int",
    "start_line": 5,
    "end_line": 7,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DistanceCount",
    "component_id": "cache_model.entity.report.DistanceCount"
  },
  "cache_model.memory.__init__.AbstractMemoryManager": {
    "id": "cache_model.memory.__init__.AbstractMemoryManager",
    "name": "AbstractMemoryManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/__init__.py",
    "relative_path": "cache_model/memory/__init__.py",
    "depends_on": [],
    "source_code": "class AbstractMemoryManager:\n    def __init__(self, config, context, next_level=None):\n        self.config = config\n        self.context = context\n        self.next_level = next_level\n\n    def process(self, request):\n        raise NotImplementedError\n\n    def post_process(self, timestamp):\n        if self.next_level:\n            self.next_level.post_process(timestamp)\n\n    def stat(self) -> Counter:\n        return Counter()\n\n    def histogram(self) -> Counter:\n        return Counter()",
    "start_line": 4,
    "end_line": 21,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AbstractMemoryManager",
    "component_id": "cache_model.memory.__init__.AbstractMemoryManager"
  },
  "cache_model.memory.addr_converter.addr_to_llc_index": {
    "id": "cache_model.memory.addr_converter.addr_to_llc_index",
    "name": "addr_to_llc_index",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/addr_converter.py",
    "relative_path": "cache_model/memory/addr_converter.py",
    "depends_on": [],
    "source_code": "def addr_to_llc_index(addr: int, cache_line_size: int, sets_per_slice: int, total_slices: int) -> int:\n    \"\"\"Simple helper that mimics the behaviour required by GCU Libra.\"\"\"\n    if total_slices <= 0:\n        return 0\n    line_addr = addr // cache_line_size\n    if sets_per_slice <= 0:\n        return line_addr % total_slices\n    return (line_addr // sets_per_slice) % total_slices",
    "start_line": 1,
    "end_line": 8,
    "has_docstring": true,
    "docstring": "Simple helper that mimics the behaviour required by GCU Libra.",
    "parameters": [
      "addr",
      "cache_line_size",
      "sets_per_slice",
      "total_slices"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function addr_to_llc_index",
    "component_id": "cache_model.memory.addr_converter.addr_to_llc_index"
  },
  "cache_model.memory.memory_manger._LRUSet": {
    "id": "cache_model.memory.memory_manger._LRUSet",
    "name": "_LRUSet",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/memory_manger.py",
    "relative_path": "cache_model/memory/memory_manger.py",
    "depends_on": [],
    "source_code": "class _LRUSet:\n    def __init__(self, ways: int):\n        self._ways = max(1, ways)\n        self._lines: OrderedDict[int, None] = OrderedDict()\n\n    def access(self, tag: int) -> bool:\n        hit = tag in self._lines\n        if hit:\n            self._lines.move_to_end(tag)\n        else:\n            if len(self._lines) >= self._ways:\n                self._lines.popitem(last=False)\n            self._lines[tag] = None\n        return hit",
    "start_line": 10,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _LRUSet",
    "component_id": "cache_model.memory.memory_manger._LRUSet"
  },
  "cache_model.memory.memory_manger._BaseCacheManager": {
    "id": "cache_model.memory.memory_manger._BaseCacheManager",
    "name": "_BaseCacheManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/memory_manger.py",
    "relative_path": "cache_model/memory/memory_manger.py",
    "depends_on": [],
    "source_code": "class _BaseCacheManager(AbstractMemoryManager):\n    def __init__(self, config, context: ModelContext, next_level=None):\n        super().__init__(config, context, next_level=next_level)\n        self._stat = Counter()\n        self._hist = Counter()\n        self._last_access = {}\n\n    def _record(self, request: Request, hit: bool):\n        op = \"read\" if request.direction == 0 else \"write\"\n        key = f\"{op}_{'hits' if hit else 'misses'}\"\n        self._stat[key] += 1\n        last = self._last_access.get((request.sip_id, request.line_addr))\n        if last is not None and hit:\n            distance = max(1, self.context.timestamp - last)\n            self._hist[distance] += 1\n        self._last_access[(request.sip_id, request.line_addr)] = self.context.timestamp\n\n    def post_process(self, timestamp):\n        super().post_process(timestamp)\n        self.context.timestamp = timestamp\n\n    def stat(self) -> Counter:\n        return Counter(self._stat)\n\n    def histogram(self) -> Counter:\n        return Counter(self._hist)",
    "start_line": 26,
    "end_line": 51,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractMemoryManager"
    ],
    "class_name": null,
    "display_name": "class _BaseCacheManager",
    "component_id": "cache_model.memory.memory_manger._BaseCacheManager"
  },
  "cache_model.memory.memory_manger.L3Manager": {
    "id": "cache_model.memory.memory_manger.L3Manager",
    "name": "L3Manager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/memory_manger.py",
    "relative_path": "cache_model/memory/memory_manger.py",
    "depends_on": [
      "cache_model.memory.memory_manger._BaseCacheManager"
    ],
    "source_code": "class L3Manager(_BaseCacheManager):\n    def __init__(self, config, context: ModelContext):\n        super().__init__(config, context, next_level=None)\n\n    def process(self, request: Request):\n        # Model L3 as an always-hit terminal level.\n        self._record(request, True)",
    "start_line": 54,
    "end_line": 60,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "_BaseCacheManager"
    ],
    "class_name": null,
    "display_name": "class L3Manager",
    "component_id": "cache_model.memory.memory_manger.L3Manager"
  },
  "cache_model.memory.memory_manger.LLCManager": {
    "id": "cache_model.memory.memory_manger.LLCManager",
    "name": "LLCManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/memory_manger.py",
    "relative_path": "cache_model/memory/memory_manger.py",
    "depends_on": [
      "cache_model.memory.memory_manger._LRUSet",
      "cache_model.memory.memory_manger._BaseCacheManager",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
    ],
    "source_code": "class LLCManager(_BaseCacheManager):\n    def __init__(\n        self,\n        config: LLC_Config,\n        context: ModelContext,\n        next_level: L3Manager,\n        addr_convert: Callable[[int], int] | None = None,\n    ):\n        super().__init__(config, context, next_level=next_level)\n        self._addr_convert = addr_convert or (lambda addr: 0)\n        self._total_slices = max(\n            1, config.NUM_OF_PARTITIONS * config.NUM_OF_SLICES_PER_PARTITION\n        )\n        sets_per_slice = max(\n            1, config.CACHE_SIZE // config.CACHE_LINE_SIZE // config.CACHE_WAYS\n        )\n        self._sets = [\n            [_LRUSet(config.CACHE_WAYS) for _ in range(sets_per_slice)]\n            for _ in range(self._total_slices)\n        ]\n        self._sets_per_slice = sets_per_slice\n\n    def process(self, request: Request):\n        slice_idx = self._addr_convert(request.address) % self._total_slices\n        set_idx = request.line_addr % self._sets_per_slice\n        cache_set = self._sets[slice_idx][set_idx]\n        hit = cache_set.access(request.line_addr)\n        if not hit and self.next_level:\n            self.next_level.process(request)\n        self._record(request, hit)",
    "start_line": 63,
    "end_line": 92,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "_BaseCacheManager"
    ],
    "class_name": null,
    "display_name": "class LLCManager",
    "component_id": "cache_model.memory.memory_manger.LLCManager"
  },
  "cache_model.memory.memory_manger.L1CManager": {
    "id": "cache_model.memory.memory_manger.L1CManager",
    "name": "L1CManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/cache_model/memory/memory_manger.py",
    "relative_path": "cache_model/memory/memory_manger.py",
    "depends_on": [
      "cache_model.memory.memory_manger._LRUSet",
      "cache_model.memory.memory_manger._BaseCacheManager",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
    ],
    "source_code": "class L1CManager(_BaseCacheManager):\n    def __init__(\n        self,\n        config: L1C_Config,\n        context: ModelContext,\n        next_level: LLCManager,\n        cache_selector: Callable[[Request, int], Tuple[int, int]],\n    ):\n        super().__init__(config, context, next_level=next_level)\n        self._cache_selector = cache_selector\n        self._num_sets = max(1, config.CACHE_SIZE // config.CACHE_LINE_SIZE)\n        self._num_slices = max(1, config.NUM_OF_SIP)\n        self._ways = max(1, config.CACHE_WAYS)\n        self._sets = [\n            [_LRUSet(self._ways) for _ in range(self._num_sets)]\n            for _ in range(self._num_slices)\n        ]\n\n    def process(self, request: Request):\n        slice_idx, set_idx = self._cache_selector(request, self._num_sets)\n        slice_idx %= self._num_slices\n        set_idx %= self._num_sets\n        cache_set = self._sets[slice_idx][set_idx]\n        hit = cache_set.access(request.line_addr)\n        if not hit and self.next_level:\n            self.next_level.process(request)\n        self._record(request, hit)",
    "start_line": 95,
    "end_line": 121,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "_BaseCacheManager"
    ],
    "class_name": null,
    "display_name": "class L1CManager",
    "component_id": "cache_model.memory.memory_manger.L1CManager"
  },
  "nova-platform.nova_lite.pipeline.SimulationResult": {
    "id": "nova-platform.nova_lite.pipeline.SimulationResult",
    "name": "SimulationResult",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_lite/pipeline.py",
    "relative_path": "nova-platform/nova_lite/pipeline.py",
    "depends_on": [],
    "source_code": "class SimulationResult:\n    \"\"\"Aggregated artifacts produced by a simulation run.\"\"\"\n\n    report: Dict\n    report_path: Path\n    trace_path: Path\n    output_dir: Path",
    "start_line": 16,
    "end_line": 22,
    "has_docstring": true,
    "docstring": "Aggregated artifacts produced by a simulation run.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class SimulationResult",
    "component_id": "nova-platform.nova_lite.pipeline.SimulationResult"
  },
  "nova-platform.nova_lite.pipeline.SimulationPipeline": {
    "id": "nova-platform.nova_lite.pipeline.SimulationPipeline",
    "name": "SimulationPipeline",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_lite/pipeline.py",
    "relative_path": "nova-platform/nova_lite/pipeline.py",
    "depends_on": [
      "nova-platform.nova_lite.pipeline.SimulationResult",
      "nova-platform.nova_platform.simulator.case.CaseInfo"
    ],
    "source_code": "class SimulationPipeline:\n    \"\"\"\n    High-level orchestrator for the GEMM → dataflow gen → execute → trace → report flow.\n\n    This wraps the lower-level ``CaseInfo`` class so downstream projects can\n    trigger a Python-only simulation without interacting with the extensive test harness.\n    \"\"\"\n\n    def __init__(self, config_template: str | Path, output_root: str | Path | None = None):\n        repo_dir = Path(__file__).resolve().parents[1]\n\n        config_template = Path(config_template)\n        if not config_template.is_absolute():\n            config_template = (repo_dir / config_template).resolve()\n        self.config_template = config_template\n        self.arch = self._detect_arch(self.config_template)\n\n        if output_root is None:\n            output_root = repo_dir / \"out\" / \"nova_lite\"\n        else:\n            output_root = Path(output_root)\n            if not output_root.is_absolute():\n                output_root = (repo_dir / output_root).resolve()\n        self.output_root = output_root\n        self.output_root.mkdir(parents=True, exist_ok=True)\n\n    def run_gemm(\n        self,\n        shape: Iterable[int],\n        dtype: DType = DType.FP16,\n        bench_version: int = 5,\n        quant_type: Optional[str] = None,\n        enable_cache: bool = False,\n        topo: TOPO = TOPO.STANDALONE,\n        force_rerun: bool = True,\n        tags: Optional[List[str]] = None,\n    ) -> SimulationResult:\n        \"\"\"\n        Execute a GEMM benchmark end-to-end and collect the generated artifacts.\n\n        Parameters\n        ----------\n        shape:\n            The GEMM tensor shape in ``[B, M, K, N]`` order.\n        dtype:\n            Datatype for the computation (defaults to ``FP16``).\n        bench_version:\n            Controls the GEMM kernel flavour. Versions ``5`` and ``6`` map to the\n            shared and local micro-architectures respectively. Other versions fall\n            back to the classic GEMM configuration.\n        quant_type:\n            Optional quantisation mode (matches existing ``bench_gemm_quant_type`` values).\n        enable_cache:\n            Toggles the cache service inside the executor.\n        topo:\n            Target topology (defaults to standalone).\n        force_rerun:\n            If ``True`` (default) the simulation ignores any cached report under ``output_root``.\n        tags:\n            Optional tag list carried through metadata for downstream tooling.\n        \"\"\"\n\n        shape = list(shape)\n        case_dir = self._build_case_dir(dtype, shape, bench_version)\n        self._ensure_file_logging(case_dir)\n        dataflow_config = {\"bench_gemm_op_version\": bench_version}\n        if quant_type:\n            dataflow_config[\"bench_gemm_quant_type\"] = quant_type\n\n        case = CaseInfo(\n            optype=\"gemm\",\n            shape=shape,\n            dtype=dtype,\n            config=str(self.config_template),\n            outdir=str(case_dir),\n            tag=tags or [\"nova-lite\"],\n            dataflow_config=dataflow_config,\n            enable_cache=enable_cache,\n            topo=topo,\n        )\n        case.do_sim(force_rerun=force_rerun)\n        return self._collect_results(case_dir)\n\n    # ------------------------------------------------------------------ #\n    # helpers\n    # ------------------------------------------------------------------ #\n\n    def _build_case_dir(self, dtype: DType, shape: List[int], bench_version: int) -> Path:\n        shape_token = \"-\".join(str(dim) for dim in shape)\n        arch_token = f\"_{self.arch.lower()}\" if self.arch else \"\"\n        dirname = f\"gemm_v{bench_version}_{dtype.name.lower()}_{shape_token}{arch_token}\"\n        case_dir = self.output_root / dirname\n        case_dir.mkdir(parents=True, exist_ok=True)\n        return case_dir\n\n    def _detect_arch(self, config_template: Path) -> str | None:\n        try:\n            with open(config_template) as f:\n                for line in f:\n                    stripped = line.strip()\n                    if not stripped or stripped.startswith(\"#\"):\n                        continue\n                    if stripped.lower().startswith(\"arch\"):\n                        parts = stripped.split(\":\")\n                        if len(parts) >= 2:\n                            val = parts[1].strip()\n                            return val or None\n        except Exception:\n            pass\n        return None\n\n    def _collect_results(self, case_dir: Path) -> SimulationResult:\n        report_files = sorted(case_dir.glob(\"gcu*/report.yaml\"))\n        if not report_files:\n            raise FileNotFoundError(f\"No report.yaml produced under {case_dir}\")\n        report_path = report_files[0]\n        with open(report_path) as f:\n            report = yaml.safe_load(f) or {}\n\n        trace_path = report_path.parent / \"trace.perfetto-trace\"\n        return SimulationResult(\n            report=report,\n            report_path=report_path,\n            trace_path=trace_path,\n            output_dir=case_dir,\n        )\n\n    def _ensure_file_logging(self, case_dir: Path) -> None:\n        \"\"\"Attach a file handler under the case directory to capture INFO logs.\"\"\"\n        log_path = case_dir / \"nova-lite.log\"\n        root = logging.getLogger()\n        # Avoid adding duplicate handlers for the same file\n        for handler in root.handlers:\n            if isinstance(handler, logging.FileHandler) and Path(handler.baseFilename) == log_path:\n                break\n        else:\n            log_path.parent.mkdir(parents=True, exist_ok=True)\n            handler = logging.FileHandler(log_path, encoding=\"utf-8\")\n            handler.setLevel(logging.INFO)\n            handler.setFormatter(logging.Formatter(\"%(asctime)s %(levelname)s %(name)s: %(message)s\"))\n            root.addHandler(handler)\n        if root.level > logging.INFO:\n            root.setLevel(logging.INFO)",
    "start_line": 25,
    "end_line": 167,
    "has_docstring": true,
    "docstring": "High-level orchestrator for the GEMM → dataflow gen → execute → trace → report flow.\n\nThis wraps the lower-level ``CaseInfo`` class so downstream projects can\ntrigger a Python-only simulation without interacting with the extensive test harness.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class SimulationPipeline",
    "component_id": "nova-platform.nova_lite.pipeline.SimulationPipeline"
  },
  "nova-platform.nova_lite.test_pipeline.test_pipeline_end_to_end": {
    "id": "nova-platform.nova_lite.test_pipeline.test_pipeline_end_to_end",
    "name": "test_pipeline_end_to_end",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_lite/test_pipeline.py",
    "relative_path": "nova-platform/nova_lite/test_pipeline.py",
    "depends_on": [
      "nova-platform.nova_lite.pipeline.SimulationPipeline"
    ],
    "source_code": "def test_pipeline_end_to_end():\n    repo_dir = Path(__file__).resolve().parent\n    config_path = Path(\"config\") / \"libra_1DIE_3.2TB_24SIP_256OST.yaml\"\n    pipeline = SimulationPipeline(config_path)\n    result = None\n\n    try:\n        result = pipeline.run_gemm(\n            shape=[1, 16, 64, 64],\n            dtype=DType.FP16,\n            bench_version=5,\n            force_rerun=True,\n        )\n\n        assert result.report_path.exists()\n        assert result.trace_path.exists()\n        assert \"total_latency\" in result.report\n        assert result.output_dir.is_relative_to(pipeline.output_root)\n    finally:\n        if result and result.output_dir.exists():\n            shutil.rmtree(result.output_dir)\n        out_root = pipeline.output_root\n        if out_root.is_dir():\n            try:\n                out_root.rmdir()\n            except OSError:\n                pass",
    "start_line": 8,
    "end_line": 34,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function test_pipeline_end_to_end",
    "component_id": "nova-platform.nova_lite.test_pipeline.test_pipeline_end_to_end"
  },
  "nova-platform.nova_platform.base_model.TensorFloat": {
    "id": "nova-platform.nova_platform.base_model.TensorFloat",
    "name": "TensorFloat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class TensorFloat:\n    pass",
    "start_line": 9,
    "end_line": 10,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TensorFloat",
    "component_id": "nova-platform.nova_platform.base_model.TensorFloat"
  },
  "nova-platform.nova_platform.base_model.DType": {
    "id": "nova-platform.nova_platform.base_model.DType",
    "name": "DType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class DType(BaseEnum):\n    FP4 = (float, 0.5)\n    FP8 = (float, 1)\n    FP16 = (float, 2)\n    FP32 = (float, 4)\n    INT8 = (int, 2)\n    INT32 = (int, 4)\n    INT64 = (int, 8)\n    TF32 = (TensorFloat, 4)\n\n    def get_bpe(self):\n        return self.value[1]",
    "start_line": 13,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class DType",
    "component_id": "nova-platform.nova_platform.base_model.DType"
  },
  "nova-platform.nova_platform.base_model.AddrDomain": {
    "id": "nova-platform.nova_platform.base_model.AddrDomain",
    "name": "AddrDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class AddrDomain(str, BaseEnum):\n    L0 = \"L0\"\n    L1C = \"L1C\"\n    LOCAL = \"LOCAL\"\n    SHARED = \"SHARED\"\n    LLC = \"LLC\"\n    L3 = \"L3\"\n    L3_FAR = \"L3_FAR\"\n    L3_REMOTE = \"L3_REMOTE\"\n    ESL = \"ESL\"\n\n    @classmethod\n    def get_addr_domain(cls, addr):\n        # DSM: [4T, 4T+1GB]\n        # L3: [5T, ]\n        # List[ addr, size, w/r ]\n        if 4 * 2**40 <= addr < 5 * 2**40:\n            return AddrDomain.SHARED\n        elif 5*2**40 <= addr:\n            return AddrDomain.L3\n        else:\n            raise Exception(\"UNKNOWN addr domain %d\", addr)",
    "start_line": 27,
    "end_line": 48,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class AddrDomain",
    "component_id": "nova-platform.nova_platform.base_model.AddrDomain"
  },
  "nova-platform.nova_platform.base_model.DataflowActionType": {
    "id": "nova-platform.nova_platform.base_model.DataflowActionType",
    "name": "DataflowActionType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class DataflowActionType(str, BaseEnum):\n    ODTE = \"ODTE\"\n    CDTE = \"CDTE\"\n    SDTE = \"SDTE\"\n    XPU = \"XPU\"\n    ESL = \"ESL\"",
    "start_line": 51,
    "end_line": 56,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class DataflowActionType",
    "component_id": "nova-platform.nova_platform.base_model.DataflowActionType"
  },
  "nova-platform.nova_platform.base_model.DataflowOpType": {
    "id": "nova-platform.nova_platform.base_model.DataflowOpType",
    "name": "DataflowOpType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class DataflowOpType(str, BaseEnum):\n    GEMM = \"GEMM\"\n    ADD = \"ADD\"\n    MUL = \"MUL\"\n    SOFTMAX = \"SOFTMAX\"\n    GELU = \"GELU\"\n    SIGMOID = \"SIGMOID\"\n    SILU = \"SILU\"\n    RELU = \"RELU\"\n    NOP = \"NOP\"\n    DTE = \"DTE\"\n    LAYERNORM = \"LAYERNORM\"\n    GATHER = \"GATHER\"",
    "start_line": 59,
    "end_line": 71,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class DataflowOpType",
    "component_id": "nova-platform.nova_platform.base_model.DataflowOpType"
  },
  "nova-platform.nova_platform.base_model.BaseActionStat": {
    "id": "nova-platform.nova_platform.base_model.BaseActionStat",
    "name": "BaseActionStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class BaseActionStat:\n    power_stat: any = None\n    relative_ts: float = 0\n    latency: float = 0\n    name: str = \"\"",
    "start_line": 75,
    "end_line": 79,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BaseActionStat",
    "component_id": "nova-platform.nova_platform.base_model.BaseActionStat"
  },
  "nova-platform.nova_platform.base_model.EDCStat": {
    "id": "nova-platform.nova_platform.base_model.EDCStat",
    "name": "EDCStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class EDCStat(BaseDataclass):\n    dtu_edc_report: Any = None\n    soc_edc_report: Any = None\n    edc_total_latency: float = 0\n    edc_acc_dict: Dict[float, float] = None\n    edc_incr_percent: float = 0",
    "start_line": 83,
    "end_line": 88,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class EDCStat",
    "component_id": "nova-platform.nova_platform.base_model.EDCStat"
  },
  "nova-platform.nova_platform.base_model.PostStat": {
    "id": "nova-platform.nova_platform.base_model.PostStat",
    "name": "PostStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class PostStat(BaseDataclass):\n    total_latency: float = 0\n    action_end_time: float = 0\n    core_util: float = 0\n    l3_rw_bw_util: float = 0\n    sic_io_r_bw_util: float = 0\n    sic_io_w_bw_util: float = 0\n    sic_io_rw_bw_util: float = 0\n    esl_bw_util: float = 0\n    workload_balance: float = 0\n    service_report_dict: Dict[str, Any] = field(default_factory=dict)\n    edc: EDCStat = field(default_factory=EDCStat)\n    d2d_tx_rw_bw_util: float = 0",
    "start_line": 92,
    "end_line": 104,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class PostStat",
    "component_id": "nova-platform.nova_platform.base_model.PostStat"
  },
  "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess": {
    "id": "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
    "name": "DataflowActionMemoryAccess",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class DataflowActionMemoryAccess:\n    base_addr: int\n    size: int\n    rw: str",
    "start_line": 108,
    "end_line": 111,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DataflowActionMemoryAccess",
    "component_id": "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
  },
  "nova-platform.nova_platform.base_model.DataflowActionMemoryStat": {
    "id": "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
    "name": "DataflowActionMemoryStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.BaseActionStat"
    ],
    "source_code": "class DataflowActionMemoryStat(BaseActionStat):\n    cache_stat: Dict[str, any] = None\n    total_count: int = 0\n    master: DataflowActionType = None\n    src: AddrDomain = None\n    dst: AddrDomain = None\n    rw: str = None\n    leading_latency: float = 0\n    memory_access_list: List[DataflowActionMemoryAccess] = None\n    remote_target_mem_access_list: List[DataflowActionMemoryAccess] = None\n    write_through = False\n    src_gcu_id: int = None\n    tar_gcu_id: int = None\n    is_done = False\n    bw_factor: bool = 1.0",
    "start_line": 115,
    "end_line": 129,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseActionStat"
    ],
    "class_name": null,
    "display_name": "class DataflowActionMemoryStat",
    "component_id": "nova-platform.nova_platform.base_model.DataflowActionMemoryStat"
  },
  "nova-platform.nova_platform.base_model.DataflowActionComputeStat": {
    "id": "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
    "name": "DataflowActionComputeStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.BaseActionStat"
    ],
    "source_code": "class DataflowActionComputeStat(BaseActionStat):\n    compute_1d_ops: Dict[DType, float] = field(default_factory=dict)\n    compute_2d_ops: Dict[DType, float] = field(default_factory=dict)\n    compute_msf_ops: int = 0\n    compute_scalar_cycle: int = 0\n    compute_nop_cycle: int = 0\n    compute_1d_efficiency: float = 1.0\n    compute_2d_efficiency: float = 1.0\n    compute_sfu_efficiency: float = 1.0\n\n    def __iadd__(lhs: 'DataflowActionComputeStat', rhs: 'DataflowActionComputeStat'):\n        for dt, val in rhs.compute_1d_ops.items():\n            if dt not in lhs.compute_1d_ops:\n                lhs.compute_1d_ops[dt] = 0\n            lhs.compute_1d_ops[dt] += val\n        for dt, val in rhs.compute_2d_ops.items():\n            if dt not in lhs.compute_2d_ops:\n                lhs.compute_2d_ops[dt] = 0\n            lhs.compute_2d_ops[dt] += val\n\n        lhs.latency += rhs.latency\n        lhs.compute_scalar_cycle += rhs.compute_scalar_cycle\n        lhs.compute_nop_cycle += rhs.compute_nop_cycle\n        lhs.compute_1d_efficiency += rhs.compute_1d_efficiency\n        lhs.compute_2d_efficiency += rhs.compute_2d_efficiency\n        lhs.compute_sfu_efficiency+rhs.compute_sfu_efficiency\n\n        return lhs",
    "start_line": 133,
    "end_line": 160,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseActionStat"
    ],
    "class_name": null,
    "display_name": "class DataflowActionComputeStat",
    "component_id": "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
  },
  "nova-platform.nova_platform.base_model.BaseFrame": {
    "id": "nova-platform.nova_platform.base_model.BaseFrame",
    "name": "BaseFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class BaseFrame(BaseDataclass):\n    begin: float = field(default=0)  # second\n    end:   float = field(default=MAX_TIME)  # second\n\n    @property\n    def dur(self):\n        return self.end-self.begin\n\n    def incr(self, frame: 'BaseFrame'):\n        raise NotImplemented()\n\n    def clone(self) -> 'BaseFrame':\n        return self.__class__(**self.__dict__)",
    "start_line": 164,
    "end_line": 176,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class BaseFrame",
    "component_id": "nova-platform.nova_platform.base_model.BaseFrame"
  },
  "nova-platform.nova_platform.base_model.BaseESLSwitch": {
    "id": "nova-platform.nova_platform.base_model.BaseESLSwitch",
    "name": "BaseESLSwitch",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/base_model.py",
    "relative_path": "nova-platform/nova_platform/base_model.py",
    "depends_on": [],
    "source_code": "class BaseESLSwitch():\n\n    def __init__(self, config, topo):\n        self.gcu_map = {}\n        self.port_map = {}\n        self.config = config\n        self.topo = topo\n\n    def add_gcu(self, gcu_id, executor):\n        self.gcu_map[gcu_id] = executor\n\n    def build_bw_resource(self, bw_res_context):\n        raise NotImplementedError\n\n    def get_unique_bw_resource(self, bw_res_context):\n        raise NotImplementedError\n\n    def get_bw_resource(self, local_gpu_id: int, src_gcu_id: int, tar_gcu_id: int, rw):\n        raise NotImplementedError\n\n    def send(self, ref, src_gcu_id, tar_gcu_id, rw, data_size, memory_list=None):\n        raise NotImplementedError",
    "start_line": 179,
    "end_line": 200,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BaseESLSwitch",
    "component_id": "nova-platform.nova_platform.base_model.BaseESLSwitch"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape",
    "name": "BatchGemmGridShape",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [],
    "source_code": "class BatchGemmGridShape(GridShape):\n    lhs_buf_cnt_l2: int = 0\n    rhs_buf_cnt_l2: int = 0\n    res_buf_cnt_l2: int = 0\n    block_row_major: bool = True\n    bias_buf_cnt_l2: int = 0\n    lhs_buf_cnt_l1: int = 0\n    rhs_buf_cnt_l1: int = 0\n    res_buf_cnt_l1: int = 0\n    subthread_row_major: bool = True\n    bias_buf_cnt_l1: int = 0\n    calc_ceil_K_l2: int = 0\n    calc_ceil_K_l1: int = 0\n\n    mem_access_l2_per_sip: int = 0\n    mem_access_l1_per_sip: int = 0\n    mem_access_total_l3: int = 0\n    imax_l3: float = 0.0\n    attaible_flops: float = 0.0\n    l1_bw_required: float = 0.0  # bytes/cycle\n    sol_cost: float = 0.0\n\n    def __post_init__(self):\n        self.block_traverse_dim_order = [0, 1, 2, 3]\n        self.subthread_traverse_dim_order = [0, 1, 2, 3]\n        if not self.block_row_major:\n            self.block_traverse_dim_order[0] = 1\n            self.block_traverse_dim_order[1] = 0\n        if not self.subthread_row_major:\n            self.subthread_traverse_dim_order[0] = 1\n            self.subthread_traverse_dim_order[1] = 0\n        self.tiled_workloads = None",
    "start_line": 20,
    "end_line": 51,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "GridShape"
    ],
    "class_name": null,
    "display_name": "class BatchGemmGridShape",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.MemArchType": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.MemArchType",
    "name": "MemArchType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [],
    "source_code": "class MemArchType(str, Enum):\n    DSM_LOCAL = \"dsm.local\"\n    DSM_SHARED = \"dsm.shared\"",
    "start_line": 53,
    "end_line": 55,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "Enum"
    ],
    "class_name": null,
    "display_name": "class MemArchType",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.MemArchType"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.QuantType": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.QuantType",
    "name": "QuantType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [],
    "source_code": "class QuantType(str, Enum):\n    No_Quant = \"No_Quant\"\n    Wf8t_Af8t = \"Wf8t_Af8t\"  # weight fp8 per tensor quant, activation fp8 per tensor quant\n    Wf4g_Af8t = \"Wf4g_Af8t\"  # weight fp4 per group quant,  activation fp8 per tensor quant\n    Wf4g_Af8k = \"Wf4g_Af8k\"  # weight fp4 per group quant,  activation fp8 per token quant",
    "start_line": 58,
    "end_line": 62,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "Enum"
    ],
    "class_name": null,
    "display_name": "class QuantType",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.QuantType"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant",
    "name": "bpe_for_quant",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [],
    "source_code": "def bpe_for_quant(dtype: DType, quant_type: QuantType):\n    orig_bpe = dtype.get_bpe()\n    if quant_type == QuantType.No_Quant:\n        return {\"lhs\": orig_bpe, \"rhs\": orig_bpe, \"res\": orig_bpe, \"scaling\": orig_bpe, \"bias\": orig_bpe}\n    elif quant_type == QuantType.Wf8t_Af8t:\n        return {\"lhs\": 1, \"rhs\": 1, \"res\": orig_bpe, \"scaling\": 2, \"bias\": 4}\n    elif quant_type == QuantType.Wf4g_Af8t:\n        return {\"lhs\": 1, \"rhs\": 0.5, \"res\": orig_bpe, \"scaling\": 2, \"bias\": 4}\n    else:\n        raise RuntimeError(f\"unsupported quant type {quant_type}\")",
    "start_line": 65,
    "end_line": 74,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "dtype",
      "quant_type"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function bpe_for_quant",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant",
    "name": "compute_dtype_for_quant",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [],
    "source_code": "def compute_dtype_for_quant(dtype: DType, quant_type: QuantType):\n    if quant_type == QuantType.No_Quant:\n        return {\"1d\": dtype, \"2d\": dtype}\n    elif quant_type == QuantType.Wf8t_Af8t:\n        return {\"1d\": DType.FP32, \"2d\": DType.FP8}\n    elif quant_type == QuantType.Wf4g_Af8t:\n        return {\"1d\": DType.FP32, \"2d\": DType.FP4}\n    else:\n        raise RuntimeError(f\"unsupported quant type {quant_type}\")",
    "start_line": 77,
    "end_line": 85,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "dtype",
      "quant_type"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function compute_dtype_for_quant",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmBase": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmBase",
    "name": "BatchGemmBase",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant",
      "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant",
      "nova-platform.nova_platform.benchmark.batch_gemm.QuantType"
    ],
    "source_code": "class BatchGemmBase(OpBase, ABC):\n    \"\"\"\n    Backend-agnostic GEMM tiler基类，子类实现各自硬件的拆分/代价/工作负载逻辑。\n    \"\"\"\n\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.in_batch = workload.inputs[0].dim[0]\n        self.in_m = workload.inputs[0].dim[1]\n        self.in_n = workload.inputs[1].dim[2]\n        self.in_k = workload.inputs[1].dim[1]\n        self.quant_type = QuantType(workload.attr.get(\"quant_type\", \"No_Quant\"))\n        self.bpe = bpe_for_quant(self.dtype, self.quant_type)\n        self.compute_dtype = compute_dtype_for_quant(self.dtype, self.quant_type)\n        self.has_bias = workload.attr.get(\"has_bias\", False)\n        if not self.has_bias:\n            self.bpe[\"bias\"] = 0\n        self.shape_list: List[Tuple[int, BatchGemmGridShape]] = []\n        self._tiled_workloads = None\n\n    @abstractmethod\n    def split(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def impl(self):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _calc_sol_cost(self, batch_gemm_shape: BatchGemmGridShape):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def _sort_shape_candidates(self, shape_list: List[Tuple[int, BatchGemmGridShape]]):\n        raise NotImplementedError()\n\n    @abstractmethod\n    def get_best_shape(self) -> BatchGemmGridShape:\n        raise NotImplementedError()\n\n    def calc_flops_total(self):\n        return self.in_batch * self.in_m * self.in_n * self.in_k * 2.0\n\n    def get_tiled_workloads(self):\n        return self._tiled_workloads",
    "start_line": 87,
    "end_line": 135,
    "has_docstring": true,
    "docstring": "Backend-agnostic GEMM tiler基类，子类实现各自硬件的拆分/代价/工作负载逻辑。",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "OpBase",
      "ABC"
    ],
    "class_name": null,
    "display_name": "class BatchGemmBase",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmBase"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm.XpuBatchGemmBase": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm.XpuBatchGemmBase",
    "name": "XpuBatchGemmBase",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant",
      "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmBase",
      "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape",
      "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant",
      "nova-platform.nova_platform.benchmark.op_base.get_layout",
      "nova-platform.nova_platform.benchmark.batch_gemm.QuantType"
    ],
    "source_code": "class XpuBatchGemmBase(BatchGemmBase):\n\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.in_batch = workload.inputs[0].dim[0]\n        self.in_m = workload.inputs[0].dim[1]\n        self.in_n = workload.inputs[1].dim[2]\n        self.in_k = workload.inputs[1].dim[1]\n        # print(\"###\", workload)\n        self.mem_arch_type = MemArchType.DSM_LOCAL\n        self.shape_list = []\n        self.quant_type = QuantType(workload.attr.get(\"quant_type\", \"No_Quant\"))\n\n        self.bpe = bpe_for_quant(self.dtype, self.quant_type)\n        self.compute_dtype = compute_dtype_for_quant(self.dtype, self.quant_type)\n        self.has_bias = workload.attr.get(\"has_bias\", False)\n        if not self.has_bias:\n            self.bpe[\"bias\"] = 0\n        self.sic_cnt = config.inst_num.NUM_OF_CLUSTER * config.inst_num.NUM_OF_DIE\n        self.sip_cnt = config.inst_num.NUM_OF_CORE_PER_CLUSTER\n        self.l1_bytes_per_sip = config.memory.l1.SIZE_PER_CORE\n        self.l2_bytes_per_sic = config.memory.l2.SIZE_PER_SIC\n        self.xpu_FLOPS = (\n            config.compute.thread_2d_throughput[self.compute_dtype[\"2d\"]] * 2 * config.freq.CORE * 1e9\n        )  # ops/s\n        self.l3_bandwidth = config.bw.mc.l3.bw * config.freq.MC * config.inst_num.NUM_OF_DIE * 1e9  # bytes/s\n        self.l3_bandwidth_per_sip = self.l3_bandwidth / self.sic_cnt / self.sip_cnt\n        if \"libra\" in str(config.name).lower():\n            self.calc_ceil_info = [([64, 64, 1, 1], 8), ([16, 32, 1, 1], 4)]\n        else:\n            self.calc_ceil_info = [([64, 64, 1, 1], 16), ([8, 8, 1, 1], 16)]\n        # self.calc_ceil_info = [([64, 64, 1, 1], 8)]\n        self.imax = self.xpu_FLOPS * self.sic_cnt * self.sip_cnt / self.l3_bandwidth\n        self.l3_to_l1_latency = 512 / config.freq.CORE / 1e9\n        self.l0_to_l3_latency = 512 / config.freq.CORE / 1e9\n        self.l1_bandwidth_per_sip = config.bw.l0.local.bw * config.freq.CORE * 1e9\n\n    def gen_buf_cnt_combines(self, grid_dims, block_dims, thread_dims, subthread_dims, l2K, l1K):\n        B = self.in_batch\n        M = self.in_m\n        N = self.in_n\n        K = self.in_k\n\n        block_stride_x = block_dims[0] * thread_dims[0] * subthread_dims[0]\n        block_stride_y = block_dims[1] * thread_dims[1] * subthread_dims[1]\n        block_stride_z = block_dims[2] * thread_dims[2] * subthread_dims[2]\n        grid_stride_x = block_stride_x * grid_dims[0]\n        grid_stride_y = block_stride_y * grid_dims[1]\n        grid_stride_z = block_stride_z * grid_dims[2]\n        block_loop_x = (N + grid_stride_x - 1) // grid_stride_x\n        block_loop_y = (M + grid_stride_y - 1) // grid_stride_y\n        block_loop_z = (B + grid_stride_z - 1) // grid_stride_z\n        block_total = block_loop_x * block_loop_y * block_loop_z  # block num on each sic\n\n        # make thread's lhs/rhs cached in L1\n        rhs_thread_cnt_l1 = thread_dims[0] * thread_dims[2]\n        lhs_thread_cnt_l1 = thread_dims[1] * thread_dims[2]\n        if block_loop_y * block_loop_z * thread_dims[1] * thread_dims[2] > 1:\n            lhs_thread_cnt_l1 = lhs_thread_cnt_l1 if lhs_thread_cnt_l1 > 2 else 2\n\n        if block_loop_x * block_loop_z * thread_dims[0] * thread_dims[2] > 1:\n            rhs_thread_cnt_l1 = rhs_thread_cnt_l1 if rhs_thread_cnt_l1 > 2 else 2\n\n        # make block's lhs/rhs totally cached in L2B and L1\n        rhs_max_cnt_l2 = (K + l2K - 1) // l2K\n        lhs_max_cnt_l2 = rhs_max_cnt_l2\n        rhs_max_cnt_l1 = (K + l1K - 1) // l1K * thread_dims[0] * thread_dims[2]\n        lhs_max_cnt_l1 = (K + l1K - 1) // l1K * thread_dims[1] * thread_dims[2]\n\n        if block_loop_y * block_loop_z > 1:\n            lhs_max_cnt_l2 = lhs_max_cnt_l2 if lhs_max_cnt_l2 > 2 else 2\n\n        if block_loop_x * block_loop_z > 1:\n            rhs_max_cnt_l2 = rhs_max_cnt_l2 if rhs_max_cnt_l2 > 2 else 2\n\n        if block_loop_y * block_loop_z * thread_dims[1] * thread_dims[2] > 1:\n            lhs_max_cnt_l1 = lhs_max_cnt_l1 if lhs_max_cnt_l1 > 2 else 2\n\n        if block_loop_x * block_loop_z * thread_dims[0] * thread_dims[2] > 1:\n            rhs_max_cnt_l1 = rhs_max_cnt_l1 if rhs_max_cnt_l1 > 2 else 2\n\n        # make input tensor's lhs/rhs totally cached in L2B and L1\n        rhs_ultra_max_cnt_l2 = (K + l2K - 1) // l2K * block_loop_x\n        lhs_ultra_max_cnt_l2 = (K + l2K - 1) // l2K * block_loop_y\n        rhs_ultra_max_cnt_l1 = (K + l1K - 1) // l1K * thread_dims[0] * thread_dims[2] * block_loop_x\n        lhs_ultra_max_cnt_l1 = (K + l1K - 1) // l1K * thread_dims[1] * thread_dims[2] * block_loop_y\n        if block_loop_y * block_loop_z > 1:\n            lhs_ultra_max_cnt_l2 = lhs_ultra_max_cnt_l2 if lhs_ultra_max_cnt_l2 > 2 else 2\n\n        if block_loop_x * block_loop_z > 1:\n            rhs_ultra_max_cnt_l2 = rhs_ultra_max_cnt_l2 if rhs_ultra_max_cnt_l2 > 2 else 2\n\n        if block_loop_y * block_loop_z * thread_dims[1] * thread_dims[2] > 1:\n            lhs_ultra_max_cnt_l1 = lhs_ultra_max_cnt_l1 if lhs_ultra_max_cnt_l1 > 2 else 2\n\n        if block_loop_x * block_loop_z * thread_dims[0] * thread_dims[2] > 1:\n            rhs_ultra_max_cnt_l1 = rhs_ultra_max_cnt_l1 if rhs_ultra_max_cnt_l1 > 2 else 2\n\n        lhs_cnt_l2_vec = []\n        rhs_cnt_l2_vec = []\n        res_cnt_l2_vec = []\n        lhs_cnt_l1_vec = []\n        rhs_cnt_l1_vec = []\n        res_cnt_l1_vec = []\n\n        res_cnt_l2_vec.append(2 if block_total > 1 else 1)\n        res_cnt_l1_vec.append(2 if block_total * thread_dims[0] * thread_dims[1] * thread_dims[2] > 1 else 1)\n\n        block_row_major_vec = []\n        if block_loop_y == 1:\n            block_row_major_vec.append(True)\n        elif block_loop_x == 1:\n            block_row_major_vec.append(False)\n        else:\n            block_row_major_vec.append(True)\n            block_row_major_vec.append(False)\n\n        subthread_row_major_vec = []\n        if thread_dims[1] == 1:\n            subthread_row_major_vec.append(True)\n        elif thread_dims[0] == 1:\n            subthread_row_major_vec.append(False)\n        else:\n            subthread_row_major_vec.append(True)\n            subthread_row_major_vec.append(False)\n\n        if self.mem_arch_type == MemArchType.DSM_SHARED:\n            subthread_row_major_vec.clear()\n            subthread_row_major_vec.append(True)\n        elif self.mem_arch_type == MemArchType.DSM_LOCAL:\n            subthread_row_major_vec.clear()\n            subthread_row_major_vec.append(True)\n\n        cb = []\n        for block_row_major in block_row_major_vec:\n            for subthread_row_major in subthread_row_major_vec:\n                lhs_cnt_l2_vec.clear()\n                lhs_cnt_l2_vec.append(lhs_max_cnt_l2)\n                if lhs_max_cnt_l2 > 2:\n                    lhs_cnt_l2_vec.append(2)\n                if not block_row_major and block_loop_x > 1:\n                    lhs_cnt_l2_vec.append(lhs_ultra_max_cnt_l2)\n                lhs_cnt_l2_vec = sorted(list(set(lhs_cnt_l2_vec)))\n\n                rhs_cnt_l2_vec.clear()\n                rhs_cnt_l2_vec.append(rhs_max_cnt_l2)\n                if rhs_max_cnt_l2 > 2:\n                    rhs_cnt_l2_vec.append(2)\n                if block_row_major and block_loop_y > 1:\n                    rhs_cnt_l2_vec.append(rhs_ultra_max_cnt_l2)\n                rhs_cnt_l2_vec = sorted(list(set(rhs_cnt_l2_vec)))\n\n                lhs_cnt_l1_vec.clear()\n                lhs_cnt_l1_vec.append(lhs_thread_cnt_l1)\n                lhs_cnt_l1_vec.append(lhs_max_cnt_l1)\n                if lhs_max_cnt_l1 > 2:\n                    lhs_cnt_l1_vec.append(2)\n                if not block_row_major:\n                    lhs_cnt_l1_vec.append(lhs_ultra_max_cnt_l1)\n                lhs_cnt_l1_vec = sorted(list(set(lhs_cnt_l1_vec)))\n\n                rhs_cnt_l1_vec.clear()\n                rhs_cnt_l1_vec.append(rhs_thread_cnt_l1)\n                rhs_cnt_l1_vec.append(rhs_max_cnt_l1)\n                if rhs_max_cnt_l1 > 2:\n                    rhs_cnt_l1_vec.append(2)\n                if block_row_major:\n                    rhs_cnt_l1_vec.append(rhs_ultra_max_cnt_l1)\n                rhs_cnt_l1_vec = sorted(list(set(rhs_cnt_l1_vec)))\n\n                if self.mem_arch_type == MemArchType.DSM_SHARED:\n                    lhs_cnt_l2_vec = [2]\n                    rhs_cnt_l2_vec = [2]\n                    res_cnt_l2_vec = [2]\n                    res_cnt_l1_vec = [2]\n                    lhs_cnt_l1_vec = [2]\n                    rhs_cnt_l1_vec = [2]\n                elif self.mem_arch_type == MemArchType.DSM_LOCAL:\n                    lhs_cnt_l2_vec = [2]\n                    rhs_cnt_l2_vec = [2]\n                    res_cnt_l2_vec = [0]\n                    res_cnt_l1_vec = [0]\n                    lhs_cnt_l1_vec = [2]\n                    rhs_cnt_l1_vec = [2]\n\n                for lhs_cnt_l2, rhs_cnt_l2, res_cnt_l2, lhs_cnt_l1, rhs_cnt_l1, res_cnt_l1 in itertools.product(\n                    lhs_cnt_l2_vec, rhs_cnt_l2_vec, res_cnt_l2_vec, lhs_cnt_l1_vec, rhs_cnt_l1_vec, res_cnt_l1_vec\n                ):\n                    grid_shape = BatchGemmGridShape(\n                        grid_dims=grid_dims,\n                        block_dims=block_dims,\n                        thread_dims=thread_dims,\n                        subthread_dims=subthread_dims,\n                        lhs_buf_cnt_l2=lhs_cnt_l2,\n                        rhs_buf_cnt_l2=rhs_cnt_l2,\n                        res_buf_cnt_l2=res_cnt_l2,\n                        lhs_buf_cnt_l1=lhs_cnt_l1,\n                        rhs_buf_cnt_l1=rhs_cnt_l1,\n                        res_buf_cnt_l1=res_cnt_l1,\n                        calc_ceil_K_l2=l2K,\n                        calc_ceil_K_l1=l1K,\n                    )\n                    if not self.check_mem_overflow(grid_shape):\n                        cb.append(\n                            (\n                                lhs_cnt_l2,\n                                rhs_cnt_l2,\n                                res_cnt_l2,\n                                block_row_major,\n                                lhs_cnt_l1,\n                                rhs_cnt_l1,\n                                res_cnt_l1,\n                                subthread_row_major,\n                            )\n                        )\n        return cb\n\n    def check_mem_overflow(self, shape: BatchGemmGridShape):\n\n        bpe = self.bpe\n        thread_stride_x, thread_stride_y, thread_stride_z = shape.thread_dims_stride()[:3]\n        lhs_bytes = bpe[\"lhs\"] * thread_stride_y * shape.calc_ceil_K_l1 * shape.lhs_buf_cnt_l1 * thread_stride_z\n        rhs_bytes = bpe[\"rhs\"] * thread_stride_x * shape.calc_ceil_K_l1 * shape.rhs_buf_cnt_l1 * thread_stride_z\n        bias_bytes = bpe[\"bias\"] * thread_stride_x * (shape.bias_buf_cnt_l1 if self.has_bias else 0) * thread_stride_z\n        res_bytes = 0\n\n        if lhs_bytes + rhs_bytes + bias_bytes + res_bytes > self.l1_bytes_per_sip:\n            return True\n\n        block_stride_x, block_stride_y, block_stride_z = shape.block_dims_stride()[:3]\n        lhs_bytes = (\n            bpe[\"lhs\"]\n            * block_stride_z\n            * block_stride_y\n            * shape.calc_ceil_K_l2\n            * shape.lhs_buf_cnt_l2\n            * shape.block_dims[0]\n        )\n\n        rhs_bytes = (\n            bpe[\"rhs\"]\n            * block_stride_z\n            * block_stride_x\n            * shape.calc_ceil_K_l2\n            * shape.rhs_buf_cnt_l2\n            * shape.block_dims[1]\n        )\n        bias_bytes = bpe[\"bias\"] * block_stride_x * (shape.bias_buf_cnt_l2 if self.has_bias else 0) * block_stride_z\n\n        res_bytes = (\n            bpe[\"res\"] * block_stride_z * block_stride_x * block_stride_y * shape.res_buf_cnt_l2\n            if self.mem_arch_type == MemArchType.DSM_SHARED\n            else 0\n        )\n        if lhs_bytes + rhs_bytes + bias_bytes + res_bytes > self.l2_bytes_per_sic:\n            return True\n        return False\n\n    def calc_mem_access_l1_per_sip(self, shape: BatchGemmGridShape):\n        bpe = self.bpe\n\n        thread_stride_x, thread_stride_y, thread_stride_z = shape.thread_dims_stride()[:3]\n        lhs_bytes = bpe[\"lhs\"] * thread_stride_y * shape.calc_ceil_K_l1 * shape.lhs_buf_cnt_l1 * thread_stride_z\n        rhs_bytes = bpe[\"rhs\"] * thread_stride_x * shape.calc_ceil_K_l1 * shape.rhs_buf_cnt_l1 * thread_stride_z\n        bias_bytes = bpe[\"bias\"] * thread_stride_x * (shape.bias_buf_cnt_l1 if self.has_bias else 0) * thread_stride_z\n        res_bytes = 0\n        return lhs_bytes + rhs_bytes + bias_bytes + res_bytes\n\n    def calc_mem_access_l2_per_sip(self, shape: BatchGemmGridShape):\n        bpe = self.bpe\n\n        thread_stride_x, thread_stride_y, thread_stride_z = shape.thread_dims_stride()[:3]\n        lhs_bytes = bpe[\"lhs\"] * thread_stride_y * shape.calc_ceil_K_l2 * shape.lhs_buf_cnt_l2 * thread_stride_z\n        rhs_bytes = bpe[\"rhs\"] * thread_stride_x * shape.calc_ceil_K_l2 * shape.rhs_buf_cnt_l2 * thread_stride_z\n        bias_bytes = bpe[\"bias\"] * thread_stride_x * (shape.bias_buf_cnt_l2 if self.has_bias else 0) * thread_stride_z\n        res_bytes = 0\n        return lhs_bytes + rhs_bytes + bias_bytes + res_bytes\n\n    def calc_l1_bw_required(self, shape: BatchGemmGridShape):\n        bpe = self.bpe\n        thread_stride_x, thread_stride_y, thread_stride_z = shape.thread_dims_stride()[:3]\n        throughput = self.config.compute.thread_2d_throughput[self.compute_dtype[\"2d\"]]\n        l1_bw_required = (\n            (bpe[\"lhs\"] * thread_stride_y + bpe[\"rhs\"] * thread_stride_x)\n            / (thread_stride_x * thread_stride_y)\n            * throughput\n        )\n        return l1_bw_required\n\n    def calc_mem_access_total_l3(self, shape: BatchGemmGridShape):\n        B = self.in_batch\n        M = self.in_m\n        N = self.in_n\n        K = self.in_k\n        bpe = self.bpe\n        grid_stride_x, grid_stride_y, grid_stride_z = shape.grid_dims_stride()[:3]\n        # loop of each sic\n        block_loop_x = (N + grid_stride_x - 1) // grid_stride_x\n        block_loop_y = (M + grid_stride_y - 1) // grid_stride_y\n        block_loop_z = (B + grid_stride_z - 1) // grid_stride_z\n\n        bytes_lhs = bpe[\"lhs\"] * block_loop_x * M * K * shape.grid_dims[0]\n        bytes_rhs = bpe[\"rhs\"] * block_loop_y * N * K * shape.grid_dims[1]\n        bytes_res = bpe[\"res\"] * M * N\n\n        if shape.block_traverse_dim_order[0] == 0:\n            if shape.lhs_buf_cnt_l2 * shape.calc_ceil_K_l2 >= K:\n                # block lhs can totally cached in l2 buf\n                bytes_lhs = bpe[\"lhs\"] * M * K * shape.grid_dims[0]\n            if shape.rhs_buf_cnt_l2 // block_loop_x * shape.calc_ceil_K_l2 >= K:\n                # all block rhs in row can totally cached in l2 buf\n                bytes_rhs = bpe[\"rhs\"] * N * K * shape.grid_dims[1]\n        else:\n            if shape.rhs_buf_cnt_l2 * shape.calc_ceil_K_l2 >= K:\n                # block rhs can totally cached in l2 buf\n                bytes_rhs = bpe[\"rhs\"] * N * K * shape.grid_dims[1]\n            if shape.lhs_buf_cnt_l2 // block_loop_y * shape.calc_ceil_K_l2 >= K:\n                # all grid lhs in col can totally cached in l2 buf\n                bytes_lhs = bpe[\"lhs\"] * M * K * shape.grid_dims[0]\n\n        k_loop_cnt = (K + shape.calc_ceil_K_l2 - 1) // shape.calc_ceil_K_l2\n        lhs_rd_cnt = (\n            k_loop_cnt\n            + (k_loop_cnt - shape.lhs_buf_cnt_l2 if k_loop_cnt > shape.lhs_buf_cnt_l2 else 0) * (block_loop_x - 1)\n        ) / k_loop_cnt\n        rhs_rd_cnt = (\n            k_loop_cnt\n            + (k_loop_cnt - shape.rhs_buf_cnt_l2 if k_loop_cnt > shape.rhs_buf_cnt_l2 else 0) * (block_loop_y - 1)\n        ) / k_loop_cnt\n\n        bytes_lhs = bpe[\"lhs\"] * M * K * shape.grid_dims[0] * lhs_rd_cnt\n        bytes_rhs = bpe[\"rhs\"] * N * K * shape.grid_dims[1] * rhs_rd_cnt\n        bytes_res = bpe[\"res\"] * M * N\n\n        bytes_lhs *= shape.block_dims[0]\n        bytes_rhs *= shape.block_dims[1]\n        bytes_total = bytes_lhs + bytes_rhs + bytes_res\n        # bytes_total *= B;\n        bytes_total *= grid_stride_z * block_loop_z\n\n        return bytes_total\n\n    def calc_flops_total(self):\n        return self.in_batch * self.in_m * self.in_n * self.in_k * 2.0\n\n    def calc_attainable_flops(self, shape: BatchGemmGridShape):\n        shape.mem_access_total_l3 = self.calc_mem_access_total_l3(shape)\n        xpu_used = shape.block_cnt_in_grid() * shape.thread_cnt_in_block()\n        flops_used = self.xpu_FLOPS * xpu_used\n        Imax = flops_used / self.l3_bandwidth\n        flops_total = self.calc_flops_total()\n        I = flops_total / shape.mem_access_total_l3\n        shape.imax_l3 = I\n        attaible_flops = self.l3_bandwidth * I if I < Imax else flops_used\n        shape.attaible_flops = attaible_flops\n        shape.mem_access_l1_per_sip = self.calc_mem_access_l1_per_sip(shape)\n        shape.mem_access_l2_per_sip = self.calc_mem_access_l2_per_sip(shape)\n        shape.l1_bw_required = self.calc_l1_bw_required(shape)\n        return attaible_flops\n\n    def split(self):\n        self.shape_list = []\n        # 这里根据instance(sip或sic数量)生成所有可能的layout(b, m, n, k=1)组合\n        def get_layout(instances):\n            layout = []\n            for B in range(1, instances + 1):\n                for M in range(1, instances + 1):\n                    if B * M > instances:\n                        continue\n                    for N in range(1, instances + 1):\n                        if B * M * N == instances:\n                            layout.append([N, M, B, 1])\n            return layout\n        # 计算好k的取值候选\n        def get_calc_ceil_k(in_k):\n            calc_ceil_K_align = 64\n            # 向上对其到64的整数倍，64因为硬件适配良好\n            k_align = (in_k + calc_ceil_K_align - 1) // calc_ceil_K_align * calc_ceil_K_align\n            kVec = list(range(256, 2048 + 1, calc_ceil_K_align))\n            if k_align not in kVec:\n                kVec.append(k_align)\n            kVec = [k for k in kVec if k <= k_align]\n            calc_ceil_k = [k for k in kVec if k % calc_ceil_K_align == 0]\n            return calc_ceil_k\n\n        def filter_this_shape(grid_dims, block_dims, thread_dims, subthread_dims, maxCeilCnt, l1K):\n            grid_x, grid_y, grid_z = grid_dims[0:3]\n            block_x, block_y, block_z = block_dims[0:3]\n            thread_x, thread_y, thread_z = thread_dims[0:3]\n            subthread_x, subthread_y, subthread_z = subthread_dims[0:3]\n            # no block-expand when k_tile less than K\n            if self.in_k > l1K and maxCeilCnt < thread_dims[0] * thread_dims[1] * thread_dims[2]:\n                return True\n            if grid_z > self.in_batch:  # sic cnt overflow in batch dim\n                return True\n\n            # ceil cnt overflow\n            if (\n                (self.in_n + grid_x * block_x * subthread_x - 1) // (grid_x * block_x * subthread_x) < thread_x\n                or (self.in_m + grid_y * block_y * subthread_y - 1) // (grid_y * block_y * subthread_y) < thread_y\n                or (self.in_batch + grid_z * block_z * subthread_z - 1) // (grid_z * block_z * subthread_z) < thread_z\n            ):\n                return True\n\n            # sic cnt overflow in 1 dim, meanwhile shortage in the other dim\n            outputDimStride = [self.in_n, self.in_m, self.in_batch]\n            sic_overflow = []\n            sic_shortage = []\n            for dim in range(3):\n                sic_overflow.append(\n                    outputDimStride[dim] < subthread_dims[dim] * thread_dims[dim] * block_dims[dim] * grid_dims[dim]\n                    and subthread_dims[dim] * thread_dims[dim] * block_dims[dim] * grid_dims[dim] - outputDimStride[dim]\n                    >= thread_dims[dim] * subthread_dims[dim] * grid_dims[dim]\n                )\n                sic_shortage.append(\n                    outputDimStride[dim] > subthread_dims[dim] * thread_dims[dim] * block_dims[dim] * grid_dims[dim]\n                )\n\n            if any(sic_overflow) and any(sic_shortage):\n                return True\n\n            return False\n\n        sicLayout = get_layout(self.sic_cnt)\n        sipLayout = get_layout(self.sip_cnt)\n        calc_ceil_k = get_calc_ceil_k(self.in_k)\n        imax = 0\n        shape_list: List[Tuple[int, BatchGemmGridShape]] = []\n        shape_idx = 0\n        # 枚举出来所有满足上限的NBM组合 作为后续的grid/block的形状\n        for ceil_info in self.calc_ceil_info:\n            # 设置block的layout组合\n            ceilLayout = []\n            # max_ceil_cnt 是 block上限， TODO: 为什么这么设置？\n            max_ceil_cnt = ceil_info[1]\n            for B in range(1, 2):\n                for M in range(1, max_ceil_cnt + 1):\n                    if B * M > max_ceil_cnt:\n                        continue\n                    for N in range(1, max_ceil_cnt + 1):\n                        if B * M * N <= max_ceil_cnt:\n                            ceilLayout.append([N, M, B, 1])\n\n            # combine all possible sic&xpu&ceil layout\n            #ceil_layout 是 thread的layout组合 B/M/N/k 方向的线程数量\n            for ceil_layout in ceilLayout: #thread dim\n                # sip_layout 是block的layout组合 代表在不同方向上的block数量\n                for sip_layout in sipLayout: #block dim\n                    # sic_layout 是 grid的layout组合 在cluster上铺设块的数量\n                    for sic_layout in sicLayout: #grid dim\n                        for K in calc_ceil_k:\n                            if filter_this_shape(sic_layout, sip_layout, ceil_layout, ceil_info[0], ceil_info[1], K):\n                                continue\n                            cb_list = self.gen_buf_cnt_combines(sic_layout, sip_layout, ceil_layout, ceil_info[0], K, K)\n                            for cb in cb_list:\n                                grid_shape = BatchGemmGridShape(\n                                    grid_dims=sic_layout,\n                                    block_dims=sip_layout,\n                                    thread_dims=ceil_layout,\n                                    subthread_dims=ceil_info[0],\n                                    lhs_buf_cnt_l2=cb[0],\n                                    rhs_buf_cnt_l2=cb[1],\n                                    res_buf_cnt_l2=cb[2],\n                                    block_row_major=cb[3],\n                                    lhs_buf_cnt_l1=cb[4],\n                                    rhs_buf_cnt_l1=cb[5],\n                                    res_buf_cnt_l1=cb[6],\n                                    subthread_row_major=cb[7],\n                                    calc_ceil_K_l2=K,\n                                    calc_ceil_K_l1=K,\n                                )\n                                self.calc_attainable_flops(grid_shape)\n                                shape_list.append((shape_idx, grid_shape))\n                                shape_idx += 1\n                                imax = max(imax, grid_shape.imax_l3)\n\n        sorted_shape_list = self._sort_shape_candidates(shape_list)\n        # pprint(shape_list[0])\n        # SYSTEM L3 BW is\n        total_flops = self.xpu_FLOPS * self.sic_cnt * self.sip_cnt\n\n        msg = f\"SYSTEM L3 BW is {self.l3_bandwidth/ 1e9 :.3f} GB/S, {total_flops / 1e9:.3f} GFLOPS, IMAX is {total_flops/self.l3_bandwidth:.3f} flops/byte \\n\"\n        msg += f\"Generated {len(sorted_shape_list)} tiling candidates:\\n\"\n\n        def tiled_shape(shape, k):\n            grid_stride = shape.grid_dims_stride()\n            block_stride = shape.block_dims_stride()\n            thread_stride = shape.thread_dims_stride()\n            return [\n                (grid_stride[2], grid_stride[1], grid_stride[0], k),\n                (block_stride[2], block_stride[1], block_stride[0], k),\n                (thread_stride[2], thread_stride[1], thread_stride[0], k),\n            ]\n\n        for idx, shape in sorted_shape_list[:100]:\n            tiled = tiled_shape(shape, shape.calc_ceil_K_l2)\n            msg += (\n                f\"Tile Dims: {shape.grid_dims} {shape.block_dims} {shape.thread_dims} {shape.subthread_dims}, \"\n                f\"Tiled Shapes: {tiled[0]} {tiled[1]} {tiled[2]} , \"\n                f\"[{'row' if shape.block_traverse_dim_order[0] == 0 else 'col'}, {'row' if shape.subthread_traverse_dim_order[0] == 0 else 'col'}] \"\n                f\"Cost: {shape.sol_cost * 1e9:.2f} ns, imax: {shape.imax_l3:.3f}, {shape.attaible_flops/1e9:.3f} GFLOPS, L3: {shape.mem_access_total_l3/ 1024 / 1024:.3f} MB, \"\n                f\"L2: {shape.mem_access_l2_per_sip / 1024 / 1024:.3f} MB \"\n                f\"L1 bw required: {shape.l1_bw_required:.2f} bytes/cycle \\n\"\n            )\n        self.shape_list = sorted_shape_list\n        logger.info(msg)\n\n    def _calc_sol_cost(self, batch_gemm_shape: BatchGemmGridShape):\n        raise NotImplementedError()\n\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        raise NotImplementedError()\n\n    def _sort_shape_candidates(self, shape_list: List[Tuple[int, BatchGemmGridShape]]):\n        num = 1000\n        prioritized_shape_list_1 = sorted(shape_list, key=lambda x: x[1].l1_bw_required)[:num]\n        prioritized_shape_list_2 = sorted(shape_list, key=lambda x: -x[1].attaible_flops)[:num]\n        prioritized_shape_list = []\n        seen_idxs = set()\n        for idx, shape in prioritized_shape_list_1 + prioritized_shape_list_2:\n            if idx not in seen_idxs:\n                prioritized_shape_list.append((idx, shape))\n                seen_idxs.add(idx)\n        for idx, shape in prioritized_shape_list:\n            shape.sol_cost = self._calc_sol_cost(shape)\n        sorted_shape_list = sorted(prioritized_shape_list, key=lambda x: x[1].sol_cost)\n        return sorted_shape_list\n\n    def impl(self):\n        best_shape = self.get_best_shape()\n        # best_shape = BatchGemmGridShape(\n        #     grid_dims=[1, 4, 1, 1],\n        #     block_dims=[6, 1, 1, 1],\n        #     thread_dims=[4, 2, 1, 1],\n        #     subthread_dims=[64, 64, 1, 1],\n        #     lhs_buf_cnt_l2=2,\n        #     rhs_buf_cnt_l2=2,\n        #     res_buf_cnt_l2=0,\n        #     block_row_major=True,\n        #     lhs_buf_cnt_l1=2,\n        #     rhs_buf_cnt_l1=2,\n        #     res_buf_cnt_l1=0,\n        #     subthread_row_major=False,\n        #     calc_ceil_K_l1=256,\n        #     calc_ceil_K_l2=256,\n        # )\n        self.tiled_workloads = self._gen_workloads(best_shape)\n        self._tiled_workloads = self.tiled_workloads\n\n    def get_best_shape(self) -> BatchGemmGridShape:\n        return self.shape_list[0][1]\n\n    def get_tiled_workloads(self):\n        return self.tiled_workloads",
    "start_line": 138,
    "end_line": 691,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BatchGemmBase"
    ],
    "class_name": null,
    "display_name": "class XpuBatchGemmBase",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm.XpuBatchGemmBase"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_gpu.BatchGemmGPU": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_gpu.BatchGemmGPU",
    "name": "BatchGemmGPU",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_gpu.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_gpu.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape",
      "nova-platform.nova_platform.benchmark.op_base.Workload"
    ],
    "source_code": "class BatchGemmGPU(BatchGemmBase):\n    \"\"\"\n    极简 GPU 占位 tiler：固定单一 tile，不做搜索，仅用于打通多后端接口。\n    \"\"\"\n\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.shape_list = []\n        self._best_shape = None\n\n    def split(self):\n        shape = BatchGemmGridShape(\n            grid_dims=[1, 1, 1, 1],\n            block_dims=[1, 1, 1, 1],\n            thread_dims=[1, 1, 1, 1],\n            subthread_dims=[1, 1, 1, 1],\n            lhs_buf_cnt_l2=1,\n            rhs_buf_cnt_l2=1,\n            res_buf_cnt_l2=1,\n            lhs_buf_cnt_l1=1,\n            rhs_buf_cnt_l1=1,\n            res_buf_cnt_l1=1,\n            calc_ceil_K_l2=self.in_k,\n            calc_ceil_K_l1=self.in_k,\n        )\n        self.shape_list = [(0, shape)]\n\n    def _calc_sol_cost(self, batch_gemm_shape: BatchGemmGridShape):\n        # 简化：用 FLOPs / 假定吞吐估计时延\n        assumed_throughput = 1e12  # ops/s，占位\n        return self.calc_flops_total() / assumed_throughput\n\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        return {0: {0: [Workload(self.workload.inputs, self.workload.outputs, dtype=self.workload.dtype, attr=self.workload.attr)]}}\n\n    def _sort_shape_candidates(self, shape_list):\n        scored = []\n        for idx, shape in shape_list:\n            shape.sol_cost = self._calc_sol_cost(shape)\n            scored.append((idx, shape))\n        return scored\n\n    def impl(self):\n        if not self.shape_list:\n            self.split()\n        self.shape_list = self._sort_shape_candidates(self.shape_list)\n        self._best_shape = self.get_best_shape()\n        self._tiled_workloads = self._gen_workloads(self._best_shape)\n\n    def get_best_shape(self) -> BatchGemmGridShape:\n        return self.shape_list[0][1]",
    "start_line": 8,
    "end_line": 58,
    "has_docstring": true,
    "docstring": "极简 GPU 占位 tiler：固定单一 tile，不做搜索，仅用于打通多后端接口。",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BatchGemmBase"
    ],
    "class_name": null,
    "display_name": "class BatchGemmGPU",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_gpu.BatchGemmGPU"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_gpu.tile_gpu_gemm_workload": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_gpu.tile_gpu_gemm_workload",
    "name": "tile_gpu_gemm_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_gpu.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_gpu.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_gpu.BatchGemmGPU"
    ],
    "source_code": "def tile_gpu_gemm_workload(config: BossaNovaConfig, chip_workload: Workload) -> Tuple[Dict, BatchGemmGridShape]:\n    gemm = BatchGemmGPU(config, chip_workload)\n    gemm.split()\n    gemm.impl()\n    return gemm.get_tiled_workloads(), gemm.get_best_shape()",
    "start_line": 61,
    "end_line": 65,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config",
      "chip_workload"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tile_gpu_gemm_workload",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_gpu.tile_gpu_gemm_workload"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local.BatchGemmLocal": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local.BatchGemmLocal",
    "name": "BatchGemmLocal",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.op_base.Operand",
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class BatchGemmLocal(XpuBatchGemmBase):\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.mem_arch_type = MemArchType.DSM_LOCAL\n    # 重写计算解的代价函数\n    def _calc_sol_cost(self, batch_gemm_shape: BatchGemmGridShape):\n        # 根据workload的inputs size和bandwidth计算输入延迟\n        def cal_input_latency(wl: Workload):\n            size = 0\n            for i in wl.inputs:\n                size += list_product(tuple(i.dim)) * i.bpe if i else 0\n            if size == 0:\n                return 0\n            # TODO: 这里为什么要加上l3_to_l1_latency？\n            return size / self.l3_bandwidth_per_sip + self.l3_to_l1_latency\n        # 根据workload的outputs size和bandwidth计算输出延迟\n        def cal_output_latency(wl):\n            size = 0\n            for i in wl.outputs:\n                size += list_product(tuple(i.dim)) * i.bpe if i else 0\n            if size == 0:\n                return 0\n            return size / self.l3_bandwidth_per_sip\n        # 根据workload的计算量和计算能力计算计算延迟\n        def cal_compute_latency(workload: Workload):\n            ops = 2 * workload.attr[\"b\"] * workload.attr[\"m\"] * workload.attr[\"n\"] * workload.attr[\"k\"]\n            l1_access = workload.attr[\"b\"] * workload.attr[\"m\"] * workload.attr[\"k\"] * workload.dtype.get_bpe()\n            l1_access += workload.attr[\"b\"] * workload.attr[\"n\"] * workload.attr[\"k\"] * workload.dtype.get_bpe()\n            # TODO: 为什么取最大值？\n            return max(ops / self.xpu_FLOPS, l1_access / self.l1_bandwidth_per_sip)\n\n        workloads = self._gen_workloads(batch_gemm_shape, True)\n        latency_list = []\n        for sic_id in workloads:\n            for sip_id in workloads[sic_id]:\n                latency = 0\n                cdte_ref = 0\n                compute_ref = 0\n                ping_buffer_release_ref = 0\n                out_ref = 0\n                for idx, wl in enumerate(workloads[sic_id][sip_id]):\n                    input_latency = cal_input_latency(wl)\n                    output_latency = cal_output_latency(wl)\n                    compute_latency = cal_compute_latency(wl)\n                    compute_ref = cdte_ref + input_latency\n                    cdte_ref = max(cdte_ref + input_latency - self.l3_to_l1_latency, ping_buffer_release_ref)\n                    ping_buffer_release_ref = compute_ref + compute_latency\n                    out_ref = compute_ref + compute_latency\n                    latency = out_ref + output_latency\n\n                latency_list.append(latency)\n        return max(latency_list)\n\n    def handle_pingpong_buf(self, batch_gemm_shape: BatchGemmGridShape, workloads: Dict[int, Dict[int, Workload]]):\n        for sic_id in workloads:\n            for sip_id in workloads[sic_id]:\n                lhs_buffer = deque(maxlen=batch_gemm_shape.lhs_buf_cnt_l2)\n                rhs_buffer = deque(maxlen=batch_gemm_shape.rhs_buf_cnt_l2)\n                scaling_buffer = deque(maxlen=batch_gemm_shape.rhs_buf_cnt_l2)\n                for wl in workloads[sic_id][sip_id]:\n                    lhs_operand = wl.inputs[0]\n                    rhs_operand = wl.inputs[1]\n                    if lhs_operand in lhs_buffer:\n                        wl.inputs[0] = None\n                    else:\n                        lhs_buffer.append(lhs_operand)\n\n                    if rhs_operand in rhs_buffer:\n                        wl.inputs[1] = None\n                    else:\n                        rhs_buffer.append(rhs_operand)\n\n                    if self.quant_type == QuantType.Wf4g_Af8t:\n                        scaling_operand = wl.inputs[2]\n                        if scaling_operand in scaling_buffer:\n                            wl.inputs[2] = None\n                        else:\n                            scaling_buffer.append(scaling_operand)\n\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        sip_workload = {}\n        batch_gemm_shape.gen_traverse_idx([self.in_n, self.in_m, self.in_batch, 1])\n\n        xpu_layout = batch_gemm_shape.block_dims\n        shape_grid = batch_gemm_shape.grid_dims_stride()\n        shape_block = batch_gemm_shape.block_dims_stride()\n        shape_thread = batch_gemm_shape.thread_dims_stride()\n\n        B_l3 = self.in_batch\n        M_l3 = self.in_m\n        N_l3 = self.in_n\n        K_l3 = self.in_k\n        B_l2 = shape_block[2]\n        M_l2 = shape_block[1]\n        N_l2 = shape_block[0]\n        K_l2 = batch_gemm_shape.calc_ceil_K_l2\n\n        shape_lhs_l2 = [K_l2, shape_thread[1], shape_thread[2], 1]  # shape after splitting\n        shape_rhs_l2 = [shape_block[0], K_l2, shape_block[2], 1]\n        lhs_cnt_l2 = batch_gemm_shape.lhs_buf_cnt_l2\n        rhs_cnt_l2 = batch_gemm_shape.rhs_buf_cnt_l2\n        lhs_tensor_l3 = self.workload.inputs[0]\n        rhs_tensor_l3 = self.workload.inputs[1]\n        if self.quant_type == QuantType.Wf4g_Af8t:\n            scaling_tensor_l3 = self.workload.inputs[2]\n        res_tensor_l3 = self.workload.outputs[0]\n\n        block_loop_x = (N_l3 + shape_grid[0] - 1) // shape_grid[0]\n        block_loop_y = (M_l3 + shape_grid[1] - 1) // shape_grid[1]\n        block_loop_z = (B_l3 + shape_grid[2] - 1) // shape_grid[2]\n        block_total = block_loop_x * block_loop_y * block_loop_z\n        assert block_total == len(batch_gemm_shape.block_traverse_idx[0])\n        thread_loop_x, thread_loop_y, thread_loop_z = batch_gemm_shape.block_dims[:3]\n        k_loop_l2 = (K_l3 + K_l2 - 1) // K_l2\n\n        sic_total = batch_gemm_shape.block_cnt_in_grid()\n        sip_total = batch_gemm_shape.thread_cnt_in_block()\n        for block_idx in range(block_total):\n            for sic_idx in range(sic_total):\n                if first_sip_only and sic_idx != 0:\n                    break\n                block_idx_vec = batch_gemm_shape.block_traverse_idx[sic_idx][block_idx]\n                if block_idx_vec[0] < 0:\n                    continue\n\n                block_idx_x, block_idx_y, block_idx_z = block_idx_vec[:3]\n                sip_workload.setdefault(sic_idx, {})\n                for sip_idx in range(sip_total):\n                    for k_l2_idx in range(k_loop_l2):\n                        sip_eng_idx = sip_idx + sic_idx * self.sip_cnt\n                        sip_idx_vec = batch_gemm_shape.thread_traverse_idx[sip_idx]\n                        # print(\"!!!sip_idx_vec\", sip_idx_vec)\n                        sip_idx_x, sip_idx_y, sip_idx_z = sip_idx_vec[:3]\n                        m_offset = shape_thread[1] * (sip_idx_y + thread_loop_y * block_idx_y)\n                        # offset from l3 tensor\n                        n_offset = shape_thread[0] * (sip_idx_x + thread_loop_x * block_idx_x)\n                        b_offset = shape_thread[2] * (sip_idx_z + thread_loop_z * block_idx_z)\n                        k_l2_idx_new = k_l2_idx\n                        if block_idx & 1:\n                            k_l2_idx_new = k_loop_l2 - 1 - k_l2_idx\n                            # snake-like\n                        k_offset = K_l2 * k_l2_idx_new\n                        # print(\n                        #     f\"!sip_idx:{sip_idx}, m_offset:{m_offset}, n_offset:{n_offset}, b_offset:{b_offset}, k_offset:{k_offset}\"\n                        # )\n                        # print(f\"!sip_idx:{sip_idx}, M_l3:{M_l3}, N_l3:{N_l3}, B_l3:{B_l3}, K_l3:{K_l3}\")\n                        if m_offset >= M_l3 or n_offset >= N_l3 or k_offset >= K_l3 or b_offset >= B_l3:\n                            continue\n\n                        m_valid = shape_thread[1] if M_l3 >= m_offset + shape_thread[1] else M_l3 - m_offset\n                        n_valid = shape_thread[0] if N_l3 >= n_offset + shape_thread[0] else N_l3 - n_offset\n                        b_valid = shape_thread[2] if B_l3 >= b_offset + shape_thread[2] else B_l3 - b_offset\n                        k_valid = K_l2 if K_l3 >= k_offset + K_l2 else (K_l3 - k_offset)\n\n                        # lhs_offset = [k_offset, m_offset, b_offset, 0]\n                        # rhs_offset = [n_offset, k_offset, b_offset, 0]\n                        # res_offset = [n_offset, m_offset, b_offset, 0]\n                        lhs_offset = [b_offset, m_offset, k_offset]\n                        rhs_offset = [b_offset, k_offset, n_offset]\n                        res_offset = [b_offset, m_offset, n_offset]\n                        bias_offset = [n_offset, 0, 0, 0]\n                        sip_workload[sic_idx].setdefault(sip_idx, [])\n                        inputs = [\n                            Operand(\n                                dim=(b_valid, m_valid, k_valid),\n                                addr=lhs_tensor_l3.addr,\n                                bpe=lhs_tensor_l3.bpe,\n                                dim_offset=lhs_offset,\n                                dim_stride=lhs_tensor_l3.dim_stride,\n                            ),\n                            Operand(\n                                dim=(b_valid, k_valid, n_valid),\n                                addr=rhs_tensor_l3.addr,\n                                bpe=rhs_tensor_l3.bpe,\n                                dim_offset=rhs_offset,\n                                dim_stride=rhs_tensor_l3.dim_stride,\n                            ),\n                        ]\n                        if self.quant_type == QuantType.Wf4g_Af8t:\n                            quant_group_size = self.workload.attr.get(\"quant_group_size\", 32)\n                            quant_group_num = scaling_tensor_l3.dim[1]\n                            group_num_offset = (k_offset + quant_group_size - 1) // quant_group_size\n                            valid_group_num = (k_valid + quant_group_size - 1) // quant_group_size\n                            valid_group_num = (\n                                valid_group_num\n                                if quant_group_num >= group_num_offset + valid_group_num\n                                else quant_group_num - group_num_offset\n                            )\n                            inputs.append(\n                                Operand(\n                                    dim=(b_valid, valid_group_num, n_valid),\n                                    addr=scaling_tensor_l3.addr,\n                                    bpe=scaling_tensor_l3.bpe,\n                                    dim_offset=(b_offset, group_num_offset, n_offset),\n                                    dim_stride=scaling_tensor_l3.dim_stride,\n                                )\n                            )\n                        if k_l2_idx == k_loop_l2 - 1:\n                            outputs = [\n                                Operand(\n                                    dim=(b_valid, m_valid, n_valid),\n                                    addr=res_tensor_l3.addr,\n                                    bpe=res_tensor_l3.bpe,\n                                    dim_offset=res_offset,\n                                    dim_stride=res_tensor_l3.dim_stride,\n                                )\n                            ]\n                        else:\n                            outputs = []\n                        attr = {**self.workload.attr, \"b\": b_valid, \"m\": m_valid, \"n\": n_valid, \"k\": k_valid}\n                        sip_workload[sic_idx][sip_idx].append(Workload(inputs, outputs, attr=attr))\n\n        self.handle_pingpong_buf(batch_gemm_shape, sip_workload)\n        return sip_workload",
    "start_line": 27,
    "end_line": 240,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XpuBatchGemmBase"
    ],
    "class_name": null,
    "display_name": "class BatchGemmLocal",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local.BatchGemmLocal"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local.tile_local_gemm_workload": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local.tile_local_gemm_workload",
    "name": "tile_local_gemm_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_local.BatchGemmLocal"
    ],
    "source_code": "def tile_local_gemm_workload(config: BossaNovaConfig, chip_workload: Workload) -> Tuple[Workload, BatchGemmGridShape]:\n    batch_gemm = BatchGemmLocal(config, chip_workload)\n    batch_gemm.split()\n    batch_gemm.impl()\n    best_shape = batch_gemm.get_best_shape()\n    return batch_gemm.get_tiled_workloads(), best_shape",
    "start_line": 244,
    "end_line": 249,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config",
      "chip_workload"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tile_local_gemm_workload",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local.tile_local_gemm_workload"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local.dsm_local_gemm_kernel": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local.dsm_local_gemm_kernel",
    "name": "dsm_local_gemm_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [],
    "source_code": "def dsm_local_gemm_kernel(\n    sip_workloads: List[Workload],\n    config: BossaNovaConfig,\n    subthread_num: int,\n    relative_ts: int,\n):\n\n    lhs_buffer = deque(maxlen=2)\n    rhs_buffer = deque(maxlen=2)\n    scaling_buffer = deque(maxlen=2)\n\n    def _iter_tensor_addr(tensor: Operand, rw) -> Generator[DataflowActionMemoryAccess, None, None]:\n        mem_access = tensor.get_contiguous_mem_accesses()\n        for addr, size in mem_access:\n            yield DataflowActionMemoryAccess(addr, size, rw)\n\n    def _get_minimum_iter_k(b, m, n, mac_dtype):\n        iter_k = 32\n        if mac_dtype == DType.FP4:\n            iter_k = 64\n        throughput = config.compute.thread_2d_throughput[dtype]\n        l1_latency = config.bw.l0.local.pre_latency\n        k = 0\n        mac_cycle = 0\n        while mac_cycle < l1_latency:\n            k += iter_k\n            mac_cycle = b * m * n * k / throughput\n        return k\n\n    def _get_operands(sip_workload: Workload, stat_ref: int, idx: int):\n\n        def cdte_operand(tensor, cdte_name, ref):\n            tensor_gen = _iter_access_gen(list(_iter_tensor_addr(tensor, \"r\")))\n            next(tensor_gen)\n            data_size = list_product(tuple(tensor.dim)) * tensor.bpe\n            tensor_read = DataflowActionMemoryStat(\n                total_count=int(data_size),\n                master=DataflowActionType.CDTE,\n                src=AddrDomain.LOCAL,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=ref,\n                memory_access_list=tensor_gen.send(data_size),\n                name=cdte_name,\n            )\n            yield tensor_read\n            return tensor_read.latency, tensor_read.leading_latency\n\n        lhs_tensor = sip_workload.inputs[0]\n        rhs_tensor = sip_workload.inputs[1]\n        quant_type = sip_workload.attr.get(\"quant_type\", QuantType.No_Quant)\n        scaling_tensor = None\n        if quant_type == QuantType.Wf4g_Af8t:\n            if len(sip_workload.inputs) >= 3 and sip_workload.inputs[2]:\n                scaling_tensor = sip_workload.inputs[2]\n\n        lhs_ref = 0\n        rhs_ref = 0\n        lhs_leading_latency = 0\n        rhs_leading_latency = 0\n        if lhs_tensor and lhs_tensor not in lhs_buffer:\n            lhs_ref, lhs_leading_latency = yield from cdte_operand(lhs_tensor, f\"{idx} lhs:l3->local\", stat_ref)\n            lhs_buffer.append(lhs_tensor)\n\n        if rhs_tensor and rhs_tensor not in rhs_buffer:\n            rhs_ref, rhs_leading_latency = yield from cdte_operand(rhs_tensor, f\"{idx} rhs:l3->local\", stat_ref)\n            rhs_buffer.append(rhs_tensor)\n\n        latency = max(lhs_ref, rhs_ref)\n        leading = max(lhs_leading_latency, rhs_leading_latency)\n\n        if scaling_tensor and rhs_tensor not in scaling_buffer:\n            scaling_ref, scaling_leading_latency = yield from cdte_operand(\n                scaling_tensor, f\"{idx} scaling:l3->local\", stat_ref\n            )\n            scaling_buffer.append(rhs_tensor)\n            latency = max(latency, scaling_ref)\n            leading = max(leading, scaling_leading_latency)\n        return latency, leading\n\n    total_latency = relative_ts\n    cdte_ref = relative_ts\n    out_release_ref = relative_ts\n    compute_release_ref = relative_ts\n    ping_buffer_release_ref = relative_ts\n\n    for idx, sip_workload in enumerate(sip_workloads):\n        quant_type = sip_workload.attr.get(\"quant_type\", QuantType.No_Quant)\n        mac_dtype = sip_workload.dtype\n        dtype = sip_workload.dtype\n        bpe = bpe_for_quant(sip_workload.dtype, quant_type)\n        if quant_type == QuantType.Wf4g_Af8t:\n            mac_dtype = DType.FP4\n            if len(sip_workload.inputs) > 3 and sip_workload.inputs[2]:\n                scaling_tensor = sip_workload.inputs[2]\n        elif quant_type == QuantType.Wf8t_Af8t:\n            mac_dtype = DType.FP8\n\n        in_b = sip_workload.attr[\"b\"]\n        in_m = sip_workload.attr[\"m\"]\n        in_n = sip_workload.attr[\"n\"]\n        in_k = sip_workload.attr[\"k\"]\n\n        local_input_size = in_b * in_m * in_k * bpe[\"lhs\"] * in_b * in_n * in_k * bpe[\"rhs\"]\n        input_name = \"lhs+rhs\"\n        quant_group_num = 0\n        if quant_type == QuantType.Wf4g_Af8t:\n            quant_group_size = sip_workload.attr.get(\"quant_group_size\", 32)\n            quant_group_num = (in_k + quant_group_size - 1) // quant_group_size\n            local_input_size += in_b * quant_group_num * in_n * bpe[\"scaling\"]\n            input_name = \"lhs+rhs+scaling\"\n        local_input_gen = _iter_access_gen([DataflowActionMemoryAccess(0x4000000000, local_input_size, \"r\")])\n        next(local_input_gen)\n\n        oprands_latency, oprands_leading = yield from _get_operands(sip_workload, cdte_ref, idx)\n        iter_ref = max(cdte_ref + oprands_latency, compute_release_ref)\n        cdte_ref = max(cdte_ref + oprands_latency - oprands_leading, ping_buffer_release_ref)\n        out_ref = iter_ref\n\n        iter_k = _get_minimum_iter_k(in_b, in_m, in_n, dtype)\n        iter_k = min(in_k, iter_k)\n        for i in range(0, in_k, iter_k):\n            k = min(iter_k, in_k - i)\n            l0_input_size = int(in_m * k * bpe[\"lhs\"] + in_n * k * bpe[\"rhs\"] + quant_group_num * k * bpe[\"scaling\"])\n            l0_input_read = DataflowActionMemoryStat(\n                total_count=l0_input_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.LOCAL,\n                rw=\"r\",\n                relative_ts=iter_ref,\n                memory_access_list=local_input_gen.send(l0_input_size),\n                name=f\"{idx} {input_name}:local->l0\",\n            )\n            yield l0_input_read\n            compute_ref = iter_ref + l0_input_read.leading_latency\n            ping_buffer_release_ref = iter_ref + l0_input_read.latency\n            scalar_ops = 4 * subthread_num\n            compute_stat1 = DataflowActionComputeStat(\n                name=f\"{idx} scalar\",\n                compute_scalar_cycle=scalar_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat1\n            scalar_cost = compute_stat1.latency\n\n            compute_mac = in_m * in_n * k\n            compute_stat2 = DataflowActionComputeStat(\n                name=f\"{idx} mac\",\n                compute_2d_ops={mac_dtype: compute_mac * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2\n            VMM = 10\n            compute_vmm = DataflowActionComputeStat(\n                name=f\"{idx} vmm\",\n                compute_nop_cycle=VMM,\n                relative_ts=compute_ref + compute_stat2.latency,\n            )\n            yield compute_vmm\n            out_ref = compute_ref\n            compute_latency = compute_ref + compute_stat2.latency + compute_vmm.latency\n            iter_ref = (\n                max(\n                    compute_latency,\n                    iter_ref + l0_input_read.latency,\n                )\n                - l0_input_read.leading_latency\n            )\n            compute_release_ref = iter_ref\n        out_ref = max(out_ref, out_release_ref)\n        if len(sip_workload.outputs):\n            out_gen = _iter_access_gen(list(_iter_tensor_addr(sip_workload.outputs[0], \"w\")))\n            next(out_gen)\n            if quant_type == QuantType.Wf4g_Af8t or quant_type == QuantType.Wf8t_Af8t:\n                compute_convert = DataflowActionComputeStat(\n                    name=f\"{idx} convert\",\n                    compute_1d_ops={DType.FP32: in_m * in_n},\n                    relative_ts=compute_ref,\n                )\n                yield compute_convert\n                compute_latency = max(compute_convert.latency, compute_latency)\n\n            out_size = in_m * in_n * bpe[\"res\"]\n            out_wirte = DataflowActionMemoryStat(\n                total_count=out_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=out_ref,\n                memory_access_list=out_gen.send(out_size),\n                name=f\"{idx} out\",\n            )\n            yield out_wirte\n            out_release_ref = out_ref + out_wirte.latency - out_wirte.leading_latency\n            total_latency = out_ref + out_wirte.latency\n    return total_latency",
    "start_line": 252,
    "end_line": 449,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "sip_workloads",
      "config",
      "subthread_num",
      "relative_ts"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dsm_local_gemm_kernel",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local.dsm_local_gemm_kernel"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local._iter_tensor_addr": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local._iter_tensor_addr",
    "name": "_iter_tensor_addr",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "    def _iter_tensor_addr(tensor: Operand, rw) -> Generator[DataflowActionMemoryAccess, None, None]:\n        mem_access = tensor.get_contiguous_mem_accesses()\n        for addr, size in mem_access:\n            yield DataflowActionMemoryAccess(addr, size, rw)",
    "start_line": 263,
    "end_line": 266,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor",
      "rw"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_tensor_addr",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local._iter_tensor_addr"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local._get_minimum_iter_k": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local._get_minimum_iter_k",
    "name": "_get_minimum_iter_k",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [],
    "source_code": "    def _get_minimum_iter_k(b, m, n, mac_dtype):\n        iter_k = 32\n        if mac_dtype == DType.FP4:\n            iter_k = 64\n        throughput = config.compute.thread_2d_throughput[dtype]\n        l1_latency = config.bw.l0.local.pre_latency\n        k = 0\n        mac_cycle = 0\n        while mac_cycle < l1_latency:\n            k += iter_k\n            mac_cycle = b * m * n * k / throughput\n        return k",
    "start_line": 268,
    "end_line": 279,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "b",
      "m",
      "n",
      "mac_dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_minimum_iter_k",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local._get_minimum_iter_k"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local._get_operands": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local._get_operands",
    "name": "_get_operands",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [],
    "source_code": "    def _get_operands(sip_workload: Workload, stat_ref: int, idx: int):\n\n        def cdte_operand(tensor, cdte_name, ref):\n            tensor_gen = _iter_access_gen(list(_iter_tensor_addr(tensor, \"r\")))\n            next(tensor_gen)\n            data_size = list_product(tuple(tensor.dim)) * tensor.bpe\n            tensor_read = DataflowActionMemoryStat(\n                total_count=int(data_size),\n                master=DataflowActionType.CDTE,\n                src=AddrDomain.LOCAL,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=ref,\n                memory_access_list=tensor_gen.send(data_size),\n                name=cdte_name,\n            )\n            yield tensor_read\n            return tensor_read.latency, tensor_read.leading_latency\n\n        lhs_tensor = sip_workload.inputs[0]\n        rhs_tensor = sip_workload.inputs[1]\n        quant_type = sip_workload.attr.get(\"quant_type\", QuantType.No_Quant)\n        scaling_tensor = None\n        if quant_type == QuantType.Wf4g_Af8t:\n            if len(sip_workload.inputs) >= 3 and sip_workload.inputs[2]:\n                scaling_tensor = sip_workload.inputs[2]\n\n        lhs_ref = 0\n        rhs_ref = 0\n        lhs_leading_latency = 0\n        rhs_leading_latency = 0\n        if lhs_tensor and lhs_tensor not in lhs_buffer:\n            lhs_ref, lhs_leading_latency = yield from cdte_operand(lhs_tensor, f\"{idx} lhs:l3->local\", stat_ref)\n            lhs_buffer.append(lhs_tensor)\n\n        if rhs_tensor and rhs_tensor not in rhs_buffer:\n            rhs_ref, rhs_leading_latency = yield from cdte_operand(rhs_tensor, f\"{idx} rhs:l3->local\", stat_ref)\n            rhs_buffer.append(rhs_tensor)\n\n        latency = max(lhs_ref, rhs_ref)\n        leading = max(lhs_leading_latency, rhs_leading_latency)\n\n        if scaling_tensor and rhs_tensor not in scaling_buffer:\n            scaling_ref, scaling_leading_latency = yield from cdte_operand(\n                scaling_tensor, f\"{idx} scaling:l3->local\", stat_ref\n            )\n            scaling_buffer.append(rhs_tensor)\n            latency = max(latency, scaling_ref)\n            leading = max(leading, scaling_leading_latency)\n        return latency, leading",
    "start_line": 281,
    "end_line": 330,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "sip_workload",
      "stat_ref",
      "idx"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_operands",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local._get_operands"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_local.cdte_operand": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_local.cdte_operand",
    "name": "cdte_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_local.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.batch_gemm_local._iter_tensor_addr",
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "        def cdte_operand(tensor, cdte_name, ref):\n            tensor_gen = _iter_access_gen(list(_iter_tensor_addr(tensor, \"r\")))\n            next(tensor_gen)\n            data_size = list_product(tuple(tensor.dim)) * tensor.bpe\n            tensor_read = DataflowActionMemoryStat(\n                total_count=int(data_size),\n                master=DataflowActionType.CDTE,\n                src=AddrDomain.LOCAL,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=ref,\n                memory_access_list=tensor_gen.send(data_size),\n                name=cdte_name,\n            )\n            yield tensor_read\n            return tensor_read.latency, tensor_read.leading_latency",
    "start_line": 283,
    "end_line": 298,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor",
      "cdte_name",
      "ref"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cdte_operand",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_local.cdte_operand"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_minimum_iter_k": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_minimum_iter_k",
    "name": "_get_minimum_iter_k",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [],
    "source_code": "def _get_minimum_iter_k(b: int, m: int, n: int, bpe: Dict[str, float], mac_dtype: DType, config: BossaNovaConfig):\n    iter_k = 32\n    if mac_dtype == DType.FP4:\n        iter_k = 64\n    k = 0\n    l1_bw = config.bw.l0.shared.bw\n    l1_latency = config.bw.l0.shared.pre_latency\n    shared_l1_cycle = 0\n    while shared_l1_cycle < l1_latency:\n        k += iter_k\n        data_size = b * m * k * bpe[\"lhs\"] + b * n * k * bpe[\"rhs\"]\n        shared_l1_cycle = data_size / l1_bw\n    return k",
    "start_line": 30,
    "end_line": 42,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "b",
      "m",
      "n",
      "bpe",
      "mac_dtype",
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_minimum_iter_k",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_minimum_iter_k"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_sip_workloads": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_sip_workloads",
    "name": "_get_sip_workloads",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm.compute_dtype_for_quant",
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.batch_gemm.bpe_for_quant",
      "nova-platform.nova_platform.benchmark.op_base.Operand",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_minimum_iter_k"
    ],
    "source_code": "def _get_sip_workloads(sic_workload: Workload, batch_gemm_shape: BatchGemmGridShape, config, sip_idx):\n    sip_workloads = []\n    B_l2 = sic_workload.attr[\"b\"]\n    M_l2 = sic_workload.attr[\"m\"]\n    N_l2 = sic_workload.attr[\"n\"]\n    K_l2 = sic_workload.attr[\"k\"]\n    quant_type = sic_workload.attr.get(\"quant_type\", QuantType.No_Quant)\n    compute_dtype = compute_dtype_for_quant(sic_workload.dtype, quant_type)\n    bpe = bpe_for_quant(sic_workload.dtype, quant_type)\n    thread_loop_x, thread_loop_y, thread_loop_z = batch_gemm_shape.block_dims[:3]\n    shape_thread = batch_gemm_shape.thread_dims_stride()\n    sip_idx_vec = batch_gemm_shape.thread_traverse_idx[sip_idx]\n\n    sip_idx_x, sip_idx_y, sip_idx_z = sip_idx_vec[:3]\n    m_offset = shape_thread[1] * (sip_idx_y)\n    n_offset = shape_thread[0] * (sip_idx_x)\n    b_offset = shape_thread[2] * (sip_idx_z)\n\n    if m_offset >= M_l2 or n_offset >= N_l2 or b_offset >= B_l2:\n        return []\n    m_valid = shape_thread[1] if M_l2 >= m_offset + shape_thread[1] else M_l2 - m_offset\n    n_valid = shape_thread[0] if N_l2 >= n_offset + shape_thread[0] else N_l2 - n_offset\n    b_valid = shape_thread[2] if B_l2 >= b_offset + shape_thread[2] else B_l2 - b_offset\n    iter_k = _get_minimum_iter_k(b_valid, m_valid, n_valid, bpe, compute_dtype[\"2d\"], config)\n    iter_k = min(K_l2, iter_k)\n    loop_k = (K_l2 + iter_k - 1) // iter_k\n    for i in range(loop_k):\n        k_offset = i * iter_k\n        if k_offset >= K_l2:\n            break\n        k_valid = iter_k if K_l2 >= k_offset + iter_k else K_l2 - k_offset\n        inputs = [\n            Operand(\n                dim=(b_valid, m_valid, k_valid),\n                addr=0x4000000000,\n                bpe=bpe[\"lhs\"],\n                dim_offset=(b_offset, m_offset, k_offset),\n                dim_stride=(B_l2, M_l2, K_l2),\n            ),\n            Operand(\n                dim=(b_valid, k_valid, n_valid),\n                addr=0x4100000000,\n                bpe=bpe[\"rhs\"],\n                dim_offset=(b_offset, k_offset, n_offset),\n                dim_stride=(B_l2, K_l2, N_l2),\n            ),\n        ]\n        if quant_type == QuantType.Wf4g_Af8t and sic_workload.inputs[2]:\n            quant_group_size = sic_workload.attr.get(\"quant_group_size\", 32)\n            scaling_tensor = sic_workload.inputs[2]\n            quant_group_num = scaling_tensor.dim[1]\n            group_num_offset = (k_offset + quant_group_size - 1) // quant_group_size\n            valid_group_num = (k_valid + quant_group_size - 1) // quant_group_size\n            valid_group_num = (\n                valid_group_num\n                if quant_group_num >= group_num_offset + valid_group_num\n                else quant_group_num - group_num_offset\n            )\n            inputs.append(\n                Operand(\n                    dim=(b_valid, valid_group_num, n_valid),\n                    addr=0x4200000000,\n                    bpe=bpe[\"scaling\"],\n                    dim_offset=(b_offset, group_num_offset, n_offset),\n                    dim_stride=(B_l2, quant_group_num, N_l2),\n                )\n            )\n        outputs = []\n        if i == loop_k - 1 and len(sic_workload.outputs) > 0:\n            outputs = [\n                Operand(\n                    dim=(b_valid, m_valid, n_valid),\n                    addr=0x4300000000,\n                    bpe=bpe[\"res\"],\n                    dim_offset=(b_offset, m_offset, n_offset),\n                    dim_stride=(B_l2, M_l2, N_l2),\n                ),\n            ]\n        attr = {**sic_workload.attr, \"b\": b_valid, \"m\": m_valid, \"n\": n_valid, \"k\": k_valid}\n        sip_workloads.append(Workload(inputs, outputs, dtype=sic_workload.dtype, attr=attr))\n    return sip_workloads",
    "start_line": 45,
    "end_line": 125,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "sic_workload",
      "batch_gemm_shape",
      "config",
      "sip_idx"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_sip_workloads",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_sip_workloads"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared.BatchGemmShared": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.BatchGemmShared",
    "name": "BatchGemmShared",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._get_sip_workloads",
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.op_base.Operand",
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class BatchGemmShared(XpuBatchGemmBase):\n\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.mem_arch_type = MemArchType.DSM_SHARED\n        self.calc_ceil_K_align = 64\n        self.l1_bandwidth_per_sip = config.bw.l0.shared.bw * config.freq.CORE * 1e9\n\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        sic_workloads = {}\n        batch_gemm_shape.gen_traverse_idx([self.in_n, self.in_m, self.in_batch, 1])\n\n        xpu_layout = batch_gemm_shape.block_dims\n        shape_grid = batch_gemm_shape.grid_dims_stride()\n        shape_block = batch_gemm_shape.block_dims_stride()\n        shape_thread = batch_gemm_shape.thread_dims_stride()\n\n        B_l3 = self.in_batch\n        M_l3 = self.in_m\n        N_l3 = self.in_n\n        K_l3 = self.in_k\n        B_l2 = shape_block[2]\n        M_l2 = shape_block[1]\n        N_l2 = shape_block[0]\n        K_l2 = batch_gemm_shape.calc_ceil_K_l2\n\n        shape_lhs_l2 = [K_l2, M_l2, B_l2, 1]  # shape after splitting\n        shape_rhs_l2 = [N_l2, K_l2, B_l2, 1]\n\n        lhs_cnt_l2 = batch_gemm_shape.lhs_buf_cnt_l2\n        rhs_cnt_l2 = batch_gemm_shape.rhs_buf_cnt_l2\n        lhs_tensor_l3 = self.workload.inputs[0]\n        rhs_tensor_l3 = self.workload.inputs[1]\n        res_tensor_l3 = self.workload.outputs[0]\n\n        block_loop_x = (N_l3 + shape_grid[0] - 1) // shape_grid[0]\n        block_loop_y = (M_l3 + shape_grid[1] - 1) // shape_grid[1]\n        block_loop_z = (B_l3 + shape_grid[2] - 1) // shape_grid[2]\n        block_total = block_loop_x * block_loop_y * block_loop_z\n        assert block_total == len(batch_gemm_shape.block_traverse_idx[0])\n        thread_loop_x, thread_loop_y, thread_loop_z = batch_gemm_shape.block_dims[:3]\n        k_loop_l2 = (K_l3 + K_l2 - 1) // K_l2\n\n        sic_total = batch_gemm_shape.block_cnt_in_grid()\n        sip_total = batch_gemm_shape.thread_cnt_in_block()\n\n        for sic_idx in range(sic_total):\n            # alloc l2 mem\n            pass\n\n        for sic_idx in range(sic_total):\n            if first_sip_only and sic_idx != 0:\n                break\n            sic_workloads.setdefault(sic_idx, [])\n            for block_idx in range(block_total):\n                block_idx_vec = batch_gemm_shape.block_traverse_idx[sic_idx][block_idx]\n                if block_idx_vec[0] < 0:\n                    continue\n                block_idx_x, block_idx_y, block_idx_z = block_idx_vec[:3]\n                for k_l2_idx in range(k_loop_l2):\n                    k_l2_idx_new = k_l2_idx\n                    if block_idx & 1:\n                        k_l2_idx_new = k_loop_l2 - 1 - k_l2_idx  # snake-like\n\n                    k_l2_offset = K_l2 * k_l2_idx_new\n                    k_l2_size = (K_l3 - k_l2_offset) if (k_l2_idx_new == k_loop_l2 - 1) else K_l2\n                    # we need rhs slice with auto padding 0\n                    k_l2_size_align = (\n                        (k_l2_size + self.calc_ceil_K_align - 1) / self.calc_ceil_K_align * self.calc_ceil_K_align\n                    )\n                    b_offset = B_l2 * block_idx_z\n                    m_offset = M_l2 * block_idx_y\n                    n_offset = N_l2 * block_idx_x\n                    k_offset = k_l2_offset\n                    if b_offset >= B_l3 or m_offset >= M_l3 or n_offset >= N_l3 or k_offset >= K_l3:\n                        continue\n                    b_valid = B_l2 if B_l3 >= b_offset + B_l2 else B_l3 - b_offset\n                    m_valid = M_l2 if M_l3 >= m_offset + M_l2 else M_l3 - m_offset\n                    n_valid = N_l2 if N_l3 >= n_offset + N_l2 else N_l3 - n_offset\n                    k_valid = K_l2 if K_l3 >= k_offset + K_l2 else (K_l3 - k_offset)\n\n                    inputs = [\n                        Operand(\n                            dim=(b_valid, m_valid, k_valid),\n                            addr=lhs_tensor_l3.addr,\n                            bpe=lhs_tensor_l3.bpe,\n                            dim_offset=(b_offset, m_offset, k_offset),\n                            dim_stride=lhs_tensor_l3.dim_stride,\n                        ),\n                        Operand(\n                            dim=(b_valid, k_valid, n_valid),\n                            addr=rhs_tensor_l3.addr,\n                            bpe=rhs_tensor_l3.bpe,\n                            dim_offset=(b_offset, k_offset, n_offset),\n                            dim_stride=rhs_tensor_l3.dim_stride,\n                        ),\n                    ]\n                    if self.quant_type == QuantType.Wf4g_Af8t:\n                        quant_group_size = self.workload.attr.get(\"quant_group_size\", 32)\n                        scaling_tensor = self.workload.inputs[2]\n                        quant_group_num = scaling_tensor.dim[1]\n                        group_num_offset = (k_offset + quant_group_size - 1) // quant_group_size\n                        valid_group_num = (k_valid + quant_group_size - 1) // quant_group_size\n                        valid_group_num = (\n                            valid_group_num\n                            if quant_group_num >= group_num_offset + valid_group_num\n                            else quant_group_num - group_num_offset\n                        )\n                        inputs.append(\n                            Operand(\n                                dim=(b_valid, valid_group_num, n_valid),\n                                addr=scaling_tensor.addr,\n                                bpe=scaling_tensor.bpe,\n                                dim_offset=(b_offset, group_num_offset, n_offset),\n                                dim_stride=scaling_tensor.dim_stride,\n                            )\n                        )\n                    if k_l2_idx == k_loop_l2 - 1:\n                        # res store\n                        outputs = [\n                            Operand(\n                                dim=(b_valid, m_valid, n_valid),\n                                addr=res_tensor_l3.addr,\n                                bpe=res_tensor_l3.bpe,\n                                dim_offset=(b_offset, m_offset, n_offset),\n                                dim_stride=res_tensor_l3.dim_stride,\n                            )\n                        ]\n                    else:\n                        outputs = []\n                    attr = {**self.workload.attr, \"b\": b_valid, \"m\": m_valid, \"n\": n_valid, \"k\": k_valid}\n                    sic_workloads[sic_idx].append(Workload(inputs, outputs, dtype=self.dtype, attr=attr))\n        self.handle_pingpong_buf(batch_gemm_shape, sic_workloads)\n        return sic_workloads\n\n    def handle_pingpong_buf(self, batch_gemm_shape: BatchGemmGridShape, workloads: Dict[int, Workload]):\n        for sic_id in workloads:\n            lhs_buffer = deque(maxlen=batch_gemm_shape.lhs_buf_cnt_l2)\n            rhs_buffer = deque(maxlen=batch_gemm_shape.rhs_buf_cnt_l2)\n            scaling_buffer = deque(maxlen=batch_gemm_shape.rhs_buf_cnt_l2)\n            for wl in workloads[sic_id]:\n                lhs_operand = wl.inputs[0]\n                rhs_operand = wl.inputs[1]\n                if lhs_operand in lhs_buffer:\n                    wl.inputs[0] = None\n                else:\n                    lhs_buffer.append(lhs_operand)\n\n                if rhs_operand in rhs_buffer:\n                    wl.inputs[1] = None\n                else:\n                    rhs_buffer.append(rhs_operand)\n                if len(wl.inputs) == 3:\n                    scaling_operand = wl.inputs[2]\n                    if scaling_operand in scaling_buffer:\n                        wl.inputs[2] = None\n                    else:\n                        scaling_buffer.append(scaling_operand)\n\n    def _calc_sol_cost(self, batch_gemm_shape: BatchGemmGridShape) -> float:\n        def _cal_input_latency(wl):\n            size = 0\n            for i in wl.inputs:\n                size += list_product(tuple(i.dim)) * i.bpe if i else 0\n            size = size / self.sip_cnt\n            if size == 0:\n                return 0\n            return size / self.l3_bandwidth_per_sip + self.l3_to_l1_latency\n\n        def _cal_output_latency(wl):\n            size = 0\n            for i in wl.outputs:\n                size += list_product(tuple(i.dim)) * i.bpe if i else 0\n\n            size = size / self.sip_cnt\n            if size == 0:\n                return 0\n            return size / self.l3_bandwidth_per_sip\n\n        def _cal_compute_latency(sip_workloads: List[Workload]):\n            latency = 0\n            for workload in sip_workloads:\n                lhs = workload.inputs[0]\n                rhs = workload.inputs[1]\n                ops = 2 * workload.attr[\"b\"] * workload.attr[\"m\"] * workload.attr[\"n\"] * workload.attr[\"k\"]\n                l1_access = lhs.dim[0] * lhs.dim[1] * lhs.dim[2] * lhs.bpe\n                l1_access += rhs.dim[0] * rhs.dim[1] * rhs.dim[2] * rhs.bpe\n                latency += max(ops / self.xpu_FLOPS, l1_access / self.l1_bandwidth_per_sip)\n            return latency + self.config.bw.l0.shared.pre_latency / self.config.freq.CORE / 1e9\n\n        sic_workloads: Dict[int, List[Workload]] = self._gen_workloads(batch_gemm_shape)\n        if len(sic_workloads) == 0:\n            return 0\n        total_latency = 0\n        cdte_ref = 0\n        compute_ref = 0\n        ping_buffer_release_ref = 0\n        out_ref = 0\n        for workload in sic_workloads[0]:\n            input_latency = _cal_input_latency(workload)\n            output_latency = _cal_output_latency(workload)\n            compute_latency = _cal_compute_latency(_get_sip_workloads(workload, batch_gemm_shape, self.config, 0))\n            compute_ref = cdte_ref + input_latency\n            cdte_ref = max(cdte_ref + input_latency - self.l3_to_l1_latency, ping_buffer_release_ref)\n            ping_buffer_release_ref = compute_ref + compute_latency\n            out_ref = compute_ref + compute_latency\n            total_latency = out_ref + output_latency\n        return total_latency",
    "start_line": 128,
    "end_line": 335,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XpuBatchGemmBase"
    ],
    "class_name": null,
    "display_name": "class BatchGemmShared",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.BatchGemmShared"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared.tile_shared_gemm_workload": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.tile_shared_gemm_workload",
    "name": "tile_shared_gemm_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared.BatchGemmShared"
    ],
    "source_code": "def tile_shared_gemm_workload(config: BossaNovaConfig, chip_workload: Workload) -> Tuple[Workload, BatchGemmGridShape]:\n    batch_gemm = BatchGemmShared(config, chip_workload)\n    batch_gemm.split()\n    batch_gemm.impl()\n    best_shape = batch_gemm.get_best_shape()\n    return batch_gemm.get_tiled_workloads(), best_shape",
    "start_line": 339,
    "end_line": 344,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config",
      "chip_workload"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tile_shared_gemm_workload",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.tile_shared_gemm_workload"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared.dsm_shared_gemm_kernel": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.dsm_shared_gemm_kernel",
    "name": "dsm_shared_gemm_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [],
    "source_code": "def dsm_shared_gemm_kernel(\n    sic_workloads: Dict[int, List[Workload]],\n    batch_gemm_shape: BatchGemmGridShape,\n    config: BossaNovaConfig,\n    kernel_id: int,\n    sic_id: int,\n    sip_id: int,\n    relative_ts: int,\n    abs_ref: int,\n):\n\n    def _iter_tensor_addr(\n        tensors: Union[List[Operand], Operand], rw: str\n    ) -> Generator[DataflowActionMemoryAccess, None, None]:\n        tensors = tensors if isinstance(tensors, (tuple, list)) else [tensors]\n        for tensor in tensors:\n            mem_access = tensor.get_contiguous_mem_accesses()\n            for addr, size in mem_access:\n                yield DataflowActionMemoryAccess(addr, size, rw)\n\n    def _tensor_transport_memory_stat(\n        tensors: Union[List[Operand], Operand], master, src, dst, rw, relative_ts, stat_name\n    ):\n        if not tensors:\n            return 0, 0\n        tensors = tensors if isinstance(tensors, (tuple, list)) else [tensors]\n        tensor_gen = _iter_access_gen(list(_iter_tensor_addr(tensors, rw)))\n        next(tensor_gen)\n        data_size = 0\n        for tensor in tensors:\n            data_size += list_product(tuple(tensor.dim)) * tensor.bpe\n        tensor_read = DataflowActionMemoryStat(\n            total_count=int(data_size),\n            master=master,\n            src=src,\n            dst=dst,\n            rw=rw,\n            relative_ts=relative_ts,\n            memory_access_list=tensor_gen.send(data_size),\n            name=stat_name,\n        )\n        yield tensor_read\n        return tensor_read.latency, tensor_read.leading_latency\n\n    def _cdte_l3_to_shared(tensor, cdte_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw=\"r\",\n            relative_ts=relative_ts,\n            stat_name=cdte_name,\n        )\n\n    def _cdte_shared_to_l3(tensor, cdte_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw=\"w\",\n            relative_ts=relative_ts,\n            stat_name=cdte_name,\n        )\n\n    def _ld_shared_to_l0(tensors: Union[List[Operand], Operand], st_name: str, relative_ts: int):\n        return _tensor_transport_memory_stat(\n            tensors=tensors,\n            master=DataflowActionType.XPU,\n            src=AddrDomain.L0,\n            dst=AddrDomain.SHARED,\n            rw=\"r\",\n            relative_ts=relative_ts,\n            stat_name=st_name,\n        )\n\n    def _st_l0_to_shared(tensor, st_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.XPU,\n            src=AddrDomain.L0,\n            dst=AddrDomain.SHARED,\n            rw=\"w\",\n            relative_ts=relative_ts,\n            stat_name=st_name,\n        )\n\n    def _split_operands(input: Operand, total_sip: int, sip_id: int):\n        if not input:\n            return None\n        for idx, dim in enumerate(input.dim):\n            if dim < total_sip:\n                continue\n            split_size = (dim + total_sip - 1) // total_sip\n            offset = split_size * sip_id\n            if offset >= dim:\n                return None\n            new_dim = split_size if dim >= split_size + offset else dim - offset\n            new_tensor_dim = tuple([new_dim if idx == i else d for i, d in enumerate(input.dim)])\n            new_tensor_offset = tuple([d + offset if idx == i else d for i, d in enumerate(input.dim_offset)])\n            return Operand(new_tensor_dim, input.addr, input.bpe, new_tensor_offset, input.dim_stride)\n        return input\n\n    def _gemm_l2_kernel(sip_workloads: List[Workload], relative_ts, idx):\n        total_latency = relative_ts\n        stat_ref = relative_ts\n        subthread_num = batch_gemm_shape.subthread_cnt_in_thread()\n        for workload in sip_workloads:\n            input_tensors = []\n            input_names = []\n            if workload.inputs[0]:\n                input_tensors.append(workload.inputs[0])\n                input_names.append(\"lhs\")\n            if workload.inputs[1]:\n                input_tensors.append(workload.inputs[1])\n                input_names.append(\"rhs\")\n\n            quant_type = workload.attr.get(\"quant_type\", QuantType.No_Quant)\n            mac_dtype = workload.dtype\n            if quant_type == QuantType.Wf4g_Af8t:\n                mac_dtype = DType.FP4\n                if len(workload.inputs) == 3:\n                    input_tensors.append(workload.inputs[2])\n                    input_names.append(\"scaling\")\n            elif quant_type == QuantType.Wf8t_Af8t:\n                mac_dtype = DType.FP8\n            input_latency, input_leading = yield from _ld_shared_to_l0(\n                input_tensors, f\"{'+'.join(input_names)}:shared->L0\", stat_ref\n            )\n\n            compute_ref = stat_ref + input_leading\n            scalar_ops = 4 * subthread_num\n            compute_stat1 = DataflowActionComputeStat(\n                name=f\"{idx} scalar\",\n                compute_scalar_cycle=scalar_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat1\n            in_b = workload.attr[\"b\"]\n            in_m = workload.attr[\"m\"]\n            in_n = workload.attr[\"n\"]\n            in_k = workload.attr[\"k\"]\n            compute_mac = in_b * in_m * in_n * in_k\n            compute_stat2 = DataflowActionComputeStat(\n                name=f\"{idx} mac\",\n                compute_2d_ops={mac_dtype: compute_mac * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2\n            compute_latency = compute_stat2.latency\n            VMM = 10\n            compute_vmm = DataflowActionComputeStat(\n                name=f\"{idx} vmm\",\n                compute_nop_cycle=VMM,\n                relative_ts=compute_ref + compute_latency,\n            )\n            yield compute_vmm\n            compute_latency += compute_vmm.latency\n\n            res_latency = 0\n            res_ref = compute_ref\n            if len(workload.outputs) > 0:\n                if quant_type == QuantType.Wf4g_Af8t or quant_type == QuantType.Wf8t_Af8t:\n                    compute_convert = DataflowActionComputeStat(\n                        name=f\"{idx} convert\",\n                        compute_1d_ops={DType.FP32: in_m * in_n},\n                        relative_ts=compute_ref,\n                    )\n                    yield compute_convert\n                    compute_latency = max(compute_convert.latency, compute_latency)\n                res_tensor = workload.outputs[0]\n                res_latency, res_leading = yield from _st_l0_to_shared(res_tensor, f\"{idx}:res:l0->shared\", res_ref)\n            total_latency = max(\n                compute_ref + compute_latency,\n                stat_ref + input_latency,\n                res_ref + res_latency,\n            )\n            stat_ref = max(stat_ref, total_latency - input_leading, compute_ref)\n\n        return total_latency\n\n    total_latency = relative_ts\n    cdte_ref = relative_ts\n    compute_release_ref = relative_ts\n    ping_buffer_release_ref = relative_ts\n    if sic_id not in sic_workloads:\n        return 0\n\n    sip_total = config.inst_num.NUM_OF_CORE_PER_CLUSTER\n    sip_compute_total = batch_gemm_shape.thread_cnt_in_block()\n    stat_ref = 0\n    for idx, workload in enumerate(sic_workloads[sic_id]):\n        lhs_tensor = _split_operands(workload.inputs[0], sip_total, sip_id)\n        # lhs_tensor = workload.inputs[0]\n        lhs_latency, lhs_leading = yield from _cdte_l3_to_shared(lhs_tensor, f\"{idx}:lhs:l3->shared\", cdte_ref)\n        rhs_tensor = _split_operands(workload.inputs[1], sip_total, sip_id)\n        # rhs_tensor = workload.inputs[1]\n        rhs_latency, rhs_leading = yield from _cdte_l3_to_shared(rhs_tensor, f\"{idx}:rhs:l3->shared\", cdte_ref)\n        cdte_latency = max(total_latency, lhs_latency + cdte_ref, rhs_latency + cdte_ref)\n        quant_type = workload.attr.get(\"quant_type\", QuantType.No_Quant)\n        if quant_type == QuantType.Wf4g_Af8t:\n            scaling_tensor = _split_operands(workload.inputs[2], sip_total, sip_id)\n            scaling_latency, scaling_leading = yield from _cdte_l3_to_shared(\n                scaling_tensor, f\"{idx}:scaling:l3->shared\", cdte_ref\n            )\n            cdte_latency = max(cdte_latency, scaling_latency + cdte_ref)\n        cdte_latency = yield from bm.get_barrier(f\"lhs_rhs_{kernel_id}_{sic_id}_{idx}\", sip_total).wait(\n            abs_ref + cdte_latency\n        )\n        cdte_latency -= abs_ref\n        if sip_id < sip_compute_total:\n            sip_workloads = _get_sip_workloads(workload, batch_gemm_shape, config, sip_id)\n            gemm_latency = yield from _gemm_l2_kernel(sip_workloads, cdte_latency, idx)\n        else:\n            gemm_latency = cdte_latency\n        gemm_latency = yield from bm.get_barrier(f\"gemm_{kernel_id}_{sic_id}_{idx}\", sip_total).wait(\n            abs_ref + gemm_latency\n        )\n        gemm_latency -= abs_ref\n        cdte_ref = max(cdte_latency - max(lhs_leading, rhs_leading), ping_buffer_release_ref)\n        ping_buffer_release_ref = gemm_latency\n        total_latency = max(total_latency, gemm_latency)\n        if len(workload.outputs) > 0:\n            res_tensor = _split_operands(workload.outputs[0], sip_total, sip_id)\n            res_latency, res_leading = yield from _cdte_shared_to_l3(res_tensor, f\"{idx}:res:shared->l3\", gemm_latency)\n            total_latency = max(total_latency, res_latency)\n\n    return total_latency",
    "start_line": 347,
    "end_line": 575,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "sic_workloads",
      "batch_gemm_shape",
      "config",
      "kernel_id",
      "sic_id",
      "sip_id",
      "relative_ts",
      "abs_ref"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dsm_shared_gemm_kernel",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared.dsm_shared_gemm_kernel"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr",
    "name": "_iter_tensor_addr",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "    def _iter_tensor_addr(\n        tensors: Union[List[Operand], Operand], rw: str\n    ) -> Generator[DataflowActionMemoryAccess, None, None]:\n        tensors = tensors if isinstance(tensors, (tuple, list)) else [tensors]\n        for tensor in tensors:\n            mem_access = tensor.get_contiguous_mem_accesses()\n            for addr, size in mem_access:\n                yield DataflowActionMemoryAccess(addr, size, rw)",
    "start_line": 358,
    "end_line": 365,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensors",
      "rw"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_tensor_addr",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat",
    "name": "_tensor_transport_memory_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr",
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "    def _tensor_transport_memory_stat(\n        tensors: Union[List[Operand], Operand], master, src, dst, rw, relative_ts, stat_name\n    ):\n        if not tensors:\n            return 0, 0\n        tensors = tensors if isinstance(tensors, (tuple, list)) else [tensors]\n        tensor_gen = _iter_access_gen(list(_iter_tensor_addr(tensors, rw)))\n        next(tensor_gen)\n        data_size = 0\n        for tensor in tensors:\n            data_size += list_product(tuple(tensor.dim)) * tensor.bpe\n        tensor_read = DataflowActionMemoryStat(\n            total_count=int(data_size),\n            master=master,\n            src=src,\n            dst=dst,\n            rw=rw,\n            relative_ts=relative_ts,\n            memory_access_list=tensor_gen.send(data_size),\n            name=stat_name,\n        )\n        yield tensor_read\n        return tensor_read.latency, tensor_read.leading_latency",
    "start_line": 367,
    "end_line": 389,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensors",
      "master",
      "src",
      "dst",
      "rw",
      "relative_ts",
      "stat_name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _tensor_transport_memory_stat",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_l3_to_shared": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_l3_to_shared",
    "name": "_cdte_l3_to_shared",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat"
    ],
    "source_code": "    def _cdte_l3_to_shared(tensor, cdte_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw=\"r\",\n            relative_ts=relative_ts,\n            stat_name=cdte_name,\n        )",
    "start_line": 391,
    "end_line": 400,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor",
      "cdte_name",
      "relative_ts"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _cdte_l3_to_shared",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_l3_to_shared"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_shared_to_l3": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_shared_to_l3",
    "name": "_cdte_shared_to_l3",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat"
    ],
    "source_code": "    def _cdte_shared_to_l3(tensor, cdte_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw=\"w\",\n            relative_ts=relative_ts,\n            stat_name=cdte_name,\n        )",
    "start_line": 402,
    "end_line": 411,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor",
      "cdte_name",
      "relative_ts"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _cdte_shared_to_l3",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._cdte_shared_to_l3"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._ld_shared_to_l0": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._ld_shared_to_l0",
    "name": "_ld_shared_to_l0",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat"
    ],
    "source_code": "    def _ld_shared_to_l0(tensors: Union[List[Operand], Operand], st_name: str, relative_ts: int):\n        return _tensor_transport_memory_stat(\n            tensors=tensors,\n            master=DataflowActionType.XPU,\n            src=AddrDomain.L0,\n            dst=AddrDomain.SHARED,\n            rw=\"r\",\n            relative_ts=relative_ts,\n            stat_name=st_name,\n        )",
    "start_line": 413,
    "end_line": 422,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensors",
      "st_name",
      "relative_ts"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _ld_shared_to_l0",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._ld_shared_to_l0"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._st_l0_to_shared": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._st_l0_to_shared",
    "name": "_st_l0_to_shared",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._tensor_transport_memory_stat"
    ],
    "source_code": "    def _st_l0_to_shared(tensor, st_name, relative_ts):\n        return _tensor_transport_memory_stat(\n            tensors=tensor,\n            master=DataflowActionType.XPU,\n            src=AddrDomain.L0,\n            dst=AddrDomain.SHARED,\n            rw=\"w\",\n            relative_ts=relative_ts,\n            stat_name=st_name,\n        )",
    "start_line": 424,
    "end_line": 433,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor",
      "st_name",
      "relative_ts"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _st_l0_to_shared",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._st_l0_to_shared"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._split_operands": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._split_operands",
    "name": "_split_operands",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "    def _split_operands(input: Operand, total_sip: int, sip_id: int):\n        if not input:\n            return None\n        for idx, dim in enumerate(input.dim):\n            if dim < total_sip:\n                continue\n            split_size = (dim + total_sip - 1) // total_sip\n            offset = split_size * sip_id\n            if offset >= dim:\n                return None\n            new_dim = split_size if dim >= split_size + offset else dim - offset\n            new_tensor_dim = tuple([new_dim if idx == i else d for i, d in enumerate(input.dim)])\n            new_tensor_offset = tuple([d + offset if idx == i else d for i, d in enumerate(input.dim_offset)])\n            return Operand(new_tensor_dim, input.addr, input.bpe, new_tensor_offset, input.dim_stride)\n        return input",
    "start_line": 435,
    "end_line": 449,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "input",
      "total_sip",
      "sip_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _split_operands",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._split_operands"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_shared._gemm_l2_kernel": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._gemm_l2_kernel",
    "name": "_gemm_l2_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_shared.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._ld_shared_to_l0",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._st_l0_to_shared"
    ],
    "source_code": "    def _gemm_l2_kernel(sip_workloads: List[Workload], relative_ts, idx):\n        total_latency = relative_ts\n        stat_ref = relative_ts\n        subthread_num = batch_gemm_shape.subthread_cnt_in_thread()\n        for workload in sip_workloads:\n            input_tensors = []\n            input_names = []\n            if workload.inputs[0]:\n                input_tensors.append(workload.inputs[0])\n                input_names.append(\"lhs\")\n            if workload.inputs[1]:\n                input_tensors.append(workload.inputs[1])\n                input_names.append(\"rhs\")\n\n            quant_type = workload.attr.get(\"quant_type\", QuantType.No_Quant)\n            mac_dtype = workload.dtype\n            if quant_type == QuantType.Wf4g_Af8t:\n                mac_dtype = DType.FP4\n                if len(workload.inputs) == 3:\n                    input_tensors.append(workload.inputs[2])\n                    input_names.append(\"scaling\")\n            elif quant_type == QuantType.Wf8t_Af8t:\n                mac_dtype = DType.FP8\n            input_latency, input_leading = yield from _ld_shared_to_l0(\n                input_tensors, f\"{'+'.join(input_names)}:shared->L0\", stat_ref\n            )\n\n            compute_ref = stat_ref + input_leading\n            scalar_ops = 4 * subthread_num\n            compute_stat1 = DataflowActionComputeStat(\n                name=f\"{idx} scalar\",\n                compute_scalar_cycle=scalar_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat1\n            in_b = workload.attr[\"b\"]\n            in_m = workload.attr[\"m\"]\n            in_n = workload.attr[\"n\"]\n            in_k = workload.attr[\"k\"]\n            compute_mac = in_b * in_m * in_n * in_k\n            compute_stat2 = DataflowActionComputeStat(\n                name=f\"{idx} mac\",\n                compute_2d_ops={mac_dtype: compute_mac * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2\n            compute_latency = compute_stat2.latency\n            VMM = 10\n            compute_vmm = DataflowActionComputeStat(\n                name=f\"{idx} vmm\",\n                compute_nop_cycle=VMM,\n                relative_ts=compute_ref + compute_latency,\n            )\n            yield compute_vmm\n            compute_latency += compute_vmm.latency\n\n            res_latency = 0\n            res_ref = compute_ref\n            if len(workload.outputs) > 0:\n                if quant_type == QuantType.Wf4g_Af8t or quant_type == QuantType.Wf8t_Af8t:\n                    compute_convert = DataflowActionComputeStat(\n                        name=f\"{idx} convert\",\n                        compute_1d_ops={DType.FP32: in_m * in_n},\n                        relative_ts=compute_ref,\n                    )\n                    yield compute_convert\n                    compute_latency = max(compute_convert.latency, compute_latency)\n                res_tensor = workload.outputs[0]\n                res_latency, res_leading = yield from _st_l0_to_shared(res_tensor, f\"{idx}:res:l0->shared\", res_ref)\n            total_latency = max(\n                compute_ref + compute_latency,\n                stat_ref + input_latency,\n                res_ref + res_latency,\n            )\n            stat_ref = max(stat_ref, total_latency - input_leading, compute_ref)\n\n        return total_latency",
    "start_line": 451,
    "end_line": 527,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "sip_workloads",
      "relative_ts",
      "idx"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _gemm_l2_kernel",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_shared._gemm_l2_kernel"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_tpu.BatchGemmTpu": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_tpu.BatchGemmTpu",
    "name": "BatchGemmTpu",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_tpu.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_tpu.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm.BatchGemmGridShape",
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.op_base.Operand",
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class BatchGemmTpu(BatchGemmBase):\n    \"\"\"\n    TPU GEMM tiler：基于阵列尺寸的简易分块，考虑 SRAM 容量与 HBM 带宽。\n    \"\"\"\n\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        super().__init__(config, workload)\n        self.shape_list: List[Tuple[int, BatchGemmGridShape]] = []\n        self._best_shape = None\n        compute = getattr(config, \"compute\", None)\n        memory = getattr(config, \"memory\", None)\n        bw = getattr(config, \"bw\", None)\n        freq = getattr(config, \"freq\", None)\n\n        freq_core = getattr(freq, \"CORE\", 1.0) if freq else 1.0\n        self.array_m = getattr(getattr(compute, \"tpu\", None), \"ARRAY_M\", 64) if compute else 64\n        self.array_n = getattr(getattr(compute, \"tpu\", None), \"ARRAY_N\", 64) if compute else 64\n        self.k_tile = getattr(getattr(compute, \"tpu\", None), \"K_TILE\", 128) if compute else 128\n        # array_flops: macs/s -> 2 flops per mac handled in cost\n        self.array_flops = self.array_m * self.array_n * freq_core * 1e9\n        # SRAM 容量\n        self.sram_bytes = getattr(getattr(memory, \"l1\", None), \"SIZE_PER_CORE\", 8 * 1024 * 1024) if memory else 8 * 1024 * 1024\n        # HBM 带宽\n        hbm_bw_cfg = getattr(getattr(getattr(bw, \"mc\", None), \"l3\", None), \"bw\", 64) if bw else 64\n        freq_mc = getattr(freq, \"MC\", freq_core) if freq else freq_core\n        num_die = getattr(getattr(config, \"inst_num\", None), \"NUM_OF_DIE\", 1)\n        self.hbm_bw = hbm_bw_cfg * freq_mc * 1e9 * num_die\n\n    def split(self):\n        candidates = []\n        idx = 0\n        tile_m_opts = [self.array_m, min(self.array_m * 2, self.in_m)]\n        tile_n_opts = [self.array_n, min(self.array_n * 2, self.in_n)]\n        tile_k_opts = [self.k_tile, min(self.k_tile * 2, self.in_k)]\n\n        for tm in tile_m_opts:\n            for tn in tile_n_opts:\n                for tk in tile_k_opts:\n                    if tm <= 0 or tn <= 0 or tk <= 0:\n                        continue\n                    if tm > self.in_m or tn > self.in_n or tk > self.in_k:\n                        continue\n                    if not self._fit_sram(tm, tn, tk):\n                        continue\n                    grid_m = math.ceil(self.in_m / tm)\n                    grid_n = math.ceil(self.in_n / tn)\n                    grid_b = math.ceil(self.in_batch / 1)\n                    shape = BatchGemmGridShape(\n                        grid_dims=[grid_n, grid_m, grid_b, 1],\n                        block_dims=[1, 1, 1, 1],\n                        thread_dims=[1, 1, 1, 1],\n                        subthread_dims=[1, 1, 1, 1],\n                        lhs_buf_cnt_l2=1,\n                        rhs_buf_cnt_l2=1,\n                        res_buf_cnt_l2=1,\n                        lhs_buf_cnt_l1=1,\n                        rhs_buf_cnt_l1=1,\n                        res_buf_cnt_l1=1,\n                        calc_ceil_K_l2=tk,\n                        calc_ceil_K_l1=tk,\n                    )\n                    shape.sol_cost = self._calc_sol_cost(shape, tm, tn, tk)\n                    candidates.append((idx, shape))\n                    idx += 1\n        if not candidates:\n            shape = BatchGemmGridShape(\n                grid_dims=[1, 1, 1, 1],\n                block_dims=[1, 1, 1, 1],\n                thread_dims=[1, 1, 1, 1],\n                subthread_dims=[1, 1, 1, 1],\n                lhs_buf_cnt_l2=1,\n                rhs_buf_cnt_l2=1,\n                res_buf_cnt_l2=1,\n                lhs_buf_cnt_l1=1,\n                rhs_buf_cnt_l1=1,\n                res_buf_cnt_l1=1,\n                calc_ceil_K_l2=self.in_k,\n                calc_ceil_K_l1=self.in_k,\n            )\n            shape.sol_cost = self._calc_sol_cost(shape, self.in_m, self.in_n, self.in_k)\n            candidates.append((0, shape))\n        self.shape_list = sorted(candidates, key=lambda x: x[1].sol_cost)\n\n    def _fit_sram(self, tm: int, tn: int, tk: int) -> bool:\n        lhs_bytes = tm * tk * self.bpe[\"lhs\"]\n        rhs_bytes = tn * tk * self.bpe[\"rhs\"]\n        res_bytes = tm * tn * self.bpe[\"res\"]\n        return lhs_bytes + rhs_bytes + res_bytes <= self.sram_bytes\n\n    def _calc_sol_cost(self, shape: BatchGemmGridShape, tm: int, tn: int, tk: int):\n        tiles_m = math.ceil(self.in_m / tm)\n        tiles_n = math.ceil(self.in_n / tn)\n        tiles_k = math.ceil(self.in_k / tk)\n        tiles_b = math.ceil(self.in_batch / 1)\n        tiles = tiles_m * tiles_n * tiles_k * tiles_b\n        macs_per_tile = tm * tn * tk\n        compute = (macs_per_tile * 2) / (self.array_flops or 1.0)\n        bytes_per_tile = tm * tk * self.bpe[\"lhs\"] + tn * tk * self.bpe[\"rhs\"] + tm * tn * self.bpe[\"res\"]\n        mem = bytes_per_tile / (self.hbm_bw or 1.0)\n        return max(compute, mem) * tiles\n\n    def _gen_workloads(self, batch_gemm_shape: BatchGemmGridShape, first_sip_only=False):\n        workloads: Dict[int, Dict[int, List[Workload]]] = {}\n        tm = min(self.array_m * 2, self.in_m)\n        tn = min(self.array_n * 2, self.in_n)\n        tk = batch_gemm_shape.calc_ceil_K_l2\n\n        lhs_tensor = self.workload.inputs[0]\n        rhs_tensor = self.workload.inputs[1]\n        res_tensor = self.workload.outputs[0]\n\n        grid_n, grid_m, grid_b, _ = batch_gemm_shape.grid_dims\n        sic_idx = 0\n        sip_idx = 0\n        workloads.setdefault(sic_idx, {}).setdefault(sip_idx, [])\n        for bz in range(grid_b):\n            for my in range(grid_m):\n                for nx in range(grid_n):\n                    m_offset = my * tm\n                    n_offset = nx * tn\n                    b_offset = bz * 1\n                    m_valid = min(tm, self.in_m - m_offset)\n                    n_valid = min(tn, self.in_n - n_offset)\n                    k_processed = 0\n                    while k_processed < self.in_k:\n                        k_valid = min(tk, self.in_k - k_processed)\n                        lhs = Operand(\n                            dim=(1, m_valid, k_valid),\n                            addr=lhs_tensor.addr,\n                            bpe=lhs_tensor.bpe,\n                            dim_offset=(b_offset, m_offset, k_processed),\n                            dim_stride=lhs_tensor.dim_stride,\n                        )\n                        rhs = Operand(\n                            dim=(1, k_valid, n_valid),\n                            addr=rhs_tensor.addr,\n                            bpe=rhs_tensor.bpe,\n                            dim_offset=(b_offset, k_processed, n_offset),\n                            dim_stride=rhs_tensor.dim_stride,\n                        )\n                        outs = []\n                        if k_processed + k_valid >= self.in_k:\n                            outs.append(\n                                Operand(\n                                    dim=(1, m_valid, n_valid),\n                                    addr=res_tensor.addr,\n                                    bpe=res_tensor.bpe,\n                                    dim_offset=(b_offset, m_offset, n_offset),\n                                    dim_stride=res_tensor.dim_stride,\n                                )\n                            )\n                        attr = {\n                            **self.workload.attr,\n                            \"b\": 1,\n                            \"m\": m_valid,\n                            \"n\": n_valid,\n                            \"k\": k_valid,\n                            \"tile_m\": tm,\n                            \"tile_n\": tn,\n                            \"tile_k\": tk,\n                            \"bytes_lhs\": list_product((1, m_valid, k_valid)) * lhs.bpe,\n                            \"bytes_rhs\": list_product((1, k_valid, n_valid)) * rhs.bpe,\n                            \"bytes_res\": list_product((1, m_valid, n_valid)) * (res_tensor.bpe if outs else 0),\n                        }\n                        workloads[sic_idx][sip_idx].append(Workload(inputs=[lhs, rhs], outputs=outs, attr=attr, dtype=self.workload.dtype))\n                        k_processed += k_valid\n        return workloads\n\n    def _sort_shape_candidates(self, shape_list):\n        return sorted(shape_list, key=lambda x: x[1].sol_cost)\n\n    def impl(self):\n        if not self.shape_list:\n            self.split()\n        self.shape_list = self._sort_shape_candidates(self.shape_list)\n        self._best_shape = self.get_best_shape()\n        self._tiled_workloads = self._gen_workloads(self._best_shape)\n\n    def get_best_shape(self) -> BatchGemmGridShape:\n        return self.shape_list[0][1]",
    "start_line": 9,
    "end_line": 188,
    "has_docstring": true,
    "docstring": "TPU GEMM tiler：基于阵列尺寸的简易分块，考虑 SRAM 容量与 HBM 带宽。",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BatchGemmBase"
    ],
    "class_name": null,
    "display_name": "class BatchGemmTpu",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_tpu.BatchGemmTpu"
  },
  "nova-platform.nova_platform.benchmark.batch_gemm_tpu.tile_tpu_gemm_workload": {
    "id": "nova-platform.nova_platform.benchmark.batch_gemm_tpu.tile_tpu_gemm_workload",
    "name": "tile_tpu_gemm_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/batch_gemm_tpu.py",
    "relative_path": "nova-platform/nova_platform/benchmark/batch_gemm_tpu.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_tpu.BatchGemmTpu"
    ],
    "source_code": "def tile_tpu_gemm_workload(config: BossaNovaConfig, chip_workload: Workload) -> Tuple[Dict, BatchGemmGridShape]:\n    gemm = BatchGemmTpu(config, chip_workload)\n    gemm.split()\n    gemm.impl()\n    return gemm.get_tiled_workloads(), gemm.get_best_shape()",
    "start_line": 191,
    "end_line": 195,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config",
      "chip_workload"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tile_tpu_gemm_workload",
    "component_id": "nova-platform.nova_platform.benchmark.batch_gemm_tpu.tile_tpu_gemm_workload"
  },
  "nova-platform.nova_platform.benchmark.eccl.EcclPrimitives": {
    "id": "nova-platform.nova_platform.benchmark.eccl.EcclPrimitives",
    "name": "EcclPrimitives",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/eccl.py",
    "relative_path": "nova-platform/nova_platform/benchmark/eccl.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class EcclPrimitives:\n    def _iter_access_gen(self, mem_acc_list: List[DataflowActionMemoryAccess]):\n        counter = 0\n        batch: List[DataflowActionMemoryAccess] = []\n        fetch_size = yield batch\n        # history = [] # for debug\n        # history_raw = [] # for debug\n        while True:\n            for acc in mem_acc_list:\n                # history_raw.append(acc)\n                batch.append(acc)\n                counter += acc.size\n                while True:\n                    last_acc = batch.pop()\n\n                    if counter >= fetch_size:\n                        last_size_right = counter - fetch_size\n                        last_size_left = last_acc.size - last_size_right\n                        batch.append(\n                            DataflowActionMemoryAccess(\n                                base_addr=last_acc.base_addr, size=last_size_left, rw=last_acc.rw\n                            )\n                        )\n\n                        new_base = last_acc.base_addr + last_size_left\n                        new_size = last_acc.size - last_size_left\n                        counter = fetch_size\n                        # history.append(batch)\n                        fetch_size = yield batch\n                        if new_size > 0:\n                            batch = [DataflowActionMemoryAccess(base_addr=new_base, size=new_size, rw=last_acc.rw)]\n                            counter = new_size\n                        else:\n                            batch = []\n                            counter = 0\n                            break\n                    else:\n                        batch.append(last_acc)\n                        break\n            if batch:\n                # history.append(batch)\n                yield batch",
    "start_line": 17,
    "end_line": 58,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EcclPrimitives",
    "component_id": "nova-platform.nova_platform.benchmark.eccl.EcclPrimitives"
  },
  "nova-platform.nova_platform.benchmark.eccl.SimpleProtoPrimitives": {
    "id": "nova-platform.nova_platform.benchmark.eccl.SimpleProtoPrimitives",
    "name": "SimpleProtoPrimitives",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/eccl.py",
    "relative_path": "nova-platform/nova_platform/benchmark/eccl.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.eccl.EcclPrimitives",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class SimpleProtoPrimitives(EcclPrimitives):\n    case_id: str = \"\"\n    channel_id: str = \"\"\n    recvPeers: List[int] = field(default_factory=list)\n    sendPeers: List[int] = field(default_factory=list)\n    sendBuff: int = 0\n    recvBuff: int = 0\n    rank: int = 0\n    ref: int = 0\n    useDirectMode: bool = False\n    slice_num: int = 1\n    esl_bw_factor: float = 1.0\n\n    def _get_fifo_addr(self, recv_id):\n        return 0x58000000000 + 0x01000000000 * (recv_id % 2)\n\n    def _get_flag_event(self, from_rank: int, to_rank: int, trans_id: int):\n        return em.get_event(f\"{self.case_id}_{self.channel_id}_{from_rank}_{to_rank}_{trans_id}\")\n\n    def directRecvReduceCopySend(self, inpIx: int, outIx: int, nelem: int, postOp: bool = False):\n        for recvPeerId in self.recvPeers:\n            for sendPeerId in self.sendPeers:\n                pass\n\n    def _esl_master_send(self, peer: int, nelem: int, relative_ts: int, src_addr_gen, dst_addr_gen, send_id: int):\n        master_send = DataflowActionMemoryStat(\n            total_count=nelem,\n            master=DataflowActionType.ESL,\n            src=AddrDomain.L3,\n            dst=AddrDomain.L3_REMOTE,\n            rw=\"w\",\n            src_gcu_id=self.rank,\n            tar_gcu_id=peer,\n            relative_ts=relative_ts,\n            memory_access_list=src_addr_gen.send(nelem),\n            remote_target_mem_access_list=dst_addr_gen.send(nelem),\n            bw_factor=self.esl_bw_factor,\n            name=f\"{self.channel_id}:l3->esl->gcu{peer}_l3\",\n        )\n        yield master_send\n        self._get_flag_event(self.rank, peer, send_id).set(self.ref + relative_ts + master_send.latency)\n        print(f\"!!!!send {self.case_id}_{self.channel_id}_{self.rank}_{peer}_{send_id}\")\n        return master_send.leading_latency, master_send.latency\n\n    def directRecvCopySend(self, outIx: int, nelem: int, relative_ts: int, recv_id: int, send_id: int):\n        latency = 0\n        for recv_peer in self.recvPeers:\n            for send_peer in self.sendPeers:\n                recv_ts = yield from self._get_flag_event(recv_peer, self.rank, recv_id).wait(self.ref + relative_ts)\n                send_ref = recv_ts - self.ref\n\n                src_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(self.sendBuff, nelem, \"r\")])\n                next(src_addr_gen)\n                dst_addr = self.recvBuff + outIx if self.useDirectMode else self._get_fifo_addr(send_id)\n                dst_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(dst_addr, nelem, \"w\")])\n                next(dst_addr_gen)\n                if not self.useDirectMode:\n                    yield from self.memcpyGtoG(self.recvBuff, outIx, self._get_fifo_addr(recv_id), 0, nelem, send_ref)\n                send_leading, send_latency = yield from self._esl_master_send(\n                    send_peer, nelem, send_ref, src_addr_gen, dst_addr_gen, send_id\n                )\n                latency = max(latency, send_ref + send_latency)\n        return latency\n\n    def directRecv(self, outIx: int, nelem: int, relative_ts: int, recv_id: int):\n        total_latency = 0\n        for recv_peer in self.recvPeers:\n            recv_ts = yield from self._get_flag_event(recv_peer, self.rank, recv_id).wait(self.ref + relative_ts)\n            latency = recv_ts - self.ref\n            if not self.useDirectMode:\n                latency = yield from self.memcpyGtoG(\n                    self.recvBuff, outIx, self._get_fifo_addr(recv_id), 0, nelem, latency\n                )\n            total_latency = max(latency, total_latency)\n        return latency\n\n    def directSend(self, inpIx: int, nelem: int, relative_ts: int, send_id: int):\n        latency = 0\n        for idx, peer in enumerate(self.sendPeers):\n            src_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(self.sendBuff + inpIx, nelem, \"r\")])\n            next(src_addr_gen)\n            dst_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(self.recvBuff, nelem, \"w\")])\n            next(dst_addr_gen)\n            send_leading, send_latency = yield from self._esl_master_send(\n                peer, nelem, relative_ts, src_addr_gen, dst_addr_gen, send_id\n            )\n            latency = max(latency, relative_ts + send_latency)\n        return latency\n\n    def directCopySend(self, inpIx: int, outIx: int, nelem: int, relative_ts: int, send_id: int):\n        latency = 0\n        for idx, peer in enumerate(self.sendPeers):\n            src_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(self.sendBuff + inpIx, nelem, \"r\")])\n            next(src_addr_gen)\n            dst_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(self.recvBuff + outIx, nelem, \"w\")])\n            next(dst_addr_gen)\n            send_leading, send_latency = yield from self._esl_master_send(\n                peer, nelem, relative_ts, src_addr_gen, dst_addr_gen, send_id\n            )\n            copy_latency = yield from self.memcpyGtoG(self.recvBuff, outIx, self.sendBuff, inpIx, nelem, relative_ts)\n            latency = max(latency, relative_ts + send_latency)\n        return latency\n\n    def recvCopyAsync(self, offset: int, nelem: int, relative_ts: int):\n        latency = 0\n        for idx, peer in enumerate(self.recvPeers):\n            recv_ts = yield from self._get_flag_event(peer, self.rank, 0).wait(self.ref + relative_ts)\n            copy_latency = yield from self.memcpyGtoG(self.recvBuff, 0, 0, 0, nelem, recv_ts - self.ref)\n            latency = max(latency, copy_latency)\n        return latency\n\n    def memcpyGtoG(self, dst: int, dstOffs: int, src: int, srcOffs: int, nelem: int, relative_ts: int):\n        if self.useDirectMode:\n            return relative_ts\n        srcAddr = src + srcOffs\n        dstAddr = dst + dstOffs\n        if dstAddr == srcAddr:\n            return relative_ts\n        src_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(srcAddr, nelem, \"r\")])\n        next(src_addr_gen)\n        dst_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(dstAddr, nelem, \"w\")])\n        next(dst_addr_gen)\n        st_l3_l3 = DataflowActionMemoryStat(\n            total_count=nelem,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.L3,\n            dst=AddrDomain.L3,\n            rw=\"w\",\n            relative_ts=relative_ts,\n            memory_access_list=src_addr_gen.send(nelem),\n            remote_target_mem_access_list=dst_addr_gen.send(nelem),\n            name=f\"{self.channel_id}:l3->l3\",\n        )\n        yield st_l3_l3\n        return relative_ts + st_l3_l3.latency\n\n    def broadCast(self, src_addr: int, src_offset: int, nelem: int, relative_ts: int):\n        total_latency = 0\n        for idx, peer in enumerate(self.sendPeers):\n            src_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(src_addr + src_offset, nelem, \"r\")])\n            next(src_addr_gen)\n            dst_addr_gen = self._iter_access_gen([DataflowActionMemoryAccess(src_addr + src_offset, nelem, \"w\")])\n            next(dst_addr_gen)\n            broadcast_ref = relative_ts\n            leading, latency = yield from self._esl_master_send(peer, nelem, broadcast_ref, src_addr_gen, dst_addr_gen)\n            total_latency = max(total_latency, broadcast_ref + latency)\n        return latency\n\n    def meshAllGather(self, offset: int, nelem: int, relative_ts: int):\n        latency = yield from self.broadCast(self.sendBuff, offset, nelem, relative_ts)\n        latency = yield from self.recvCopyAsync(offset, nelem, relative_ts)\n        return latency",
    "start_line": 62,
    "end_line": 213,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "EcclPrimitives"
    ],
    "class_name": null,
    "display_name": "class SimpleProtoPrimitives",
    "component_id": "nova-platform.nova_platform.benchmark.eccl.SimpleProtoPrimitives"
  },
  "nova-platform.nova_platform.benchmark.op_base.list_product": {
    "id": "nova-platform.nova_platform.benchmark.op_base.list_product",
    "name": "list_product",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [],
    "source_code": "def list_product(input):\n    return reduce(operator.mul, input, 1)",
    "start_line": 16,
    "end_line": 17,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "input"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function list_product",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.list_product"
  },
  "nova-platform.nova_platform.benchmark.op_base.get_layout": {
    "id": "nova-platform.nova_platform.benchmark.op_base.get_layout",
    "name": "get_layout",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [],
    "source_code": "def get_layout(instances):\n    layout = []\n    for b in range(1, instances + 1):\n        for m in range(1, instances + 1):\n            if b * m > instances:\n                continue\n            for n in range(1, instances + 1):\n                if b * m * n == instances:\n                    layout.append([b, m, n])\n    return layout",
    "start_line": 21,
    "end_line": 30,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "instances"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_layout",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.get_layout"
  },
  "nova-platform.nova_platform.benchmark.op_base.GridShape": {
    "id": "nova-platform.nova_platform.benchmark.op_base.GridShape",
    "name": "GridShape",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class GridShape:\n    grid_dims: list = field(default_factory=lambda: [1, 1, 1, 1])\n    block_dims: list = field(default_factory=lambda: [1, 1, 1, 1])\n    thread_dims: list = field(default_factory=lambda: [1, 1, 1, 1])\n    subthread_dims: list = field(default_factory=lambda: [1, 1, 1, 1])\n    block_row_major: bool = True\n    subthread_row_major: bool = True\n    block_traverse_idx: list = field(default_factory=list)\n    thread_traverse_idx: list = field(default_factory=list)\n    subthread_traverse_idx: list = field(default_factory=list)\n\n    def block_cnt_in_grid(self):\n        return list_product(tuple(self.grid_dims))\n\n    def thread_cnt_in_block(self):\n        return list_product(tuple(self.block_dims))\n\n    def subthread_cnt_in_thread(self):\n        return list_product(tuple(self.thread_dims))\n\n    def subthread_dims_stride(self):\n        return self.subthread_dims\n\n    def thread_dims_stride(self):\n        return [thread_dim * subthread_dim for thread_dim, subthread_dim in zip(self.thread_dims, self.subthread_dims)]\n\n    def block_dims_stride(self):\n        return [\n            block_dim * thread_dim * subthread_dim\n            for block_dim, thread_dim, subthread_dim in zip(self.block_dims, self.thread_dims, self.subthread_dims)\n        ]\n\n    def grid_dims_stride(self):\n        return [\n            grid_dim * block_dim * thread_dim * subthread_dim\n            for grid_dim, block_dim, thread_dim, subthread_dim in zip(\n                self.grid_dims, self.block_dims, self.thread_dims, self.subthread_dims\n            )\n        ]\n\n    def gen_traverse_idx(self, traverse_total_size):\n        shape_grid = self.grid_dims_stride()\n        shape_block = self.block_dims_stride()\n        gridLoopNum = [0, 0, 0, 0]\n        grid_total = 1\n        blockLoopNum = [0, 0, 0, 0]\n        block_total = 1\n        for i in range(4):\n            gridLoopNum[i] = (traverse_total_size[i] + shape_grid[i] - 1) // shape_grid[i]\n            grid_total *= gridLoopNum[i]\n            blockLoopNum[i] = (traverse_total_size[i] + shape_block[i] - 1) // shape_block[i]\n            block_total *= blockLoopNum[i]\n\n        self.block_traverse_idx = [[] for i in range(self.block_cnt_in_grid())]\n        # print(\"!!!!self.block_traverse_idx\", self.block_traverse_idx)\n        gridFlattenIdxX = 0\n        gridFlattenIdxY = 0\n        for i in range(gridLoopNum[self.block_traverse_dim_order[3]]):\n            for j in range(gridLoopNum[self.block_traverse_dim_order[2]]):\n                for k in range(gridLoopNum[self.block_traverse_dim_order[1]]):\n                    for l in range(gridLoopNum[self.block_traverse_dim_order[0]]):\n                        idx = [0, 0, 0, 0]\n                        idx[self.block_traverse_dim_order[0]] = l\n                        idx[self.block_traverse_dim_order[1]] = k\n                        idx[self.block_traverse_dim_order[2]] = j\n                        idx[self.block_traverse_dim_order[3]] = i\n                        # snake-like\n                        if self.block_traverse_dim_order[0] == 0 and (gridFlattenIdxY & 1):  # row major\n                            idx[self.block_traverse_dim_order[0]] = (\n                                gridLoopNum[self.block_traverse_dim_order[0]] - l - 1\n                            )\n                        if self.block_traverse_dim_order[3] == 0 and (gridFlattenIdxX & 1):  # col major\n                            idx[self.block_traverse_dim_order[0]] = (\n                                gridLoopNum[self.block_traverse_dim_order[0]] - l - 1\n                            )\n                            idx[self.block_traverse_dim_order[1]] = (\n                                gridLoopNum[self.block_traverse_dim_order[1]] - k - 1\n                            )\n                            idx[self.block_traverse_dim_order[2]] = (\n                                gridLoopNum[self.block_traverse_dim_order[2]] - j - 1\n                            )\n                        c_idx = 0\n                        for n in range(self.grid_dims[3]):\n                            for z in range(self.grid_dims[2]):\n                                for y in range(self.grid_dims[1]):\n                                    for x in range(self.grid_dims[0]):\n                                        block_idx = [0, 0, 0, 0]\n                                        block_idx[0] = idx[0] * self.grid_dims[0] + x\n                                        block_idx[1] = idx[1] * self.grid_dims[1] + y\n                                        block_idx[2] = idx[2] * self.grid_dims[2] + z\n                                        block_idx[3] = idx[3] * self.grid_dims[3] + n\n                                        if (\n                                            block_idx[0] >= blockLoopNum[0]\n                                            or block_idx[1] >= blockLoopNum[1]\n                                            or block_idx[2] >= blockLoopNum[2]\n                                            or block_idx[3] >= blockLoopNum[3]\n                                        ):\n                                            block_idx = [-1, -1, -1, -1]\n                                        self.block_traverse_idx[c_idx].append(block_idx)\n                                        c_idx += 1\n\n                    gridFlattenIdxY += 1\n            gridFlattenIdxX += 1\n\n        # gen thread traverse index vec\n        threadLoopNum = [0, 0, 0, 0]\n        thread_total = 1\n        for i in range(4):\n            threadLoopNum[i] = self.block_dims[i]\n            thread_total *= threadLoopNum[i]\n        self.thread_traverse_idx = []\n        for i in range(threadLoopNum[3]):\n            for j in range(threadLoopNum[2]):\n                for k in range(threadLoopNum[1]):\n                    for l in range(threadLoopNum[0]):\n                        self.thread_traverse_idx.append([l, k, j, i])\n\n        # gen thread traverse index vec\n        subthreadLoopNum = [0, 0, 0, 0]\n        subthread_total = 1\n        for i in range(4):\n            subthreadLoopNum[i] = self.thread_dims[i]\n            subthread_total *= subthreadLoopNum[i]\n        self.subthread_traverse_idx = []\n        subthreadFlattenIdxX = 0\n        subthreadFlattenIdxY = 0\n        for i in range(subthreadLoopNum[self.subthread_traverse_dim_order[3]]):\n            for j in range(subthreadLoopNum[self.subthread_traverse_dim_order[2]]):\n                for k in range(subthreadLoopNum[self.subthread_traverse_dim_order[1]]):\n                    for l in range(subthreadLoopNum[self.subthread_traverse_dim_order[1]]):\n                        idx = [0, 0, 0, 0]\n                        idx[self.subthread_traverse_dim_order[0]] = l\n                        idx[self.subthread_traverse_dim_order[1]] = k\n                        idx[self.subthread_traverse_dim_order[2]] = j\n                        idx[self.subthread_traverse_dim_order[3]] = i\n                        if self.subthread_traverse_dim_order[0] == 0 and (subthreadFlattenIdxY & 1):  # row major\n                            idx[self.subthread_traverse_dim_order[0]] = (\n                                subthreadLoopNum[self.subthread_traverse_dim_order[0]] - l - 1\n                            )\n                        if self.subthread_traverse_dim_order[3] == 0 and (subthreadFlattenIdxX & 1):  # col major\n                            idx[self.subthread_traverse_dim_order[0]] = (\n                                subthreadLoopNum[self.subthread_traverse_dim_order[0]] - l - 1\n                            )\n                            idx[self.subthread_traverse_dim_order[1]] = (\n                                subthreadLoopNum[self.subthread_traverse_dim_order[1]] - k - 1\n                            )\n                            idx[self.subthread_traverse_dim_order[2]] = (\n                                subthreadLoopNum[self.subthread_traverse_dim_order[2]] - j - 1\n                            )\n\n                        self.subthread_traverse_idx.append(idx)",
    "start_line": 34,
    "end_line": 184,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GridShape",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.GridShape"
  },
  "nova-platform.nova_platform.benchmark.op_base.Operand": {
    "id": "nova-platform.nova_platform.benchmark.op_base.Operand",
    "name": "Operand",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class Operand:\n    dim: tuple = field(default_factory=tuple)\n    addr: int = 0\n    bpe: int = 2\n    dim_offset: tuple = field(default_factory=tuple)\n    dim_stride: tuple = field(default_factory=tuple)\n    level: int = 3\n\n    def __hash__(self):\n        return hash((tuple(self.dim), tuple(self.dim_offset), tuple(self.dim_stride), self.level, self.addr, self.bpe))\n\n    def get_phy_addr_by_offset(self, off_):\n        assert all([o < s for o, s in zip(off_, self.dim_stride)])\n\n        def calculate_dim_stride(shape):\n            strides = []\n            current_stride = 1\n            for size in reversed(shape):\n                strides.append(current_stride)\n                current_stride *= size\n            strides.reverse()\n            return strides\n\n        stride = calculate_dim_stride(self.dim_stride)\n        offset = 0\n        for o, s in zip(off_, stride):\n            offset += o * s\n        return self.addr + offset * self.bpe\n\n    def get_contiguous_mem_accesses(self):\n        if all(o == 0 for o in self.dim_offset) and all([d == s for d, s in zip(self.dim, self.dim_stride)]):\n            return [(self.addr, list_product(tuple(self.dim)) * self.bpe)]\n        mem_access = []\n\n        size = self.dim[-1] * self.bpe\n        dim = list(self.dim[:-1]) + [1]\n        for idx in itertools.product(*(range(i) for i in dim)):\n            offset = [i + o for i, o in zip(idx, self.dim_offset)]\n            addr = self.get_phy_addr_by_offset(offset)\n            mem_access.append((addr, size))\n        return mem_access",
    "start_line": 188,
    "end_line": 228,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Operand",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.Operand"
  },
  "nova-platform.nova_platform.benchmark.op_base.Workload": {
    "id": "nova-platform.nova_platform.benchmark.op_base.Workload",
    "name": "Workload",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [],
    "source_code": "class Workload:\n    inputs: List[Operand] = field(default_factory=list)\n    outputs: List[Operand] = field(default_factory=list)\n    dtype: DType = DType.FP16\n    name: str = \"\"\n    attr: dict = field(default_factory=dict)\n\n    def __hash__(self):\n        return hash((tuple(self.inputs), tuple(self.outputs), self.dtype, self.name))",
    "start_line": 232,
    "end_line": 240,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Workload",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.Workload"
  },
  "nova-platform.nova_platform.benchmark.op_base.OpBase": {
    "id": "nova-platform.nova_platform.benchmark.op_base.OpBase",
    "name": "OpBase",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_base.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_base.py",
    "depends_on": [],
    "source_code": "class OpBase:\n    def __init__(self, config: BossaNovaConfig, workload: Workload) -> None:\n        self.config = config\n        self.workload = workload\n        self.dtype = self.workload.dtype",
    "start_line": 243,
    "end_line": 247,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpBase",
    "component_id": "nova-platform.nova_platform.benchmark.op_base.OpBase"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.DataType": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.DataType",
    "name": "DataType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "class DataType(str, Enum):\n    f32 = \"f32\"\n    f16 = \"f16\"\n    bf16 = \"bf16\"\n    f8e4m3 = \"f8e4m3\"\n    f8e5m2 = \"f8e5m2\"\n    u32 = \"u32\"\n    i32 = \"i32\"\n    u16 = \"u16\"\n    i16 = \"i16\"\n    u8 = \"u8\"\n    i8 = \"i8\"\n    b8 = \"b8\"\n    str = \"str\"",
    "start_line": 8,
    "end_line": 21,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "Enum"
    ],
    "class_name": null,
    "display_name": "class DataType",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.DataType"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.Scalar": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Scalar",
    "name": "Scalar",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "class Scalar:\n    value_name: str = \"\"\n    value: Any = None\n    dtype: DataType = \"\"",
    "start_line": 25,
    "end_line": 28,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Scalar",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Scalar"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.Tensor": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Tensor",
    "name": "Tensor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "class Tensor:\n    id: str\n    dtype: DataType\n    shape: List[int] = field(default_factory=list)\n    stride: List[int] = field(default_factory=list)\n    offset: int = 0",
    "start_line": 32,
    "end_line": 37,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Tensor",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Tensor"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.Struct": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Struct",
    "name": "Struct",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "class Struct:\n    value_name: str\n    struct_name: str\n    values: List[Union[Tensor, Scalar]]",
    "start_line": 41,
    "end_line": 44,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Struct",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.Struct"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.OpTraceRecord": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.OpTraceRecord",
    "name": "OpTraceRecord",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "class OpTraceRecord:\n    process_id: Union[int, None]\n    rank_id: Union[int, None]\n    stream_id: Union[int, None]\n    domain_name: str = \"\"\n    version_number: str = \"\"\n    op_name: str = \"\"\n    operands: List[Union[Struct, Tensor, Scalar]] = field(default_factory=list)\n    results: List[Union[Struct, Tensor, Scalar]] = field(default_factory=list)",
    "start_line": 48,
    "end_line": 56,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OpTraceRecord",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.OpTraceRecord"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_op_trace": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_op_trace",
    "name": "parse_op_trace",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "def parse_op_trace(traces: str) -> Generator[OpTraceRecord, None, None]:\n    record_pattern = r\"(?P<op_ids>\\w+:\\w+:\\w+)\\s+(?P<results>.+)\\s+=\\s*(?P<operator>.+)\\((?P<args>.+)\\)\"\n\n    def parse_value(arg):\n        if \"list{\" in arg:\n            args = split_args(arg[5:-1])\n            values = []\n            for v in args:\n                if value := parse_value(v):\n                    values.append(value)\n            return Struct(value_name=\"\", struct_name=\"list\", values=values)\n        if match := re.search(r\"(%\\d+):<(\\w+)>\\s*(\\{[\\d,]+\\})?\\s*(\\+\\d+)?\\s*\", arg):  # tensor\n            shape_n_dtype = match.group(2).split(\"x\")\n            stride = match.group(3)\n            stride = [int(i) for i in stride[1:-1].split(\",\")] if stride else []\n            offset = match.group(4)\n            offset = int(offset[1:]) if offset else 0\n\n            t = Tensor(\n                id=match.group(1),\n                dtype=shape_n_dtype[-1],\n                shape=[int(i) for i in shape_n_dtype[:-1]],\n                stride=stride,\n                offset=offset,\n            )\n            if len(t.shape) == 0:\n                t.shape = [1]\n            return t\n        if match := re.search(r\"([\\w.]+)\\s*=\\s*([\\w.-]+):([\\w.]+)\", arg):  # scalar with name\n            s = Scalar(value_name=match.group(1), value=match.group(2), dtype=match.group(3))\n            return s\n        if match := re.search(r\"([\\w.-]+):([\\w.]+)\", arg):  # scalar without name\n            s = Scalar(value_name=\"\", value=match.group(1), dtype=match.group(2))\n            return s\n        return None\n\n    def split_args(args):\n        stack = []\n        splits = []\n        start_index = 0\n        end_index = len(args)\n        for i, char in enumerate(args):\n            if char == \"{\":\n                stack.append(i)\n            elif char == \"}\":\n                if stack:\n                    stack.pop()\n            elif char == \",\":\n                if not stack:\n                    splits.append(args[start_index:i].strip())\n                    start_index = i + 1\n        splits.append(args[start_index:end_index].strip())\n        return splits\n\n    def parse_operands(args):\n        operands = []\n        args = split_args(args)\n        for arg in args:\n            if operand := parse_value(arg):\n                operands.append(operand)\n        return operands\n\n    trace_lines = traces.split(\"\\n\")\n    for trace_str in trace_lines:\n        # print(trace_str)\n\n        match = re.search(record_pattern, trace_str)\n        if not match:\n            continue\n        op_ids = match.group(\"op_ids\").split(\":\")\n        op_ids = [int(i) if i.isdigit() else i for i in op_ids]\n        results = match.group(\"results\")\n        operator = match.group(\"operator\")\n        args = match.group(\"args\")\n        operator = operator.split(\".\")\n        operands = parse_operands(args)\n        results = parse_operands(results)\n        record = OpTraceRecord(\n            process_id=op_ids[0],\n            rank_id=op_ids[1],\n            stream_id=op_ids[2],\n            domain_name=operator[0],\n            version_number=operator[1],\n            op_name=operator[2],\n            operands=operands,\n            results=results,\n        )\n        yield record",
    "start_line": 59,
    "end_line": 146,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "traces"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_op_trace",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_op_trace"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_value": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_value",
    "name": "parse_value",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.Scalar",
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.Tensor",
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_value",
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.split_args",
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.Struct"
    ],
    "source_code": "    def parse_value(arg):\n        if \"list{\" in arg:\n            args = split_args(arg[5:-1])\n            values = []\n            for v in args:\n                if value := parse_value(v):\n                    values.append(value)\n            return Struct(value_name=\"\", struct_name=\"list\", values=values)\n        if match := re.search(r\"(%\\d+):<(\\w+)>\\s*(\\{[\\d,]+\\})?\\s*(\\+\\d+)?\\s*\", arg):  # tensor\n            shape_n_dtype = match.group(2).split(\"x\")\n            stride = match.group(3)\n            stride = [int(i) for i in stride[1:-1].split(\",\")] if stride else []\n            offset = match.group(4)\n            offset = int(offset[1:]) if offset else 0\n\n            t = Tensor(\n                id=match.group(1),\n                dtype=shape_n_dtype[-1],\n                shape=[int(i) for i in shape_n_dtype[:-1]],\n                stride=stride,\n                offset=offset,\n            )\n            if len(t.shape) == 0:\n                t.shape = [1]\n            return t\n        if match := re.search(r\"([\\w.]+)\\s*=\\s*([\\w.-]+):([\\w.]+)\", arg):  # scalar with name\n            s = Scalar(value_name=match.group(1), value=match.group(2), dtype=match.group(3))\n            return s\n        if match := re.search(r\"([\\w.-]+):([\\w.]+)\", arg):  # scalar without name\n            s = Scalar(value_name=\"\", value=match.group(1), dtype=match.group(2))\n            return s\n        return None",
    "start_line": 62,
    "end_line": 93,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "arg"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_value",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_value"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.split_args": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.split_args",
    "name": "split_args",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [],
    "source_code": "    def split_args(args):\n        stack = []\n        splits = []\n        start_index = 0\n        end_index = len(args)\n        for i, char in enumerate(args):\n            if char == \"{\":\n                stack.append(i)\n            elif char == \"}\":\n                if stack:\n                    stack.pop()\n            elif char == \",\":\n                if not stack:\n                    splits.append(args[start_index:i].strip())\n                    start_index = i + 1\n        splits.append(args[start_index:end_index].strip())\n        return splits",
    "start_line": 95,
    "end_line": 111,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "args"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function split_args",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.split_args"
  },
  "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_operands": {
    "id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_operands",
    "name": "parse_operands",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "relative_path": "nova-platform/nova_platform/benchmark/op_trace_obsolete.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_value",
      "nova-platform.nova_platform.benchmark.op_trace_obsolete.split_args"
    ],
    "source_code": "    def parse_operands(args):\n        operands = []\n        args = split_args(args)\n        for arg in args:\n            if operand := parse_value(arg):\n                operands.append(operand)\n        return operands",
    "start_line": 113,
    "end_line": 119,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "args"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_operands",
    "component_id": "nova-platform.nova_platform.benchmark.op_trace_obsolete.parse_operands"
  },
  "nova-platform.nova_platform.benchmark.utils._iter_access_gen": {
    "id": "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
    "name": "_iter_access_gen",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/benchmark/utils.py",
    "relative_path": "nova-platform/nova_platform/benchmark/utils.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "def _iter_access_gen(mem_acc_list: List[DataflowActionMemoryAccess]):\n    counter = 0\n    batch: List[DataflowActionMemoryAccess] = []\n    fetch_size = yield batch\n    # history = [] # for debug\n    # history_raw = [] # for debug\n    while True:\n        for acc in mem_acc_list:\n            # history_raw.append(acc)\n            batch.append(acc)\n            counter += acc.size\n            while True:\n                last_acc = batch.pop()\n\n                if counter >= fetch_size:\n                    last_size_right = counter - fetch_size\n                    last_size_left = last_acc.size - last_size_right\n                    batch.append(\n                        DataflowActionMemoryAccess(base_addr=last_acc.base_addr, size=last_size_left, rw=last_acc.rw)\n                    )\n\n                    new_base = last_acc.base_addr + last_size_left\n                    new_size = last_acc.size - last_size_left\n                    counter = fetch_size\n                    # history.append(batch)\n                    fetch_size = yield batch\n                    if new_size > 0:\n                        batch = [DataflowActionMemoryAccess(base_addr=new_base, size=new_size, rw=last_acc.rw)]\n                        counter = new_size\n                    else:\n                        batch = []\n                        counter = 0\n                        break\n                else:\n                    batch.append(last_acc)\n                    break\n        if batch:\n            # history.append(batch)\n            yield batch",
    "start_line": 12,
    "end_line": 50,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "mem_acc_list"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_access_gen",
    "component_id": "nova-platform.nova_platform.benchmark.utils._iter_access_gen"
  },
  "nova-platform.nova_platform.config.AbstractCacheConfig": {
    "id": "nova-platform.nova_platform.config.AbstractCacheConfig",
    "name": "AbstractCacheConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class AbstractCacheConfig:\n    # Important: all below values are the value per cache bank\n\n    CACHE_LINE_SIZE: int = field(default=128)\n    CACHE_WAYS: int = field(default=None)\n    CACHE_SIZE: int = field(default=None)\n    MEM_LATENCY: int = field(default=0)\n    NON_MEM_LATENCY: int = field(default=0)\n    NUM_MSHR: int = field(default=INF)",
    "start_line": 10,
    "end_line": 18,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AbstractCacheConfig",
    "component_id": "nova-platform.nova_platform.config.AbstractCacheConfig"
  },
  "nova-platform.nova_platform.config.L1C_Config": {
    "id": "nova-platform.nova_platform.config.L1C_Config",
    "name": "L1C_Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [
      "nova-platform.nova_platform.config.AbstractCacheConfig"
    ],
    "source_code": "class L1C_Config(AbstractCacheConfig):\n    NUM_OF_CORE: int = field(default=None)\n    CACHE_SIZE_PER_CORE: int = field(default=None)",
    "start_line": 22,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractCacheConfig"
    ],
    "class_name": null,
    "display_name": "class L1C_Config",
    "component_id": "nova-platform.nova_platform.config.L1C_Config"
  },
  "nova-platform.nova_platform.config.LLC_Config": {
    "id": "nova-platform.nova_platform.config.LLC_Config",
    "name": "LLC_Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [
      "nova-platform.nova_platform.config.AbstractCacheConfig"
    ],
    "source_code": "class LLC_Config(AbstractCacheConfig):\n    NUM_OF_PARTITIONS: int = field(default=None)\n    NUM_OF_SLICES_PER_PARTITION: int = field(default=None)",
    "start_line": 28,
    "end_line": 30,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractCacheConfig"
    ],
    "class_name": null,
    "display_name": "class LLC_Config",
    "component_id": "nova-platform.nova_platform.config.LLC_Config"
  },
  "nova-platform.nova_platform.config.InstNumConfig": {
    "id": "nova-platform.nova_platform.config.InstNumConfig",
    "name": "InstNumConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class InstNumConfig:\n    NUM_OF_CLUSTER:           int\n    NUM_OF_CORE_PER_CLUSTER:  int\n    NUM_OF_DSM:               int\n    NUM_OF_DIE:               int = 1\n    NUM_OF_ESL_PER_DIE:       int = 1",
    "start_line": 34,
    "end_line": 39,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstNumConfig",
    "component_id": "nova-platform.nova_platform.config.InstNumConfig"
  },
  "nova-platform.nova_platform.config.FreqConfig": {
    "id": "nova-platform.nova_platform.config.FreqConfig",
    "name": "FreqConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class FreqConfig:\n    CORE:         float  # GHz\n    INTERCONNECT: float\n    NOC:          float\n    LLC:          float\n    MC:           float\n    L3:           float\n    ESL:          float = None\n\n    def get_freq(self, domain: 'FreqDomain'):\n        return getattr(self, domain.name)*1e9  # GHz => Hz",
    "start_line": 43,
    "end_line": 53,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class FreqConfig",
    "component_id": "nova-platform.nova_platform.config.FreqConfig"
  },
  "nova-platform.nova_platform.config.FreqDomain": {
    "id": "nova-platform.nova_platform.config.FreqDomain",
    "name": "FreqDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class FreqDomain(BaseEnum):\n    CORE = \"CORE\"\n    INTERCONNECT = \"INTERCONNECT\"\n    NOC = \"NOC\"\n    LLC = \"LLC\"\n    MC = \"MC\"\n    L3 = \"L3\"\n    ESL = \"ESL\"",
    "start_line": 56,
    "end_line": 63,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class FreqDomain",
    "component_id": "nova-platform.nova_platform.config.FreqDomain"
  },
  "nova-platform.nova_platform.config.TPUComputeConfig": {
    "id": "nova-platform.nova_platform.config.TPUComputeConfig",
    "name": "TPUComputeConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class TPUComputeConfig:\n    ARRAY_M: int = 64\n    ARRAY_N: int = 64\n    K_TILE: int = 128",
    "start_line": 67,
    "end_line": 70,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TPUComputeConfig",
    "component_id": "nova-platform.nova_platform.config.TPUComputeConfig"
  },
  "nova-platform.nova_platform.config.ComputeConfig": {
    "id": "nova-platform.nova_platform.config.ComputeConfig",
    "name": "ComputeConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class ComputeConfig:\n    thread_2d_throughput: Dict[DType, float]\n    thread_1d_throughput: Dict[DType, float]\n    thread_sfu_throughput: int = field(default=None)\n    compute_parallel_capability: int = 0\n    tpu: TPUComputeConfig = field(default=None)",
    "start_line": 74,
    "end_line": 79,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ComputeConfig",
    "component_id": "nova-platform.nova_platform.config.ComputeConfig"
  },
  "nova-platform.nova_platform.config.DTEConfig": {
    "id": "nova-platform.nova_platform.config.DTEConfig",
    "name": "DTEConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class DTEConfig:\n    THREAD_NUMBER:    int\n    BW_PER_DTE_THEAD: int  # Byte/cycle",
    "start_line": 83,
    "end_line": 85,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DTEConfig",
    "component_id": "nova-platform.nova_platform.config.DTEConfig"
  },
  "nova-platform.nova_platform.config.BossaNovaBaseConfig": {
    "id": "nova-platform.nova_platform.config.BossaNovaBaseConfig",
    "name": "BossaNovaBaseConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class BossaNovaBaseConfig:\n    def deserialize(o: any):\n        raise NotImplementedError()",
    "start_line": 88,
    "end_line": 90,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BossaNovaBaseConfig",
    "component_id": "nova-platform.nova_platform.config.BossaNovaBaseConfig"
  },
  "nova-platform.nova_platform.config.BWEle": {
    "id": "nova-platform.nova_platform.config.BWEle",
    "name": "BWEle",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [
      "nova-platform.nova_platform.config.BWEle"
    ],
    "source_code": "class BWEle(Deserializable):\n    pre_latency:        int = field(default=0)\n    post_latency:       int = field(default=0)\n    rw_ratio:         float = field(default=1)\n    bw:                 int = field(default=0)\n    bw_per_second:    float = field(default=0)\n\n    def get_bw(self, freq):\n        if self.bw:\n            return self.bw\n        else:\n            return self.bw_per_second/freq\n\n    def deserialize(o):\n        # example 1: [[ 32,0], 256, 1]\n        # example 2: [[355,0],   0, 1, !eval 8/2*10*2**40]\n        # [[pre_latency,post_latency], Byte/Cycle, rw_ratio, Byte/s]\n        # [[#1.1,#1.2],                #2,         #3,       #4]\n        # 1.1: pre-latency/cycle\n        # 1.2: post-latency/cycle\n        # 2:   参数2和参数4互斥，带宽Byte/Cycle\n        # 3:   rw_ratio=read bw/write bw\n        # 4:   可选，当设置#4时，#2必须为0，带宽Byte/s\n        if not isinstance(o, list):\n            raise RuntimeError(f\"bwele config is invalid {str(o)}\")\n        if len(o) == 3:\n            pre, post = o[0]\n            bw = o[1]\n            rw = o[2]\n            return BWEle(pre, post, rw, bw)\n        elif len(o) == 4:\n            if o[1] != 0:\n                raise RuntimeError(\n                    f\"bwele config is invalid. when set #4, #2 must equal to 0. {str(o)}\")\n            pre, post = o[0]\n            bw_per_second = o[3]\n            rw = o[2]\n            return BWEle(pre, post, rw, None, bw_per_second)\n\n        raise RuntimeError(f\"bwele config is invalid {str(o)}\")",
    "start_line": 94,
    "end_line": 133,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Deserializable"
    ],
    "class_name": null,
    "display_name": "class BWEle",
    "component_id": "nova-platform.nova_platform.config.BWEle"
  },
  "nova-platform.nova_platform.config.BWFile": {
    "id": "nova-platform.nova_platform.config.BWFile",
    "name": "BWFile",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class BWFile:\n    freq_domain:   FreqDomain = field(default=None)\n    otsd:                 int = field(default=256)\n    shared:             BWEle = field(default=None)\n    local:              BWEle = field(default=None)\n    sic_io:             BWEle = field(default=None)\n    noc:                BWEle = field(default=None)\n    llc:                BWEle = field(default=None)\n    llc_far:            BWEle = field(default=None)\n    mc:                 BWEle = field(default=None)\n    l3:                 BWEle = field(default=None)",
    "start_line": 137,
    "end_line": 147,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BWFile",
    "component_id": "nova-platform.nova_platform.config.BWFile"
  },
  "nova-platform.nova_platform.config.BWConfig": {
    "id": "nova-platform.nova_platform.config.BWConfig",
    "name": "BWConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class BWConfig:\n    xpu:        BWFile = field(default=None)\n    cdte:       BWFile = field(default=None)\n    l0:         BWFile = field(default=None)\n    local:      BWFile = field(default=None)\n    shared:     BWFile = field(default=None)\n    sic_io:     BWFile = field(default=None)\n    noc:        BWFile = field(default=None)\n    llc:        BWFile = field(default=None)\n    llc_far:    BWFile = field(default=None)\n    mc:         BWFile = field(default=None)\n    l3:         BWFile = field(default=None)\n    esl:        BWFile = field(default=None)",
    "start_line": 151,
    "end_line": 163,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BWConfig",
    "component_id": "nova-platform.nova_platform.config.BWConfig"
  },
  "nova-platform.nova_platform.config.MemoryCommonConfig": {
    "id": "nova-platform.nova_platform.config.MemoryCommonConfig",
    "name": "MemoryCommonConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryCommonConfig:\n    DSM_PRIVATE_SHARED_RATIO: float",
    "start_line": 167,
    "end_line": 168,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryCommonConfig",
    "component_id": "nova-platform.nova_platform.config.MemoryCommonConfig"
  },
  "nova-platform.nova_platform.config.MemoryL3Config": {
    "id": "nova-platform.nova_platform.config.MemoryL3Config",
    "name": "MemoryL3Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryL3Config:\n    TOTAL_SIZE: int",
    "start_line": 172,
    "end_line": 173,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryL3Config",
    "component_id": "nova-platform.nova_platform.config.MemoryL3Config"
  },
  "nova-platform.nova_platform.config.MemoryL1Config": {
    "id": "nova-platform.nova_platform.config.MemoryL1Config",
    "name": "MemoryL1Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryL1Config:\n    SIZE_PER_CORE: int",
    "start_line": 177,
    "end_line": 178,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryL1Config",
    "component_id": "nova-platform.nova_platform.config.MemoryL1Config"
  },
  "nova-platform.nova_platform.config.MemoryL2Config": {
    "id": "nova-platform.nova_platform.config.MemoryL2Config",
    "name": "MemoryL2Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryL2Config:\n    SIZE_PER_SIC: int",
    "start_line": 182,
    "end_line": 183,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryL2Config",
    "component_id": "nova-platform.nova_platform.config.MemoryL2Config"
  },
  "nova-platform.nova_platform.config.MemoryL0Config": {
    "id": "nova-platform.nova_platform.config.MemoryL0Config",
    "name": "MemoryL0Config",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryL0Config:\n    IV_SIZE: int\n    SMR_SIZE: int\n    OA_SIZE: int",
    "start_line": 187,
    "end_line": 190,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryL0Config",
    "component_id": "nova-platform.nova_platform.config.MemoryL0Config"
  },
  "nova-platform.nova_platform.config.MemoryConfig": {
    "id": "nova-platform.nova_platform.config.MemoryConfig",
    "name": "MemoryConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class MemoryConfig:\n    common: MemoryCommonConfig\n    l3: MemoryL3Config\n    l2: MemoryL2Config\n    l1: MemoryL1Config\n    l0: MemoryL0Config\n\n    llc: LLC_Config\n    l1c: L1C_Config = None",
    "start_line": 194,
    "end_line": 202,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryConfig",
    "component_id": "nova-platform.nova_platform.config.MemoryConfig"
  },
  "nova-platform.nova_platform.config.PowerSIPLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerSIPLibConfig",
    "name": "PowerSIPLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerSIPLibConfig:\n    voltage:                          float\n    voltage_scaling:                  float\n\n    idle_power:                       float\n    leakage_power:                    float\n    plc_power:                        float\n\n    xpu_l0_dsm_local_energy:          float\n    xpu_l0_dsm_shared_energy:         float\n    xpu_l0_l3_energy:                 float\n\n    compute_2d_mac_energy:            Dict[DType, float]\n    compute_1d_op_energy:             Dict[DType, float]\n    compute_msf_op_energy:            float",
    "start_line": 206,
    "end_line": 220,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerSIPLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerSIPLibConfig"
  },
  "nova-platform.nova_platform.config.PowerL1LibConfig": {
    "id": "nova-platform.nova_platform.config.PowerL1LibConfig",
    "name": "PowerL1LibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerL1LibConfig:\n    voltage:                          float\n    voltage_scaling:                  float\n    idle_power:                       float\n    leakage_power:                    float\n    sip_master_energy:                float\n    dte_master_energy:                float\n\n    xpu_dsm_local_l3_energy:          float\n    xpu_dsm_shared_l3_energy:         float\n    xpu_dsm_shared_dsm_shared_energy: float\n    xpu_l3_l3_energy:                 float\n\n    dte_dsm_local_l3_energy:          float\n    dte_dsm_shared_l3_energy:         float\n    dte_dsm_shared_dsm_shared_energy: float\n    dte_l3_l3_energy:                 float",
    "start_line": 224,
    "end_line": 240,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerL1LibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerL1LibConfig"
  },
  "nova-platform.nova_platform.config.PowerSOCLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerSOCLibConfig",
    "name": "PowerSOCLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerSOCLibConfig:\n    voltage:                     float\n    voltage_scaling:             float\n    mc_idle_power:               float\n    dataflow_idle_power:         float\n    esl_idle_power:              float\n    d2d_idle_power:              float\n    leakage_power:               float\n    other_power:                 float\n    cdte_master_energy:          float\n    llc_hit_data_energy:         float\n    llc_miss_data_energy:        float\n    sic_io:                      float",
    "start_line": 244,
    "end_line": 256,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerSOCLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerSOCLibConfig"
  },
  "nova-platform.nova_platform.config.PowerMemLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerMemLibConfig",
    "name": "PowerMemLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerMemLibConfig:\n    voltage_scaling:             float\n    leakage_power:               float\n    hbm_active_power:            float\n    hbm_idle_power:              float",
    "start_line": 260,
    "end_line": 264,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerMemLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerMemLibConfig"
  },
  "nova-platform.nova_platform.config.PowerD2DLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerD2DLibConfig",
    "name": "PowerD2DLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerD2DLibConfig:\n    leakage_power:               float\n    active_power:                float\n    idle_power:                  float",
    "start_line": 268,
    "end_line": 271,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerD2DLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerD2DLibConfig"
  },
  "nova-platform.nova_platform.config.PowerESLLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerESLLibConfig",
    "name": "PowerESLLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerESLLibConfig:\n    idle_power:                  float\n    leakage_power:               float",
    "start_line": 275,
    "end_line": 277,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerESLLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerESLLibConfig"
  },
  "nova-platform.nova_platform.config.PowerLibConfig": {
    "id": "nova-platform.nova_platform.config.PowerLibConfig",
    "name": "PowerLibConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class PowerLibConfig:\n    sip:                     PowerSIPLibConfig\n    l1:                      PowerL1LibConfig\n    soc:                     PowerSOCLibConfig\n    mem:                     PowerMemLibConfig\n    esl:                     PowerESLLibConfig\n    d2d:                     PowerD2DLibConfig\n    dtu_edc:                 float = 1000\n    board_efficiency:        float = 1\n    edc_filter_glitch:       float = 10e-9\n    edc_current_interval:    int = 10",
    "start_line": 281,
    "end_line": 291,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerLibConfig",
    "component_id": "nova-platform.nova_platform.config.PowerLibConfig"
  },
  "nova-platform.nova_platform.config.TOPO": {
    "id": "nova-platform.nova_platform.config.TOPO",
    "name": "TOPO",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class TOPO(BaseEnum):\n    STANDALONE = (\"STANDALONE\", 1)\n    FULLMESH8 = (\"FULLMESH8\", 8)\n    SUPERNODE4 = (\"SUPERNODE4\", 4)\n    SUPERNODE8 = (\"SUPERNODE8\", 8)\n    SUPERNODE16 = (\"SUPERNODE16\", 16)\n    SUPERNODE32 = (\"SUPERNODE32\", 32)",
    "start_line": 294,
    "end_line": 300,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class TOPO",
    "component_id": "nova-platform.nova_platform.config.TOPO"
  },
  "nova-platform.nova_platform.config.BossaNovaConfig": {
    "id": "nova-platform.nova_platform.config.BossaNovaConfig",
    "name": "BossaNovaConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/config.py",
    "relative_path": "nova-platform/nova_platform/config.py",
    "depends_on": [],
    "source_code": "class BossaNovaConfig:\n    name: str\n    arch_name: str\n    arch: str\n    inst_num: InstNumConfig\n    freq: FreqConfig\n    compute: ComputeConfig\n    dte: DTEConfig\n    # latency: LatencyConfig\n    bw: BWConfig\n    memory: MemoryConfig\n    power: PowerLibConfig\n    gather_mu: int = 1\n    gcu_id: int = 0\n\n    def __hash__(self):\n        return hash(self.name)",
    "start_line": 304,
    "end_line": 320,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BossaNovaConfig",
    "component_id": "nova-platform.nova_platform.config.BossaNovaConfig"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.__init__.RequestType": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.RequestType",
    "name": "RequestType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "depends_on": [],
    "source_code": "class RequestType(str, Enum):\n    L1C = \"L1C\"\n    LLC = \"LLC\"",
    "start_line": 9,
    "end_line": 11,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "Enum"
    ],
    "class_name": null,
    "display_name": "class RequestType",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.RequestType"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleRequest": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleRequest",
    "name": "SimpleRequest",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "depends_on": [],
    "source_code": "class SimpleRequest(Request):\n    request_type: RequestType | None = None\n    sip_id: int = 0",
    "start_line": 15,
    "end_line": 17,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Request"
    ],
    "class_name": null,
    "display_name": "class SimpleRequest",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleRequest"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleAccess": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleAccess",
    "name": "SimpleAccess",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "depends_on": [],
    "source_code": "class SimpleAccess(Access):\n    request_type: RequestType | None = None\n    sip_id: int = 0",
    "start_line": 21,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Access"
    ],
    "class_name": null,
    "display_name": "class SimpleAccess",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleAccess"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleStrategy": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleStrategy",
    "name": "SimpleStrategy",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/__init__.py",
    "depends_on": [
      "cache_model.entity.report.DistanceCount",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process",
      "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleRequest"
    ],
    "source_code": "class SimpleStrategy():\n    def __init__(self, hardware: HardwareConfig, _cls: AbstractGCU):\n        self.hardware = hardware\n        self.memory_device: AbstractGCU = _cls(self.get_hardware(hardware))\n        self.fake_time = 0\n        self.queue = []\n\n    def get_hardware(self, hardware: HardwareConfig):\n        return hardware\n\n    def process_request(self, request: SimpleRequest):\n        try:\n            self.memory_device.process(\n                request, self.fake_time)\n\n        except No_MSHR as e:\n            self.queue.append(request)\n\n    def process(\n        self,\n        access: SimpleAccess\n    ):\n        if access.width != 0:\n            start_line_addr = int(\n                access.address/self.hardware.CACHE_LINE_SIZE)\n            end_line_addr = int(\n                access.end_address/self.hardware.CACHE_LINE_SIZE)\n            for line_addr in range(start_line_addr, end_line_addr+1):\n                request = SimpleRequest(\n                    access.direction,\n                    int(access.address) +\n                    (line_addr-start_line_addr) *\n                    self.hardware.CACHE_LINE_SIZE,\n                    line_addr,\n                    thread=None,\n                    request_type=access.request_type,\n                    sip_id=access.sip_id,\n                )\n                if self.queue:\n                    self.queue.append(request)\n                else:\n                    self.process_request(request)\n\n    def reuse_distance(\n        self,\n        access: SimpleAccess\n    ):\n        if self.queue:\n            # TODO: unsafe for multi-threads\n            # check performance when hit MSHR limit\n            _queue = self.queue\n            self.queue = []\n            for i, _request in enumerate(_queue):\n                if not self.queue:\n                    self.process_request(_request)\n                else:\n                    self.queue.extend(_queue[i:])\n                    break\n\n        self.process(access)\n        self.memory_device.post_process(self.fake_time)\n        self.fake_time += 1\n\n    def finish_queue(self):\n        while self.queue:\n            if self.queue:\n                # TODO: unsafe for multi-threads\n                # check performance when hit MSHR limit\n                _queue = self.queue\n                self.queue = []\n                for i, _request in enumerate(_queue):\n                    if not self.queue:\n                        self.process_request(_request)\n                    else:\n                        self.queue.extend(_queue[i:])\n                        break\n            self.memory_device.post_process(self.fake_time)\n            self.fake_time += 1\n\n    def histogram(self) -> Mapping[str, List[DistanceCount]]:\n        # distance = self.read_distance+self.write_distance\n        distance_dict = self.memory_device.histogram_dict()\n        his = {}\n        for k, distance in distance_dict.items():\n            v = [DistanceCount(k, v)\n                 for k, v in sorted(distance.items(), reverse=True)]\n            his[k] = v\n\n        return his",
    "start_line": 26,
    "end_line": 114,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class SimpleStrategy",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleStrategy"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.eltanin.GCU_Eltanin": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.eltanin.GCU_Eltanin",
    "name": "GCU_Eltanin",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/eltanin.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/eltanin.py",
    "depends_on": [
      "cache_model.memory.memory_manger.L3Manager",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process",
      "cache_model.memory.memory_manger.LLCManager",
      "cache_model.memory.memory_manger.L1CManager",
      "cache_model.entity.model.ModelContext"
    ],
    "source_code": "class GCU_Eltanin(AbstractGCU):\n    def __init__(self, hardware: HardwareConfig) -> None:\n        super().__init__(hardware)\n        self.hardware = hardware\n        self.context = ModelContext(self.hardware)\n\n        def cache_selector(request: SimpleRequest, num_of_sets):\n            slice_id = request.sip_id\n            set_index = request.line_addr % num_of_sets\n            return slice_id, set_index\n\n        self.L3 = L3Manager(hardware.MEMORY.L3, self.context)\n        self.LLC = LLCManager(hardware.MEMORY.LLC, self.context,\n                              self.L3, lambda addr: (addr//128) % 96)\n        self.L1C = L1CManager(hardware.MEMORY.L1C,\n                              self.context, self.LLC, cache_selector)\n\n    def get_last_memory_manager(self) -> AbstractMemoryManager:\n        return self.LLC\n\n    def process(self, request: SimpleRequest, timestamp: int):\n        self.context.timestamp = timestamp\n        if request.request_type == RequestType.LLC:\n            self.LLC.process(request)\n        elif request.request_type == RequestType.L1C:\n            self.L1C.process(request)\n\n    def post_process(self, timestamp):\n        self.LLC.post_process(timestamp)\n        self.L1C.post_process(timestamp)\n\n    def stat_dict(self):\n        _stat_dict = {\n            \"L1C\": self.L1C.stat(),\n            \"LLC\": self.LLC.stat(),\n            \"L3\": self.L3.stat(),\n        }\n        return _stat_dict\n\n    def histogram_dict(self) -> Counter:\n        distance_dict = {\n            \"LLC\": self.LLC.histogram()\n        }\n        return distance_dict",
    "start_line": 11,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractGCU"
    ],
    "class_name": null,
    "display_name": "class GCU_Eltanin",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.eltanin.GCU_Eltanin"
  },
  "nova-platform.nova_platform.cost_service.cache.arch.libra.GCU_Libra": {
    "id": "nova-platform.nova_platform.cost_service.cache.arch.libra.GCU_Libra",
    "name": "GCU_Libra",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/arch/libra.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/arch/libra.py",
    "depends_on": [
      "cache_model.entity.model.ModelContext",
      "cache_model.memory.memory_manger.LLCManager",
      "cache_model.memory.memory_manger.L3Manager",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
    ],
    "source_code": "class GCU_Libra(AbstractGCU):\n    def __init__(self, hardware: HardwareConfig) -> None:\n        super().__init__(hardware)\n        self.hardware = hardware\n        self.context = ModelContext(self.hardware)\n\n        self.L3 = L3Manager(hardware.MEMORY.L3, self.context)\n        # TODO: addr_to_llc_index only support llc 8/16\n\n        CACHE_SETS = int(hardware.MEMORY.LLC.CACHE_SIZE /\n                         hardware.MEMORY.LLC.CACHE_LINE_SIZE/hardware.MEMORY.LLC.CACHE_WAYS)\n\n        def addr_convert(addr):\n            line_addr = addr//hardware.MEMORY.LLC.CACHE_LINE_SIZE\n            llc_index = (line_addr//CACHE_SETS) % (hardware.MEMORY.LLC.NUM_OF_PARTITIONS *\n                                                   hardware.MEMORY.LLC.NUM_OF_SLICES_PER_PARTITION)\n            return llc_index\n        self.LLC = LLCManager(hardware.MEMORY.LLC, self.context,\n                              self.L3, addr_convert)\n\n    def get_last_memory_manager(self) -> AbstractMemoryManager:\n        return self.LLC\n\n    def process(self, request: SimpleRequest, timestamp: int):\n        self.context.timestamp = timestamp\n        if request.request_type == RequestType.LLC:\n            self.LLC.process(request)\n        else:\n            raise Exception(\n                \"request type %s is not supported in Libra\", request.request_type)\n\n    def post_process(self, timestamp):\n        self.LLC.post_process(timestamp)\n\n    def stat_dict(self):\n        _stat_dict = {\n            \"LLC\": self.LLC.stat(),\n            \"L3\": self.L3.stat(),\n        }\n        return _stat_dict\n\n    def histogram_dict(self) -> Counter:\n        distance_dict = {\n            \"LLC\": self.LLC.histogram()\n        }\n        return distance_dict",
    "start_line": 12,
    "end_line": 57,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractGCU"
    ],
    "class_name": null,
    "display_name": "class GCU_Libra",
    "component_id": "nova-platform.nova_platform.cost_service.cache.arch.libra.GCU_Libra"
  },
  "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat": {
    "id": "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat",
    "name": "CacheStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/base_cache_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/base_cache_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat"
    ],
    "source_code": "class CacheStat:\n    write_count: int = 0\n    write_hit_count: int = 0\n    read_count: int = 0\n    read_hit_count: int = 0\n\n    @property\n    def write_hit_rate(self):\n        return self.write_hit_count/self.write_count if self.write_count > 0 else 0\n\n    @property\n    def read_hit_rate(self):\n        return self.read_hit_count/self.read_count if self.read_count > 0 else 0\n\n    def __sub__(self, b: 'CacheStat'):\n        diff_write_count = self.write_count - b.write_count\n        diff_write_hit_count = self.write_hit_count - b.write_hit_count\n\n        diff_read_count = self.read_count - b.read_count\n        diff_read_hit_count = self.read_hit_count - b.read_hit_count\n\n        return CacheStat(\n            write_count=diff_write_count,\n            write_hit_count=diff_write_hit_count,\n            read_count=diff_read_count,\n            read_hit_count=diff_read_hit_count\n        )\n\n    def __add__(self, b: 'CacheStat'):\n        diff_write_count = self.write_count + b.write_count\n        diff_write_hit_count = self.write_hit_count + b.write_hit_count\n\n        diff_read_count = self.read_count + b.read_count\n        diff_read_hit_count = self.read_hit_count + b.read_hit_count\n\n        return CacheStat(\n            write_count=diff_write_count,\n            write_hit_count=diff_write_hit_count,\n            read_count=diff_read_count,\n            read_hit_count=diff_read_hit_count\n        )",
    "start_line": 16,
    "end_line": 56,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CacheStat",
    "component_id": "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat"
  },
  "nova-platform.nova_platform.cost_service.cache.base_cache_model.BaseCacheCostService": {
    "id": "nova-platform.nova_platform.cost_service.cache.base_cache_model.BaseCacheCostService",
    "name": "BaseCacheCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/base_cache_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/base_cache_model.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class BaseCacheCostService(BaseCostService):\n    def __init__(self, config: BossaNovaConfig):\n        super().__init__(config)\n\n    def get_raw_stat_dict(self) -> Dict[str, Counter]:\n        raise NotImplementedError()\n\n    def process(self, action: DataflowAction, context: BossaNovaContext, ref: float) -> Generator[None, DataflowActionMemoryStat | None, None]:\n        action_stat = defaultdict(CacheStat)\n\n        while True:\n            mem_stat = yield\n            if not mem_stat:\n                break\n\n            memory_access_list: List[DataflowActionMemoryAccess] = mem_stat.memory_access_list\n            start_stat = self.get_cache_stat_dict()\n            self._process_access(action, memory_access_list)\n            end_stat = self.get_cache_stat_dict()\n            stat = {}\n            for k, v in end_stat.items():\n                stat[k] = v-start_stat[k]\n\n            for k, v in stat.items():\n                action_stat[k] += v\n\n            mem_stat.cache_stat = stat\n\n            # log the mem access\n            # logger.info(\"memory access list: %s\", memory_access_list)\n            # for access in memory_access_list:\n            #     logging.info(\n            #         \"cache service - action: %s, memory_access: base_addr=%s, size=%s, rw=%s\",\n            #         action.get_action_id(),\n            #         access.base_addr,\n            #         access.size,\n            #         access.rw\n            #     )\n\n            # # 打印 llc_stat 的详细信息\n            # llc_stat = self.get_cache_stat_dict()[\"LLC\"]\n            # logging.info(\n            #     \"llc_stat: write_count=%s, write_hit_rate=%.2f, read_count=%s, read_hit_rate=%.2f, \\n\\n\",\n            #     llc_stat.write_count,\n            #     llc_stat.write_hit_rate,\n            #     llc_stat.read_count,\n            #     llc_stat.read_hit_rate\n            # )\n        cost_book = context.get_cost_book(action)\n\n        # action_stat = {}\n        # for k, v in last_stat.items():\n        #     action_stat[k] = v-action_begin_stat[k]\n\n        cost_book.cache_stat_dict = dict(action_stat)\n        logger.debug(\"cache service - action: %s, stat: %s\",\n                     action.get_action_id(), cost_book.cache_stat_dict)\n\n    def process_old(self, action: DataflowAction, context: BossaNovaContext, ref: float) -> Generator[bool, DataflowActionMemoryStat | None, None]:\n        action_begin_stat = self.get_raw_stat_dict()\n        last_stat = action_begin_stat\n        access_gen = action.get_memory_access()\n        memory_access_queue: List[DataflowActionMemoryAccess] = []\n        while True:\n            local_counter = reduce(\n                lambda acc, curr: acc+curr.size, memory_access_queue, 0)\n            stat: DataflowActionMemoryStat | None = yield self.get_cache_stat_dict(last_stat)\n            if not stat:\n                break\n            while True:\n                if local_counter >= stat.total_count:\n                    break\n                else:\n                    access = next(access_gen)\n                    local_counter += access.size\n                    memory_access_queue.append(access)\n\n            if local_counter > stat.total_count:\n                # split last access to match stat total count\n                last_access = memory_access_queue[-1]\n                # assume stat.total_count =10\n                # access size: 5, 3, 6   local_counter: 14\n                # last_size: 6 - (14-10) = 2\n                # access size: 5, 3, 2 >>> sum=10 match stat.total_counter\n                new_size = last_access.size - \\\n                    (local_counter-stat.total_count)\n                curr_queue = memory_access_queue[:-1]\n\n                curr_queue.append(\n                    DataflowActionMemoryAccess(\n                        last_access.base_addr, new_size, last_access.rw)\n                )\n\n                next_base_addr = last_access.base_addr+new_size\n                next_size = last_access.size-new_size\n                memory_access_queue = [DataflowActionMemoryAccess(\n                    next_base_addr, next_size, last_access.rw)]\n            else:\n                curr_queue = memory_access_queue\n                memory_access_queue = []\n            self._process_access(action, curr_queue)\n            curr_stat = self.get_raw_stat_dict()\n            stat = {}\n            for k, v in curr_stat.items():\n                stat[k] = v-last_stat[k]\n\n            last_stat = curr_stat\n\n        cost_book = context.cost_dict[action.get_action_id()]\n\n        action_stat = {}\n        for k, v in last_stat.items():\n            action_stat[k] = v-action_begin_stat[k]\n\n        cost_book.cache_stat_dict[k] = self.get_cache_stat_dict(action_stat)\n        logger.debug(\"cache service - action: %s, stat: %s\",\n                     action.get_action_id(), cost_book.cache_stat_dict[k])\n        yield  # using yield instead of return to avoid from StopIterator exception\n\n    def _process_access(self, action: DataflowAction, access_list: List[DataflowActionMemoryAccess]):\n        raise NotImplementedError()\n\n    def get_cache_stat_dict(self) -> Dict[str, CacheStat]:\n        raise NotImplementedError()",
    "start_line": 59,
    "end_line": 182,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCostService"
    ],
    "class_name": null,
    "display_name": "class BaseCacheCostService",
    "component_id": "nova-platform.nova_platform.cost_service.cache.base_cache_model.BaseCacheCostService"
  },
  "nova-platform.nova_platform.cost_service.cache.cache_cost_service.CacheCostService": {
    "id": "nova-platform.nova_platform.cost_service.cache.cache_cost_service.CacheCostService",
    "name": "CacheCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/cache_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/cache_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.config.LLC_Config",
      "cache_model.entity.model.HardwareConfig",
      "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleStrategy",
      "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat",
      "cache_model.entity.model.Memory",
      "nova-platform.nova_platform.config.L1C_Config",
      "cache_model.entity.model.L3_Config",
      "nova-platform.nova_platform.cost_service.cache.arch.__init__.SimpleAccess"
    ],
    "source_code": "    class CacheCostService(BaseCacheCostService):\n        def __init__(self, config: BossaNovaConfig):\n            super().__init__(config)\n            self._has_l1c = bool(getattr(config.memory, \"l1c\", None))\n            self._stat_template = {\"LLC\": CacheStat(), \"L3\": CacheStat()}\n            if self._has_l1c:\n                self._stat_template[\"L1C\"] = CacheStat()\n            logger.info(\"Using no-op CacheCostService; cache stats remain zero.\")\n\n        def __deepcopy__(self, memo):\n            return None\n\n        def _process_access(self, action: DataflowAction, access_list: List[DataflowActionMemoryAccess]):\n            # cache_model unavailable: skip access simulation\n            return\n\n        def get_cache_stat_dict(self):\n            return {level: CacheStat() for level in self._stat_template}\n\n        def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n            report = {}\n            for level in self._stat_template:\n                report[level] = {\n                    \"read_count\": 0,\n                    \"read_hit_rate\": 0,\n                    \"write_count\": 0,\n                    \"write_hit_rate\": 0,\n                }\n            return report",
    "start_line": 157,
    "end_line": 185,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCacheCostService"
    ],
    "class_name": null,
    "display_name": "class CacheCostService",
    "component_id": "nova-platform.nova_platform.cost_service.cache.cache_cost_service.CacheCostService"
  },
  "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheCostService": {
    "id": "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheCostService",
    "name": "ParallelCacheCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/parallel_cache_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/parallel_cache_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.config.AbstractCacheConfig",
      "nova-platform.nova_platform.cost_service.cache.base_cache_model.CacheStat",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process",
      "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheImpl"
    ],
    "source_code": "class ParallelCacheCostService(BaseCacheCostService):\n    def __init__(self, config: BossaNovaConfig, device_id=0):\n        super().__init__(config)\n        os.environ['CUDA_DEVICE'] = str(device_id)\n        import pycuda.autoinit\n\n        # 编译并加载CUDA模块\n        mod = SourceModule(\n            Path(\"nova_platform/cost_service/cache/cache_kernal.cu\").read_text())\n        # 获取内核函数\n        global cache_kernel, addr_convert_kernel\n        cache_kernel = mod.get_function(\"cache_kernel\")\n        addr_convert_kernel = mod.get_function(\"addr_convert_kernel\")\n\n        # TODO: pass cache directly, may need refactor\n        if config.arch_name != \"libra\":\n            raise Exception(\"arch %s is not supported\", config.arch_name)\n        self.llc_config = self.config.memory.llc\n\n        cfg = asdict(self.config.memory.llc)\n        NUM_OF_PARTITIONS = cfg.pop(\"NUM_OF_PARTITIONS\")\n        NUM_OF_SLICES_PER_PARTITION = cfg.pop(\"NUM_OF_SLICES_PER_PARTITION\")\n        cfg = AbstractCacheConfig(**cfg)\n        cfg.CACHE_SIZE = self.llc_config.CACHE_SIZE * \\\n            NUM_OF_PARTITIONS * \\\n            NUM_OF_SLICES_PER_PARTITION * \\\n            config.inst_num.NUM_OF_DIE\n        self.llc_cache = ParallelCacheImpl(cfg)\n        self.raw_stat_dict = {\n            \"r_hit\": 0,\n            \"w_hit\": 0,\n            \"r_miss\": 0,\n            \"w_miss\": 0\n        }\n    def __deepcopy__(self, memo):\n        # deepcopy逻辑，None表示跳过\n        return None\n    \n    def _process_access(self, action: DataflowAction, access_list: List[DataflowActionMemoryAccess]):\n        # def process(self, action: DataflowAction, context: BossaNovaContext, ref: float) -> Generator[bool, None, None]:\n        base_addr_list = []\n        size_list = []\n        rw_list = []\n        total_size = 0\n        CACHE_LINE_SIZE = self.llc_config.CACHE_LINE_SIZE\n        for access in access_list:\n            base_addr = int(access.base_addr)\n            # TODO: need refactor\n            if base_addr < 5*2**40:\n                continue\n            base_addr_list.append(base_addr)\n            line_addr_start = base_addr//CACHE_LINE_SIZE\n            line_addr_end = int(base_addr+access.size-1)//CACHE_LINE_SIZE\n            line_size = line_addr_end-line_addr_start+1\n            size_list.append(line_size)\n            rw_list.append(0 if access.rw == 'r' else 1)\n            total_size += line_size\n\n        if base_addr_list:\n            r_hit, w_hit, r_miss, w_miss = self.llc_cache.process(\n                base_addr_list, size_list, rw_list, total_size)\n            self.raw_stat_dict = {\n                \"r_hit\": self.raw_stat_dict['r_hit'] + r_hit,\n                \"w_hit\": self.raw_stat_dict['w_hit'] + w_hit,\n                \"r_miss\": self.raw_stat_dict['r_miss'] + r_miss,\n                \"w_miss\": self.raw_stat_dict['w_miss'] + w_miss\n            }\n\n    def get_cache_stat_dict(self) -> Dict[str, CacheStat]:\n        stat_dict = {}\n        r_hit = self.raw_stat_dict[\"r_hit\"]\n        w_hit = self.raw_stat_dict[\"w_hit\"]\n        r_miss = self.raw_stat_dict[\"r_miss\"]\n        w_miss = self.raw_stat_dict[\"w_miss\"]\n        stat = CacheStat(\n            w_hit+w_miss,\n            w_hit,\n            r_hit+r_miss,\n            r_hit,\n        )\n        stat_dict[\"LLC\"] = stat\n        return stat_dict\n\n    def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n        stat = self.llc_cache.stat_dict()\n\n        return {\n            \"LLC\": stat\n        }",
    "start_line": 24,
    "end_line": 112,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCacheCostService"
    ],
    "class_name": null,
    "display_name": "class ParallelCacheCostService",
    "component_id": "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheCostService"
  },
  "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheImpl": {
    "id": "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheImpl",
    "name": "ParallelCacheImpl",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/cache/parallel_cache_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/cache/parallel_cache_cost_service.py",
    "depends_on": [],
    "source_code": "class ParallelCacheImpl():\n    def __init__(self, cache_config: AbstractCacheConfig):\n        self.cache_line_size = cache_config.CACHE_LINE_SIZE\n        self.cache_ways = cache_config.CACHE_WAYS\n        assert cache_config.CACHE_SIZE % self.cache_line_size == 0 and cache_config.CACHE_SIZE % self.cache_line_size % self.cache_ways == 0\n\n        self.cache_sets = cache_config.CACHE_SIZE//self.cache_ways//self.cache_line_size\n\n        self.cache_cell = np.zeros(\n            self.cache_sets*self.cache_ways, dtype=np.int64)\n        self.cache_cell[:] = -1\n        self.cache_cell_offset = np.zeros(self.cache_sets, dtype=np.int64)\n        self.cache_cell_offset[:] = -1\n        self.read_hits_per_set = np.zeros(self.cache_sets, dtype=np.int32)\n        self.write_hits_per_set = np.zeros(self.cache_sets, dtype=np.int32)\n        self.read_misses_per_set = np.zeros(self.cache_sets, dtype=np.int32)\n        self.write_misses_per_set = np.zeros(self.cache_sets, dtype=np.int32)\n        # allocate gpu memory\n        self.cache_cell_gpu = cuda.mem_alloc(self.cache_cell.nbytes)\n        self.cache_cell_offset_gpu = cuda.mem_alloc(\n            self.cache_cell_offset.nbytes)\n        self.read_hits_per_set_gpu = cuda.mem_alloc(\n            self.read_hits_per_set.nbytes)\n        self.write_hits_per_set_gpu = cuda.mem_alloc(\n            self.write_hits_per_set.nbytes)\n        self.read_misses_per_set_gpu = cuda.mem_alloc(\n            self.read_misses_per_set.nbytes)\n        self.write_misses_per_set_gpu = cuda.mem_alloc(\n            self.write_misses_per_set.nbytes)\n        # h2d\n        cuda.memcpy_htod(self.cache_cell_gpu, self.cache_cell)\n        cuda.memcpy_htod(self.cache_cell_offset_gpu, self.cache_cell_offset)\n        cuda.memcpy_htod(self.read_hits_per_set_gpu, self.read_hits_per_set)\n        cuda.memcpy_htod(self.write_hits_per_set_gpu, self.write_hits_per_set)\n        cuda.memcpy_htod(self.read_misses_per_set_gpu,\n                         self.read_misses_per_set)\n        cuda.memcpy_htod(self.write_misses_per_set_gpu,\n                         self.write_misses_per_set)\n\n    def process(self, base_addr_list, size_list, rw_list, total_size):\n        # step 1: compute line_addr, set_idx, rw\n        list_size = len(base_addr_list)\n        base_addr_list = np.array(base_addr_list, dtype=np.int64)\n        size_list = np.array(size_list, dtype=np.int32)\n        rw_list = np.array(rw_list, dtype=np.int32)\n\n        output_set_idx_list = np.zeros(total_size, dtype=np.int32)\n        output_line_addr_list = np.zeros(total_size, dtype=np.int64)\n        output_rw_list = np.zeros(total_size, dtype=np.int32)\n\n        chunk_size = 1024\n\n        set_num = self.cache_sets\n\n        base_addr_list_gpu = cuda.mem_alloc(base_addr_list.nbytes)\n        size_list_gpu = cuda.mem_alloc(size_list.nbytes)\n        rw_list_gpu = cuda.mem_alloc(rw_list.nbytes)\n\n        output_set_idx_list_gpu = cuda.mem_alloc(output_set_idx_list.nbytes)\n        output_line_addr_list_gpu = cuda.mem_alloc(\n            output_line_addr_list.nbytes)\n        output_rw_list_gpu = cuda.mem_alloc(output_rw_list.nbytes)\n\n        cuda.memcpy_htod(base_addr_list_gpu, base_addr_list)\n        cuda.memcpy_htod(size_list_gpu, size_list)\n        cuda.memcpy_htod(rw_list_gpu, rw_list)\n        block_size = 256\n        grid_size = ((total_size+chunk_size-1)//chunk_size +\n                     block_size - 1) // block_size\n        global addr_convert_kernel, cache_kernel\n        addr_convert_kernel(\n            base_addr_list_gpu,\n            size_list_gpu,\n            rw_list_gpu,\n            np.int32(list_size),\n            output_set_idx_list_gpu,\n            output_line_addr_list_gpu,\n            output_rw_list_gpu,\n            np.int32(total_size),\n            np.int32(chunk_size),\n            np.int32(set_num),\n            block=(block_size, 1, 1),\n            grid=(grid_size, 1)\n        )\n\n        cuda.memcpy_dtoh(output_set_idx_list, output_set_idx_list_gpu)\n        cuda.memcpy_dtoh(output_line_addr_list, output_line_addr_list_gpu)\n        cuda.memcpy_dtoh(output_rw_list, output_rw_list_gpu)\n\n        base_addr_list_gpu.free()\n        size_list_gpu.free()\n        rw_list_gpu.free()\n\n        # step2: compute hit rate\n        block_size = 256\n        grid_size = (self.cache_sets + block_size - 1) // block_size\n\n        access_hit_flag = np.zeros(total_size, dtype=np.int32)\n\n        # 分配GPU内存\n        access_hit_flag_gpu = cuda.mem_alloc(access_hit_flag.nbytes)\n        cuda.memcpy_htod(access_hit_flag_gpu, access_hit_flag)\n\n        cache_kernel(\n            output_set_idx_list_gpu,\n            output_line_addr_list_gpu,\n            output_rw_list_gpu,\n            np.int32(total_size),\n            access_hit_flag_gpu,\n            self.cache_cell_gpu,\n            self.cache_cell_offset_gpu,\n            self.read_hits_per_set_gpu,\n            self.write_hits_per_set_gpu,\n            self.read_misses_per_set_gpu,\n            self.write_misses_per_set_gpu,\n            np.int32(set_num),\n            np.int32(self.cache_ways),\n            block=(block_size, 1, 1),\n            grid=(grid_size, 1)\n        )\n\n        cuda.memcpy_dtoh(access_hit_flag, access_hit_flag_gpu)\n\n        unique, counts = np.unique(access_hit_flag, return_counts=True)\n        stat = dict(zip(unique, counts))\n        access_hit_flag_gpu.free()\n\n        output_set_idx_list_gpu.free()\n        output_line_addr_list_gpu.free()\n        output_rw_list_gpu.free()\n\n        r_hit = stat.get(1, 0)\n        w_hit = stat.get(-1, 0)\n        r_miss = stat.get(2, 0)\n        w_miss = stat.get(-2, 0)\n\n        return int(r_hit), int(w_hit), int(r_miss), int(w_miss)\n\n    def stat_dict(self):\n        cuda.memcpy_dtoh(self.read_hits_per_set, self.read_hits_per_set_gpu)\n        cuda.memcpy_dtoh(self.write_hits_per_set, self.write_hits_per_set_gpu)\n        cuda.memcpy_dtoh(self.read_misses_per_set,\n                         self.read_misses_per_set_gpu)\n        cuda.memcpy_dtoh(self.write_misses_per_set,\n                         self.write_misses_per_set_gpu)\n        read_hits = np.sum(self.read_hits_per_set)\n        write_hits = np.sum(self.write_hits_per_set)\n        read_misses = np.sum(self.read_misses_per_set)\n        write_misses = np.sum(self.write_misses_per_set)\n        return {\n            \"read_hits\": int(read_hits),\n            \"read_misses\": int(read_misses),\n            \"write_hits\": int(write_hits),\n            \"write_misses\": int(write_misses),\n            \"read_hit_rate\": float(read_hits/(read_hits+read_misses)) if read_hits+read_misses else 0,\n            \"write_hit_rate\": float(write_hits/(write_hits+write_misses)) if write_hits+write_misses else 0,\n        }",
    "start_line": 115,
    "end_line": 271,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ParallelCacheImpl",
    "component_id": "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheImpl"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWMode": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWMode",
    "name": "BWMode",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BWMode(str, BaseEnum):\n    PORT = \"port\"\n    BANDWIDTH = \"bandwidth\"",
    "start_line": 19,
    "end_line": 21,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "str",
      "BaseEnum"
    ],
    "class_name": null,
    "display_name": "class BWMode",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWMode"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat",
    "name": "BaseCoreStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BaseCoreStat(BaseActionStat):\n    ld_l3_l3: int = 0\n    st_l3_l3: int = 0\n    tensor_macs: Dict[DType, int] = field(\n        default_factory=dict)\n    vector_ops: Dict[DType, int] = field(\n        default_factory=dict)\n    sfu_ops: int = 0\n    scalar_cycle: int = 0\n    ld_l0_local:  int = 0\n    st_l0_local:  int = 0\n    ld_l0_shared:  int = 0\n    st_l0_shared:  int = 0\n    ld_l0_l3: int = 0\n    st_l0_l3: int = 0\n    ld_local_l3: int = 0\n    st_local_l3: int = 0\n    ld_shared_l3: int = 0\n    st_shared_l3: int = 0\n    st_l3_l3_remote: int = 0\n    st_esl_l3: int = 0\n\n    def __iadd__(self, stat: DataflowActionComputeStat | DataflowActionMemoryStat):\n        if isinstance(stat, DataflowActionComputeStat):\n            for dt, v in stat.compute_1d_ops.items():\n                if dt not in self.vector_ops:\n                    self.vector_ops[dt] = 0.0\n                self.vector_ops[dt] += v\n            for dt, v in stat.compute_2d_ops.items():\n                if dt not in self.tensor_macs:\n                    self.tensor_macs[dt] = 0.0\n                self.tensor_macs[dt] += v/2\n            self.sfu_ops += stat.compute_msf_ops\n            self.scalar_cycle += stat.compute_scalar_cycle\n        elif isinstance(stat, DataflowActionMemoryStat):\n            src, dst = stat.src.name.lower(), stat.dst.name.lower()\n            if stat.rw == 'r':\n                tar = f\"ld_{src}_{dst}\"\n            else:\n                tar = f\"st_{src}_{dst}\"\n\n            v = getattr(self, tar)\n            setattr(self, tar, v+stat.total_count)\n        elif isinstance(stat, BaseCoreStat):\n            for f in fields(BaseCoreStat):\n                if f.type in (int, float):\n                    v = getattr(self, f.name)+getattr(stat, f.name)\n                    setattr(self, f.name, v)\n                elif isinstance(f.type, _GenericAlias) and f.type.__origin__ is dict:\n                    lhs_dict = getattr(self, f.name)\n                    rhs_dict = getattr(stat, f.name)\n                    for k, v in rhs_dict.items():\n                        if k not in lhs_dict:\n                            lhs_dict[k] = 0\n                        lhs_dict[k] += rhs_dict[k]\n                elif f.name in ('power_stat', 'name'):\n                    pass\n                else:\n                    raise NotImplemented\n        else:\n            raise NotImplemented\n\n        return self\n\n    def to_dict(self):\n        res = asdict(self)\n        res[\"tensor_macs\"] = sum([v for v in self.tensor_macs.values()])\n        res[\"vector_ops\"] = sum([v for v in self.vector_ops.values()])\n        return res",
    "start_line": 25,
    "end_line": 93,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseActionStat"
    ],
    "class_name": null,
    "display_name": "class BaseCoreStat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWCost": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWCost",
    "name": "BWCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BWCost:\n    start: float\n    end: float\n    action_id: int",
    "start_line": 97,
    "end_line": 100,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BWCost",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWCost"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWFrame": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWFrame",
    "name": "BWFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BWFrame(BaseFrame):\n    allocated_bw: int = field(default=0)  # Byte/cycle",
    "start_line": 104,
    "end_line": 105,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class BWFrame",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWFrame"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline",
    "name": "Timeline",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class Timeline(BaseDataclass, Generic[IFrame]):\n    data: List[IFrame] = field(default_factory=list)\n\n    def get_frame(self, ref) -> Tuple[int, IFrame]:\n        assert 0 <= ref < MAX_TIME, \"ref time out of range [0,%d)\" % (MAX_TIME)\n        data_len = len(self.data)\n        for i in reversed(range(data_len)):\n            if self.data[i].begin <= ref:\n                # ..., Frame left, ref, Frame right, ...\n                return i, self.data[i]\n\n    def insert(self, new_frame: IFrame):\n        s_idx, s_frame = self.get_frame(new_frame.begin)\n        # frame_cls: IFrame = new_frame.__class__\n        if s_frame.begin < new_frame.begin:\n            # split s_frame\n            # rhs_frame = frame_cls(**s_frame.__dict__)\n            rhs_frame = s_frame.clone()\n            rhs_frame.begin = new_frame.begin\n            self.data.insert(s_idx+1, rhs_frame)\n            s_frame.end = new_frame.begin\n\n            s_idx += 1\n            s_frame = rhs_frame\n\n        # now s_frame.begin==new_frame.begin\n\n        if s_frame.end > new_frame.end:\n            # split s_frame\n            # rhs_frame = frame_cls(**s_frame.__dict__)\n            rhs_frame = s_frame.clone()\n            rhs_frame.begin = new_frame.end\n            self.data.insert(s_idx+1, rhs_frame)\n            s_frame.end = new_frame.end\n            s_frame.incr(new_frame)\n        elif s_frame.end == new_frame.end:\n            s_frame.incr(new_frame)\n        else:  # s_frame.end < new_frame.end\n            # s_frame.incr(new_frame)\n            # next_frame: IFrame = new_frame.clone()\n            # next_frame.begin = s_frame.end\n            # self.insert(next_frame)\n\n            while s_frame.end < new_frame.end:\n                s_frame.incr(new_frame)\n                s_idx += 1\n                s_frame = self.data[s_idx]\n\n            next_frame: IFrame = new_frame.clone()\n            next_frame.begin = s_frame.begin\n            self.insert(next_frame)",
    "start_line": 112,
    "end_line": 162,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class Timeline",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource",
    "name": "BWResource",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWFrame",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline"
    ],
    "source_code": "class BWResource(BaseDataclass):\n    name: str\n    mode: BWMode\n    max_bw: int\n    timeline: Timeline[BWFrame] = field(\n        default_factory=lambda: Timeline([BWFrame(0, MAX_TIME, 0)]))\n    acc_read: int = 0\n    acc_write: int = 0\n    _lock: threading.Lock = field(default_factory=threading.Lock)\n\n    def get_frame(self, ref):\n        assert 0 <= ref < MAX_TIME, \"ref time out of range [0,%d)\" % (MAX_TIME)\n        data_len = len(self.timeline.data)\n        for i in reversed(range(data_len)):\n            if self.timeline.data[i].begin <= ref:\n                # ..., Frame left, ref, Frame right, ...\n\n                if self.timeline.data[i].allocated_bw == self.max_bw:\n                    # no available bw in this frame\n                    start_i = i\n                    end_i = data_len - 1\n                    for j in range(i+1, data_len):\n                        if self.timeline.data[j].allocated_bw < self.max_bw:\n                            end_i = j\n                            break\n                    # compact timeline to avoid too deep recusion\n                    self.timeline.data[i].end = self.timeline.data[end_i-1].end\n                    del self.timeline.data[start_i+1:end_i]\n                    # i = i+1  # use next allocatable frame\n\n                return i, self.timeline.data[i]\n\n    def get_last_valid_frame(self):\n        frame_list = self.timeline.data\n        if len(frame_list) == 0:\n            return None\n        elif frame_list[-1].end != MAX_TIME:\n            return frame_list[-1]\n        elif len(frame_list) > 1:\n            return frame_list[-2]\n        else:\n            return None",
    "start_line": 166,
    "end_line": 207,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class BWResource",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResourceContext": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResourceContext",
    "name": "BWResourceContext",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource"
    ],
    "source_code": "class BWResourceContext:\n    config: BossaNovaConfig\n    esl_switch: BaseESLSwitch\n    sic_io_dict: Dict = field(default_factory=dict)\n    l3_dict: Dict = field(default_factory=dict)\n    d2d_dict: Dict = field(default_factory=dict)\n    esl_dict: Dict = field(default_factory=dict)\n\n    def __post_init__(self):\n        # for libra-like arch\n        # TODO: extend when introducing new arch\n\n        for die_id in range(self.config.inst_num.NUM_OF_DIE):\n            # sic io\n            for cluster_id in range(self.config.inst_num.NUM_OF_CLUSTER):\n                sic_io_config = self.config.bw.sic_io\n                sic_io_r = BWResource(\n                    f\"die{die_id}_sic_io{cluster_id}_r\", BWMode.PORT, sic_io_config.noc.bw)\n                sic_io_w = BWResource(\n                    f\"die{die_id}_sic_io{cluster_id}_w\", BWMode.PORT, sic_io_config.noc.bw/sic_io_config.noc.rw_ratio)\n                self.sic_io_dict[(die_id, cluster_id, 'r')] = sic_io_r\n                self.sic_io_dict[(die_id, cluster_id, 'w')] = sic_io_w\n\n            # l3\n            l3_rw = BWResource(f\"die{die_id}_l3\",\n                               BWMode.BANDWIDTH, self.config.bw.mc.l3.bw)\n            self.l3_dict[die_id] = l3_rw\n\n            # d2d\n            if self.config.inst_num.NUM_OF_DIE == 2:\n                # define Tx for each bw\n                die0_d2d_bw_resource = BWResource(f\"die0_d2d_tx\",\n                                                  BWMode.BANDWIDTH, self.config.bw.noc.llc_far.bw_per_second/self.config.freq.NOC/1e9)\n                die1_d2d_bw_resource = BWResource(f\"die1_d2d_tx\",\n                                                  BWMode.BANDWIDTH, self.config.bw.noc.llc_far.bw_per_second/self.config.freq.NOC/1e9)\n                # Die 0 ST: transmit data from die0 to die1\n                self.d2d_dict[(0, 'w')] = die0_d2d_bw_resource\n                # Die 0 LD: transmit data from die1 to die0\n                self.d2d_dict[(0, 'r')] = die1_d2d_bw_resource\n                # Die 1 ST: transmit data from die1 to die0\n                self.d2d_dict[(1, 'w')] = die1_d2d_bw_resource\n                # Die 0 LD: transmit data from die0 to die1\n                self.d2d_dict[(1, 'r')] = die0_d2d_bw_resource\n\n        # esl\n        self.esl_switch.build_bw_resource(self)\n\n    def get_unique_bw_resource_list(self) -> Generator[Tuple[str, str, int, int, int, BWResource], None, None]:\n        # TODO: need overwrite when introducing new arch\n        for die_id in range(self.config.inst_num.NUM_OF_DIE):\n            for cluster_id in range(self.config.inst_num.NUM_OF_CLUSTER):\n                yield (\"sic_io\", \"r\", die_id, cluster_id, None, self.sic_io_dict[(die_id, cluster_id, 'r')])\n                yield (\"sic_io\", \"w\", die_id, cluster_id, None, self.sic_io_dict[(die_id, cluster_id, 'w')])\n            yield (\"l3\", \"rw\", die_id, None, None, self.l3_dict[(die_id)])\n            if self.config.inst_num.NUM_OF_DIE > 1:\n                yield (\"d2d_tx\", \"rw\", die_id, None, None, self.d2d_dict[(die_id, 'w')])\n        # get esl unique bw_resource\n        yield from self.esl_switch.get_unique_bw_resource(self)\n\n    def get_bw_resource(self, src, dst, die_id, cluster_id, mem_stat, rw) -> BWResource | None:\n        def _sic_io():\n            key = (die_id, cluster_id, rw)\n            return self.sic_io_dict[key]\n\n        def _l3():\n            key = (die_id)\n            return self.l3_dict[key]\n\n        def _d2d():\n            key = (die_id, rw)\n            return self.d2d_dict[key]\n\n        def _esl():\n            return self.esl_switch.get_bw_resource(self.config.gcu_id, mem_stat.src_gcu_id, mem_stat.tar_gcu_id, rw)\n\n        res_dict = {\n            (\"sic_io\", \"noc\"): _sic_io,\n            (\"mc\", \"l3\"): _l3,\n            ('esl', 'noc'): _esl,\n            (\"noc\", \"llc_far\"): _d2d,\n        }\n\n        if (src, dst) in res_dict:\n            return res_dict[(src, dst)]()\n        else:\n            return None",
    "start_line": 211,
    "end_line": 296,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BWResourceContext",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResourceContext"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.CostBook": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.CostBook",
    "name": "CostBook",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class CostBook(BaseDataclass):\n    cache_stat_dict: Dict[str, any] = field(default_factory=dict)\n    power_stat: PowerStat = None\n    core_stat: BaseCoreStat = None\n    latency: float = 0\n    r_datasize: int = 0\n    w_datasize: int = 0",
    "start_line": 300,
    "end_line": 306,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseDataclass"
    ],
    "class_name": null,
    "display_name": "class CostBook",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.CostBook"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.EDCFrame": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.EDCFrame",
    "name": "EDCFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class EDCFrame(BaseFrame):\n    current: float = 0\n    # triggered: bool = False\n\n    def incr(self, frame: 'EDCFrame'):\n        self.current += frame.current",
    "start_line": 310,
    "end_line": 315,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class EDCFrame",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.EDCFrame"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.PowerContext": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.PowerContext",
    "name": "PowerContext",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerFrame",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.EDCFrame"
    ],
    "source_code": "class PowerContext:\n    # (die_id,cluster_id,sip_id)\n    sip_power_timeline_dict: Dict[Tuple[int, int, int],\n                                  Timeline[PowerFrame]] = field(default_factory=lambda: {})\n    l1_power_timeline_dict: Dict[Tuple[int, int, int],\n                                 Timeline[PowerFrame]] = field(default_factory=lambda: {})\n    dtu_power_timeline:  Timeline[PowerFrame] = field(\n        default_factory=lambda: Timeline([PowerFrame()]))\n\n    dtu_edc: Timeline[EDCFrame] = field(\n        default_factory=lambda: Timeline([EDCFrame()]))\n\n    soc_edc: Timeline[EDCFrame] = field(\n        default_factory=lambda: Timeline([EDCFrame()]))\n\n    l3_power: List[PowerL3Frame] = field(default_factory=list)",
    "start_line": 320,
    "end_line": 335,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerContext",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.PowerContext"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BossaNovaContext": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BossaNovaContext",
    "name": "BossaNovaContext",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BossaNovaContext():\n    initial_ref: float = 0\n    cost_dict: DefaultDict[int, CostBook] = field(\n        default_factory=lambda: defaultdict(CostBook))\n\n    bw_resource_context: BWResourceContext = None\n\n    power_context: PowerContext = None\n\n    post_stat: PostStat = None\n    tgen: TraceGenerator = None\n\n    def get_cluster_tgen(self, die_id: int, cid: int):\n        return self.tgen.get_cluster_tgen(die_id, cid)\n\n    def get_cost_book(self, action: 'DataflowAction') -> CostBook:\n        return self.cost_dict[action.get_action_id()]",
    "start_line": 339,
    "end_line": 355,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BossaNovaContext",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BossaNovaContext"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.DataflowAction": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.DataflowAction",
    "name": "DataflowAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class DataflowAction():\n    def __post_init__(self):\n        self.ref = 0\n\n    def get_die_id(self) -> int:\n        raise NotImplemented()\n\n    def get_action_id(self) -> int:\n        raise NotImplemented()\n\n    def get_engine_id(self) -> int:\n        raise NotImplemented()\n\n    def get_engine_sub_id(self) -> int:\n        raise NotImplemented()\n\n    def get_cluster_id(self) -> int:\n        raise NotImplemented()\n\n    def get_local_engine_id(self) -> int:\n        raise NotImplemented()\n\n    def get_port_id(self) -> int:\n        raise NotImplemented()\n\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        # List[ addr, size, w/r ]\n        raise NotImplemented()\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        # List[ count, src_domain, dst_domain, w/r ]\n        raise NotImplemented()\n\n    def get_child_ids(self):\n        raise NotImplemented()\n\n    def get_parent_ids(self):\n        raise NotImplemented()\n\n    def get_action_type(self) -> DataflowActionType:\n        raise NotImplemented()\n\n    def get_optype(self) -> DataflowOpType:\n        raise NotImplemented()\n\n    def get_trace_label(self) -> str:\n        # 默认使用动作类型名作为 trace 标签\n        return self.get_action_type().name.lower()\n\n    def get_dtype(self) -> DType:\n        raise NotImplemented()\n\n    def compute(self, context: 'BossaNovaContext') -> Generator[DataflowActionMemoryStat | DataflowActionComputeStat | None, None, BaseCoreStat]:\n        raise NotImplemented()\n\n    def get_core_stat(self) -> BaseActionStat:\n        raise NotImplemented()",
    "start_line": 359,
    "end_line": 415,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DataflowAction",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.DataflowAction"
  },
  "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCostService": {
    "id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCostService",
    "name": "BaseCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/base_compute_model.py",
    "depends_on": [],
    "source_code": "class BaseCostService():\n    def __init__(self, config: BossaNovaConfig):\n        self.config = config\n\n    def process(self, action: DataflowAction, context: BossaNovaContext, ref: float) -> Generator[bool, None, None]:\n        raise NotImplementedError()\n\n    def post_process(self, context: BossaNovaContext):\n        pass\n\n    def post_stat(self, context: BossaNovaContext, dataflow):\n        pass\n\n    def get_report(self):\n        return {}",
    "start_line": 418,
    "end_line": 432,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BaseCostService",
    "component_id": "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCostService"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EngineStat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EngineStat",
    "name": "EngineStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "class EngineStat:\n    start_op_ref: float = MAX_TIME\n    end_op_ref: float = 0\n    engine_end_ref: float = 0",
    "start_line": 22,
    "end_line": 25,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EngineStat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EngineStat"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.ComputeCostService": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.ComputeCostService",
    "name": "ComputeCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "class ComputeCostService(BaseCostService):\n    def __init__(\n        self,\n        config: BossaNovaConfig,\n        power_svc: PowerCostService,\n        dump_addr,\n        cache_svc: CacheCostService | None = None,\n        esl_switch: BaseESLSwitch = None,\n    ):\n        super().__init__(config)\n        if cache_svc:\n            self.cache_svc = cache_svc\n        else:\n            # make an empty cache_gen\n            class EmptyCacheSvc:\n                def process(self, action, context, ref):\n                    while True:\n                        stat: DataflowActionMemoryStat = yield\n                        if not stat:\n                            break\n                        stat.cache_stat = {}\n                    yield  # using yield instead of return to avoid from StopIterator exception\n            self.cache_svc = EmptyCacheSvc()\n        self.data_transport_service = DataTransportService(\n            config,  esl_switch)\n        self.power_svc = power_svc\n\n        self.engine_stat_dict: Dict[Tuple[DataflowActionType, int, int, int], EngineStat] = defaultdict(\n            EngineStat)  # key: (act_type,die_id,cluster_id,sip_id)\n\n        self.dump_addr = dump_addr\n\n    def process_compute_stat(\n        self,\n        stat: DataflowActionComputeStat,\n        edc_freq_cfg: FreqConfig,\n    ):\n        def _process_throughput(ops_dict: Dict[DType, float], throughput_cfg: Dict[DType, float], factor=1):\n            cyc = 0\n            for dtype, ops in ops_dict.items():\n                throughput = throughput_cfg.get(dtype, 0)\n                cyc = max(cyc, ops/throughput)\n            return cyc/factor\n\n        cyc_1d = _process_throughput(\n            stat.compute_1d_ops, self.config.compute.thread_1d_throughput, factor=stat.compute_1d_efficiency\n        )\n        cyc_2d = _process_throughput(\n            stat.compute_2d_ops, self.config.compute.thread_2d_throughput, factor=2 *\n            stat.compute_2d_efficiency\n        )  # config unit [mac]\n        cyc_sfu = stat.compute_msf_ops / \\\n            self.config.compute.thread_sfu_throughput / stat.compute_sfu_efficiency\n        cyc_scalar = stat.compute_scalar_cycle  # TODO: config\n        cyc_nop = stat.compute_nop_cycle\n        cycle = max(cyc_1d, cyc_2d, cyc_sfu, cyc_scalar, cyc_nop)\n        stat.latency = cycle/edc_freq_cfg.CORE/1e9\n        return stat.latency\n\n    def process_memory_stat(\n        self,\n        stat: DataflowActionMemoryStat,\n        context: BossaNovaContext,\n        edc_freq_cfg,\n        cache_gen,\n        die_id,\n        cluster_id,\n        engine_id,\n        action_name,\n        ref: float,\n    ):\n        try:\n            cache_gen.send(stat)\n        except (StopIteration, RuntimeError) as e:\n            stat.cache_stat = {}\n            logger.warning(\n                \"cache gen stopped before mem stat. %s\", e)\n        if stat.total_count > 10 * 2**10:  # >10kB\n            chunk_size = (stat.total_count // 10 // 128 + 1)*128\n            # chunk_size = stat.total_count // 10\n        # if stat.total_count > 10*1024*1024:  # >10kB\n        #     chunk_size = 10*1024*1024\n        else:\n            chunk_size = stat.total_count\n        # chunk_size = stat.total_count\n\n        def split_array_with_remainder_first(total_count, chunk_size):\n            remainder = total_count % chunk_size  # 计算余数部分大小\n            offset = 0\n\n            # 如果有余数部分，先生成余数部分\n            if remainder > 0:\n                yield offset, remainder\n                offset += remainder\n\n            # 生成完整的 chunk\n            for start in range(remainder, total_count, chunk_size):\n                yield start, chunk_size\n                offset += chunk_size\n\n        def _iter_stat(_stat: DataflowActionMemoryStat):\n            chunk_stat = DataflowActionMemoryStat(**_stat.__dict__)\n            if chunk_stat.dst == 'L3_REMOTE':\n                chunk_stat.rw = 'r' if _stat.rw == 'w' else 'w'\n            # for offset in range(0, _stat.total_count, chunk_size):\n            #     chunk_stat.is_done = False\n            #     chunk_stat.total_count = min(\n            #         chunk_size, _stat.total_count - offset)\n            #     yield chunk_stat\n            for offset, size in split_array_with_remainder_first(_stat.total_count, chunk_size):\n                chunk_stat.is_done = False\n                chunk_stat.total_count = size\n                yield chunk_stat\n\n        def _iter_stat_list(stat_list: List[DataflowActionMemoryStat]):\n            iter_list = [_iter_stat(s) for s in stat_list]\n            while iter_list:\n                _stat_g = iter_list.pop(0)\n                try:\n                    yield next(_stat_g)\n                    iter_list.append(_stat_g)\n                except StopIteration as si:\n                    pass\n\n        stat_list = []\n        if stat.dst == AddrDomain.L3:\n            _dst = AddrDomain.L3_FAR\n            for i in range(self.config.inst_num.NUM_OF_DIE):\n                _stat = DataflowActionMemoryStat(**stat.__dict__)\n                _stat.total_count = int(\n                    _stat.total_count/self.config.inst_num.NUM_OF_DIE)\n                if i != die_id:\n                    _stat.dst = _dst\n                stat_list.append(_stat)\n            bw_factor = 1/self.config.inst_num.NUM_OF_DIE\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.ESL, AddrDomain.L3, AddrDomain.L3_REMOTE, 'w'):  # esl master write\n            _stat = DataflowActionMemoryStat(**stat.__dict__)\n            _stat.src = AddrDomain.ESL\n            _stat.dst = AddrDomain.L3\n            _stat.rw = 'r'\n            for i in range(self.config.inst_num.NUM_OF_DIE):\n                __stat = DataflowActionMemoryStat(**_stat.__dict__)\n                __stat.total_count = int(\n                    __stat.total_count/self.config.inst_num.NUM_OF_DIE)\n                if i != die_id:\n                    __stat.dst = AddrDomain.L3_FAR\n                stat_list.append(__stat)\n            bw_factor = 1/self.config.inst_num.NUM_OF_DIE\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.ESL, AddrDomain.L3, AddrDomain.L3_REMOTE, 'r'):  # esl master read\n            raise NotImplemented\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.XPU, AddrDomain.L3, AddrDomain.L3_REMOTE, 'w'):  # esl slave store\n            raise NotImplemented\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.XPU, AddrDomain.L0, AddrDomain.L3_REMOTE, 'r'):  # esl slave load\n            raise NotImplemented\n        else:\n            stat_list.append(stat)\n            bw_factor = 1\n\n        # consider mem stat bw\n        bw_factor *= stat.bw_factor\n\n        latency_stat_list = []\n\n        # track_detail1 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail1\")\n        # track_detail2 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail2\")\n        # track_detail3 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail3\")\n        for idx, chunk_stat in enumerate(_iter_stat_list(stat_list)):\n            # calculate latency\n            while not chunk_stat.is_done:\n                _latency, leading_latency, latency_stat = yield from self.data_transport_service.compute_latency(\n                    context, chunk_stat,\n                    die_id,\n                    cluster_id,\n\n                    ref + chunk_stat.relative_ts, stat.cache_stat, edc_freq_cfg,\n                    bw_factor=bw_factor\n                )\n                if not chunk_stat.is_done:\n                    next_ref = ref + chunk_stat.relative_ts\n                    yield next_ref, chunk_stat\n\n            latency_stat_list.append(latency_stat)\n\n            last_leading_dur = latency_stat[-1][\"data_start_ref\"] - \\\n                latency_stat[-1][\"ref\"]\n            next_ref = (\n                latency_stat[-1][\"data_end_ref\"] -\n                last_leading_dur\n            )\n\n            if stat.master == DataflowActionType.ESL and stat.name != 'esl_remote':\n                next_ref = latency_stat[-1]['raw_data_lat'] + \\\n                    stat.relative_ts+ref\n\n            chunk_stat.relative_ts = next_ref-ref\n            # i += 1\n            yield next_ref, stat\n\n        stat_far_data_start_ref = 0\n        stat_end_ref = 0\n        # latency_stat_list  >>> [\n        #   [near_die_chunk0_subpath0, near_die_chunk0_path1, ...], [far_die_chunk0_subpath0, far_die_chunk1_subpath1, ...],\n        #   [near_die_chunk1_subpath0, near_die_chunk1_path1, ...], [far_die_chunk1_subpath0, far_die_chunk1_subpath1, ...],\n        # ]\n        first_chunk_list = latency_stat_list[0:len(stat_list)]\n        last_chunk_list = latency_stat_list[-len(stat_list):]\n        for i in range(len(stat_list)):\n            stat_far_data_start_ref = max(\n                stat_far_data_start_ref, first_chunk_list[i][0]['data_start_ref'])\n\n            _stat_end_ref = max([s['data_end_ref']\n                                for s in last_chunk_list[i]])\n\n            stat_end_ref = max(stat_end_ref, _stat_end_ref)\n\n        # for _s in latency_stat_list:\n        #     _stat_end_ref = max([_sub['data_end_ref']\n        #                         for _sub in _s])\n        #     stat_end_ref = max(stat_end_ref, _stat_end_ref)\n\n        if (stat.rw == 'w' and stat.write_through == False and stat.master != DataflowActionType.ESL) or \\\n           (stat.rw == 'w' and stat.write_through == False and stat.master == DataflowActionType.ESL and stat.name == 'esl_remote'):\n            # 当write back且经过l3时, 寻找llc/llc_far数据传输end time作为respond时间\n            _stat_far_data_start_ref = 0\n            for i in range(len(stat_list)):\n                for _sub_path_stat in first_chunk_list[i]:\n                    _src, _dst = _sub_path_stat['src'], _sub_path_stat['dst']\n                    if (_src.lower(), _dst.lower()) in [('noc', 'llc'), ('noc', 'llc_far')]:\n                        _stat_far_data_start_ref = max(\n                            _stat_far_data_start_ref, _sub_path_stat[\"data_start_ref\"])\n                        break\n            if _stat_far_data_start_ref > 0:\n                stat_far_data_start_ref = min(\n                    stat_far_data_start_ref, _stat_far_data_start_ref)\n            _stat_end_ref = 0\n            for i in range(len(stat_list)):\n                for _sub_path_stat in last_chunk_list[i]:\n                    _src, _dst = _sub_path_stat['src'], _sub_path_stat['dst']\n                    if (_src.lower(), _dst.lower()) in [('noc', 'llc'), ('noc', 'llc_far')]:\n                        _stat_end_ref = max(\n                            _stat_end_ref, _sub_path_stat[\"data_end_ref\"])\n                        break\n            if _stat_end_ref > 0:\n                stat_end_ref = min(stat_end_ref, _stat_end_ref)\n\n        stat.latency = stat_end_ref-(stat.relative_ts+ref)\n\n        # stat.leading_latency = leading_latency\n        # stat.leading_latency = stat_data_start_ref - \\\n        #     (ref+stat.relative_ts)\n        stat.leading_latency = stat_far_data_start_ref - \\\n            (ref+stat.relative_ts)\n        track = context.get_cluster_tgen(die_id, cluster_id).create_track(\n            f\"{action_name}:{engine_id}:dataflow\", tid=engine_id)\n        stat_name = stat.name if stat.name else f\"m\"\n        track.duration(\n            ref + stat.relative_ts,\n            stat.leading_latency,\n            f\"{stat_name}:leading:{stat.rw}\",\n            latency_stat_list,\n            category_list=[\"memory\", \"leading\"],\n        )\n        track.duration(\n            ref + stat.relative_ts + stat.leading_latency,\n            stat.latency - stat.leading_latency,\n            f\"{stat_name}:latency:{stat.rw}\",\n            stat,\n            category_list=[\"memory\", \"latency\"],\n        )\n        return latency_stat_list\n        # for debug\n        # for c, chunk in enumerate(latency_stat_list):\n        #     for i, layer in enumerate(chunk):\n        #         seq = layer['seq']\n        #         if c % 2 == 0:\n        #             near = 'near'\n        #         else:\n        #             near = 'far'\n        #         name = f\"c{c}:l{i}:{layer['src']}->{layer['dst']}:{seq}\"\n        #         _track = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #             f\"{action_type}:{engine_id}:dataflow_{near}:{stat_name}\", tid=engine_id)\n        #         # start = layer['ref']\n        #         # dur = layer['end_ts']-start\n        #         start = layer['data_start_ref']\n        #         esl_dur = layer['esl_data_lat'] if 'esl_data_lat' in layer else 0\n        #         total_dur = layer['data_lat']\n        #         _track.duration(\n        #             start,\n        #             total_dur,\n        #             name,\n        #             layer,\n        #             category_list=[\"memory\", \"detail\"],\n        #         )\n        #         if (layer['src'], layer['dst']) == ('ESL', 'NOC'):\n        #             name = f\"c{c}:l{i}:{layer['src']}->{layer['dst']}->remote:{seq}\"\n        #             _track.duration(\n        #                 start+total_dur-esl_dur,\n        #                 esl_dur,\n        #                 name,\n        #                 layer,\n        #                 category_list=[\"memory\", \"detail\"],\n        #             )\n\n    def process(self, action: DiagDataflowAction, context: BossaNovaContext, ref: float, trace_label: str | None = None) -> Generator[float, None, None]:\n        die_id = action.get_die_id()\n        cid = action.get_cluster_id()\n        engine_id = action.get_local_engine_id()\n        action_type = action.get_action_type()\n        trace_label = trace_label or action_type.name\n\n        cost_book = context.get_cost_book(action)\n        stat_gen = action.compute(context)\n\n        cache_gen = self.cache_svc.process(\n            action, context, ref)\n        next(cache_gen)\n\n        power_gen = self.power_svc.process(\n            action, context, ref)\n        next(power_gen)\n\n        engine_stat = self.engine_stat_dict[(\n            action_type, die_id, cid, engine_id)]\n\n        core_stat = BaseCoreStat()\n        try:\n            i = 0\n            while stat := next(stat_gen):\n                if isinstance(stat, BARRIER) or isinstance(stat, BossaNovaEvent):\n                    yield stat.max_t, stat\n                    continue\n\n                # temp_context = None\n                # llc_stat = self.cache_svc.post_stat(temp_context)\n                # logging.info(f\"LLC in every action: {llc_stat}\")\n                # addr_stat, data_size = self.cache_svc.get_access_addr(stat)\n                edc_freq_cfg = self.power_svc.get_edc_freq(\n                    ref+stat.relative_ts, context)\n                if issubclass(type(stat), DataflowActionComputeStat):\n                    compute_latency = self.process_compute_stat(\n                        stat, edc_freq_cfg)\n                    track = context.get_cluster_tgen(die_id, cid).create_track(\n                        f\"{trace_label}:{engine_id}:compute\", tid=engine_id)\n                    stat_name = stat.name if stat.name else f\"compute\"\n                    track.duration(\n                        ref + stat.relative_ts, stat.latency, stat_name, stat, category_list=[\"compute\"]\n                    )\n\n                    # update stat\n                    start_ref = ref + stat.relative_ts\n                    if start_ref < engine_stat.start_op_ref:\n                        engine_stat.start_op_ref = start_ref\n                    end_ref = ref + stat.relative_ts+stat.latency\n                    if end_ref > engine_stat.end_op_ref:\n                        engine_stat.end_op_ref = end_ref\n\n                    yield ref+stat.relative_ts+compute_latency, stat\n                elif issubclass(type(stat), DataflowActionMemoryStat):\n\n                    # calculate cache\n                    # TODO: need review\n                    yield from self.process_memory_stat(stat, context, edc_freq_cfg, cache_gen, die_id, cid, engine_id, trace_label, ref)\n                    self.dump_addr(action, stat, ref)\n\n                    # track = context.tgen._create_track(\n                    #     track._uuid, f\"{action_type}:{engine_id}:Detail\", 0)\n                # if edc_freq_cfg.CORE_CLOCK_DOMAIN < self.config.freq.CORE_CLOCK_DOMAIN:\n                #     track.instant(ref+stat.relative_ts, \"edc triggered\", {\n                #                   \"freq\": edc_freq_cfg.CORE_CLOCK_DOMAIN})\n                power_gen.send(stat)\n                core_stat += stat\n\n                engine_end_ref = ref + stat.relative_ts+stat.latency\n                if engine_end_ref > engine_stat.engine_end_ref:\n                    engine_stat.engine_end_ref = engine_end_ref\n\n            next(stat_gen)\n        except StopIteration as res:\n            # cost_book.core_stat = res.value\n            cost_book.core_stat = core_stat\n\n            cost_book.latency = res.value.latency\n        finally:\n            # must send None at the end to notify cache gen finish work of this action\n            try:\n                cache_gen.send(None)\n            except StopIteration as e:\n                pass\n            try:\n                power_gen.send(None)\n            except StopIteration as e:\n                pass\n\n        logger.debug(\"cost servcie - action %03d:%025s done\",\n                     action.get_action_id(), action.get_action_type())\n\n    def _longest_sip_stat(self, context: BossaNovaContext, per_sip_stat):\n        index = None\n        max_end_ref = 0\n        for _index, _stat in self.engine_stat_dict.items():\n            if _index[0] == DataflowActionType.XPU:\n                engine_end_ref = _stat.engine_end_ref\n                if engine_end_ref > max_end_ref:\n                    max_end_ref = engine_end_ref\n                    index = _index\n\n        if index:\n            _stat = self.engine_stat_dict[index]\n            initial_ref = context.initial_ref\n            return {\n                'die_id': index[1],\n                'cluster_id': index[2],\n                'sip_id': index[3],\n                'ops': per_sip_stat[index[1:]].to_dict(),\n                'prolog': f\"{_stat.start_op_ref-initial_ref:.3E}\",\n                'epilog': f\"{context.post_stat.total_latency-(_stat.end_op_ref-initial_ref):.3E}\",\n                'main_body': f\"{_stat.end_op_ref-_stat.start_op_ref:.3E}\"\n            }\n        return None\n\n    def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n\n        stat = BaseCoreStat()\n        per_sip_stat = {}\n\n        for action_id, cost_book in context.cost_dict.items():\n            action = dataflow._action_map[action_id]\n            die_id = action.get_die_id()\n            cid = action.get_cluster_id()\n            engine_id = action.get_local_engine_id()\n            per_sip_stat.setdefault(\n                (die_id, cid, engine_id), BaseCoreStat())\n            per_sip_stat[(die_id, cid, engine_id)] += cost_book.core_stat\n            if cost_book.core_stat:\n                stat += cost_book.core_stat\n\n        report = stat.to_dict()\n\n        sip_max_vector_ops = 0\n        sip_max_tensor_macs = 0\n        sip_max_sfu_ops = 0\n        for die_id, cid, engine_id in per_sip_stat.keys():\n            _vector_ops = per_sip_stat[(\n                die_id, cid, engine_id)].vector_ops.values()\n            _vector_ops = max(_vector_ops) if _vector_ops else 0\n            _tensor_macs = per_sip_stat[(\n                die_id, cid, engine_id)].tensor_macs.values()\n            _tensor_macs = max(_tensor_macs) if _tensor_macs else 0\n            _sfu_ops = per_sip_stat[(die_id, cid, engine_id)].sfu_ops\n            sip_max_vector_ops = max(sip_max_vector_ops, _vector_ops)\n            sip_max_tensor_macs = max(sip_max_tensor_macs, _tensor_macs)\n            sip_max_sfu_ops = max(sip_max_sfu_ops, _sfu_ops)\n\n        num_of_cores = self.config.inst_num.NUM_OF_CORE_PER_CLUSTER * \\\n            self.config.inst_num.NUM_OF_CLUSTER*self.config.inst_num.NUM_OF_DIE\n        workload_balance = {\n            \"sip_max_vector_ops\": sip_max_vector_ops,\n            \"sip_max_tensor_macs\": sip_max_tensor_macs,\n            \"sip_max_sfu_ops\": sip_max_sfu_ops,\n            \"vector_ops_rate\": max(stat.vector_ops.values())/sip_max_vector_ops / num_of_cores if sip_max_vector_ops else 0,\n            \"tensor_macs_rate\": max(stat.tensor_macs.values())/sip_max_tensor_macs / num_of_cores if sip_max_tensor_macs else 0,\n            \"sfu_ops_rate\": stat.sfu_ops/sip_max_sfu_ops / num_of_cores if sip_max_sfu_ops else 0,\n        }\n        report.update({\"workload_balance\": workload_balance})\n        longest_sip_stat = self._longest_sip_stat(context, per_sip_stat)\n        report.update({\"longest_sip_stat\": longest_sip_stat})\n\n        # context.post_stat.core_util = stat['action_time'] / \\\n        #     num_of_cores/context.post_stat.total_latency\n        if workload_balance[\"tensor_macs_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"tensor_macs_rate\"]\n        elif workload_balance[\"vector_ops_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"vector_ops_rate\"]\n        elif workload_balance[\"sfu_ops_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"sfu_ops_rate\"]\n        # report.update({\n        #     \"action_count\": f\"{stat['action_count']:,d}\",\n        #     \"action_time(ns)\": f\"{int(stat['action_time']*1e9):,d}\",\n        # })\n        # report = dict(sorted(report.items()))\n\n        for (lvl, rw, die_id, cluster_id, esl_port_id, bw_resource) in context.bw_resource_context.get_unique_bw_resource_list():\n            key = f\"{lvl}_{rw}_total\"\n            if key not in report:\n                report[key] = 0\n            if rw == 'r':\n                report[key] += bw_resource.acc_read\n            elif rw == 'w':\n                report[key] += bw_resource.acc_write\n            elif rw == 'rw':\n                report[key] += bw_resource.acc_read + bw_resource.acc_write\n            else:\n                raise Exception(f\"unsupported rw: {rw}\")\n\n        for k, v in report.items():\n            if type(v) == int:\n                report[k] = f\"{v:,d}\"\n\n        return report",
    "start_line": 28,
    "end_line": 529,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCostService"
    ],
    "class_name": null,
    "display_name": "class ComputeCostService",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.ComputeCostService"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EmptyCacheSvc": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EmptyCacheSvc",
    "name": "EmptyCacheSvc",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "            class EmptyCacheSvc:\n                def process(self, action, context, ref):\n                    while True:\n                        stat: DataflowActionMemoryStat = yield\n                        if not stat:\n                            break\n                        stat.cache_stat = {}\n                    yield  # using yield instead of return to avoid from StopIterator exception",
    "start_line": 42,
    "end_line": 49,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EmptyCacheSvc",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.EmptyCacheSvc"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_compute_stat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_compute_stat",
    "name": "process_compute_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "    def process_compute_stat(\n        self,\n        stat: DataflowActionComputeStat,\n        edc_freq_cfg: FreqConfig,\n    ):\n        def _process_throughput(ops_dict: Dict[DType, float], throughput_cfg: Dict[DType, float], factor=1):\n            cyc = 0\n            for dtype, ops in ops_dict.items():\n                throughput = throughput_cfg.get(dtype, 0)\n                cyc = max(cyc, ops/throughput)\n            return cyc/factor\n\n        cyc_1d = _process_throughput(\n            stat.compute_1d_ops, self.config.compute.thread_1d_throughput, factor=stat.compute_1d_efficiency\n        )\n        cyc_2d = _process_throughput(\n            stat.compute_2d_ops, self.config.compute.thread_2d_throughput, factor=2 *\n            stat.compute_2d_efficiency\n        )  # config unit [mac]\n        cyc_sfu = stat.compute_msf_ops / \\\n            self.config.compute.thread_sfu_throughput / stat.compute_sfu_efficiency\n        cyc_scalar = stat.compute_scalar_cycle  # TODO: config\n        cyc_nop = stat.compute_nop_cycle\n        cycle = max(cyc_1d, cyc_2d, cyc_sfu, cyc_scalar, cyc_nop)\n        stat.latency = cycle/edc_freq_cfg.CORE/1e9\n        return stat.latency",
    "start_line": 60,
    "end_line": 85,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "stat",
      "edc_freq_cfg"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function process_compute_stat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_compute_stat"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service._process_throughput": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._process_throughput",
    "name": "_process_throughput",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "        def _process_throughput(ops_dict: Dict[DType, float], throughput_cfg: Dict[DType, float], factor=1):\n            cyc = 0\n            for dtype, ops in ops_dict.items():\n                throughput = throughput_cfg.get(dtype, 0)\n                cyc = max(cyc, ops/throughput)\n            return cyc/factor",
    "start_line": 65,
    "end_line": 70,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "ops_dict",
      "throughput_cfg",
      "factor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _process_throughput",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._process_throughput"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_memory_stat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_memory_stat",
    "name": "process_memory_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "    def process_memory_stat(\n        self,\n        stat: DataflowActionMemoryStat,\n        context: BossaNovaContext,\n        edc_freq_cfg,\n        cache_gen,\n        die_id,\n        cluster_id,\n        engine_id,\n        action_name,\n        ref: float,\n    ):\n        try:\n            cache_gen.send(stat)\n        except (StopIteration, RuntimeError) as e:\n            stat.cache_stat = {}\n            logger.warning(\n                \"cache gen stopped before mem stat. %s\", e)\n        if stat.total_count > 10 * 2**10:  # >10kB\n            chunk_size = (stat.total_count // 10 // 128 + 1)*128\n            # chunk_size = stat.total_count // 10\n        # if stat.total_count > 10*1024*1024:  # >10kB\n        #     chunk_size = 10*1024*1024\n        else:\n            chunk_size = stat.total_count\n        # chunk_size = stat.total_count\n\n        def split_array_with_remainder_first(total_count, chunk_size):\n            remainder = total_count % chunk_size  # 计算余数部分大小\n            offset = 0\n\n            # 如果有余数部分，先生成余数部分\n            if remainder > 0:\n                yield offset, remainder\n                offset += remainder\n\n            # 生成完整的 chunk\n            for start in range(remainder, total_count, chunk_size):\n                yield start, chunk_size\n                offset += chunk_size\n\n        def _iter_stat(_stat: DataflowActionMemoryStat):\n            chunk_stat = DataflowActionMemoryStat(**_stat.__dict__)\n            if chunk_stat.dst == 'L3_REMOTE':\n                chunk_stat.rw = 'r' if _stat.rw == 'w' else 'w'\n            # for offset in range(0, _stat.total_count, chunk_size):\n            #     chunk_stat.is_done = False\n            #     chunk_stat.total_count = min(\n            #         chunk_size, _stat.total_count - offset)\n            #     yield chunk_stat\n            for offset, size in split_array_with_remainder_first(_stat.total_count, chunk_size):\n                chunk_stat.is_done = False\n                chunk_stat.total_count = size\n                yield chunk_stat\n\n        def _iter_stat_list(stat_list: List[DataflowActionMemoryStat]):\n            iter_list = [_iter_stat(s) for s in stat_list]\n            while iter_list:\n                _stat_g = iter_list.pop(0)\n                try:\n                    yield next(_stat_g)\n                    iter_list.append(_stat_g)\n                except StopIteration as si:\n                    pass\n\n        stat_list = []\n        if stat.dst == AddrDomain.L3:\n            _dst = AddrDomain.L3_FAR\n            for i in range(self.config.inst_num.NUM_OF_DIE):\n                _stat = DataflowActionMemoryStat(**stat.__dict__)\n                _stat.total_count = int(\n                    _stat.total_count/self.config.inst_num.NUM_OF_DIE)\n                if i != die_id:\n                    _stat.dst = _dst\n                stat_list.append(_stat)\n            bw_factor = 1/self.config.inst_num.NUM_OF_DIE\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.ESL, AddrDomain.L3, AddrDomain.L3_REMOTE, 'w'):  # esl master write\n            _stat = DataflowActionMemoryStat(**stat.__dict__)\n            _stat.src = AddrDomain.ESL\n            _stat.dst = AddrDomain.L3\n            _stat.rw = 'r'\n            for i in range(self.config.inst_num.NUM_OF_DIE):\n                __stat = DataflowActionMemoryStat(**_stat.__dict__)\n                __stat.total_count = int(\n                    __stat.total_count/self.config.inst_num.NUM_OF_DIE)\n                if i != die_id:\n                    __stat.dst = AddrDomain.L3_FAR\n                stat_list.append(__stat)\n            bw_factor = 1/self.config.inst_num.NUM_OF_DIE\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.ESL, AddrDomain.L3, AddrDomain.L3_REMOTE, 'r'):  # esl master read\n            raise NotImplemented\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.XPU, AddrDomain.L3, AddrDomain.L3_REMOTE, 'w'):  # esl slave store\n            raise NotImplemented\n        elif (stat.master, stat.src, stat.dst, stat.rw) == (DataflowActionType.XPU, AddrDomain.L0, AddrDomain.L3_REMOTE, 'r'):  # esl slave load\n            raise NotImplemented\n        else:\n            stat_list.append(stat)\n            bw_factor = 1\n\n        # consider mem stat bw\n        bw_factor *= stat.bw_factor\n\n        latency_stat_list = []\n\n        # track_detail1 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail1\")\n        # track_detail2 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail2\")\n        # track_detail3 = context.get_cluster_tgen(die_id, cluster_id).create_track(\n        #     f\"{action_type}:{engine_id}:dataflow_detail3\")\n        for idx, chunk_stat in enumerate(_iter_stat_list(stat_list)):\n            # calculate latency\n            while not chunk_stat.is_done:\n                _latency, leading_latency, latency_stat = yield from self.data_transport_service.compute_latency(\n                    context, chunk_stat,\n                    die_id,\n                    cluster_id,\n\n                    ref + chunk_stat.relative_ts, stat.cache_stat, edc_freq_cfg,\n                    bw_factor=bw_factor\n                )\n                if not chunk_stat.is_done:\n                    next_ref = ref + chunk_stat.relative_ts\n                    yield next_ref, chunk_stat\n\n            latency_stat_list.append(latency_stat)\n\n            last_leading_dur = latency_stat[-1][\"data_start_ref\"] - \\\n                latency_stat[-1][\"ref\"]\n            next_ref = (\n                latency_stat[-1][\"data_end_ref\"] -\n                last_leading_dur\n            )\n\n            if stat.master == DataflowActionType.ESL and stat.name != 'esl_remote':\n                next_ref = latency_stat[-1]['raw_data_lat'] + \\\n                    stat.relative_ts+ref\n\n            chunk_stat.relative_ts = next_ref-ref\n            # i += 1\n            yield next_ref, stat\n\n        stat_far_data_start_ref = 0\n        stat_end_ref = 0\n        # latency_stat_list  >>> [\n        #   [near_die_chunk0_subpath0, near_die_chunk0_path1, ...], [far_die_chunk0_subpath0, far_die_chunk1_subpath1, ...],\n        #   [near_die_chunk1_subpath0, near_die_chunk1_path1, ...], [far_die_chunk1_subpath0, far_die_chunk1_subpath1, ...],\n        # ]\n        first_chunk_list = latency_stat_list[0:len(stat_list)]\n        last_chunk_list = latency_stat_list[-len(stat_list):]\n        for i in range(len(stat_list)):\n            stat_far_data_start_ref = max(\n                stat_far_data_start_ref, first_chunk_list[i][0]['data_start_ref'])\n\n            _stat_end_ref = max([s['data_end_ref']\n                                for s in last_chunk_list[i]])\n\n            stat_end_ref = max(stat_end_ref, _stat_end_ref)\n\n        # for _s in latency_stat_list:\n        #     _stat_end_ref = max([_sub['data_end_ref']\n        #                         for _sub in _s])\n        #     stat_end_ref = max(stat_end_ref, _stat_end_ref)\n\n        if (stat.rw == 'w' and stat.write_through == False and stat.master != DataflowActionType.ESL) or \\\n           (stat.rw == 'w' and stat.write_through == False and stat.master == DataflowActionType.ESL and stat.name == 'esl_remote'):\n            # 当write back且经过l3时, 寻找llc/llc_far数据传输end time作为respond时间\n            _stat_far_data_start_ref = 0\n            for i in range(len(stat_list)):\n                for _sub_path_stat in first_chunk_list[i]:\n                    _src, _dst = _sub_path_stat['src'], _sub_path_stat['dst']\n                    if (_src.lower(), _dst.lower()) in [('noc', 'llc'), ('noc', 'llc_far')]:\n                        _stat_far_data_start_ref = max(\n                            _stat_far_data_start_ref, _sub_path_stat[\"data_start_ref\"])\n                        break\n            if _stat_far_data_start_ref > 0:\n                stat_far_data_start_ref = min(\n                    stat_far_data_start_ref, _stat_far_data_start_ref)\n            _stat_end_ref = 0\n            for i in range(len(stat_list)):\n                for _sub_path_stat in last_chunk_list[i]:\n                    _src, _dst = _sub_path_stat['src'], _sub_path_stat['dst']\n                    if (_src.lower(), _dst.lower()) in [('noc', 'llc'), ('noc', 'llc_far')]:\n                        _stat_end_ref = max(\n                            _stat_end_ref, _sub_path_stat[\"data_end_ref\"])\n                        break\n            if _stat_end_ref > 0:\n                stat_end_ref = min(stat_end_ref, _stat_end_ref)\n\n        stat.latency = stat_end_ref-(stat.relative_ts+ref)\n\n        # stat.leading_latency = leading_latency\n        # stat.leading_latency = stat_data_start_ref - \\\n        #     (ref+stat.relative_ts)\n        stat.leading_latency = stat_far_data_start_ref - \\\n            (ref+stat.relative_ts)\n        track = context.get_cluster_tgen(die_id, cluster_id).create_track(\n            f\"{action_name}:{engine_id}:dataflow\", tid=engine_id)\n        stat_name = stat.name if stat.name else f\"m\"\n        track.duration(\n            ref + stat.relative_ts,\n            stat.leading_latency,\n            f\"{stat_name}:leading:{stat.rw}\",\n            latency_stat_list,\n            category_list=[\"memory\", \"leading\"],\n        )\n        track.duration(\n            ref + stat.relative_ts + stat.leading_latency,\n            stat.latency - stat.leading_latency,\n            f\"{stat_name}:latency:{stat.rw}\",\n            stat,\n            category_list=[\"memory\", \"latency\"],\n        )\n        return latency_stat_list",
    "start_line": 87,
    "end_line": 300,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "stat",
      "context",
      "edc_freq_cfg",
      "cache_gen",
      "die_id",
      "cluster_id",
      "engine_id",
      "action_name",
      "ref"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function process_memory_stat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_memory_stat"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.split_array_with_remainder_first": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.split_array_with_remainder_first",
    "name": "split_array_with_remainder_first",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "        def split_array_with_remainder_first(total_count, chunk_size):\n            remainder = total_count % chunk_size  # 计算余数部分大小\n            offset = 0\n\n            # 如果有余数部分，先生成余数部分\n            if remainder > 0:\n                yield offset, remainder\n                offset += remainder\n\n            # 生成完整的 chunk\n            for start in range(remainder, total_count, chunk_size):\n                yield start, chunk_size\n                offset += chunk_size",
    "start_line": 114,
    "end_line": 126,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "total_count",
      "chunk_size"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function split_array_with_remainder_first",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.split_array_with_remainder_first"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat",
    "name": "_iter_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.split_array_with_remainder_first",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat"
    ],
    "source_code": "        def _iter_stat(_stat: DataflowActionMemoryStat):\n            chunk_stat = DataflowActionMemoryStat(**_stat.__dict__)\n            if chunk_stat.dst == 'L3_REMOTE':\n                chunk_stat.rw = 'r' if _stat.rw == 'w' else 'w'\n            # for offset in range(0, _stat.total_count, chunk_size):\n            #     chunk_stat.is_done = False\n            #     chunk_stat.total_count = min(\n            #         chunk_size, _stat.total_count - offset)\n            #     yield chunk_stat\n            for offset, size in split_array_with_remainder_first(_stat.total_count, chunk_size):\n                chunk_stat.is_done = False\n                chunk_stat.total_count = size\n                yield chunk_stat",
    "start_line": 128,
    "end_line": 140,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "_stat"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_stat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat_list": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat_list",
    "name": "_iter_stat_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat"
    ],
    "source_code": "        def _iter_stat_list(stat_list: List[DataflowActionMemoryStat]):\n            iter_list = [_iter_stat(s) for s in stat_list]\n            while iter_list:\n                _stat_g = iter_list.pop(0)\n                try:\n                    yield next(_stat_g)\n                    iter_list.append(_stat_g)\n                except StopIteration as si:\n                    pass",
    "start_line": 142,
    "end_line": 150,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "stat_list"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _iter_stat_list",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._iter_stat_list"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process",
    "name": "process",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_compute_stat",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat",
      "nova-platform.nova_platform.cost_service.power.power_cost_service.duration",
      "nova-platform.nova_platform.executor.nova_platform_executor.dump_addr",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_memory_stat",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
    ],
    "source_code": "    def process(self, action: DiagDataflowAction, context: BossaNovaContext, ref: float, trace_label: str | None = None) -> Generator[float, None, None]:\n        die_id = action.get_die_id()\n        cid = action.get_cluster_id()\n        engine_id = action.get_local_engine_id()\n        action_type = action.get_action_type()\n        trace_label = trace_label or action_type.name\n\n        cost_book = context.get_cost_book(action)\n        stat_gen = action.compute(context)\n\n        cache_gen = self.cache_svc.process(\n            action, context, ref)\n        next(cache_gen)\n\n        power_gen = self.power_svc.process(\n            action, context, ref)\n        next(power_gen)\n\n        engine_stat = self.engine_stat_dict[(\n            action_type, die_id, cid, engine_id)]\n\n        core_stat = BaseCoreStat()\n        try:\n            i = 0\n            while stat := next(stat_gen):\n                if isinstance(stat, BARRIER) or isinstance(stat, BossaNovaEvent):\n                    yield stat.max_t, stat\n                    continue\n\n                # temp_context = None\n                # llc_stat = self.cache_svc.post_stat(temp_context)\n                # logging.info(f\"LLC in every action: {llc_stat}\")\n                # addr_stat, data_size = self.cache_svc.get_access_addr(stat)\n                edc_freq_cfg = self.power_svc.get_edc_freq(\n                    ref+stat.relative_ts, context)\n                if issubclass(type(stat), DataflowActionComputeStat):\n                    compute_latency = self.process_compute_stat(\n                        stat, edc_freq_cfg)\n                    track = context.get_cluster_tgen(die_id, cid).create_track(\n                        f\"{trace_label}:{engine_id}:compute\", tid=engine_id)\n                    stat_name = stat.name if stat.name else f\"compute\"\n                    track.duration(\n                        ref + stat.relative_ts, stat.latency, stat_name, stat, category_list=[\"compute\"]\n                    )\n\n                    # update stat\n                    start_ref = ref + stat.relative_ts\n                    if start_ref < engine_stat.start_op_ref:\n                        engine_stat.start_op_ref = start_ref\n                    end_ref = ref + stat.relative_ts+stat.latency\n                    if end_ref > engine_stat.end_op_ref:\n                        engine_stat.end_op_ref = end_ref\n\n                    yield ref+stat.relative_ts+compute_latency, stat\n                elif issubclass(type(stat), DataflowActionMemoryStat):\n\n                    # calculate cache\n                    # TODO: need review\n                    yield from self.process_memory_stat(stat, context, edc_freq_cfg, cache_gen, die_id, cid, engine_id, trace_label, ref)\n                    self.dump_addr(action, stat, ref)\n\n                    # track = context.tgen._create_track(\n                    #     track._uuid, f\"{action_type}:{engine_id}:Detail\", 0)\n                # if edc_freq_cfg.CORE_CLOCK_DOMAIN < self.config.freq.CORE_CLOCK_DOMAIN:\n                #     track.instant(ref+stat.relative_ts, \"edc triggered\", {\n                #                   \"freq\": edc_freq_cfg.CORE_CLOCK_DOMAIN})\n                power_gen.send(stat)\n                core_stat += stat\n\n                engine_end_ref = ref + stat.relative_ts+stat.latency\n                if engine_end_ref > engine_stat.engine_end_ref:\n                    engine_stat.engine_end_ref = engine_end_ref\n\n            next(stat_gen)\n        except StopIteration as res:\n            # cost_book.core_stat = res.value\n            cost_book.core_stat = core_stat\n\n            cost_book.latency = res.value.latency\n        finally:\n            # must send None at the end to notify cache gen finish work of this action\n            try:\n                cache_gen.send(None)\n            except StopIteration as e:\n                pass\n            try:\n                power_gen.send(None)\n            except StopIteration as e:\n                pass\n\n        logger.debug(\"cost servcie - action %03d:%025s done\",\n                     action.get_action_id(), action.get_action_type())",
    "start_line": 334,
    "end_line": 425,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "action",
      "context",
      "ref",
      "trace_label"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function process",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service._longest_sip_stat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._longest_sip_stat",
    "name": "_longest_sip_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [],
    "source_code": "    def _longest_sip_stat(self, context: BossaNovaContext, per_sip_stat):\n        index = None\n        max_end_ref = 0\n        for _index, _stat in self.engine_stat_dict.items():\n            if _index[0] == DataflowActionType.XPU:\n                engine_end_ref = _stat.engine_end_ref\n                if engine_end_ref > max_end_ref:\n                    max_end_ref = engine_end_ref\n                    index = _index\n\n        if index:\n            _stat = self.engine_stat_dict[index]\n            initial_ref = context.initial_ref\n            return {\n                'die_id': index[1],\n                'cluster_id': index[2],\n                'sip_id': index[3],\n                'ops': per_sip_stat[index[1:]].to_dict(),\n                'prolog': f\"{_stat.start_op_ref-initial_ref:.3E}\",\n                'epilog': f\"{context.post_stat.total_latency-(_stat.end_op_ref-initial_ref):.3E}\",\n                'main_body': f\"{_stat.end_op_ref-_stat.start_op_ref:.3E}\"\n            }\n        return None",
    "start_line": 427,
    "end_line": 449,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "context",
      "per_sip_stat"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _longest_sip_stat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service._longest_sip_stat"
  },
  "nova-platform.nova_platform.cost_service.compute.compute_cost_service.post_stat": {
    "id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.post_stat",
    "name": "post_stat",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/compute_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service._longest_sip_stat",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat"
    ],
    "source_code": "    def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n\n        stat = BaseCoreStat()\n        per_sip_stat = {}\n\n        for action_id, cost_book in context.cost_dict.items():\n            action = dataflow._action_map[action_id]\n            die_id = action.get_die_id()\n            cid = action.get_cluster_id()\n            engine_id = action.get_local_engine_id()\n            per_sip_stat.setdefault(\n                (die_id, cid, engine_id), BaseCoreStat())\n            per_sip_stat[(die_id, cid, engine_id)] += cost_book.core_stat\n            if cost_book.core_stat:\n                stat += cost_book.core_stat\n\n        report = stat.to_dict()\n\n        sip_max_vector_ops = 0\n        sip_max_tensor_macs = 0\n        sip_max_sfu_ops = 0\n        for die_id, cid, engine_id in per_sip_stat.keys():\n            _vector_ops = per_sip_stat[(\n                die_id, cid, engine_id)].vector_ops.values()\n            _vector_ops = max(_vector_ops) if _vector_ops else 0\n            _tensor_macs = per_sip_stat[(\n                die_id, cid, engine_id)].tensor_macs.values()\n            _tensor_macs = max(_tensor_macs) if _tensor_macs else 0\n            _sfu_ops = per_sip_stat[(die_id, cid, engine_id)].sfu_ops\n            sip_max_vector_ops = max(sip_max_vector_ops, _vector_ops)\n            sip_max_tensor_macs = max(sip_max_tensor_macs, _tensor_macs)\n            sip_max_sfu_ops = max(sip_max_sfu_ops, _sfu_ops)\n\n        num_of_cores = self.config.inst_num.NUM_OF_CORE_PER_CLUSTER * \\\n            self.config.inst_num.NUM_OF_CLUSTER*self.config.inst_num.NUM_OF_DIE\n        workload_balance = {\n            \"sip_max_vector_ops\": sip_max_vector_ops,\n            \"sip_max_tensor_macs\": sip_max_tensor_macs,\n            \"sip_max_sfu_ops\": sip_max_sfu_ops,\n            \"vector_ops_rate\": max(stat.vector_ops.values())/sip_max_vector_ops / num_of_cores if sip_max_vector_ops else 0,\n            \"tensor_macs_rate\": max(stat.tensor_macs.values())/sip_max_tensor_macs / num_of_cores if sip_max_tensor_macs else 0,\n            \"sfu_ops_rate\": stat.sfu_ops/sip_max_sfu_ops / num_of_cores if sip_max_sfu_ops else 0,\n        }\n        report.update({\"workload_balance\": workload_balance})\n        longest_sip_stat = self._longest_sip_stat(context, per_sip_stat)\n        report.update({\"longest_sip_stat\": longest_sip_stat})\n\n        # context.post_stat.core_util = stat['action_time'] / \\\n        #     num_of_cores/context.post_stat.total_latency\n        if workload_balance[\"tensor_macs_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"tensor_macs_rate\"]\n        elif workload_balance[\"vector_ops_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"vector_ops_rate\"]\n        elif workload_balance[\"sfu_ops_rate\"] > 0:\n            context.post_stat.workload_balance = workload_balance[\"sfu_ops_rate\"]\n        # report.update({\n        #     \"action_count\": f\"{stat['action_count']:,d}\",\n        #     \"action_time(ns)\": f\"{int(stat['action_time']*1e9):,d}\",\n        # })\n        # report = dict(sorted(report.items()))\n\n        for (lvl, rw, die_id, cluster_id, esl_port_id, bw_resource) in context.bw_resource_context.get_unique_bw_resource_list():\n            key = f\"{lvl}_{rw}_total\"\n            if key not in report:\n                report[key] = 0\n            if rw == 'r':\n                report[key] += bw_resource.acc_read\n            elif rw == 'w':\n                report[key] += bw_resource.acc_write\n            elif rw == 'rw':\n                report[key] += bw_resource.acc_read + bw_resource.acc_write\n            else:\n                raise Exception(f\"unsupported rw: {rw}\")\n\n        for k, v in report.items():\n            if type(v) == int:\n                report[k] = f\"{v:,d}\"\n\n        return report",
    "start_line": 451,
    "end_line": 529,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "context",
      "dataflow"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function post_stat",
    "component_id": "nova-platform.nova_platform.cost_service.compute.compute_cost_service.post_stat"
  },
  "nova-platform.nova_platform.cost_service.compute.data_transport_service.DataTransportService": {
    "id": "nova-platform.nova_platform.cost_service.compute.data_transport_service.DataTransportService",
    "name": "DataTransportService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/compute/data_transport_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/compute/data_transport_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWFrame"
    ],
    "source_code": "class DataTransportService():\n    def __init__(self, config: BossaNovaConfig, esl_switch: BaseESLSwitch):\n        self.config = config\n        self.esl_switch = esl_switch\n\n    def _compute_competition_latency_wrapper(self, resource_ctx: BWResource, data_size, cluster_id, bw, freq, ref):\n        if resource_ctx:\n            resource_ctx._lock.acquire()\n            res = self._compute_competition_latency(\n                resource_ctx, data_size, cluster_id, bw, freq, ref)\n            resource_ctx._lock.release()\n        else:\n            res = self._compute_competition_latency(\n                resource_ctx, data_size, cluster_id, bw, freq, ref)\n        return res\n        # assume default frame [0,MAX_TIME)\n        # frames are continue without any gap\n\n    def _compute_competition_latency(self, resource_ctx: BWResource, data_size, cluster_id, bw, freq, ref):\n        if not resource_ctx:\n            return data_size/bw/freq\n        # look for left_frame, ref, right_frame\n        # frame_index, frame = resource_ctx.timeline.get_frame(ref)\n        frame_index, frame = resource_ctx.get_frame(ref)\n\n        def get_allocatable_bw(frame: BWFrame):\n            return min(resource_ctx.max_bw-frame.allocated_bw, bw)\n        allocateable_bw = get_allocatable_bw(frame)\n        affordable_data_size = int(\n            allocateable_bw*(frame.end-ref)*freq)  # need review precision\n\n        if ref > frame.begin:\n            right_frame = BWFrame(ref, frame.end, frame.allocated_bw)\n            if ref == frame.end:\n                pass\n            resource_ctx.timeline.data.insert(frame_index+1, right_frame)\n            dur = ref-frame.begin\n            frame.end = ref\n            latency = self._compute_competition_latency(\n                resource_ctx, data_size, cluster_id, bw, freq, ref)\n            logger.debug(\n                \"%02d, case   I, ref > frame.begin, split, nested call, current frame=%s, new ref=%.3E\", cluster_id, frame, ref)\n        elif allocateable_bw == 0:\n            # case I : get_allocatable_bw==0\n            dur = frame.end-ref\n            ref = frame.end\n            logger.debug(\n                \"%02d, case  II, current frame allocateable_bw==0, nested call by set ref=frame.end, current begin=%.3E, new ref=%.3E\", cluster_id, frame.begin, ref)\n            latency = dur + \\\n                self._compute_competition_latency(\n                    resource_ctx, data_size, cluster_id, bw, freq, ref)\n        elif data_size <= affordable_data_size:\n            # case II: current frame can afford all data transportation\n            dur = data_size/allocateable_bw/freq\n            if data_size < affordable_data_size:\n                right_frame = BWFrame(ref+dur,\n                                      frame.end, frame.allocated_bw)\n                if ref+dur == frame.end:\n                    pass\n                resource_ctx.timeline.data.insert(frame_index+1, right_frame)\n                frame.end = right_frame.begin\n                logger.debug(\n                    \"%02d, case III, split frame left=%s, right=%s\", cluster_id, frame, right_frame)\n            frame.allocated_bw += allocateable_bw\n            latency = dur\n            logger.debug(\n                \"%02d, case  IV, estimate_lat <= frame.dur, current frame=%s\", cluster_id, frame)\n        else:\n            # case V: allocate partial data in current frame and nested call remain\n            frame.allocated_bw += allocateable_bw\n            remaining_data_size = data_size-int(allocateable_bw*frame.dur*freq)\n            ref += frame.dur\n            logger.debug(\n                \"%02d, case V, estimate_lat > frame.dur, allocate current frame and nested call remaining data, new ref=%.3E, curr=%s, remain=%d\", cluster_id, ref, frame, remaining_data_size)\n            latency = frame.dur\n            if remaining_data_size > 0:\n                latency += self._compute_competition_latency(\n                    resource_ctx, remaining_data_size, cluster_id, bw, freq, ref)\n\n        return latency\n\n    def _path_latency(self, src, dst, freq_config: FreqConfig):\n        config = self.config\n        bw_file: BWFile = getattr(config.bw, src.lower())\n        bw_ele: BWEle = getattr(bw_file, dst.lower())\n        freq = freq_config.get_freq(bw_file.freq_domain)  # GHz=>Hz\n\n        pre_lat, post_lat = bw_ele.pre_latency/freq, bw_ele.post_latency/freq\n        return pre_lat, post_lat\n\n    def _get_route(self, master: DataflowActionType, src: AD, dst: AD):\n        route_map = {\n            (DataflowActionType.CDTE, AD.L0, AD.L3): [\"L0\", \"L1C\", \"SIC_IO\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.L0, AD.L3_FAR): [\"L0\", \"L1C\", \"SIC_IO\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.SHARED, AD.L3): [\"SHARED\", \"SIC_IO\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.SHARED, AD.L3_FAR): [\"SHARED\", \"SIC_IO\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.LOCAL, AD.L3): [\"LOCAL\", \"SIC_IO\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.LOCAL, AD.L3_FAR): [\"LOCAL\", \"SIC_IO\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.L3, AD.L3): [\"SIC_IO\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.CDTE, AD.L3, AD.L3_FAR): [\"SIC_IO\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n            (DataflowActionType.XPU, AD.L0, AD.L3): [\"L0\", \"SIC_IO\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.XPU, AD.L0, AD.L3_FAR): [\"L0\", \"SIC_IO\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n            (DataflowActionType.XPU, AD.L0, AD.LOCAL): [\"L0\", \"LOCAL\"],\n            (DataflowActionType.XPU, AD.L0, AD.SHARED): [\"L0\", \"SHARED\"],\n            (DataflowActionType.ESL, AD.ESL, AD.L3): [\"ESL\", \"NOC\", \"LLC\", \"MC\", \"L3\"],\n            (DataflowActionType.ESL, AD.ESL, AD.L3_FAR): [\"ESL\", \"NOC\", \"LLC_FAR\", \"MC\", \"L3\"],\n        }\n        return route_map.get((master, src, dst))\n\n    def _get_bw(self, master: DataflowActionType, src, dst, rw, last_bw_per_second, bw_resource: BWResource | None, freq_cfg: FreqConfig):\n        bw_dict = self.config.bw\n        bw_file: BWFile = getattr(bw_dict, src.lower())\n        bw_ele: BWEle = getattr(bw_file, dst.lower())\n        freq = freq_cfg.get_freq(bw_file.freq_domain)\n        # bw_Bps = bw * freq\n\n        last_bw = int(last_bw_per_second/freq)\n        if bw_resource and bw_resource.mode == BWMode.BANDWIDTH:\n            bw = last_bw\n            logger.debug(\"bw bandwidth mode, last bw/s=%.2f, bw=%d\",\n                         last_bw_per_second, bw)\n        else:\n            bw = bw_ele.get_bw(freq)\n            if rw == 'w':\n                # TODO: check rw_ratio apply to private or global, now assume private\n                rw_ratio = bw_ele.rw_ratio\n                bw = bw/rw_ratio\n                logger.debug(\"bw ratio %d, bw= %.2f\", rw_ratio, bw)\n\n            # bw = 128\n            if master == DataflowActionType.CDTE:\n                # TODO: workaround for SPMD _bw*4\n                SPMD = self.config.dte.THREAD_NUMBER\n                bw = bw*SPMD\n                logger.debug(\"bw SPMD %d, bw= %.2f\", SPMD, bw)\n            bw = min(bw, last_bw)\n\n        return bw, freq\n\n    # @functools.lru_cache\n    def _compute_leading(self, master, src, dst, freq_cfg):\n        router = self._get_route(master, src, dst)\n        if router is None:\n            raise ValueError(\n                f\"Invalid route: master={master}, src={src}, dst={dst}\")\n        leading_latency_group = []\n        for i in range(len(router)-1):\n            _src = router[i]\n            _dst = router[i+1]\n            # leading\n            pre_lat, post_lat = self._path_latency(_src, _dst, freq_cfg)\n            leading_latency_group.append((_src, _dst, pre_lat+post_lat))\n\n        return leading_latency_group\n\n    def get_edc_freq_by_src(self, freq_cfg: FreqConfig, src):\n        freq_domain_map = {\n            \"L0\":     freq_cfg.CORE,\n            \"L1C\":    freq_cfg.CORE,\n            \"LOCAL\":  freq_cfg.CORE,\n            \"SHARED\": freq_cfg.CORE,\n            \"SIC_IO\": freq_cfg.INTERCONNECT,\n            \"NOC\":    freq_cfg.INTERCONNECT,\n            \"LLC\":    freq_cfg.LLC,\n            \"LLC_FAR\": freq_cfg.LLC,\n            \"MC\":     freq_cfg.MC,\n            \"L3\":     freq_cfg.L3,\n        }\n        return freq_domain_map[src.upper()]*1e9\n\n    def _compute_sub_path(self, mem_stat: DataflowActionMemoryStat, master, route, index, count, last_bw_per_second, freq_cfg: FreqConfig, die_id, cluster_id, rw, hit_stat_dict: Dict[str, CacheStat], context: BossaNovaContext, ref, latency_stat):\n        src = route[index]\n        dst = route[index+1]\n        pre_lat, post_lat = self._path_latency(src, dst, freq_cfg)\n        pre_ref = ref+pre_lat\n\n        resource_ctx = context.bw_resource_context.get_bw_resource(\n            src.lower(), dst.lower(), die_id, cluster_id, mem_stat, rw)\n        bw, freq = self._get_bw(\n            master, src, dst, rw, last_bw_per_second, resource_ctx, freq_cfg)  # bw: byte/cycle\n        _last_bw_per_second = bw*freq\n        end_ts = 0\n        _post_ref = pre_ref\n        if index < len(route)-1:\n            if dst.upper() == \"LLC_FAR\":\n                _dst = \"LLC\"\n            else:\n                _dst = dst\n            is_cache_layer = _dst.upper() in hit_stat_dict\n            if is_cache_layer:\n                cache_stat = hit_stat_dict.get(_dst)\n                if rw == 'w':\n                    # hit_count = cache_stat.write_hit_count\n                    write_hit_rate = cache_stat.write_hit_rate\n                    hit_count = min(int(count*write_hit_rate), count)\n                else:\n                    # hit_count = cache_stat.read_hit_count\n                    # check fixed hit_rate\n                    read_hit_rate = cache_stat.read_hit_rate\n                    # logging.info(hit_rate)\n                    hit_count = min(int(count*read_hit_rate), count)\n            else:\n                hit_count = 0\n                hit_rate = 0\n\n            next_count = count - hit_count\n            if next_count > 0 and index < len(route)-2:\n                _die_id = die_id if route[index+1][-4:] != '_FAR' else (\n                    die_id+1) % self.config.inst_num.NUM_OF_DIE\n                _ref, _pre_ref, _post_ref, _end_ts, _count = yield from self._compute_sub_path(\n                    mem_stat, master, route, index+1, next_count, _last_bw_per_second, freq_cfg, _die_id, cluster_id, rw, hit_stat_dict, context, pre_ref, latency_stat)\n                end_ts = max(end_ts, _end_ts)\n\n        data_start_ts = 99999\n        data_end_ts = 0\n\n        if rw == 'r':\n            # no sub path\n            post_ref = _post_ref + post_lat  # compute backward latency for read\n        else:\n            post_ref = pre_ref\n\n        data_start_ts = min(data_start_ts, post_ref)\n        global seq\n        seq += 1\n        data_lat = self._compute_competition_latency_wrapper(\n            resource_ctx, count, cluster_id, bw, freq, post_ref)\n        _stat = {}\n\n        def __compute_esl(ref, src_gcu_id, tar_gcu_id, rw, count, memory_list):\n            lat, detail = yield from self.esl_switch.send(\n                ref,\n                src_gcu_id=src_gcu_id,\n                tar_gcu_id=tar_gcu_id,\n                rw=rw, data_size=count, memory_list=memory_list)\n            return lat, detail\n\n        if src == 'ESL' and rw == 'r':\n            _lat, esl_detail = yield from __compute_esl(post_ref, mem_stat.src_gcu_id, mem_stat.tar_gcu_id, 'w', count, mem_stat.remote_target_mem_access_list)\n            _stat['raw_data_lat'] = data_lat\n            _stat['esl_data_lat'] = _lat\n            data_lat = max(_lat, data_lat)\n            _stat['esl_detail'] = esl_detail\n\n        end_ts = max(end_ts, post_ref+data_lat)\n        data_end_ts = max(data_end_ts, post_ref+data_lat)\n\n        if resource_ctx:  # TODO: consider move below code to compute_competition_latency\n            if rw == 'r':\n                resource_ctx.acc_read += count\n            else:\n                resource_ctx.acc_write += count\n\n        _stat.update(\n            **{\"src\": src,\n               \"dst\": dst,\n               \"ref\": ref,\n               \"data_lat\": data_end_ts-data_start_ts,\n               \"data_start_ref\": data_start_ts,\n               \"data_end_ref\": data_end_ts,\n               \"pre_ref\": pre_ref,\n               \"post_ref\": post_ref,\n               \"end_ts\": end_ts,\n               \"count\": count,\n               \"last_bw_per_second\": last_bw_per_second,\n               \"_last_bw_per_second\": _last_bw_per_second,\n               \"required_bw\": bw*freq,\n               \"actual_bw\": count/data_lat,\n               \"seq\": seq,\n               }\n        )\n\n        latency_stat.append(_stat)\n        return ref, pre_ref, post_ref, end_ts, count\n\n    def compute_latency(\n        self,\n        context: BossaNovaContext,\n        mem_stat: DataflowActionMemoryStat,\n        die_id,\n        cluster_id,\n        ref: float,\n        hit_stat_dict: Dict[str, CacheStat],\n        freq_cfg: FreqConfig,\n        bw_factor=1\n    ):\n        master = mem_stat.master\n\n        src, dst, rw, total_count = mem_stat.src, mem_stat.dst, mem_stat.rw, mem_stat.total_count\n\n        leading_latency_group = self._compute_leading(\n            master, src, dst, freq_cfg)\n        total_leading_latency = 0\n        total_latency = 0\n        count = total_count\n        router = self._get_route(mem_stat.master, src, dst)\n        logger.debug(f\"({src},{dst},{rw})=>{router}\")\n        # calculate and accumlate latency for each sub path\n        # e.g. [\"L0\",\"L1C\",\"LLC\",\"MC\"]=> (\"L0\",\"L1C\"),(\"L1C\",\"LLC\"),(\"LLC\",\"MC\")\n        # bw_min = 9999\n        # last_bw_per_second = 0\n\n        # master_freq = self.freq_domain_map[src.upper()]\n        master_file: BWFile = getattr(\n            self.config.bw, master.value.lower())  # TODO: need review\n        master_freq = freq_cfg.get_freq(master_file.freq_domain)\n        _src, _dst, _ = leading_latency_group[0]\n        bw_file: BWFile = getattr(self.config.bw, _src.lower())\n        bw_ele: BWEle = getattr(bw_file, _dst.lower())\n        if rw == 'w':\n            spmd = self.config.dte.THREAD_NUMBER\n            # bw_dict = getattr(self.config.bw, master.value.lower())\n            bw_per_cycle = bw_ele.get_bw(\n                freq_cfg.get_freq(bw_file.freq_domain))\n            last_bytes_per_cycle = bw_per_cycle if master == DataflowActionType.XPU else bw_per_cycle*spmd\n        else:\n            # TODO: how to fix?\n            otsd = master_file.otsd\n            _count = total_count\n            round_chip_latency = 0\n            for _src, _dst, leading_latency in leading_latency_group:\n                is_cache_layer = _dst.upper() in hit_stat_dict\n                hit_rate = 0\n                if is_cache_layer:\n                    cache_stat = hit_stat_dict.get(_dst)\n                    hit_rate = cache_stat.read_hit_rate\n                round_chip_latency += _count*leading_latency  # count*[s]\n                _hit_count = min(int(_count*hit_rate), count)\n                _count -= _hit_count\n                if _count == 0:\n                    break\n            round_chip_latency = (round_chip_latency /\n                                  total_count)*master_freq  # [s]*[cycle/s]=[cycle]\n            # 256 * 128 / 5xx # 128：TODO: 取src到dst上最小的bw_per_cycle\n            last_bytes_per_cycle = otsd*128/round_chip_latency\n            bw_per_cycle = bw_ele.get_bw(\n                freq_cfg.get_freq(bw_file.freq_domain))\n            last_bytes_per_cycle = min(last_bytes_per_cycle, bw_per_cycle)\n            logger.debug(\"round_chip_latency=%f, effective bpc=%f\",\n                         round_chip_latency, last_bytes_per_cycle)\n        last_bw_per_second = last_bytes_per_cycle*master_freq*bw_factor\n\n        route = self._get_route(master, src, dst)\n        latency_stat = []\n        start_ts, pre_ref, post_ref, end_ts, _count = yield from self._compute_sub_path(\n            mem_stat, master, route, 0, count, last_bw_per_second, freq_cfg,\n            die_id, cluster_id, rw, hit_stat_dict, context, ref, latency_stat\n        )\n        total_leading_latency = post_ref-start_ts\n        total_latency = end_ts-start_ts\n\n        mem_stat.is_done = True\n        return total_latency, total_leading_latency, latency_stat",
    "start_line": 18,
    "end_line": 370,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DataTransportService",
    "component_id": "nova-platform.nova_platform.cost_service.compute.data_transport_service.DataTransportService"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerField": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerField",
    "name": "PowerField",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class PowerField(Field):\n    def __init__(self, default, default_factory, init, repr, hash, compare, metadata, kw_only, formula: PW_FORMULA_TYPE, pre_check: VALID_TYPE):\n        super().__init__(default, default_factory, init,\n                         repr, hash, compare, metadata, kw_only)\n        self.formula = formula\n        self.pre_check = pre_check",
    "start_line": 15,
    "end_line": 20,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Field"
    ],
    "class_name": null,
    "display_name": "class PowerField",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerField"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.power_field": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.power_field",
    "name": "power_field",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerField"
    ],
    "source_code": "def power_field(*, default=MISSING, default_factory=MISSING, init=True, repr=True,\n                hash=None, compare=True, metadata=None, kw_only=MISSING,\n                formula: PW_FORMULA_TYPE = None, pre_check=None):\n    if default is not MISSING and default_factory is not MISSING:\n        raise ValueError('cannot specify both default and default_factory')\n    if default is MISSING and default_factory is MISSING:\n        default = 0\n    return PowerField(default, default_factory, init, repr, hash, compare,\n                      metadata, kw_only, formula, pre_check)",
    "start_line": 23,
    "end_line": 31,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function power_field",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.power_field"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.sum_sub_items": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.sum_sub_items",
    "name": "sum_sub_items",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def sum_sub_items(ops_dict: Dict[DType, float], pw_cfg: Dict[DType, float]):\n    sum_energy = 0\n    for dtype, ops in ops_dict.items():\n        # compute_tensor_fp16_energy\n        energy_lib = pw_cfg[dtype]\n        sum_energy += ops*energy_lib\n    return sum_energy",
    "start_line": 36,
    "end_line": 42,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "ops_dict",
      "pw_cfg"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function sum_sub_items",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.sum_sub_items"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain",
    "name": "BasePowerDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class BasePowerDomain:\n    def __add__(self, o: 'BasePowerDomain'):\n        sum_obj = self.__class__()\n        for f in dataclasses.fields(self):\n            left = getattr(self, f.name)\n            right = getattr(o, f.name)\n            setattr(sum_obj, f.name, left+right)\n        return sum_obj\n\n    def __mul__(self, scalar):\n        mul_obj = self.__class__()\n        for f in dataclasses.fields(self):\n            val = getattr(self, f.name)*scalar\n            setattr(mul_obj, f.name, val)\n        return mul_obj",
    "start_line": 46,
    "end_line": 60,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BasePowerDomain",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_type": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_type",
    "name": "check_stat_type",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def check_stat_type(stat_type: DataflowActionMemoryStat | DataflowActionComputeStat):\n    def _check(stat: DataflowActionMemoryStat | DataflowActionComputeStat):\n        return isinstance(stat, stat_type)\n    return _check",
    "start_line": 63,
    "end_line": 66,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "stat_type"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_stat_type",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_type"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model._check": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model._check",
    "name": "_check",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "    def _check(stat: DataflowActionMemoryStat | DataflowActionComputeStat):\n        if isinstance(stat, DataflowActionComputeStat):\n            return False\n        if master != stat.master:\n            return False\n        if src == stat.src and dst == stat.dst:\n            return True\n        return False",
    "start_line": 97,
    "end_line": 104,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "stat"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _check",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model._check"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.check_master": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_master",
    "name": "check_master",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def check_master(master):\n    def _check(stat: DataflowActionMemoryStat | DataflowActionComputeStat):\n        return stat.master == master\n    return _check",
    "start_line": 69,
    "end_line": 72,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "master"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_master",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_master"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.check_list": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_list",
    "name": "check_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def check_list(*args):\n    def _check(stat):\n        for checker in args:\n            if not checker(stat):\n                return False\n        return True\n    return _check",
    "start_line": 75,
    "end_line": 81,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_list",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_list"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_src_dst": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_src_dst",
    "name": "check_stat_src_dst",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def check_stat_src_dst(master, src, dst) -> VALID_TYPE:\n    def _check(stat: DataflowActionMemoryStat | DataflowActionComputeStat):\n        if isinstance(stat, DataflowActionComputeStat):\n            return False\n        if master != stat.master:\n            return False\n        if src == stat.src and dst == stat.dst:\n            return True\n        return False\n\n    return _check",
    "start_line": 96,
    "end_line": 106,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "master",
      "src",
      "dst"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_stat_src_dst",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.check_stat_src_dst"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSIPDomain": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSIPDomain",
    "name": "PowerSIPDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.sum_sub_items",
      "nova-platform.nova_platform.cost_service.power.base_power_model.power_field"
    ],
    "source_code": "class PowerSIPDomain(BasePowerDomain):\n    # autopep8: off\n    xpu_2d_compute:                    float = power_field(\n        pre_check=CHECK_C,\n        formula=lambda stat, pw_cfg: sum_sub_items(\n            stat.compute_2d_ops, pw_cfg.sip.compute_2d_mac_energy)/2*pw_cfg.sip.voltage_scaling/stat.latency\n        # for 2d, the energy in power lib is energy/mac, thus a factor 2 is in above expression to convert ops to mac\n    )\n    xpu_1d_compute:                    float = power_field(\n        pre_check=CHECK_C,\n        formula=lambda stat, pw_cfg: sum_sub_items(\n            stat.compute_1d_ops, pw_cfg.sip.compute_1d_op_energy)*pw_cfg.sip.voltage_scaling/stat.latency\n    )\n    xpu_msf_compute:                   float = power_field(\n        pre_check=CHECK_C,\n        formula=lambda stat, pw_cfg: stat.compute_msf_ops *\n        pw_cfg.sip.compute_msf_op_energy*pw_cfg.sip.voltage_scaling/stat.latency\n    )\n    xpu_ld_st_l0_dsm_local:            float = power_field(\n        pre_check=CHECK_XPU_L0_DSM_LOCAL,\n        formula=lambda stat, pw_cfg: stat.total_count *\n        pw_cfg.sip.xpu_l0_dsm_local_energy*pw_cfg.sip.voltage_scaling/stat.latency\n    )\n    xpu_ld_st_l0_dsm_shared:           float = power_field(\n        pre_check=CHECK_XPU_L0_DSM_SHARED,\n        formula=lambda stat, pw_cfg: stat.total_count *\n        pw_cfg.sip.xpu_l0_dsm_shared_energy*pw_cfg.sip.voltage_scaling/stat.latency\n    )\n    xpu_ld_st_l0_l3:                   float = power_field(\n        pre_check=CHECK_XPU_L0_L3,\n        formula=lambda stat, pw_cfg: stat.total_count *\n        pw_cfg.sip.xpu_l0_l3_energy*pw_cfg.sip.voltage_scaling/stat.latency\n    )",
    "start_line": 138,
    "end_line": 170,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BasePowerDomain"
    ],
    "class_name": null,
    "display_name": "class PowerSIPDomain",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSIPDomain"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL1Domain": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL1Domain",
    "name": "PowerL1Domain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.power_field"
    ],
    "source_code": "class PowerL1Domain(BasePowerDomain):\n    # autopep8: off\n\n    xpu_ld_st_dsm_local_l3:            float = power_field( \n        pre_check=CHECK_XPU_DSM_LOCAL_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*(pw_cfg.l1.xpu_dsm_local_l3_energy+pw_cfg.l1.sip_master_energy)*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n    xpu_ld_st_dsm_shared_l3:           float = power_field( \n        pre_check=CHECK_XPU_DSM_SHARED_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*(pw_cfg.l1.xpu_dsm_shared_l3_energy+pw_cfg.l1.sip_master_energy)*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n    xpu_ld_st_dsm_shared_dsm_shared:   float = power_field( \n        pre_check=CHECK_XPU_DSM_SHARED_DSM_SHARED,\n        formula=lambda stat, pw_cfg: stat.total_count*(pw_cfg.l1.xpu_dsm_shared_dsm_shared_energy+pw_cfg.l1.sip_master_energy)*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n\n    xpu_ld_st_l3_l3:                   float = power_field( \n        pre_check=CHECK_XPU_L3_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*(pw_cfg.l1.xpu_l3_l3_energy+pw_cfg.l1.sip_master_energy)*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n\n    cdte_op_dsm_local_l3:              float = power_field( \n        pre_check=CHECK_DTE_DSM_LOCAL_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*pw_cfg.l1.dte_dsm_local_l3_energy*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n    cdte_op_dsm_shared_l3:             float = power_field( \n        pre_check=CHECK_DTE_DSM_SHARED_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*pw_cfg.l1.dte_dsm_shared_l3_energy*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n    cdte_op_dsm_shared_dsm_shared:     float = power_field( \n        pre_check=CHECK_DTE_DSM_SHARED_DSM_SHARED,\n        formula=lambda stat, pw_cfg: stat.total_count*pw_cfg.l1.dte_dsm_shared_dsm_shared_energy*pw_cfg.l1.voltage_scaling/stat.latency\n    )\n    cdte_op_l3_l3:                     float = power_field( \n        pre_check=CHECK_DTE_L3_L3,\n        formula=lambda stat, pw_cfg: stat.total_count*pw_cfg.l1.dte_l3_l3_energy*pw_cfg.l1.voltage_scaling/stat.latency\n    )",
    "start_line": 175,
    "end_line": 211,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BasePowerDomain"
    ],
    "class_name": null,
    "display_name": "class PowerL1Domain",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL1Domain"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.get_hit_rate": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.get_hit_rate",
    "name": "get_hit_rate",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "def get_hit_rate(stat: DataflowActionMemoryStat | DataflowActionComputeStat) -> float:\n    llc_stat = stat.cache_stat.get(\"LLC\", None)\n    if llc_stat == None:\n        return 0\n    if stat.rw == 'r':\n        hit_rate = llc_stat.read_hit_rate\n    else:\n        hit_rate = llc_stat.write_hit_rate\n    return hit_rate",
    "start_line": 215,
    "end_line": 223,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "stat"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_hit_rate",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.get_hit_rate"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.get_miss_rate": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.get_miss_rate",
    "name": "get_miss_rate",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.get_hit_rate"
    ],
    "source_code": "def get_miss_rate(stat) -> float:\n    return 1-get_hit_rate(stat)",
    "start_line": 226,
    "end_line": 227,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "stat"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_miss_rate",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.get_miss_rate"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSOCDomain": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSOCDomain",
    "name": "PowerSOCDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.power_field"
    ],
    "source_code": "class PowerSOCDomain(BasePowerDomain):\n    # autopep8: off\n    xpu_ld_st_l0_l3_llc_hit:            float = power_field( \n        pre_check=CHECK_XPU_L0_L3,\n        # LLC Hit Ratio * L3 Data Transcation * LLC Hit Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_HIT_SOC_PW\n    )\n    xpu_ld_st_l0_l3_llc_miss:           float = power_field( \n        pre_check=CHECK_XPU_L0_L3,\n        # (1-LLC Hit Ratio )* L3 Data Transcation * LLC Miss Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_MIS_SOC_PW\n    )\n    xpu_ld_st_dsm_local_l3_llc_hit:     float = power_field( \n        pre_check=CHECK_XPU_DSM_LOCAL_L3,\n        # LLC Hit Ratio * L3 Data Transcation * LLC Hit Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_HIT_SOC_PW\n    )\n    xpu_ld_st_dsm_local_l3_llc_miss:    float = power_field( \n        pre_check=CHECK_XPU_DSM_LOCAL_L3,\n        # (1-LLC Hit Ratio) * LLC Data Transcation * LLC Miss Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_MIS_SOC_PW\n    )\n    xpu_ld_st_dsm_shared_l3_llc_hit:    float = power_field( \n        pre_check=CHECK_XPU_DSM_SHARED_L3,\n        # LLC Hit Ratio * L3 Data Transcation * LLC Hit Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_HIT_SOC_PW\n    )\n    xpu_ld_st_dsm_shared_l3_llc_miss:   float = power_field( \n        pre_check=CHECK_XPU_DSM_SHARED_L3,\n        # (1-LLC Hit Ratio) * LLC Data Transcation * LLC Miss Data energy lib  *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_XPU_MIS_SOC_PW\n    )\n    xpu_ld_st_l3_l3:                    float = power_field( \n        pre_check=CHECK_XPU_L3_L3,\n        # (LLC read HIT Data Transcation *  LLC Hit Data energy lib + LLC read Miss Data Transcation *  LLC Miss Data energy lib + LLC write HIT Data Transcation *  LLC Hit Data energy lib + LLC write Miss Data Transcation *  LLC Miss Data energy lib) *Voltage Scaling / Data Transcation execution time \n        # we treat L3-L3 as two stat whcih are L3-L3 read mem stat and L3-L3 write mem stat\n        formula=(\n            lambda stat, pw_cfg:\n                FORMULA_XPU_HIT_SOC_PW(stat,pw_cfg)+FORMULA_XPU_MIS_SOC_PW(stat,pw_cfg)\n        )\n    )\n\n    cdte_op_dsm_local_l3_llc_hit:       float = power_field( \n        pre_check=CHECK_DTE_DSM_LOCAL_L3,\n        # LLC Hit Ratio * LLC Data Transcation * ( LLC Hit Data energy lib + CDTE Master energy lib ) *Voltage Scaling/ Data Transcation execution time \n        formula=FORMULA_DTE_HIT_SOC_PW\n    )\n    cdte_op_dsm_local_l3_llc_miss:      float = power_field( \n        pre_check=CHECK_DTE_DSM_LOCAL_L3,\n        formula=FORMULA_DTE_MIS_SOC_PW\n    )\n    cdte_op_dsm_shared_l3_llc_hit:      float = power_field( \n        pre_check=CHECK_DTE_DSM_SHARED_L3,\n        formula=FORMULA_DTE_HIT_SOC_PW\n    )\n    cdte_op_dsm_shared_l3_llc_miss:     float = power_field( \n        pre_check=CHECK_DTE_DSM_SHARED_L3,\n        formula=FORMULA_DTE_MIS_SOC_PW\n    )\n    cdte_op_dsm_shared_dsm_shared:      float = power_field( \n        pre_check=CHECK_DTE_DSM_SHARED_DSM_SHARED,\n        # (LLC read HIT Data Transcation *  LLC Hit Data energy lib + LLC read Miss Data Transcation *  LLC Miss Data energy lib + LLC write HIT Data Transcation *  LLC Hit Data energy lib + LLC write Miss Data Transcation *  LLC Miss Data energy lib + CDTE Master energy lib* LLC Data Transcation ) *Voltage Scaling / Data Transcation execution time \n        # we treat L3-L3 as two stat whcih are L3-L3 read mem stat and L3-L3 write mem stat\n        formula=(\n            lambda stat, pw_cfg:\n                FORMULA_DTE_HIT_SOC_PW(stat,pw_cfg)+FORMULA_DTE_HIT_SOC_PW(stat,pw_cfg)\n        )\n    )",
    "start_line": 263,
    "end_line": 330,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BasePowerDomain"
    ],
    "class_name": null,
    "display_name": "class PowerSOCDomain",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSOCDomain"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerMemDomain": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerMemDomain",
    "name": "PowerMemDomain",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.BasePowerDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.power_field"
    ],
    "source_code": "class PowerMemDomain(BasePowerDomain):\n    hbm: float = power_field(\n        pre_check=lambda stat: True,\n        formula=lambda stat, pw_stat: 0\n    )",
    "start_line": 336,
    "end_line": 340,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BasePowerDomain"
    ],
    "class_name": null,
    "display_name": "class PowerMemDomain",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerMemDomain"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerStat": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerStat",
    "name": "PowerStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSOCDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerSIPDomain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerStat",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL1Domain",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerMemDomain"
    ],
    "source_code": "class PowerStat:\n    sip: PowerSIPDomain = field(default_factory=lambda: PowerSIPDomain())\n    l1: PowerL1Domain = field(default_factory=lambda: PowerL1Domain())\n    soc: PowerSOCDomain = field(default_factory=lambda: PowerSOCDomain())\n    mem: PowerMemDomain = field(default_factory=lambda: PowerMemDomain())\n\n    def __add__(self, o: 'PowerStat'):\n        return PowerStat(\n            sip=self.sip+o.sip,\n            l1=self.l1+o.l1,\n            soc=self.soc+o.soc,\n            mem=self.mem+o.mem\n        )\n\n    def __mul__(self, scalar):\n        return PowerStat(\n            sip=self.sip*scalar,\n            l1=self.l1*scalar,\n            soc=self.soc*scalar,\n            mem=self.mem*scalar\n        )",
    "start_line": 344,
    "end_line": 364,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class PowerStat",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerStat"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerFrame": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerFrame",
    "name": "PowerFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class PowerFrame(BaseFrame):\n    power: float = 0\n\n    def incr(self, frame: 'PowerFrame'):\n        self.power += frame.power",
    "start_line": 368,
    "end_line": 372,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class PowerFrame",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerFrame"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerActiveFrame": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerActiveFrame",
    "name": "PowerActiveFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class PowerActiveFrame(BaseFrame):\n    active: bool = False\n\n    def incr(self, frame: 'PowerActiveFrame'):\n        self.active |= frame.active",
    "start_line": 376,
    "end_line": 380,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class PowerActiveFrame",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerActiveFrame"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.ActiveFrame": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.ActiveFrame",
    "name": "ActiveFrame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class ActiveFrame(BaseFrame):\n    active: bool = False\n\n    def incr(self, frame: 'ActiveFrame'):\n        self.active |= frame.active",
    "start_line": 384,
    "end_line": 388,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class ActiveFrame",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.ActiveFrame"
  },
  "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL3Frame": {
    "id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL3Frame",
    "name": "PowerL3Frame",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/base_power_model.py",
    "depends_on": [],
    "source_code": "class PowerL3Frame(BaseFrame):\n    l3_power: int = field(default=0)  # Byte/cycle",
    "start_line": 392,
    "end_line": 393,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseFrame"
    ],
    "class_name": null,
    "display_name": "class PowerL3Frame",
    "component_id": "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL3Frame"
  },
  "nova-platform.nova_platform.cost_service.power.power_cost_service.duration": {
    "id": "nova-platform.nova_platform.cost_service.power.power_cost_service.duration",
    "name": "duration",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "depends_on": [],
    "source_code": "def duration(func):\n    def wrap(*args, **kwargs):\n        start = time.perf_counter_ns()\n        result = func(*args, **kwargs)\n        end = time.perf_counter_ns()\n        print(func.__name__, f\"{(end-start)/1e6:.2f}ms\")\n        return result\n    return wrap",
    "start_line": 17,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "func"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function duration",
    "component_id": "nova-platform.nova_platform.cost_service.power.power_cost_service.duration"
  },
  "nova-platform.nova_platform.cost_service.power.power_cost_service.wrap": {
    "id": "nova-platform.nova_platform.cost_service.power.power_cost_service.wrap",
    "name": "wrap",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "depends_on": [],
    "source_code": "    def wrap(*args, **kwargs):\n        start = time.perf_counter_ns()\n        result = func(*args, **kwargs)\n        end = time.perf_counter_ns()\n        print(func.__name__, f\"{(end-start)/1e6:.2f}ms\")\n        return result",
    "start_line": 18,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function wrap",
    "component_id": "nova-platform.nova_platform.cost_service.power.power_cost_service.wrap"
  },
  "nova-platform.nova_platform.cost_service.power.power_cost_service.merge_timeline": {
    "id": "nova-platform.nova_platform.cost_service.power.power_cost_service.merge_timeline",
    "name": "merge_timeline",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "depends_on": [],
    "source_code": "def merge_timeline(pairs, agg_fun, init_fac):\n    ts_map = defaultdict(list)\n    for key, timeline in pairs:\n        for frame in timeline.data:\n            ts_map[frame.begin].append((0, key, frame))\n            ts_map[frame.end].append((1, key, frame))\n\n    ts_list = sorted(ts_map.keys())\n    ts_res = [init_fac() for _ in range(len(ts_list))]\n    ts_len = len(ts_list)\n    progress = 0\n    for i, ts in enumerate(ts_list):\n        frame_list = ts_map[ts]\n        for flag, key, frame in frame_list:\n            if flag == 1:\n                continue\n            e = frame.end\n            val = agg_fun(key, frame)\n            for offset, ts_end in enumerate(ts_list[i:]):\n                if ts_end < e:\n                    for k, _v in enumerate(val):\n                        ts_res[i+offset][k] += _v\n                else:\n                    break\n        _progress = int((i+1)/ts_len*100)\n        if _progress > progress:\n            progress = _progress\n            logger.info(\"merge_timeline progress %d%%\" % progress)\n    return ts_list, ts_res",
    "start_line": 28,
    "end_line": 56,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "pairs",
      "agg_fun",
      "init_fac"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function merge_timeline",
    "component_id": "nova-platform.nova_platform.cost_service.power.power_cost_service.merge_timeline"
  },
  "nova-platform.nova_platform.cost_service.power.power_cost_service.PowerCostService": {
    "id": "nova-platform.nova_platform.cost_service.power.power_cost_service.PowerCostService",
    "name": "PowerCostService",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "relative_path": "nova-platform/nova_platform/cost_service/power/power_cost_service.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerFrame",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerStat",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.PowerContext",
      "nova-platform.nova_platform.cost_service.power.base_power_model.PowerL3Frame",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.EDCFrame",
      "nova-platform.nova_platform.cost_service.power.power_cost_service.merge_timeline",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.Timeline"
    ],
    "source_code": "class PowerCostService(BaseCostService):\n    def __init__(self, config: BossaNovaConfig):\n        super().__init__(config)\n\n    def init_context(self):\n        die_num=self.config.inst_num.NUM_OF_DIE\n        cluster_num=self.config.inst_num.NUM_OF_CLUSTER\n        sip_num=self.config.inst_num.NUM_OF_CORE_PER_CLUSTER\n        sip_power_timeline_dict={}\n        l1_power_timeline_dict={}\n        for die_id,cluster_id,sip_id in itertools.product(\n            range(die_num),\n            range(cluster_num),\n            range(sip_num),\n        ):\n            key=die_id,cluster_id,sip_id\n            sip_power_timeline_dict[key]=Timeline([PowerFrame()])\n            l1_power_timeline_dict[key]=Timeline([PowerFrame()])\n\n\n        power_context=PowerContext(\n            sip_power_timeline_dict=sip_power_timeline_dict,\n            l1_power_timeline_dict=l1_power_timeline_dict,\n            # soc_power_timeline_dict={die_id:Timeline([PowerFrame()]) for die_id in range(die_num)}\n        )\n        return power_context\n\n    def _compute_domain_power(self,power_domain_stat, act_stat: DataflowActionComputeStat | DataflowActionMemoryStat):\n        sum_power = 0\n        for f in dataclasses.fields(power_domain_stat):\n            f: PowerField\n            if f.pre_check(act_stat):\n                fun = f.formula\n                val=fun(act_stat, self.config.power) if act_stat.latency else 0\n                setattr(power_domain_stat,f.name,val)\n                sum_power += val\n        sum_power /= 1e12  # [pW] -> [W]\n        return sum_power\n\n    def process(self, action: DataflowAction, context: BossaNovaContext, ref: float) -> Generator[bool, BaseActionStat, None]:\n        die_id=action.get_die_id()\n        cluster_id=action.get_cluster_id()\n        engine_id = action.get_local_engine_id() if action.get_action_type()!=DT.CDTE else 0\n        action_type = action.get_action_type()\n        key=(die_id,cluster_id,engine_id)\n        cost_book=context.get_cost_book(action)\n        cost_book.power_stat=PowerStat()\n        sub_power_stat=[]\n        while True:\n            act_stat = yield\n            if not act_stat:\n                break\n            power_stat=PowerStat()\n            sub_power_stat.append((power_stat,act_stat.latency))\n            act_stat.power_stat=power_stat\n\n            # sip\n            sum_sip_power=self._compute_domain_power(power_stat.sip,act_stat)\n            # volt = self.config.power.dtu.voltage\n            context.power_context.sip_power_timeline_dict[key].insert(PowerFrame(\n                ref+act_stat.relative_ts,\n                ref+act_stat.relative_ts+act_stat.latency,\n                power=sum_sip_power\n            ))\n\n            # l1\n            sum_l1_power=self._compute_domain_power(power_stat.l1,act_stat)\n            # volt = self.config.power.dtu.voltage\n            context.power_context.l1_power_timeline_dict[key].insert(PowerFrame(\n                ref+act_stat.relative_ts,\n                ref+act_stat.relative_ts+act_stat.latency,\n                power=sum_l1_power\n            ))\n\n            # soc\n            sum_soc_energy=self._compute_domain_power(power_stat.soc,act_stat)\n            # context.power_context.soc_power_timeline_dict[die_id].insert(PowerFrame(\n            #     ref+act_stat.relative_ts,\n            #     ref+act_stat.relative_ts+act_stat.latency,\n            #     power=sum_soc_power\n            # )) \n\n            soc_volt = self.config.power.soc.voltage\n            soc_current = sum_soc_energy/soc_volt\n            context.power_context.soc_edc.insert(EDCFrame(\n                ref+act_stat.relative_ts,\n                ref+act_stat.relative_ts+act_stat.latency,\n                soc_current\n            ))\n\n        tot=PowerStat()\n        if cost_book.latency:\n            for sub,lat in sub_power_stat:\n                sub: PowerStat\n                tot+=sub*lat\n            tot=tot*(1/(cost_book.latency*1e12))\n        cost_book.power_stat=tot\n\n    def _derive_sip_power_timeline(self, context: BossaNovaContext):\n        sip_idle_power=self.config.power.sip.idle_power/1e12\n        sip_leakage_power=self.config.power.sip.leakage_power/1e12\n        sip_plc_power=self.config.power.sip.plc_power/1e12\n\n        DIE_NUM=self.config.inst_num.NUM_OF_DIE\n        sip_idle_energy=[0]*DIE_NUM\n        sip_active_energy=[0]*DIE_NUM\n        sip_plc_energy=[0]*DIE_NUM\n        sip_leakage_energy=[0]*DIE_NUM\n        total_latency=context.post_stat.total_latency\n        total_items=len(context.power_context.sip_power_timeline_dict)\n        item_count=0\n        tot_sip_dur=0\n        def agg_fun(key,pw_frame):\n            nonlocal tot_sip_dur\n            die_id,cluster_id,sip_id=key\n            # sip \n            sip_power=pw_frame.power\n            sip_dur=pw_frame.dur if pw_frame.end<MAX_TIME else total_latency-pw_frame.begin\n            if sip_power:\n                # active\n                sip_active_energy[die_id]+=sip_power*sip_dur\n                sip_plc_energy[die_id]+=sip_plc_power*sip_dur\n                sip_power+=sip_plc_power+sip_leakage_power\n                tot_sip_dur+=sip_dur\n            else:\n                sip_idle_energy[die_id]+=sip_idle_power*sip_dur\n                sip_power+=sip_idle_power+sip_leakage_power\n\n            sip_leakage_energy[die_id]+=sip_leakage_power*sip_dur\n            current=sip_power/self.config.power.sip.voltage\n            return sip_power, current\n\n        logger.info(\"start to merge sip power timeline\")\n        ts_list,ts_res=merge_timeline(context.power_context.sip_power_timeline_dict.items(),agg_fun,lambda:[0,0])\n        dtu_power_timeline_data_part1=[PowerFrame(ts_list[i],ts_list[i+1],ts_res[i][0]) for i in range(len(ts_list)-1)]\n        dtu_edc_timeline_data_part1=[EDCFrame(ts_list[i],ts_list[i+1],ts_res[i][1]) for i in range(len(ts_list)-1)]\n\n        sip_util=tot_sip_dur/total_latency/len(context.power_context.sip_power_timeline_dict)\n        # update edc\n        # for dtu_frame in context.power_context.dtu_power_timeline.data:\n        #     volt = self.config.power.dtu.voltage\n        #     current = dtu_frame.power/volt\n        #     context.power_context.dtu_edc.insert(EDCFrame(\n        #         dtu_frame.begin,\n        #         dtu_frame.end,\n        #         current\n        #     ))\n        sip_active_power_tot=[e/total_latency for e in sip_active_energy]\n        sip_idle_power_tot=[e/total_latency for e in sip_idle_energy]\n        sip_leakage_power_tot=[e/total_latency for e in sip_leakage_energy]\n        sip_plc_power_tot=[e/total_latency for e in sip_plc_energy]\n        return sip_active_power_tot,sip_idle_power_tot,sip_leakage_power_tot,sip_plc_power_tot,sip_util,dtu_power_timeline_data_part1,dtu_edc_timeline_data_part1\n\n    def _derive_l1_power_timeline(self, context: BossaNovaContext):\n        l1_idle_power=self.config.power.l1.idle_power/1e12\n        l1_leakage_power=self.config.power.l1.leakage_power/1e12\n        DIE_NUM=self.config.inst_num.NUM_OF_DIE\n\n        l1_active_energy=[0]*DIE_NUM\n        l1_leakage_energy=[0]*DIE_NUM\n        l1_idle_energy=[0]*DIE_NUM\n        total_latency=context.post_stat.total_latency\n        total_items=len(context.power_context.sip_power_timeline_dict)\n        item_count=0\n        # with open(\"l1_power_timeline.timeline\",\"wb\") as f:\n        #     import pickle\n        #     pickle.dump(context.power_context.l1_power_timeline_dict,f)\n\n\n        def agg_fun(key,pw_frame):\n            die_id,cluster_id,sip_id=key\n            l1_power=pw_frame.power\n            l1_dur=pw_frame.dur if pw_frame.end<MAX_TIME else total_latency-pw_frame.begin\n            if l1_power:\n                # active\n                l1_active_energy[die_id]+=l1_power*l1_dur\n                l1_power+=l1_leakage_power\n            else:\n                l1_idle_energy[die_id]+=l1_idle_power*l1_dur\n                l1_power+=l1_idle_power+l1_leakage_power\n            l1_leakage_energy[die_id]+=l1_leakage_power*l1_dur\n\n            l1_power/=1e12\n            current=l1_power/self.config.power.l1.voltage\n            return l1_power,current\n\n        logger.info(\"start to merge l1 power timeline\")\n\n        ts_list,ts_res=merge_timeline(context.power_context.l1_power_timeline_dict.items(),agg_fun,lambda:[0,0])\n        dtu_power_timeline_data_part2=[PowerFrame(ts_list[i],ts_list[i+1],ts_res[i][0]) for i in range(len(ts_list)-1)]\n        dtu_edc_timeline_data_part2=[EDCFrame(ts_list[i],ts_list[i+1],ts_res[i][1]) for i in range(len(ts_list)-1)]\n\n        l1_active_power_tot=[e/total_latency for e in l1_active_energy]\n        l1_idle_power_tot=[e/total_latency for e in l1_idle_energy]\n        l1_leakage_power_tot=[e/total_latency for e in l1_leakage_energy]\n        return l1_active_power_tot,l1_idle_power_tot,l1_leakage_power_tot, dtu_power_timeline_data_part2,dtu_edc_timeline_data_part2\n\n\n    def _derive_soc_power_timeline(self, context: BossaNovaContext):\n        soc_idle_power=self.config.power.soc.mc_idle_power/1e12\n        soc_leakage_power=self.config.power.dtu.leakage_power/1e12\n        for key,pw_timeline in context.power_context.soc_power_timeline_dict.items():\n            for pw_frame in pw_timeline.data:\n                # sip \n                if pw_frame.power:\n                    # active\n                    pw_frame.power+=xpu_plc_power+soc_leakage_power\n                else:\n                    pw_frame.power+=xpu_idle_power+soc_leakage_power\n                context.power_context.soc_power_timeline.insert(pw_frame) \n\n\n    def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n        # TODO: dual_die\n        die_id=0\n        mem_bw: BWResource = context.bw_resource_context.get_bw_resource(\n            'mc', 'l3', die_id, 0, None, 'r')\n\n        sip_active_power,sip_idle_power,sip_leakage_power,sip_plc_power,sip_util,dtu_pw_part1,dtu_edc_part1=self._derive_sip_power_timeline(context)\n        l1_active_power,l1_idle_power,l1_leakage_power,dtu_pw_part2,dtu_edc_part2=self._derive_l1_power_timeline(context)\n        # self._derive_soc_power_timeline(context)\n        logger.info(\"sip_active_power=%s\",[f\"{e/1e12:03.2f}\" for e in sip_active_power])\n        logger.info(\"sip_idle_power=%s\",[f\"{e/1e12:03.2f}\" for e in sip_idle_power])\n        logger.info(\"sip_leakage_power=%s\",[f\"{e/1e12:03.2f}\" for e in sip_leakage_power])\n        logger.info(\"sip_plc_power=%s\",[f\"{e/1e12:03.2f}\" for e in sip_plc_power])\n        logger.info(\"l1_active_power=%s\",[f\"{e/1e12:03.2f}\" for e in l1_active_power])\n        logger.info(\"l1_idle_power=%s\",[f\"{e/1e12:03.2f}\" for e in l1_idle_power])\n        logger.info(\"l1_leakage_power=%s\",[f\"{e/1e12:03.2f}\" for e in l1_leakage_power])\n\n        logger.info(\"start to merge dtu power timeline\")\n        dtu_pw_dict={\n            \"part1\": Timeline(dtu_pw_part1),\n            \"part2\": Timeline(dtu_pw_part2),\n        }\n        ts_list,ts_res=merge_timeline(dtu_pw_dict.items(),lambda k,frame:[frame.power],lambda:[0])\n        context.power_context.dtu_power_timeline.data=[PowerFrame(ts_list[i],ts_list[i+1],ts_res[i][0]) for i in range(len(ts_list)-1)]\n\n        logger.info(\"start to merge dtu edc timeline\")\n        dtu_edc_dict={\n            \"part1\": Timeline(dtu_edc_part1),\n            \"part2\": Timeline(dtu_edc_part2),\n        }\n        ts_list,ts_res=merge_timeline(dtu_edc_dict.items(),lambda k,frame:[frame.current],lambda:[0])\n        context.power_context.dtu_edc.data=[EDCFrame(ts_list[i],ts_list[i+1],ts_res[i][0]) for i in range(len(ts_list)-1)]\n\n        for frame in mem_bw.timeline.data:\n            util = frame.allocated_bw/mem_bw.max_bw\n            total_power = util*self.config.power.mem.hbm_active_power + \\\n                (1-util)*self.config.power.mem.hbm_idle_power\n            total_power /= 1e12\n            context.power_context.l3_power.append(PowerL3Frame(\n                frame.begin, frame.end, total_power))\n\n        domain_total_energy = defaultdict(float)\n        for id, cost_book in context.cost_dict.items():\n            for domain_name, domain in asdict(cost_book.power_stat).items():\n                domain_total_energy[domain_name] += sum(domain.values()) * \\\n                    cost_book.latency\n        for frame in context.power_context.l3_power:\n            dur = frame.dur if frame.dur < 999 else 0\n            domain_total_energy[\"mem\"] += dur*frame.l3_power\n\n        # TODO: need to implement ESL power calculation\n        domain_total_energy[\"esl\"] = 0\n\n        # total_avg_power = 0\n        for k in list(domain_total_energy):\n            energy = domain_total_energy.pop(k)\n            domain_total_energy[k+\":active_energy(J)\"] = energy\n            domain_total_energy[k+\":active_power(W)\"] = energy / \\\n                context.post_stat.total_latency\n\n        # DTU Domain\n        DIE_NUM=self.config.inst_num.NUM_OF_DIE\n        for die_id in range(DIE_NUM):\n            die_key=f'die:{die_id}'\n            domain_total_energy[die_key]={}\n            data_dict=domain_total_energy[die_key]\n            data_dict[\"xpu:compute_power(W)\"] = sip_active_power[die_id]+sip_plc_power[die_id]+sip_leakage_power[die_id]+sip_idle_power[die_id]\n            data_dict[\"l1:dataflow_power(W)\"] = l1_active_power[die_id]+l1_leakage_power[die_id]+l1_idle_power[die_id]\n            data_dict[\"dtu:idle_power(W)\"] = sip_idle_power[die_id]+l1_idle_power[die_id]\n            data_dict[\"dtu:leakage_power(W)\"] = sip_leakage_power[die_id]+l1_leakage_power[die_id]\n            data_dict[\"dtu:active_power(W)\"] = sip_active_power[die_id]+l1_active_power[die_id]\n            data_dict[\"dtu:total_power(W)\"] = data_dict[\"xpu:compute_power(W)\"]+data_dict[\"l1:dataflow_power(W)\"]\n\n        domain_total_energy[\"dtu:idle_power(W)\"] = sum(sip_idle_power)+sum(l1_idle_power)\n        domain_total_energy[\"dtu:active_power(W)\"] = sum(sip_active_power)+sum(l1_active_power)\n        domain_total_energy[\"dtu:leakage_power(W)\"] = sum(sip_leakage_power)+sum(l1_leakage_power)\n        domain_total_energy[\"dtu:total_power(W)\"] = sum([domain_total_energy[f'die:{die_id}'][\"dtu:total_power(W)\"] for die_id in range(DIE_NUM)])\n\n\n        # SOC\n        domain_total_energy[\"soc:idle_power(W)\"] = (\n            (1-context.post_stat.l3_rw_bw_util) *\n            self.config.power.soc.mc_idle_power\n            + (1-context.post_stat.sic_io_rw_bw_util) *\n            self.config.power.soc.dataflow_idle_power\n            + (1-context.post_stat.esl_bw_util) *\n            self.config.power.soc.esl_idle_power\n        )*DIE_NUM/1e12\n        domain_total_energy[\"soc:leakage_power(W)\"] = self.config.power.soc.leakage_power*DIE_NUM/1e12\n        domain_total_energy[\"soc:other_power(W)\"] = self.config.power.soc.other_power*DIE_NUM/1e12\n        domain_total_energy[\"soc:total_power(W)\"] = (domain_total_energy[\"soc:active_power(W)\"]\n                                                     + domain_total_energy[\"soc:idle_power(W)\"]\n                                                     + domain_total_energy[\"soc:leakage_power(W)\"]\n                                                     + domain_total_energy[\"soc:other_power(W)\"]\n                                                     )\n\n        domain_total_energy[\"mem:idle_power(W)\"] = (\n            1-context.post_stat.l3_rw_bw_util)*self.config.power.mem.hbm_idle_power/1e12\n        domain_total_energy[\"mem:leakage_power(W)\"] = self.config.power.mem.leakage_power/1e12\n        domain_total_energy[\"mem:total_power(W)\"] = (domain_total_energy[\"mem:active_power(W)\"]\n                                                     + domain_total_energy[\"mem:idle_power(W)\"]\n                                                     + domain_total_energy[\"mem:leakage_power(W)\"]\n                                                     )\n\n        domain_total_energy[\"esl:idle_power(W)\"] = (\n            1-context.post_stat.esl_bw_util)*self.config.power.esl.idle_power/1e12\n        domain_total_energy[\"esl:leakage_power(W)\"] = self.config.power.esl.leakage_power/1e12\n        domain_total_energy[\"esl:total_power(W)\"] = (domain_total_energy[\"esl:active_power(W)\"]\n                                                     + domain_total_energy[\"esl:idle_power(W)\"]\n                                                     + domain_total_energy[\"esl:leakage_power(W)\"]\n                                                     )\n\n        # d2d\n        if DIE_NUM>1:\n            domain_total_energy[\"d2d:active_power(W)\"] = (\n                context.post_stat.d2d_tx_rw_bw_util*self.config.power.d2d.active_power/1e12\n            )\n            domain_total_energy[\"d2d:idle_power(W)\"] = (\n                1-context.post_stat.d2d_tx_rw_bw_util)*self.config.power.d2d.idle_power/1e12\n            domain_total_energy[\"d2d:leakage_power(W)\"] = self.config.power.d2d.leakage_power/1e12\n            domain_total_energy[\"d2d:total_power(W)\"] = (domain_total_energy[\"d2d:active_power(W)\"]\n                                                        + domain_total_energy[\"d2d:idle_power(W)\"]\n                                                        + domain_total_energy[\"d2d:leakage_power(W)\"]\n                                                        )     \n\n        domain_total_energy[\"total:asic_power(W)\"] = (\n            domain_total_energy[\"dtu:total_power(W)\"]\n            + domain_total_energy[\"soc:total_power(W)\"]\n            + domain_total_energy[\"mem:total_power(W)\"]\n            + domain_total_energy[\"esl:total_power(W)\"]\n            + domain_total_energy[\"d2d:total_power(W)\"]\n        )\n        domain_total_energy[\"total:board_power(W)\"] = domain_total_energy[\"total:asic_power(W)\"] / \\\n            self.config.power.board_efficiency\n\n        for k in domain_total_energy:\n            if \"(J)\" in k:\n                domain_total_energy[k] = round(domain_total_energy[k]/1e12, 6)\n        domain_total_energy = sorted(domain_total_energy.items())\n        res = dict(domain_total_energy)\n        res['sip_util']=sip_util\n        # calculate total latency affected by edc\n        EDC_LIMIT = context.bw_resource_context.config.power.dtu_edc\n        base_freq = context.bw_resource_context.config.freq.CORE\n        sum_edc_total_latency = 0\n        edc_acc_dict = defaultdict(lambda: 0)\n        raw_edc_total_latency=0\n        for edc_frame in context.power_context.dtu_edc.data[:-1]:\n            edc_dtu_freq = min(EDC_LIMIT/edc_frame.current, 1) * \\\n                base_freq if edc_frame.current else base_freq  # MHz\n            edc_dtu_freq = ((edc_dtu_freq*1000)//50)*50 / 1000\n            sum_edc_total_latency += edc_frame.dur*base_freq/edc_dtu_freq if edc_dtu_freq else 0\n            raw_edc_total_latency += edc_frame.dur\n            edc_acc_dict[edc_dtu_freq] += edc_frame.dur\n        context.post_stat.edc.edc_total_latency = sum_edc_total_latency\n        context.post_stat.edc.edc_acc_dict = dict(edc_acc_dict)\n        if raw_edc_total_latency != 0:\n             context.post_stat.edc.edc_incr_percent = (\n            sum_edc_total_latency-raw_edc_total_latency)/raw_edc_total_latency*100\n        else:\n            context.post_stat.edc.edc_incr_percent = 0  # or another appropriate default value\n        return res\n\n    def get_edc_freq(self, ref, context: BossaNovaContext) -> FreqConfig:\n        base_freq_cfg = context.bw_resource_context.config.freq\n        return base_freq_cfg",
    "start_line": 60,
    "end_line": 437,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCostService"
    ],
    "class_name": null,
    "display_name": "class PowerCostService",
    "component_id": "nova-platform.nova_platform.cost_service.power.power_cost_service.PowerCostService"
  },
  "nova-platform.nova_platform.data_visual.__init__.AbstractPostProcessor": {
    "id": "nova-platform.nova_platform.data_visual.__init__.AbstractPostProcessor",
    "name": "AbstractPostProcessor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/data_visual/__init__.py",
    "relative_path": "nova-platform/nova_platform/data_visual/__init__.py",
    "depends_on": [],
    "source_code": "class AbstractPostProcessor:\n    def get_trace_generator(self) -> TraceGenerator:\n        raise NotImplementedError()",
    "start_line": 4,
    "end_line": 6,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class AbstractPostProcessor",
    "component_id": "nova-platform.nova_platform.data_visual.__init__.AbstractPostProcessor"
  },
  "nova-platform.nova_platform.data_visual.post_processor.ts_convert": {
    "id": "nova-platform.nova_platform.data_visual.post_processor.ts_convert",
    "name": "ts_convert",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/data_visual/post_processor.py",
    "relative_path": "nova-platform/nova_platform/data_visual/post_processor.py",
    "depends_on": [],
    "source_code": "def ts_convert(ns):\n    return round(ns*1e6, 6)",
    "start_line": 20,
    "end_line": 21,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "ns"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function ts_convert",
    "component_id": "nova-platform.nova_platform.data_visual.post_processor.ts_convert"
  },
  "nova-platform.nova_platform.data_visual.post_processor.PostProcessor": {
    "id": "nova-platform.nova_platform.data_visual.post_processor.PostProcessor",
    "name": "PostProcessor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/data_visual/post_processor.py",
    "relative_path": "nova-platform/nova_platform/data_visual/post_processor.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.post_stat",
      "nova-platform.nova_platform.perfetto_protobuf._profile.count",
      "nova-platform.nova_platform.base_model.PostStat",
      "nova-platform.nova_platform.data_visual.post_processor.ts_convert"
    ],
    "source_code": "class PostProcessor(AbstractPostProcessor):\n    def __init__(self, outdir=\"./\", tgen=None) -> None:\n        self.outdir = outdir\n        self.tgen = tgen\n        self.visited_ref = {}\n        self.post_stat: PostStat = None\n\n    def get_trace_generator(self) -> TraceGenerator:\n        return self.tgen\n\n    def _get_post_stat(self, dataflow, context, config):\n        post_stat = PostStat()\n        cost_dict = context.cost_dict\n        action_end_time, longest_path = 0, []\n        visited = {}\n        for action_id in dataflow._roots:\n            l, p = self._get_longest(\n                action_id, cost_dict, dataflow.dag, visited)\n            if l > action_end_time:\n                action_end_time = l\n                longest_path = p\n\n        post_stat.action_end_time = action_end_time\n        l3_end_time = 0\n        for die_id in range(config.inst_num.NUM_OF_DIE):\n            bw_resource = context.bw_resource_context.l3_dict[die_id]\n            last_frame = bw_resource.get_last_valid_frame()\n            if last_frame:\n                l3_end_time = max(l3_end_time, last_frame.end) - \\\n                    context.initial_ref\n\n        total_latency = max(action_end_time, l3_end_time)\n        post_stat.total_latency = total_latency\n\n        self.longest_path = longest_path\n        self.edge_pairs = [(longest_path[i], longest_path[i+1])\n                           for i in range(len(longest_path)-1)]\n\n        bw_total_stat = defaultdict(float)\n        bw_max_bw = defaultdict(float)\n        for (lvl, rw, die_id, cluster_id, esl_port_id, bw_resource) in context.bw_resource_context.get_unique_bw_resource_list():\n            key = f\"{lvl}_{rw}_bw_util\"\n            bw_max_bw[key] += bw_resource.max_bw\n            for frame in bw_resource.timeline.data:\n                bw_total_stat[key] += frame.dur*frame.allocated_bw\n\n            if rw != 'rw':\n                key_rw = f\"{lvl}_rw_bw_util\"\n                bw_max_bw[key_rw] += bw_resource.max_bw\n                for frame in bw_resource.timeline.data:\n                    bw_total_stat[key_rw] += frame.dur*frame.allocated_bw\n        for k, v in bw_total_stat.items():\n            util = bw_total_stat[k]/total_latency / \\\n                bw_max_bw[k] if total_latency > 0 else 0\n\n            if not hasattr(post_stat, k):\n                logger.warning(\n                    \"no attr %s(util=%.1f%%) in post_stat, skipped\", k, util*100)\n                continue\n            setattr(post_stat, k, util)\n\n        return post_stat\n\n    def _get_longest(self, action_id, cost_dict, dag,  visited={}):\n        stack = [action_id]\n        if action_id in visited:\n            return visited[action_id]\n        curr_latency = cost_dict[action_id].latency\n        next_max_latency = 0\n        next_child_stack = []\n        for child_id in dag.successors(action_id):\n            # child = _action_map[child_id]\n            latency, child_stack = self._get_longest(\n                child_id, cost_dict, dag,  visited)\n            if latency > next_max_latency:\n                next_max_latency = latency\n                next_child_stack = child_stack\n        visited[action_id] = curr_latency + \\\n            next_max_latency, stack\n        stack.extend(next_child_stack)\n        return curr_latency+next_max_latency, stack\n\n    def _get_ref(self, action_id, dag, cost_dict):\n        if action_id in self.visited_ref:\n            return self.visited_ref[action_id]\n\n        ref = 0\n        parents = dag.predecessors(action_id)\n        if parents:\n            refs = [self._get_ref(parent_action_id, dag, cost_dict)\n                    for parent_action_id in parents]\n            ref = max(refs) if refs else 0\n\n        self.visited_ref[action_id] = cost_dict[action_id].latency+ref\n\n        return self.visited_ref[action_id]\n\n    def _prepare_key_frame(self, context, dataflow, config):\n        cost_dict = context.cost_dict\n        dag = dataflow.dag\n        _action_map = dataflow._action_map\n        key_frame_per_cluster = defaultdict(set)\n        key_frame_global = set()\n        for id, cost_book in cost_dict.items():\n            if id not in _action_map:\n                continue\n            action = _action_map[id]\n            action: DataflowAction\n\n            x, dx = (self._get_ref(id, dag, cost_dict) -\n                     cost_book.latency, cost_book.latency)\n\n            die_id = action.get_die_id()\n            cluster_id = action.get_cluster_id()\n\n            key_frame_global.add(x)\n            key_frame_global.add(x+dx)\n            key_frame_per_cluster[(die_id, cluster_id)].add(x)\n            key_frame_per_cluster[(die_id, cluster_id)].add(x+dx)\n\n        key_frame_global_dict = defaultdict(list)\n        key_frame_per_cluster_dict = defaultdict(lambda: defaultdict(list))\n        for id, cost_book in cost_dict.items():\n            if id not in _action_map:\n                continue\n            action = _action_map[id]\n            action: DataflowAction\n\n            x, dx = (self._get_ref(id, dag, cost_dict) -\n                     cost_book.latency, cost_book.latency)\n\n            for key_frame in sorted(key_frame_global):\n                if x <= key_frame < x+dx:\n                    key_frame_global_dict[key_frame].append(cost_book)\n            for (die_id, cluster_id), sub_key_frame_list in key_frame_per_cluster.items():\n                for key_frame in sorted(sub_key_frame_list):\n                    if x <= key_frame < x+dx:\n                        key_frame_per_cluster_dict[(die_id, cluster_id)][key_frame].append(\n                            cost_book)\n\n        self.key_frame_global_dict = key_frame_global_dict\n        self.key_frame_per_cluster_list = key_frame_per_cluster_dict\n\n    def _post_trace(self, context, dataflow, config):\n        # trace_arr = []\n        # if not hasattr(context, 'track_dict'):\n        #    context.track_dict = {}\n        # track_dict = context.track_dict\n\n        _action_map = dataflow._action_map\n        dag = dataflow.dag\n        cost_dict = context.cost_dict\n        # key_frame_per_cluster = defaultdict(set)\n        # key_frame_global = set()\n        # total_per_domain_cluster = defaultdict(lambda: defaultdict(list))\n        # action_by_cluster = defaultdict(set)\n\n        # for id, cost_book in cost_dict.items():\n        #     if id not in _action_map:\n        #         continue\n        #     action = _action_map[id]\n        #     action: DataflowAction\n\n        #     x, dx = (self._get_ref(id, dag, cost_dict) -\n        #              cost_book.latency, cost_book.latency)\n        #     die_id = action.get_die_id()\n        #     cluster_id = action.get_cluster_id()\n        #     args = {\n        #         \"parent_ids\": action.get_parent_ids(),\n        #         \"child_ids\": action.get_child_ids(),\n        #         \"action_id\": action.get_action_id(),\n        #     }\n\n        #     for k, v in asdict(cost_book).items():\n        #         args[k] = v\n\n        #     key_frame_global.add(x)\n        #     key_frame_global.add(x+dx)\n        #     key = (die_id, cluster_id)\n        #     key_frame_per_cluster[key].add(x)\n        #     key_frame_per_cluster[key].add(x+dx)\n        #     action_by_cluster[key].add(id)\n\n        # init track dict\n        # 1. power\n        # track_dict: Dict[Tuple[int, int, str], CounterTrack] = {}\n        # die_ids = list(range(config.inst_num.NUM_OF_DIE))\n        # cids = list(range(config.inst_num.NUM_OF_CLUSTER))\n        # for cls in [PowerSIPDomain, PowerL1Domain, PowerSOCDomain, PowerMemDomain]:\n        #     pat = r'Power(.*)Domain'\n        #     import re\n        #     domain_name = re.match(\n        #         pat, cls.__name__).group(1).lower()\n        #     for die_id in die_ids:\n        #         for cluster_id in cids:\n        #             for field in fields(cls):\n        #                 field_name = field.name\n        #                 _name = f\"pw_stat {domain_name}:{field_name}\"\n        #                 key = (die_id, cluster_id,\n        #                        f\"{domain_name}:{field_name}\")\n        #                 track_dict[key] = context.get_cluster_tgen(\n        #                     die_id, cluster_id).create_counter_track(_name)\n\n        #             # add cluster total track\n        #             key = (die_id, cluster_id, f\"{domain_name}:cluster_total\")\n        #             track_dict[key] = context.get_cluster_tgen(\n        #                 die_id, cluster_id).create_counter_track(f\"pw_stat {domain_name}:cluster_total\")\n\n        #         # global\n        #         key = (None, None, f\"{domain_name}:total\")\n        #         _name = f\"pw_stat:total {domain_name}:total\"\n        #         track_dict[key] = context.get_cluster_tgen(\n        #             None, None).create_counter_track(_name)\n        # # power\n        # for (die_id, cluster_id), frame_set in key_frame_per_cluster.items():\n        #     action_list = action_by_cluster[(die_id, cluster_id)]\n        #     key_frame_dict = {k: [] for k in frame_set}\n        #     for id in action_list:\n        #         cost_book = cost_dict[id]\n        #         x, dx = (self._get_ref(id, dag, cost_dict) -\n        #                  cost_book.latency, cost_book.latency)\n        #         for key_frame in sorted(frame_set):\n        #             if x <= key_frame < x+dx:\n        #                 key_frame_dict[key_frame].append(cost_book)\n\n        #     for key_frame in sorted(key_frame_dict):\n        #         cost_book_list = key_frame_dict[key_frame]\n        #         sum_dict = defaultdict(Counter)\n\n        #         for cost_book in cost_book_list:\n        #             cost_book: CostBook\n        #             for k, v in asdict(cost_book.power_stat).items():\n        #                 if k == 'mem':  # skip mem domain as it is counted in global\n        #                     continue\n        #                 sum_dict[k].update(v)\n\n        #         args = {}\n        #         for pd_domain_name, pd_domain in sum_dict.items():\n        #             sum_domain = 0\n        #             for pd_sub_domain_name, v in pd_domain.items():\n        #                 name = f\"{pd_domain_name}:{pd_sub_domain_name}\"\n        #                 args[name] = round(v/1e12, 3)\n        #                 track_dict[die_id, cluster_id,\n        #                            name].count(key_frame, round(v/1e12, 3))\n\n        #                 sum_domain += v\n        #             name = f\"{pd_domain_name}:cluster_total\"\n        #             args[name] = round(sum_domain/1e12, 3)\n        #             track_dict[die_id, cluster_id, name].count(\n        #                 key_frame, round(v/1e12, 3))\n        #             total_per_domain_cluster[pd_domain_name][cluster_id].append(\n        #                 (key_frame, sum_domain))\n\n        # bw_util\n        for (lvl, rw, die_id, cluster_id, esl_port_id, bw_resource) in context.bw_resource_context.get_unique_bw_resource_list():\n            name = f\"bw:{lvl}_{rw}\"\n            if esl_port_id is not None:\n                name += f\"_{esl_port_id}\"\n            last_end = 0\n            key_frame = None\n            max_bw = bw_resource.max_bw\n\n            track = context.get_cluster_tgen(\n                die_id, cluster_id).create_counter_track(f\"{name} bw_util\")\n\n            for frame in bw_resource.timeline.data:\n                if frame.begin > last_end:\n                    track.count(last_end, 0)\n                track.count(frame.begin, frame.allocated_bw/max_bw)\n                last_end = frame.end\n\n        # global total\n        # cluster_idx = defaultdict(int)\n        # for key_frame in sorted(key_frame_global):\n        #     for domain, c in total_per_domain_cluster.items():\n        #         domain_sum = defaultdict(float)\n        #         key = f\"{domain}:total\"\n        #         for cluster_id, domain_total_list in c.items():\n        #             ts, domain_val = domain_total_list[cluster_idx[cluster_id]]\n\n        #             ts_next, domain_val_next = domain_total_list[cluster_idx[cluster_id]+1] if cluster_idx[cluster_id]+1 < len(\n        #                 domain_total_list) else (999999, domain_val)\n        #             if ts <= key_frame < ts_next:\n        #                 domain_sum[key] += round(domain_val/1e12, 3)\n        #             else:\n        #                 domain_sum[key] += round(domain_val_next/1e12, 3)\n        #                 if cluster_idx[cluster_id]+1 < len(domain_total_list):\n        #                     cluster_idx[cluster_id] += 1\n        #         for k, v in domain_sum.items():\n        #             track_dict[(None, None, key)].count(key_frame, v)\n\n        # l3 power\n        if context.power_context:\n            track = context.get_cluster_tgen(\n                None, None).create_counter_track(\"pw:l3 l3_power\")\n            for key_frame in context.power_context.l3_power:\n\n                track.count(key_frame.begin, round(key_frame.l3_power/1e12, 3))\n\n            track.count(min(key_frame.end, context.post_stat.total_latency),\n                        round(key_frame.l3_power/1e12, 3))\n\n            # edc frame current\n            dtu_track = context.get_cluster_tgen(\n                None, None).create_counter_track(\"EDC: DTU [A]\")\n            dtu_timeline: Timeline[EDCFrame] = context.power_context.dtu_edc\n            for key_frame in dtu_timeline.data:\n                if key_frame.begin > 100:\n                    continue\n                dtu_track.count(key_frame.begin, key_frame.current)\n\n            soc_track = context.get_cluster_tgen(\n                None, None).create_counter_track(\"EDC: SOC [A]\")\n            soc_timeline: Timeline[EDCFrame] = context.power_context.soc_edc\n            for key_frame in soc_timeline.data:\n                if key_frame.begin > 100:\n                    continue\n                soc_track.count(key_frame.begin, key_frame.current)\n\n        # update track_dict\n        # context.track_dict = track_dict\n        logger.info(\"trace file generated\")\n\n    def generate_report(\n        self,\n        context: BossaNovaContext,\n        dataflow: Dataflow,\n        config: BossaNovaConfig,\n        service_list: List[BaseCostService],\n    ):\n        # BossaNovaContext类型的对象context记录post_stat生成的信息\n        post_stat = self._get_post_stat(dataflow, context, config)\n        context.post_stat = post_stat\n        # pass out for fusion_report\n        self.post_stat = post_stat\n        DIE_NUM = config.inst_num.NUM_OF_DIE\n\n        for cost_svc in service_list:\n            post_stat.service_report_dict[cost_svc.__class__.__name__] = cost_svc.post_stat(\n                context, dataflow)\n        self._post_trace(context, dataflow, config)\n\n        report_path = f\"{self.outdir}/report.yaml\"\n        self._prepare_key_frame(context, dataflow, config)\n\n        logger.info(\"total_latency:%.2fns,len(longest_path):%s\",\n                    post_stat.total_latency*1e9, len(self.longest_path))\n\n        INTERVAL = config.power.edc_current_interval\n        FILTER_GLITCH = config.power.edc_filter_glitch\n\n        # context.power_context.dtu_edc\n        if context.power_context:\n            def edc_stat(current_timeline: Timeline[EDCFrame]):\n                max_current_lvl = 0\n                current_dict = defaultdict(lambda: [])\n                for key_frame in current_timeline.data:\n                    if key_frame.end > 100:\n                        continue\n                    current = int(key_frame.current // INTERVAL * INTERVAL)\n                    max_current_lvl = max(max_current_lvl, current)\n                    for lvl in range(0, max_current_lvl+INTERVAL, INTERVAL):\n                        if not current_dict[lvl]:\n                            current_dict[lvl].append({\n                                \"begin\": key_frame.begin,\n                            })\n                        if current >= lvl:\n                            current_dict[lvl][-1][\"end\"] = key_frame.end\n                        else:\n                            if \"end\" not in current_dict[lvl][-1]:\n                                current_dict[lvl][-1][\"begin\"] = key_frame.end\n                            else:\n                                current_dict[lvl].append({\n                                    \"begin\": key_frame.end,\n                                })\n                # filter duration < FILTER_GLITCH and stat count and total duration\n                current_dict_agg = {}\n                for lvl in range(0, max_current_lvl+INTERVAL, INTERVAL):\n                    dur = 0\n                    count = 0\n                    min_dur = 9999\n                    max_dur = 0\n                    for current in current_dict[lvl]:\n                        if \"end\" not in current:\n                            continue\n                        _dur = current[\"end\"]-current[\"begin\"]\n                        if _dur < FILTER_GLITCH:\n                            continue\n                        count += 1\n                        dur += _dur\n                        min_dur = min(min_dur, _dur)\n                        max_dur = max(max_dur, _dur)\n                    if count == 0:\n                        continue  # 没有符合要求的数据, e.g. 所有的dur<FILTER_GLITCH\n                    current_dict_agg[lvl] = {\n                        \"count\": count,\n                        \"duration(tot)(us)\": ts_convert(dur),\n                        \"duration(min)(us)\": ts_convert(min_dur),\n                        \"duration(max)(us)\": ts_convert(max_dur),\n                        \"duration(avg)(us)\": ts_convert(dur/count if count > 0 else 0),\n                        \"current(A)\": lvl,\n                    }\n\n                return current_dict_agg\n            dtu_edc_stat = edc_stat(context.power_context.dtu_edc)\n            soc_edc_stat = edc_stat(context.power_context.soc_edc)\n\n            dtu_edc_report = [dtu_edc_stat[lvl]\n                              for lvl in sorted(dtu_edc_stat.keys(), reverse=True)[:5]]\n            soc_edc_report = [soc_edc_stat[lvl]\n                              for lvl in sorted(soc_edc_stat.keys(), reverse=True)[:5]]\n            post_stat.edc.dtu_edc_report = dtu_edc_report\n            post_stat.edc.soc_edc_report = soc_edc_report\n\n        report = asdict(post_stat)\n        report_str = yaml.dump(report, sort_keys=False, indent=2)\n\n        Path(report_path).write_text(report_str)\n        logger.debug(report)\n        logger.info(\"report generated at %s\", report_path)\n        return report",
    "start_line": 24,
    "end_line": 444,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "AbstractPostProcessor"
    ],
    "class_name": null,
    "display_name": "class PostProcessor",
    "component_id": "nova-platform.nova_platform.data_visual.post_processor.PostProcessor"
  },
  "nova-platform.nova_platform.data_visual.post_processor.FusionPostProcessor": {
    "id": "nova-platform.nova_platform.data_visual.post_processor.FusionPostProcessor",
    "name": "FusionPostProcessor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/data_visual/post_processor.py",
    "relative_path": "nova-platform/nova_platform/data_visual/post_processor.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.PostStat",
      "nova-platform.nova_platform.base_model.EDCStat"
    ],
    "source_code": "class FusionPostProcessor:\n    '''\n    record last post_stat and merge it with current one\n    '''\n\n    def __init__(self, outdir=\"./\") -> None:\n        self.outdir = outdir\n\n    def fusion_post_stats(self, post_stats: List[PostStat]) -> PostStat:\n        if not post_stats:\n            return PostStat()  # 返回一个默认的 PostStat\n        # 过滤掉 None 的情况\n        post_stats = [ps for ps in post_stats if ps is not None]\n        # 如果过滤后为空，返回一个默认的 PostStat 实例\n        if not post_stats:\n            return PostStat()\n        # TODO: need to sync with the latest version of post_stat\n        total_latency = sum(\n            post_stat.total_latency for post_stat in post_stats)\n        action_end_time = sum(\n            post_stat.action_end_time for post_stat in post_stats)\n\n        def w_avg(attr: str) -> float:\n            if total_latency == 0:\n                raise ValueError(\"Total latency is zero, cannot compute weighted average.\")\n            return sum(getattr(post_stat, attr) * post_stat.total_latency for post_stat in post_stats) / total_latency\n        \n        core_util = w_avg('core_util')\n        l3_rw_bw_util = w_avg('l3_rw_bw_util')\n        sic_io_r_bw_util = w_avg('sic_io_r_bw_util')\n        sic_io_w_bw_util = w_avg('sic_io_w_bw_util')\n        sic_io_rw_bw_util = w_avg('sic_io_rw_bw_util')\n        esl_bw_util = w_avg('esl_bw_util')\n        d2d_tx_rw_bw_util = w_avg('d2d_tx_rw_bw_util')\n\n        # merge edc\n        edc = self.merge_edc_stats([post_stat.edc for post_stat in post_stats])\n        # merge service report\n        service_report_dict = self.merge_service_report_dicts(\n            [post_stat.service_report_dict for post_stat in post_stats])\n\n        return PostStat(\n            total_latency=total_latency,\n            action_end_time=action_end_time,\n            core_util=core_util,\n            l3_rw_bw_util=l3_rw_bw_util,\n            sic_io_r_bw_util=sic_io_r_bw_util,\n            sic_io_w_bw_util=sic_io_w_bw_util,\n            sic_io_rw_bw_util=sic_io_rw_bw_util,\n            esl_bw_util=esl_bw_util,\n            service_report_dict=service_report_dict,\n            edc=edc,\n            d2d_tx_rw_bw_util=d2d_tx_rw_bw_util\n        )\n    \n    def fusion_post_stats_list(self, fus_post_stats: List[PostStat], caseinfo_post_stats: List[PostStat]) -> List[PostStat]:\n        \"\"\"\n        Fusion multiple lists of post_stats for different GCUs\n        Input: \n            arg0: List[self.post_stat0, ...],\n            arg1: List[caseinfo.post_stat0, ...]\n\n        \"\"\"\n        num_gcus = len(fus_post_stats)  # Get GCU count from first list\n        \n        # Fusion post_stats for each GCU\n        for gcu_id in range(num_gcus):\n            fus_post_stats[gcu_id] = self.fusion_post_stats([fus_post_stats[gcu_id], caseinfo_post_stats[gcu_id]])\n                \n        return fus_post_stats\n\n    @staticmethod\n    def merge_service_report_dicts(service_report_dicts: List[Dict[str, Any]]) -> Dict[str, Any]:\n            merged_service_report = {}\n            last_cache_service_report = None\n            for service_report_dict in service_report_dicts:\n                for service_name, metrics in service_report_dict.items():\n                    if service_name in [\"CacheCostService\", \"ParallelCacheCostService\"]:\n                        # 记录最后一个 CacheCostService 的报告\n                        last_cache_service_report = service_report_dict\n                    else:\n                        service_metrics = merged_service_report.setdefault(service_name, {})\n                        for metric_key, metric_value in metrics.items():\n                            try:\n                                metric_value_num = float(str(metric_value).replace(',', ''))\n                            except ValueError:\n                                continue\n                            if metric_value_num != 0:\n                                service_metrics[metric_key] = service_metrics.get(metric_key, 0) + metric_value_num\n                            else:\n                                service_metrics.setdefault(metric_key, 0)\n            if last_cache_service_report:\n                if \"CacheCostService\" in last_cache_service_report:\n                    merged_service_report[\"CacheCostService\"] = last_cache_service_report[\"CacheCostService\"]\n                if \"ParallelCacheCostService\" in last_cache_service_report:\n                    merged_service_report[\"ParallelCacheCostService\"] = last_cache_service_report[\"ParallelCacheCostService\"]\n\n            return merged_service_report\n\n    @staticmethod\n    def merge_edc_stats(edc_stats: List[EDCStat]) -> EDCStat:\n        def merge_reports(reports: List[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n            merged_report = []\n            for report in reports:\n                for entry in report:\n                    if entry is None:\n                        continue\n                    current = entry[\"current(A)\"]\n                    existing_entry = next(\n                        (e for e in merged_report if e[\"current(A)\"] == current), None)\n                    if existing_entry:\n                        existing_entry[\"count\"] += entry[\"count\"]\n                        existing_entry[\"duration(tot)(us)\"] += entry[\"duration(tot)(us)\"]\n                        existing_entry[\"duration(max)(us)\"] = max(\n                            existing_entry[\"duration(max)(us)\"], entry[\"duration(max)(us)\"])\n                        existing_entry[\"duration(min)(us)\"] = min(\n                            existing_entry[\"duration(min)(us)\"], entry[\"duration(min)(us)\"])\n                        existing_entry[\"duration(avg)(us)\"] = (\n                            existing_entry[\"duration(tot)(us)\"] / existing_entry[\"count\"])\n                    else:\n                        merged_report.append(entry.copy())\n            return merged_report\n\n        def get_top_currents(report: List[Dict[str, Any]], top_n: int) -> List[Dict[str, Any]]:\n            return sorted(report, key=lambda x: x[\"current(A)\"], reverse=True)[:top_n]\n\n        dtu_edc_reports = [edc.dtu_edc_report for edc in edc_stats]\n        soc_edc_reports = [edc.soc_edc_report for edc in edc_stats]\n\n        merged_dtu_edc_report = merge_reports(dtu_edc_reports)\n        merged_soc_edc_report = merge_reports(soc_edc_reports)\n\n        top_dtu_edc_report = get_top_currents(merged_dtu_edc_report, 5)\n\n        # 累加 edc_total_latency\n        edc_total_latency = sum(edc.edc_total_latency for edc in edc_stats)\n\n        # 累加 edc_acc_dict\n        edc_acc_dict = {}\n        for edc in edc_stats:\n            if edc.edc_acc_dict is not None:\n                for key, value in edc.edc_acc_dict.items():\n                    edc_acc_dict[key] = edc_acc_dict.get(key, 0) + value\n\n        # 求均值 edc_incr_percent\n        edc_incr_percent = sum(\n            edc.edc_incr_percent for edc in edc_stats) / len(edc_stats) if edc_stats else 0\n\n        return EDCStat(\n            dtu_edc_report=top_dtu_edc_report,\n            soc_edc_report=merged_soc_edc_report,\n            edc_total_latency=edc_total_latency,\n            edc_acc_dict=edc_acc_dict,\n            edc_incr_percent=edc_incr_percent\n        )\n\n    def generate_fus_reports(self, post_stats):\n        \"\"\"Generate fusion reports for multiple GCUs\"\"\"\n        for gcu_id, post_stat in enumerate(post_stats):\n            if post_stat is not None:\n                fus_report = asdict(post_stat)\n                fus_report_str = yaml.dump(fus_report, sort_keys=False, indent=2)\n                \n                fus_report_dir = Path(self.outdir) / \"fus_report\"\n                fus_report_dir.mkdir(parents=True, exist_ok=True)\n\n                fus_report_path = f\"{self.outdir}/fus_report/fus_report_gcu{gcu_id}.yaml\"\n                Path(fus_report_path).write_text(fus_report_str)\n                logger.debug(f\"GCU{gcu_id} fusion report: {fus_report}\")\n                logger.info(\"report generated at %s\", fus_report_path)",
    "start_line": 447,
    "end_line": 616,
    "has_docstring": true,
    "docstring": "record last post_stat and merge it with current one",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class FusionPostProcessor",
    "component_id": "nova-platform.nova_platform.data_visual.post_processor.FusionPostProcessor"
  },
  "nova-platform.nova_platform.data_visual.trace_post_processor.BossaNovaTraceProcessor": {
    "id": "nova-platform.nova_platform.data_visual.trace_post_processor.BossaNovaTraceProcessor",
    "name": "BossaNovaTraceProcessor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/data_visual/trace_post_processor.py",
    "relative_path": "nova-platform/nova_platform/data_visual/trace_post_processor.py",
    "depends_on": [],
    "source_code": "class BossaNovaTraceProcessor(TraceProcessor):\n    def __init__(self, trace_path):\n        super().__init__(trace=trace_path)\n\n    def run_sql(self, sql):\n        qr_it = self.query(sql)\n        return qr_it\n\n    def get_esl_bw_stat(self):\n        sql = \"\"\"\nwith xpu_track as (\n    SELECT name, id FROM track where name like '%%:dataflow'\n),\ntemp1 as (\n    select \n    track_id,ts as leading_ts,\n    dur as leading_dur, \n    LEAD(ts) OVER (PARTITION BY track_id ORDER BY ts) AS data_ts,\n    LEAD(dur) OVER (PARTITION BY track_id ORDER BY ts) AS data_dur,\n    LEAD(arg_set_id) OVER (PARTITION BY track_id ORDER BY ts) AS arg_set_id,\n    arg_set_id\n    from slice \n    where track_id in (select id from xpu_track) and name like '%%->esl->%%'\n    order by track_id, ts\n),\nrst1 as (\n        select leading_ts,leading_dur,data_ts,data_dur, display_value as bytes, display_value/(leading_dur+data_dur) as bw\n        from temp1 \n        left join args on temp1.arg_set_id=args.arg_set_id and flat_key='debug.total_count'\n        where data_dur is not null\n)\n        select max(bw) as max_bw,min(bw) as min_bw,avg(bw) as avg_bw,count(1) as total_count from rst1 \n\"\"\"\n        qr = self.run_sql(sql)\n        return qr",
    "start_line": 4,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "TraceProcessor"
    ],
    "class_name": null,
    "display_name": "class BossaNovaTraceProcessor",
    "component_id": "nova-platform.nova_platform.data_visual.trace_post_processor.BossaNovaTraceProcessor"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorId": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorId",
    "name": "DiagTensorId",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class DiagTensorId:\n    id_key: int\n    id_val: int",
    "start_line": 12,
    "end_line": 14,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DiagTensorId",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorId"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor",
    "name": "DiagTensor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class DiagTensor:\n    addr: int\n    offsets: List[int]\n    dims: List[int]\n    stride_dims: List[int]\n    bpe: int",
    "start_line": 18,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DiagTensor",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorContainer": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorContainer",
    "name": "DiagTensorContainer",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class DiagTensorContainer:\n    id: DiagTensorId\n    tensor: List[DiagTensor]",
    "start_line": 27,
    "end_line": 29,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DiagTensorContainer",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorContainer"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID",
    "name": "DiagTriggerID",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class DiagTriggerID:\n    id: int\n    values: List[int]",
    "start_line": 33,
    "end_line": 35,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DiagTriggerID",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.BufCnt": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.BufCnt",
    "name": "BufCnt",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class BufCnt:\n    lhs: int = 1\n    rhs: int = 1\n    res: int = 1\n    traverse_dim_order: str = \"\"",
    "start_line": 39,
    "end_line": 43,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BufCnt",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.BufCnt"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.TileInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.TileInfo",
    "name": "TileInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class TileInfo:\n    cube_dim: List[int] = field(default_factory=list)\n    grid_dim: List[int] = field(default_factory=list)\n    block_dim: List[int] = field(default_factory=list)\n    tile_shape: List[int] = field(default_factory=list)\n    l2_buf_cnt: BufCnt = None  # lhs, rhs, res,\n    l1_buf_cnt: BufCnt = None",
    "start_line": 47,
    "end_line": 53,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TileInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.TileInfo"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflowAction": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflowAction",
    "name": "DiagDataflowAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class DiagDataflowAction(DataflowAction):\n    config: BossaNovaConfig\n    action_id: int\n    topo: TOPO\n    action_type: DataflowActionType\n    engine_id: int\n    engine_sub_id: int\n    inputs: List[DiagTensorContainer]\n    outputs: List[DiagTensorContainer]\n    child_action_ids: List[int]\n    # child_actions: List[\"DiagAction\"]\n    parent_action_ids: List[int]\n    # parent_actions: List[\"DiagAction\"]\n    depth: int\n    setup_parent_action_id: int\n    setup_child_action_id: int\n    exe_sem_id: int\n    setup_sem_id: int\n    trigger_id: DiagTriggerID\n    input_hints: List[int]\n    tile_info: TileInfo\n    dataflow_config: dict[str, str]\n    die_id: int\n    case_id: int\n\n    def get_die_id(self):\n        return (self.engine_id//self.config.inst_num.NUM_OF_CORE_PER_CLUSTER) % self.config.inst_num.NUM_OF_DIE\n        # return self.get_cluster_id() % 2\n\n    def get_action_id(self):\n        return self.action_id\n\n    def get_engine_id(self):\n        return self.engine_id\n\n    def get_cluster_id(self):\n        return self.engine_id//self.config.inst_num.NUM_OF_CORE_PER_CLUSTER//self.config.inst_num.NUM_OF_DIE\n\n    def get_engine_sub_id(self):\n        return self.engine_sub_id\n\n    def get_port_id(self):\n        return 0\n\n    def get_child_ids(self):\n        return self.child_action_ids\n\n    def get_parent_ids(self):\n        return self.parent_action_ids\n\n    def get_action_type(self) -> DataflowActionType:\n        return self.action_type\n\n    def get_core_stat(self):\n        return self.core_cost\n\n    def _iter_tensor_addr(self, base_addr, tensor: DiagTensor, rw) -> Generator[DataflowActionMemoryAccess, None, None]:\n        # return addr, size\n        bpe = tensor.bpe\n        stride0, stride1, stride2, _ = tensor.stride_dims\n        stride0 *= bpe\n        shape0, shape1, shape2, shape3 = tensor.dims\n        shape0 *= bpe\n        offset0, offset1, offset2, offset3 = tensor.offsets\n        offset0 *= bpe\n        for k in range(offset3, offset3+shape3):\n            for j in range(offset2, offset2+shape2):\n                for i in range(offset1, offset1+shape1):\n                    addr = base_addr + \\\n                        (\n                            i * stride0\n                            + j * stride1 * stride0\n                            + k * stride2 * stride1 * stride0\n                            + offset0\n                        )\n                    yield DataflowActionMemoryAccess(addr, shape0, rw)\n\n    def _iter_access_gen(self, mem_acc_list: List[DataflowActionMemoryAccess]):\n        counter = 0\n        batch: List[DataflowActionMemoryAccess] = []\n        fetch_size = yield batch\n        # history = [] # for debug\n        # history_raw = [] # for debug\n        while True:\n            for acc in mem_acc_list:\n                # history_raw.append(acc)\n                batch.append(acc)\n                counter += acc.size\n                while True:\n                    last_acc = batch.pop()\n\n                    if counter >= fetch_size:\n                        last_size_right = counter-fetch_size\n                        last_size_left = last_acc.size - last_size_right\n                        batch.append(\n                            DataflowActionMemoryAccess(\n                                base_addr=last_acc.base_addr,\n                                size=last_size_left,\n                                rw=last_acc.rw\n                            )\n                        )\n\n                        new_base = last_acc.base_addr + last_size_left\n                        new_size = last_acc.size - last_size_left\n                        counter = fetch_size\n                        # history.append(batch)\n                        fetch_size = yield batch\n                        if new_size > 0:\n                            batch = [\n                                DataflowActionMemoryAccess(\n                                    base_addr=new_base,\n                                    size=new_size,\n                                    rw=last_acc.rw\n                                )\n                            ]\n                            counter = new_size\n                        else:\n                            batch = []\n                            counter = 0\n                            break\n                    else:\n                        batch.append(last_acc)\n                        break\n            if batch:\n                # history.append(batch)\n                yield batch",
    "start_line": 57,
    "end_line": 182,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowAction"
    ],
    "class_name": null,
    "display_name": "class DiagDataflowAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflowAction"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow",
    "name": "DiagDataflow",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "class DiagDataflow(Dataflow):\n    dataflow_name: str\n    dataflow_id: int\n    odte_total_bytes: int\n    cdte_total_bytes: int\n    sdte_total_bytes: int\n    action_list: List[DiagDataflowAction]\n\n    def __post_init__(self):\n        self._build_dag(self.action_list)",
    "start_line": 186,
    "end_line": 195,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Dataflow"
    ],
    "class_name": null,
    "display_name": "class DiagDataflow",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action._parse_input_output": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action._parse_input_output",
    "name": "_parse_input_output",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorContainer",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorId",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor"
    ],
    "source_code": "def _parse_input_output(input_string: str) -> List[DiagTensorContainer]:\n    pattern = r'(\\d+\\.\\d+):\\{\\{.*?\\}\\}'\n\n    # 查找所有匹配项\n    matches = re.findall(pattern, input_string)\n\n    # 初始化结果字典\n    result_dict = {}\n\n    # 处理每个匹配项\n    actions = []\n    for match in matches:\n        #   # dec param # hex PA\n        key_pattern = r'(\\d+).(\\d+):\\{\\{(\\d+,\\d+,\\d+,\\d+)\\},\\{(\\d+,\\d+,\\d+,\\d+)\\},\\{([0-9a-fA-F]+)\\}'\n        key_match = re.search(key_pattern, input_string)\n        if key_match:\n            actions.append(\n                DiagTensorContainer(\n                    DiagTensorId(\n                        int(key_match.group(1)), int(key_match.group(2))),\n                    [\n                        DiagTensor(\n                            int(key_match.group(5), base=16),  # hex\n                            [int(offset)\n                             for offset in key_match.group(3).split(',')],  # dec\n                            [int(dim)\n                             for dim in key_match.group(4).split(',')],  # dec\n                            [int(dim)\n                             for dim in key_match.group(4).split(',')],  # TODO: stride dim, need upstream to provide!!! fix later,\n                            2,  # TODO: bpe\n                        )\n                    ]\n                )\n            )\n    return actions",
    "start_line": 215,
    "end_line": 249,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "input_string"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _parse_input_output",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action._parse_input_output"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.parse_diag_dataflow": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.parse_diag_dataflow",
    "name": "parse_diag_dataflow",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID",
      "nova-platform.nova_platform.base_model.DataflowActionType",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflowAction",
      "nova-platform.nova_platform.dataflow.action.diag_action._parse_input_output",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow"
    ],
    "source_code": "def parse_diag_dataflow(path: str) -> DiagDataflow:\n    with open(path) as f:\n        input_str = f.read()\n    lines = input_str.split('\\n')\n    line0 = re.match(\n        r'# dataflow:\\{name\\{(\\w+)\\},id\\{(\\w+)\\}\\}', lines[0]).groups()\n    line1 = re.match(\n        r'# odte:\\{(\\w+)\\},cdte:\\{(\\w+)\\},sdte:\\{(\\w+)\\}', lines[1]).groups()\n    dataflow_name, dataflow_id = line0[0], int(line0[1])  # dec\n    (\n        odte_total_bytes,\n        cdte_total_bytes,\n        sdte_total_bytes\n    ) = [int(i) for i in line1]  # dec\n\n    actions = []\n    for i, line in enumerate(lines[2:]):\n        if line.strip() == '':\n            continue\n        match = re.search(pattern, line)\n        if match:\n            (\n                action_id,  # hex\n                action_type,  # str\n                engine_id,  # hex\n                dte_op_or_xpu_code,  # str\n                inputs,\n                outputs,\n                child_action_ids,  # hex\n                parent_action_ids,  # hex\n                depth,  # hex\n                setup_action_id_pair,  # hex\n                sem_id_pair,  # dec\n                trigger_id,  # dec\n                input_hints,  # hex\n                vc_ctx_id,\n                vc_ctx_data\n            ) = match.groups()\n\n            action_id = int(action_id, base=16)\n            action_type = DataflowActionType(action_type.upper())\n            engine_id = int(engine_id, base=16)\n\n            inputs = _parse_input_output(inputs)\n            outputs = _parse_input_output(outputs)\n\n            child_action_ids = re.findall(r'-?[0-9a-fA-F]+', child_action_ids)\n            child_action_ids = [int(i, base=16)\n                                for i in child_action_ids]  # hex\n            parent_action_ids = re.findall(\n                r'-?[0-9a-fA-F]+', parent_action_ids)\n            parent_action_ids = [int(i, base=16)\n                                 for i in parent_action_ids]  # hex\n            depth = int(re.findall(\n                r'-?[0-9a-fA-F]+', depth)[0], base=16)  # hex\n\n            # '{-1,-1}'\n            setup_parent_action_id, setup_child_action_id = re.findall(\n                r'-?[0-9a-fA-F]+', setup_action_id_pair)\n            setup_parent_action_id, setup_child_action_id = int(\n                setup_parent_action_id, base=16), int(setup_child_action_id, base=16)  # hex\n\n            # '{0,-1}'\n            exe_sem_id, setup_sem_id = re.findall(\n                r'-?[0-9a-fA-F]+', sem_id_pair)\n            exe_sem_id, setup_sem_id = int(\n                exe_sem_id), int(setup_sem_id)  # dec\n\n            # '{0->0,1,2,3,4,}' => [0,0,1,2,3,4]\n            trigger_id = re.findall(\n                r'-?[0-9a-fA-F]+', trigger_id)\n            if len(trigger_id) > 0:\n                trigger_id = DiagTriggerID(\n                    int(trigger_id[0]),  # dec\n                    [int(i) for i in trigger_id[1:]]  # dec\n                )\n            else:\n                trigger_id = None\n\n            # {80000000,80000001,}\n            input_hints = re.findall(r'[0-9a-fA-F]+', input_hints)\n            input_hints = [int(i, base=16) for i in input_hints]  # hex\n\n            # vc0/xpu0\n            engine_sub_id = re.findall(r'[vc|ctx](\\d+)', vc_ctx_id)\n            if len(engine_sub_id) > 0:\n                engine_sub_id = int(engine_sub_id[0])  # dec\n            else:\n                engine_sub_id = None\n\n            if action_type in [DataflowActionType.CDTE, DataflowActionType.SDTE, DataflowActionType.ODTE]:\n                dte_data = re.findall(r'[0-9a-fA-F]+', vc_ctx_data)\n                # dec\n                param0 = [int(d) for d in dte_data[:4]]\n                param1 = [int(d) for d in dte_data[4:8]]\n                param2 = [int(d) for d in dte_data[8:12]]\n                param3 = [int(d) for d in dte_data[12:]]\n\n                action = DiagDataflowAction(\n                    action_id=action_id,\n                    action_type=action_type,\n                    engine_id=engine_id,\n                    engine_sub_id=engine_sub_id,\n                    inputs=inputs,\n                    outputs=outputs,\n                    child_action_ids=child_action_ids,\n                    parent_action_ids=parent_action_ids,\n                    depth=depth,\n                    setup_parent_action_id=setup_parent_action_id,\n                    setup_child_action_id=setup_child_action_id,\n                    exe_sem_id=exe_sem_id,\n                    setup_sem_id=setup_sem_id,\n                    trigger_id=trigger_id,\n                    input_hints=input_hints,\n                    op=dte_op_or_xpu_code,\n                    param0=param0, param1=param1, param2=param2, param3=param3\n                )\n            else:\n                xpu_data = re.findall(r'[0-9a-fA-F]+', vc_ctx_data)\n                xpu_data = [int(d, base=16) for d in xpu_data]  # hex\n                action = DiagDataflowAction(\n                    action_id=action_id,\n                    action_type=action_type,\n                    engine_id=engine_id,\n                    engine_sub_id=engine_sub_id,\n                    inputs=inputs,\n                    outputs=outputs,\n                    child_action_ids=child_action_ids,\n                    parent_action_ids=parent_action_ids,\n                    depth=depth,\n                    setup_parent_action_id=setup_parent_action_id,\n                    setup_child_action_id=setup_child_action_id,\n                    exe_sem_id=exe_sem_id,\n                    setup_sem_id=setup_sem_id,\n                    trigger_id=trigger_id,\n                    input_hints=input_hints,\n                    code=dte_op_or_xpu_code,\n                    data=xpu_data\n                )\n            actions.append(action)\n        else:\n            raise Exception(i, line)\n\n    dataflow = DiagDataflow(\n        dataflow_name=dataflow_name,\n        dataflow_id=dataflow_id,\n        odte_total_bytes=odte_total_bytes,\n        cdte_total_bytes=cdte_total_bytes,\n        sdte_total_bytes=sdte_total_bytes,\n        action_list=actions\n    )\n    return dataflow",
    "start_line": 252,
    "end_line": 403,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_diag_dataflow",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.parse_diag_dataflow"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.parse_shape_tile_info": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.parse_shape_tile_info",
    "name": "parse_shape_tile_info",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "def parse_shape_tile_info(path):\n    def to_list(s: str):\n        elements = [i.strip() for i in s[1:-1].split(\",\")]\n        l = [int(i) if i.isdigit() else i for i in elements]\n        return l\n\n    p = Path(path)\n    if not p.exists():\n        raise Exception(f\"{path} not exists!\")\n    with open(path) as f:\n        input_str = f.read()\n    lines = input_str.split(\"\\n\")\n    if \"# SYSTEM\" in lines[0]:\n        line1 = lines[1].split(\"-\")\n    else:\n        line1 = lines[0].split(\"-\")\n    l2_buf_cnt = to_list(line1[4]) if len(line1) > 4 else None\n    l1_buf_cnt = to_list(line1[5]) if len(line1) > 5 else None\n    info = TileInfo(\n        cube_dim=to_list(line1[0]),\n        grid_dim=to_list(line1[1]),\n        block_dim=to_list(line1[2]),\n        tile_shape=to_list(line1[3]),\n        l2_buf_cnt=l2_buf_cnt,\n        l1_buf_cnt=l1_buf_cnt,\n    )\n    print(line1)\n    return info",
    "start_line": 406,
    "end_line": 433,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "path"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_shape_tile_info",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.parse_shape_tile_info"
  },
  "nova-platform.nova_platform.dataflow.action.diag_action.to_list": {
    "id": "nova-platform.nova_platform.dataflow.action.diag_action.to_list",
    "name": "to_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/diag_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/diag_action.py",
    "depends_on": [],
    "source_code": "    def to_list(s: str):\n        elements = [i.strip() for i in s[1:-1].split(\",\")]\n        l = [int(i) if i.isdigit() else i for i in elements]\n        return l",
    "start_line": 407,
    "end_line": 410,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "s"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function to_list",
    "component_id": "nova-platform.nova_platform.dataflow.action.diag_action.to_list"
  },
  "nova-platform.nova_platform.dataflow.action.dte_action.BaseDTEStat": {
    "id": "nova-platform.nova_platform.dataflow.action.dte_action.BaseDTEStat",
    "name": "BaseDTEStat",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/dte_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/dte_action.py",
    "depends_on": [],
    "source_code": "class BaseDTEStat(BaseActionStat):\n    r_datasize: int = 0\n    w_datasize: int = 0",
    "start_line": 14,
    "end_line": 16,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseActionStat"
    ],
    "class_name": null,
    "display_name": "class BaseDTEStat",
    "component_id": "nova-platform.nova_platform.dataflow.action.dte_action.BaseDTEStat"
  },
  "nova-platform.nova_platform.dataflow.action.dte_action.DTEAction": {
    "id": "nova-platform.nova_platform.dataflow.action.dte_action.DTEAction",
    "name": "DTEAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/dte_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/dte_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.dte_action.BaseDTEStat"
    ],
    "source_code": "class DTEAction(DiagDataflowAction):\n    op: str\n    param0: list[int]\n    param1: list[int]\n    param2: list[int]\n    param3: list[int]\n    core_cost: BaseDTEStat\n\n    def get_local_engine_id(self):\n        return self.engine_sub_id\n\n    def compute(self, context: BossaNovaContext) -> Generator[DataflowActionMemoryStat, None, BaseCoreStat]:\n        # cost_book = context.get_cost_book(self)\n        self.core_cost = BaseDTEStat()\n        latency = 0\n        for stat in self.get_memory_stat():\n            yield stat\n            latency = max(latency, stat.latency+stat.relative_ts)\n            if stat.rw == 'r':\n                self.core_cost.r_datasize += stat.total_count\n                # cost_book.r_datasize += stat.total_count\n            else:\n                self.core_cost.w_datasize += stat.total_count\n                # cost_book.w_datasize += stat.total_count\n\n        # cost_book.latency = latency\n        self.core_cost.latency = latency\n        return self.core_cost",
    "start_line": 20,
    "end_line": 47,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DiagDataflowAction"
    ],
    "class_name": null,
    "display_name": "class DTEAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.dte_action.DTEAction"
  },
  "nova-platform.nova_platform.dataflow.action.dte_action.CDTESliceAction": {
    "id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTESliceAction",
    "name": "CDTESliceAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/dte_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/dte_action.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.dataflow.action.dte_action.DTEAction",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr"
    ],
    "source_code": "class CDTESliceAction(DTEAction):\n\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        src: DiagTensor = self.inputs[0].tensor[0]\n        dst: DiagTensor = self.outputs[0].tensor[0]\n        src_addr_domain = AddrDomain.get_addr_domain(src.addr)\n        dst_addr_domain = AddrDomain.get_addr_domain(dst.addr)\n        if src_addr_domain == AddrDomain.SHARED and dst_addr_domain == AddrDomain.L3:\n            param0 = self.param0\n            tensor = DiagTensor(**src.__dict__)\n            tensor.dims = param0\n            tensor.stride_dims = dst.stride_dims\n            yield from self._iter_tensor_addr(dst.addr, tensor, 'w')\n        elif src_addr_domain == AddrDomain.L3 and dst_addr_domain == AddrDomain.SHARED:\n            param0 = self.param0\n            tensor = DiagTensor(**dst.__dict__)\n            tensor.dims = param0\n            tensor.stride_dims = src.stride_dims\n            yield from self._iter_tensor_addr(src.addr, tensor, 'r')\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        if logger.level == logging.DEBUG:\n            src_addr = self.inputs[0].tensor[0].addr\n            dst_addr = self.outputs[0].tensor[0].addr\n            src_addr_domain = AddrDomain.get_addr_domain(src_addr)\n            dst_addr_domain = AddrDomain.get_addr_domain(dst_addr)\n            assert src_addr_domain == AddrDomain.L3\n            assert dst_addr_domain == AddrDomain.SHARED\n\n        data_dims = self.param0\n        data_size = reduce(lambda a, b: a*b, data_dims, 1) * \\\n            self.inputs[0].tensor[0].bpe\n        yield DataflowActionMemoryStat(\n            total_count=data_size,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw='r',\n            memory_access_list=list(self.get_memory_access())\n        )",
    "start_line": 50,
    "end_line": 89,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DTEAction"
    ],
    "class_name": null,
    "display_name": "class CDTESliceAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTESliceAction"
  },
  "nova-platform.nova_platform.dataflow.action.dte_action.CDTEDesliceAction": {
    "id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTEDesliceAction",
    "name": "CDTEDesliceAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/dte_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/dte_action.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.dataflow.action.dte_action.CDTESliceAction"
    ],
    "source_code": "class CDTEDesliceAction(CDTESliceAction):\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        if logger.level == logging.DEBUG:\n            src_addr = self.inputs[0].tensor[0].addr\n            dst_addr = self.outputs[0].tensor[0].addr\n            src_addr_domain = AddrDomain.get_addr_domain(src_addr)\n            dst_addr_domain = AddrDomain.get_addr_domain(dst_addr)\n            assert src_addr_domain == AddrDomain.SHARED\n            assert dst_addr_domain == AddrDomain.L3\n\n        data_dims = self.param0\n        data_size = reduce(lambda a, b: a*b, data_dims, 1) * \\\n            self.inputs[0].tensor[0].bpe\n        yield DataflowActionMemoryStat(\n            total_count=data_size,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.SHARED,\n            dst=AddrDomain.L3,\n            rw='w',\n            memory_access_list=list(self.get_memory_access())\n        )",
    "start_line": 92,
    "end_line": 112,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CDTESliceAction"
    ],
    "class_name": null,
    "display_name": "class CDTEDesliceAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTEDesliceAction"
  },
  "nova-platform.nova_platform.dataflow.action.dte_action.CDTEReshapeAction": {
    "id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTEReshapeAction",
    "name": "CDTEReshapeAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/dte_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/dte_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.dataflow.action.dte_action.DTEAction",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr"
    ],
    "source_code": "class CDTEReshapeAction(DTEAction):\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        if logger.level == logging.DEBUG:\n            src_addr = self.inputs[0].tensor[0].addr\n            dst_addr = self.outputs[0].tensor[0].addr\n            src_addr_domain = AddrDomain.get_addr_domain(src_addr)\n            dst_addr_domain = AddrDomain.get_addr_domain(dst_addr)\n            assert src_addr_domain == AddrDomain.L3\n            assert dst_addr_domain == AddrDomain.L3\n        data_size = reduce(\n            lambda a, b: a*b, self.outputs[0].tensor[0].dims, 1) * self.outputs[0].tensor[0].bpe\n\n        input_tensor = self.inputs[0].tensor[0]\n        input_access = self._iter_tensor_addr(\n            input_tensor.addr, input_tensor, 'r')\n        input_gen = self._iter_access_gen(input_access)\n        next(input_gen)\n\n        output_tensor = self.outputs[0].tensor[0]\n        output_access = self._iter_tensor_addr(\n            output_tensor.addr, output_tensor, 'w')\n        output_gen = self._iter_access_gen(output_access)\n        next(output_gen)\n\n        # read\n        read_stat = DataflowActionMemoryStat(\n            total_count=data_size,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.L3,\n            dst=AddrDomain.L3,\n            rw='r',\n            memory_access_list=input_gen.send(data_size)\n        )\n        yield read_stat\n        # write\n        yield DataflowActionMemoryStat(\n            total_count=data_size,\n            master=DataflowActionType.CDTE,\n            src=AddrDomain.L3,\n            dst=AddrDomain.L3,\n            rw='w',\n            relative_ts=read_stat.leading_latency,\n            memory_access_list=output_gen.send(data_size)\n        )",
    "start_line": 115,
    "end_line": 158,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DTEAction"
    ],
    "class_name": null,
    "display_name": "class CDTEReshapeAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.dte_action.CDTEReshapeAction"
  },
  "nova-platform.nova_platform.dataflow.action.nop_action.XPUNopAction": {
    "id": "nova-platform.nova_platform.dataflow.action.nop_action.XPUNopAction",
    "name": "XPUNopAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/nop_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/nop_action.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BaseCoreStat"
    ],
    "source_code": "class XPUNopAction(XPUAction):\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        yield from ()\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        yield from ()\n\n    def compute(self, context: BossaNovaContext) -> Generator[None, None, BaseCoreStat]:\n        self.core_cost = BaseCoreStat()\n        yield\n        return self.core_cost",
    "start_line": 11,
    "end_line": 21,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUNopAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.nop_action.XPUNopAction"
  },
  "nova-platform.nova_platform.dataflow.action.tpu_gemm_action._action_tensor_to_operand": {
    "id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action._action_tensor_to_operand",
    "name": "_action_tensor_to_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "depends_on": [],
    "source_code": "def _action_tensor_to_operand(tensor: DiagTensor) -> Operand:\n    # 现有 diag tensor 采用 reversed 维度，保持与 XPU 行为一致的适配\n    def reversed_dims(t):\n        return tuple(reversed(t))\n\n    return Operand(\n        dim=reversed_dims(tensor.dims[:3]),\n        addr=tensor.addr,\n        bpe=tensor.bpe,\n        dim_offset=reversed_dims(tensor.offsets[:3]),\n        dim_stride=reversed_dims(tensor.stride_dims[:3]),\n    )",
    "start_line": 20,
    "end_line": 31,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _action_tensor_to_operand",
    "component_id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action._action_tensor_to_operand"
  },
  "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.reversed_dims": {
    "id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.reversed_dims",
    "name": "reversed_dims",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "depends_on": [],
    "source_code": "    def reversed_dims(t):\n        return tuple(reversed(t))",
    "start_line": 22,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "t"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function reversed_dims",
    "component_id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.reversed_dims"
  },
  "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuCoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuCoreCost",
    "name": "TpuCoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "depends_on": [],
    "source_code": "class TpuCoreCost(BaseCoreStat):\n    instruction_info: dict = field(default_factory=dict)",
    "start_line": 35,
    "end_line": 36,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class TpuCoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuCoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuGemmAction",
    "name": "TpuGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/tpu_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.batch_gemm_tpu.tile_tpu_gemm_workload",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.benchmark.op_base.list_product",
      "nova-platform.nova_platform.dataflow.action.tpu_gemm_action._action_tensor_to_operand"
    ],
    "source_code": "class TpuGemmAction(XPUAction):\n    \"\"\"\n    TPU GEMM action：使用 TPU tiler 输出，生成基本访存/计算统计和延迟。\n    \"\"\"\n\n    core_cost: TpuCoreCost = field(default_factory=TpuCoreCost)\n\n    def get_kernel_idx(self):\n        if len(self.data) >= 2:\n            return self.data[0], self.data[1]\n        return self.data[0], 0\n\n    def get_trace_label(self) -> str:\n        # TPU 阵列轨道名称使用 SA 前缀\n        return \"sa\"\n\n    def get_chip_workload(self) -> Workload:    \n        lhs = _action_tensor_to_operand(self.inputs[0].tensor[0])\n        rhs = _action_tensor_to_operand(self.inputs[1].tensor[0])\n        res = _action_tensor_to_operand(self.outputs[0].tensor[0])\n        inputs = [lhs, rhs]\n        attr = {\"b\": lhs.dim[0], \"m\": lhs.dim[1], \"k\": lhs.dim[2], \"n\": rhs.dim[2]}\n        return Workload(inputs=inputs, outputs=[res], attr=attr, dtype=self.get_dtype())\n\n    def _array_flops(self):\n        freq_core = getattr(getattr(self.config, \"freq\", None), \"CORE\", 1.0)\n        compute = getattr(self.config, \"compute\", None)\n        arr_m = getattr(getattr(compute, \"tpu\", None), \"ARRAY_M\", 64) if compute else 64\n        arr_n = getattr(getattr(compute, \"tpu\", None), \"ARRAY_N\", 64) if compute else 64\n        return arr_m * arr_n * freq_core * 1e9  # mac/s\n\n    def _hbm_bw(self):\n        bw = getattr(self.config, \"bw\", None)\n        freq = getattr(self.config, \"freq\", None)\n        freq_mc = getattr(freq, \"MC\", getattr(freq, \"CORE\", 1.0)) if freq else 1.0\n        num_die = getattr(getattr(self.config, \"inst_num\", None), \"NUM_OF_DIE\", 1)\n        hbm_bw_cfg = getattr(getattr(getattr(bw, \"mc\", None), \"l3\", None), \"bw\", 64) if bw else 64\n        return hbm_bw_cfg * freq_mc * 1e9 * num_die  # bytes/s\n\n    def _vmem_bw(self):\n        # 简化：使用 HBM 带宽作为 VMEM 带宽占位\n        return self._hbm_bw()\n\n    def _vregs_bw(self):\n        # 寄存器层假设比 HBM 快 4 倍\n        return self._hbm_bw() * 4\n\n    def _single_access(self, base_addr: int, size: int, rw: str) -> list:\n        return [DataflowActionMemoryAccess(base_addr=base_addr, size=size, rw=rw)]\n\n    def _calc_time(self, bytes_total: float, bw_bytes_per_s: float) -> float:\n        if bw_bytes_per_s <= 0:\n            return 0.0\n        return bytes_total / bw_bytes_per_s\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        sic_id, sip_id = self.get_kernel_idx()\n        chip_workload = self.get_chip_workload()\n        workloads, best_shape = tile_tpu_gemm_workload(self.config, chip_workload)\n        self.core_cost.instruction_info = {\n            \"grid_dims\": best_shape.grid_dims,\n            \"block_dims\": best_shape.block_dims,\n            \"thread_dims\": best_shape.thread_dims,\n            \"tile_k\": best_shape.calc_ceil_K_l2,\n        }\n        if sic_id not in workloads or sip_id not in workloads[sic_id]:\n            self.core_cost.latency = 0\n            return\n\n        array_flops = self._array_flops()\n        hbm_bw = self._hbm_bw()\n        vmem_bw = self._vmem_bw()\n        vregs_bw = self._vregs_bw()\n        total_ref = 0.0\n\n        for idx, wl in enumerate(workloads[sic_id][sip_id]):\n            bytes_lhs = wl.attr.get(\"bytes_lhs\") or sum(list_product(tuple(t.dim)) * t.bpe for t in wl.inputs if t)\n            bytes_rhs = wl.attr.get(\"bytes_rhs\") or 0\n            bytes_res = wl.attr.get(\"bytes_res\") or sum(list_product(tuple(t.dim)) * t.bpe for t in wl.outputs)\n            bytes_in = bytes_lhs + bytes_rhs\n\n            # HBM -> VMEM (映射为 LOCAL->L3，与 XPU 路由兼容)\n            mem_hbm = DataflowActionMemoryStat(\n                total_count=int(bytes_in),\n                master=DataflowActionType.CDTE,\n                src=AddrDomain.LOCAL,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=total_ref,\n                memory_access_list=self._single_access(wl.inputs[0].addr if wl.inputs else 0, int(bytes_in), \"r\"),\n                name=f\"hbm->vmem-{idx}\",\n            )\n            yield mem_hbm\n            hbm_time = self._calc_time(bytes_in, hbm_bw)\n\n            # VMEM -> VREGS (映射为 L0->LOCAL，与 XPU 路由兼容)\n            mem_vmem = DataflowActionMemoryStat(\n                total_count=int(bytes_in),\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.LOCAL,\n                rw=\"r\",\n                relative_ts=total_ref + hbm_time,\n                memory_access_list=self._single_access(wl.inputs[0].addr if wl.inputs else 0, int(bytes_in), \"r\"),\n                name=f\"vmem->vregs-{idx}\",\n            )\n            yield mem_vmem\n            vmem_time = self._calc_time(bytes_in, vmem_bw)\n\n            # Compute\n            macs = wl.attr[\"m\"] * wl.attr[\"n\"] * wl.attr[\"k\"]\n            compute_ref = total_ref + hbm_time + vmem_time\n            compute_stat = DataflowActionComputeStat(\n                name=\"tpu_gemm_mac\",\n                compute_2d_ops={self.get_dtype(): macs * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat\n            compute_time = (macs * 2) / (array_flops or 1.0)\n\n            # VREGS -> HBM (写回)\n            out_ref = compute_ref + compute_time\n            mem_out = DataflowActionMemoryStat(\n                total_count=int(bytes_res),\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=out_ref,\n                memory_access_list=self._single_access(wl.outputs[0].addr if wl.outputs else 0, int(bytes_res), \"w\"),\n                name=f\"vregs->hbm-{idx}\",\n            )\n            yield mem_out\n            out_time = self._calc_time(bytes_res, hbm_bw)\n\n            total_ref = out_ref + out_time\n\n        self.core_cost.latency = total_ref\n\n    def _basic_stat_info(self):\n        self.core_cost.tensor_dtype = self.get_dtype()",
    "start_line": 40,
    "end_line": 180,
    "has_docstring": true,
    "docstring": "TPU GEMM action：使用 TPU tiler 输出，生成基本访存/计算统计和延迟。",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class TpuGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.tpu_gemm_action.TpuGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_action.XPUAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_action.XPUAction",
    "name": "XPUAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_action.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DType",
      "nova-platform.nova_platform.base_model.DataflowOpType",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class XPUAction(DiagDataflowAction):\n    code: str\n    data: list\n    core_cost: BaseCoreStat\n\n    def get_local_engine_id(self):\n        return self.engine_id % self.config.inst_num.NUM_OF_CORE_PER_CLUSTER\n\n    def get_valid_shape(self):\n        return [\n            self.tile_info.tile_shape[0] * self.tile_info.block_dim[1],\n            self.tile_info.tile_shape[1] * self.tile_info.block_dim[2],\n            self.tile_info.tile_shape[2],\n        ]\n\n    def get_optype(self) -> DataflowOpType:\n        return DataflowOpType(self.code.split(\"_\")[0])\n\n    def get_dtype(self) -> DType:\n        return DType(self.code.split(\"_\")[-1])\n\n    def _basic_stat_info(self):\n        raise NotImplementedError()\n\n    def compute(self, context: BossaNovaContext) -> Generator[DataflowActionMemoryStat, any, BaseCoreStat]:\n        yield from self.get_memory_stat()\n        self._basic_stat_info()\n        return self.core_cost\n\n    def _iter_addr(self, tensor: DiagTensor, rw) -> DataflowActionMemoryAccess:\n        data_num = self.data[0]\n        base_addr = tensor.addr\n        data_size = data_num*tensor.bpe\n        return DataflowActionMemoryAccess(base_addr, data_size, rw)",
    "start_line": 9,
    "end_line": 42,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DiagDataflowAction"
    ],
    "class_name": null,
    "display_name": "class XPUAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_action.XPUAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    ld_ins_num: int = 0\n    scalar_ins_num: int = 0\n    st_ins_num: int = 0\n    compute_1d_ins_num: int = 0\n    compute_sfu_ins_num: int = 0\n    compute_ins_shape: int = 0",
    "start_line": 17,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    unroll_num: int = 16\n    subthread_num: int = 8\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = None",
    "start_line": 28,
    "end_line": 37,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction",
    "name": "XPUActivationAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
    ],
    "source_code": "class XPUActivationAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        dtype = self.core_cost.instruction_info.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.vector_dtype = dtype\n        self.core_cost.vector_ops = info.compute_ins_shape * info.compute_1d_ins_num\n        self.core_cost.ld_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.ld_ins_num\n        self.core_cost.st_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.st_ins_num\n\n    def kernel(self):\n        raise NotImplementedError()\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 3 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = int(\n            self.config.memory.l0.OA_SIZE / self.core_cost.oa_occupation)\n        self.core_cost.main_body_length = 64\n        self.main_body_num = float(\n            self.core_cost.instruction_info.ld_ins_num) / self.core_cost.main_body_length\n\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        lhs_tensor = self.inputs[0].tensor[0]\n        lhs_access = self._iter_addr(lhs_tensor, 'r')\n        out_tensor = self.outputs[0].tensor[0]\n        out_access = self._iter_addr(out_tensor, 'w')\n        yield from self._iter_access_gen([lhs_access, out_access])\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        self.core_cost.instruction_info = self.kernel()\n        self._get_l0_occupation()\n        self._get_main_body_length()\n        stat_ref = 0\n        total_latency = 0\n        total_compute_latency = 0\n        dtype = self.core_cost.instruction_info.dtype\n\n        throughput_1d = self.config.compute.thread_1d_throughput[dtype]\n        throughput_sfu = self.config.compute.thread_sfu_throughput\n\n        input_tensor = self.inputs[0].tensor[0]\n        input_access = self._iter_addr(input_tensor, 'r')\n        output_tensor = self.outputs[0].tensor[0]\n        output_access = self._iter_addr(output_tensor, 'w')\n        input_gen = self._iter_access_gen([input_access])\n        next(input_gen)\n        output_gen = self._iter_access_gen([output_access])\n        next(output_gen)\n\n        for i in range(0, self.core_cost.instruction_info.ld_ins_num, self.core_cost.main_body_length):\n            inst_num = min(self.core_cost.main_body_length,\n                           self.core_cost.instruction_info.ld_ins_num - i)\n            lhs, out = [inst_num * BYPTES_PER_OA] * 2\n\n            lhs_read = DataflowActionMemoryStat(\n                total_count=lhs,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(lhs)\n            )\n            yield lhs_read\n            vld_latency, vld_leading_latency = lhs_read.latency, lhs_read.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - vld_latency:{vld_latency}, vld_leading_latency:{vld_leading_latency}\"\n            )\n\n            compute_1d_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_1d_ins_num / self.core_cost.instruction_info.ld_ins_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_sfu_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_sfu_ins_num / self.core_cost.instruction_info.ld_ins_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_cycle = (\n                max(compute_1d_ops / throughput_1d,\n                    compute_sfu_ops / throughput_sfu)\n                + inst_num * self.core_cost.instruction_info.scalar_ins_num /\n                self.core_cost.instruction_info.ld_ins_num\n            )\n            # compute_latency = compute_cycle / \\\n            #     (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n            compute_ref = stat_ref + vld_leading_latency\n\n            compute_stat = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_1d_ops\n                },\n                compute_msf_ops=compute_sfu_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat\n            compute_latency = compute_stat.latency\n            total_compute_latency += compute_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - compute_cycle: {compute_cycle}, compute latency:{compute_latency}\"\n            )\n\n            out_write = DataflowActionMemoryStat(\n                total_count=out,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=stat_ref + vld_leading_latency,\n                memory_access_list=output_gen.send(out)\n            )\n            yield out_write\n            out_latency, out_leading_latency = out_write.latency, out_write.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - out_latency:{out_latency}, out_leading_latency:{out_leading_latency}\"\n            )\n\n            total_latency = max(\n                total_latency,\n                compute_ref + compute_latency,  # compute latency,\n                stat_ref + vld_latency,  # vld latency\n                stat_ref + vld_leading_latency + out_latency,  # vst latency\n            )\n            stat_ref = total_latency - vld_leading_latency  # update ref\n        print(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n        self.core_cost.compute_cost = total_compute_latency",
    "start_line": 41,
    "end_line": 175,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUActivationAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSigmoidAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSigmoidAction",
    "name": "XPUSigmoidAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction",
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo"
    ],
    "source_code": "class XPUSigmoidAction(XPUActivationAction):\n    def kernel(self):\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        dtype = self.get_dtype()\n        compute_1d_ins_num = 0\n        st_ins_num = 0\n        scalar_ins_num = 18\n        ld_ins_num = 0\n        compute_sfu_ins_num = 0\n        elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n        for i in range(0, size, elements_per_OA):\n            ld_ins_num += 1\n            compute_sfu_ins_num += 1\n            st_ins_num += 1\n            scalar_ins_num += 3\n        return InstructionInfo(\n            dtype=dtype,\n            ld_ins_num=ld_ins_num,\n            scalar_ins_num=scalar_ins_num,\n            st_ins_num=st_ins_num,\n            compute_1d_ins_num=compute_1d_ins_num,\n            compute_sfu_ins_num=compute_sfu_ins_num,\n            compute_ins_shape=elements_per_OA,\n        )",
    "start_line": 178,
    "end_line": 202,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUActivationAction"
    ],
    "class_name": null,
    "display_name": "class XPUSigmoidAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSigmoidAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSiluAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSiluAction",
    "name": "XPUSiluAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction",
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo"
    ],
    "source_code": "class XPUSiluAction(XPUActivationAction):\n    def kernel(self):\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        dtype = self.get_dtype()\n        compute_1d_ins_num = 0\n        st_ins_num = 0\n        scalar_ins_num = 18\n        ld_ins_num = 0\n        compute_sfu_ins_num = 0\n        elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n        for i in range(0, size, elements_per_OA):\n            ld_ins_num += 1\n            compute_sfu_ins_num += 1\n            compute_1d_ins_num += 1\n            st_ins_num += 1\n            scalar_ins_num += 3\n        return InstructionInfo(\n            dtype=dtype,\n            ld_ins_num=ld_ins_num,\n            scalar_ins_num=scalar_ins_num,\n            st_ins_num=st_ins_num,\n            compute_1d_ins_num=compute_1d_ins_num,\n            compute_sfu_ins_num=compute_sfu_ins_num,\n            compute_ins_shape=elements_per_OA,\n        )",
    "start_line": 205,
    "end_line": 230,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUActivationAction"
    ],
    "class_name": null,
    "display_name": "class XPUSiluAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUSiluAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUGeluAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUGeluAction",
    "name": "XPUGeluAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction",
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo"
    ],
    "source_code": "class XPUGeluAction(XPUActivationAction):\n    def kernel(self):\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        dtype = self.get_dtype()\n        compute_1d_ins_num = 0\n        st_ins_num = 0\n        scalar_ins_num = 18\n        ld_ins_num = 0\n        compute_sfu_ins_num = 0\n        elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n        for i in range(0, size, elements_per_OA):\n            ld_ins_num += 1\n            compute_sfu_ins_num += 1\n            st_ins_num += 1\n            scalar_ins_num += 3\n\n        return InstructionInfo(\n            dtype=dtype,\n            ld_ins_num=ld_ins_num,\n            scalar_ins_num=scalar_ins_num,\n            st_ins_num=st_ins_num,\n            compute_1d_ins_num=compute_1d_ins_num,\n            compute_sfu_ins_num=compute_sfu_ins_num,\n            compute_ins_shape=elements_per_OA,\n        )",
    "start_line": 233,
    "end_line": 258,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUActivationAction"
    ],
    "class_name": null,
    "display_name": "class XPUGeluAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUGeluAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUReluAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUReluAction",
    "name": "XPUReluAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_activation_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUActivationAction",
      "nova-platform.nova_platform.dataflow.action.xpu_activation_action.InstructionInfo"
    ],
    "source_code": "class XPUReluAction(XPUActivationAction):\n    def kernel(self):\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        dtype = self.get_dtype()\n        compute_1d_ins_num = 0\n        st_ins_num = 0\n        scalar_ins_num = 18\n        ld_ins_num = 0\n        compute_sfu_ins_num = 0\n        elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n        for i in range(0, size, elements_per_OA):\n            ld_ins_num += 1\n            compute_1d_ins_num += 2\n            st_ins_num += 1\n            scalar_ins_num += 3\n\n        return InstructionInfo(\n            dtype=dtype,\n            ld_ins_num=ld_ins_num,\n            scalar_ins_num=scalar_ins_num,\n            st_ins_num=st_ins_num,\n            compute_1d_ins_num=compute_1d_ins_num,\n            compute_sfu_ins_num=compute_sfu_ins_num,\n            compute_ins_shape=elements_per_OA,\n        )",
    "start_line": 261,
    "end_line": 286,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUActivationAction"
    ],
    "class_name": null,
    "display_name": "class XPUReluAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_activation_action.XPUReluAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.action_tensor_to_operand": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.action_tensor_to_operand",
    "name": "action_tensor_to_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "def action_tensor_to_operand(tensor):\n    return Operand(\n        dim=tuple(tensor.dims),\n        addr=tensor.addr,\n        bpe=tensor.bpe,\n        dim_offset=tuple(tensor.offsets),\n        dim_stride=tuple(tensor.stride_dims),\n    )",
    "start_line": 17,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function action_tensor_to_operand",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.action_tensor_to_operand"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16",
    "start_line": 28,
    "end_line": 29,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    sip_workloads: list = field(default_factory=list)",
    "start_line": 33,
    "end_line": 35,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.XPUAllGatherAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.XPUAllGatherAction",
    "name": "XPUAllGatherAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.action_tensor_to_operand",
      "nova-platform.nova_platform.benchmark.op_base.list_product",
      "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.get_peer_ranks",
      "nova-platform.nova_platform.benchmark.eccl.SimpleProtoPrimitives"
    ],
    "source_code": "class XPUAllGatherAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_kernel_idx(self):\n        return self.data[0], self.data[1]\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        if self.topo in [TOPO.FULLMESH8]:\n            yield from self.fullmesh_allgather()\n        elif self.topo in [TOPO.SUPERNODE4, TOPO.SUPERNODE8, TOPO.SUPERNODE16, TOPO.SUPERNODE32]:\n            yield from self.ring_allgather()\n        else:\n            raise NotImplementedError\n\n    def ring_allgather(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        channel_cnt = 8\n        rank_num = self.topo.value[1]\n        def get_peer_ranks(channel, rank, rank_cnt):\n            pre = rank - 1 if rank != 0 else rank_cnt - 1\n            next = rank + 1 if rank < rank_cnt - 1 else 0\n            return pre, next\n\n        def get_channels(sic_id, sic_num):\n            num_channels = channel_cnt // sic_num\n            start_channel = sic_id * num_channels\n            end_channel = start_channel + num_channels\n            return list(range(start_channel, end_channel))\n\n        sic_id, sip_id = self.get_kernel_idx()\n        if sic_id not in [0, 1] or sip_id != 0:\n            return\n        rank = self.config.gcu_id\n        in_tensor = action_tensor_to_operand(self.inputs[0].tensor[0])\n        out_tensor = action_tensor_to_operand(self.outputs[0].tensor[0])\n        dtype = self.get_dtype()\n        bpe = dtype.get_bpe()\n\n        element_cnt = list_product(in_tensor.dim) * bpe\n        element_cnt_per_channel = element_cnt // channel_cnt\n        stat_ref = 0\n        total_latency = 0\n        channel_list = get_channels(sic_id, 2)\n        for channel_id in channel_list:\n            # print(\n            #     f\"!!!gcu:{self.config.gcu_id}, sic: {sic_id}, channel:{channel_id}\",\n            # )\n            channel_offset = channel_id * element_cnt_per_channel\n            dst_offset = element_cnt * rank + channel_offset\n            pre_rank, next_rank = get_peer_ranks(channel_id, rank, rank_num)\n            prims = SimpleProtoPrimitives(\n                case_id=self.case_id,\n                channel_id=channel_id,\n                recvPeers=[pre_rank],\n                sendPeers=[next_rank],\n                sendBuff=in_tensor.addr,\n                recvBuff=out_tensor.addr,\n                rank=rank,\n                ref=self.ref,\n                esl_bw_factor=16,\n            )\n            # step 0\n            if in_tensor.addr + channel_offset == out_tensor.addr + dst_offset:\n                latency = yield from prims.directSend(channel_offset, element_cnt_per_channel, stat_ref, 0)\n            else:\n                latency = yield from prims.directCopySend(\n                    channel_offset, dst_offset, element_cnt_per_channel, stat_ref, 0\n                )\n\n            # step 1 ~ k-2\n            for idx in range(1, rank_num - 1):\n                latency = yield from prims.directRecvCopySend(0, element_cnt_per_channel, latency, idx - 1, idx)\n\n            # final\n            latency = yield from prims.directRecv(0, element_cnt_per_channel, latency, rank_num - 2)\n            total_latency = max(total_latency, latency)\n            # print(\n            #     f\"!!!gcu:{self.config.gcu_id}, sic: {sic_id}, channel:{channel_id} done\",\n            # )\n        self.core_cost.latency = total_latency\n        print(\n            f\"!!!gcu:{self.config.gcu_id}, sic: {sic_id} done\",\n        )\n\n    def fullmesh_allgather(self) -> Generator[DataflowActionMemoryStat, None, None]:\n\n        def get_peer_ranks(rank_num, rank):\n            peer_rank = list(range(0, rank_num))\n            peer_rank = peer_rank[rank + 1 :] + peer_rank[0:rank]\n            return peer_rank\n\n        sic_id, sip_id = self.get_kernel_idx()\n        if sic_id not in [0, 1] or sip_id != 0:\n            return\n        # sic 0 for mesh 0, sic 1 for mesh 1\n        mesh_num = 2\n        esl_bw_factor = 1 / mesh_num\n        mesh_id = sic_id\n        in_tensor = action_tensor_to_operand(self.inputs[0].tensor[0])\n        out_tensor = action_tensor_to_operand(self.outputs[0].tensor[0])\n        rank_cnt = 8\n        element_cnt = list_product(in_tensor.dim)\n        element_cnt_per_mesh = element_cnt // mesh_num\n        element_cnt_per_rank = element_cnt_per_mesh // rank_cnt\n        rank = self.config.gcu_id\n        peer_ranks = get_peer_ranks(rank_cnt, rank)\n        dtype = self.get_dtype()\n        bpe = dtype.get_bpe()\n        addr_l3_base = 0x51000000000 + (mesh_id + 1) * 0x00010000000\n        flag_l3_base = 0x52000000000 + (mesh_id + 1) * 0x00010000000\n        data_l3_base = in_tensor.addr + mesh_id * element_cnt_per_mesh * bpe\n        total_latency = 0\n        stat_ref = 0\n\n        prims = SimpleProtoPrimitives(\n            case_id=self.case_id,\n            channel_id=mesh_id,\n            recvPeers=peer_ranks,\n            sendPeers=peer_ranks,\n            sendBuff=in_tensor.addr,\n            recvBuff=out_tensor.addr,\n            rank=rank,\n            ref=self.ref,\n        )\n        yield from prims.meshAllGather(mesh_id * element_cnt_per_mesh * bpe, element_cnt_per_mesh * bpe, stat_ref)\n\n    def _basic_stat_info(self):\n        pass",
    "start_line": 39,
    "end_line": 165,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUAllGatherAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_action.XPUAllGatherAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.action_tensor_to_operand": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.action_tensor_to_operand",
    "name": "action_tensor_to_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "def action_tensor_to_operand(tensor):\n    return Operand(\n        dim=tuple(tensor.dims),\n        addr=tensor.addr,\n        bpe=tensor.bpe,\n        dim_offset=tuple(tensor.offsets),\n        dim_stride=tuple(tensor.stride_dims),\n    )",
    "start_line": 26,
    "end_line": 33,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function action_tensor_to_operand",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.action_tensor_to_operand"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16",
    "start_line": 37,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    sip_workloads: list = field(default_factory=list)",
    "start_line": 42,
    "end_line": 44,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_peer_ranks": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_peer_ranks",
    "name": "get_peer_ranks",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [],
    "source_code": "def get_peer_ranks(channel, rank, rank_cnt):\n    pre = rank - 1 if rank != 0 else rank_cnt - 1\n    next = rank + 1 if rank < rank_cnt - 1 else 0\n    return pre, next",
    "start_line": 47,
    "end_line": 50,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "channel",
      "rank",
      "rank_cnt"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_peer_ranks",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_peer_ranks"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.split_workload": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.split_workload",
    "name": "split_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "def split_workload(workload: Workload, nranks: int, rank: int) -> List[Tuple[Workload]]:\n    b = workload.inputs[0].dim[0]\n    m = workload.inputs[0].dim[1]\n    k = workload.inputs[0].dim[2]\n    workloads = []\n    tmp_buffer = Operand(\n        dim=workload.inputs[0].dim,\n        addr=0x53000000000,\n        dim_offset=(0, 0, 0, 0),\n        bpe=workload.inputs[0].bpe,\n        dim_stride=workload.inputs[0].dim,\n    )\n    output = workload.outputs[0]\n\n    def get_received_peer_ranks(rank, rank_cnt):\n        peer_rank = list(range(0, rank_cnt))\n        peer_rank = list(reversed(peer_rank[0 : rank + 1])) + list(reversed(peer_rank[rank + 1 :]))\n        return peer_rank\n\n    peers = get_received_peer_ranks(rank, nranks)\n    for step, peer in enumerate(peers):\n        step_output = Operand(\n            dim=output.dim,\n            addr=output.addr,\n            bpe=output.bpe,\n            dim_offset=(0, m * peer, 0, 0),\n            dim_stride=output.dim_stride,\n        )\n\n        if step % 2 == 0:\n            ag_work = Workload((workload.inputs[0],), (tmp_buffer,), dtype=workload.dtype)\n            gemm_work = Workload((workload.inputs[0], workload.inputs[1]), (step_output,), dtype=workload.dtype)\n        else:\n            ag_work = Workload((tmp_buffer,), (workload.inputs[0],))\n            gemm_work = Workload((tmp_buffer, workload.inputs[1]), (step_output,), dtype=workload.dtype)\n        workloads.append((ag_work, gemm_work))\n    return workloads",
    "start_line": 54,
    "end_line": 90,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "workload",
      "nranks",
      "rank"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function split_workload",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.split_workload"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_received_peer_ranks": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_received_peer_ranks",
    "name": "get_received_peer_ranks",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [],
    "source_code": "    def get_received_peer_ranks(rank, rank_cnt):\n        peer_rank = list(range(0, rank_cnt))\n        peer_rank = list(reversed(peer_rank[0 : rank + 1])) + list(reversed(peer_rank[rank + 1 :]))\n        return peer_rank",
    "start_line": 68,
    "end_line": 71,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "rank",
      "rank_cnt"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_received_peer_ranks",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_received_peer_ranks"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.XPUAllGatherGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.XPUAllGatherGemmAction",
    "name": "XPUAllGatherGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared.dsm_shared_gemm_kernel",
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared.tile_shared_gemm_workload",
      "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.get_peer_ranks",
      "nova-platform.nova_platform.benchmark.op_base.list_product",
      "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._tile_workload",
      "nova-platform.nova_platform.benchmark.eccl.SimpleProtoPrimitives",
      "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.action_tensor_to_operand"
    ],
    "source_code": "class XPUAllGatherGemmAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def _basic_stat_info(self):\n        pass\n\n    def get_kernel_idx(self):\n        return self.data[0], self.data[1]\n\n    def get_workload(self) -> Workload:\n        workload = Workload(\n            inputs=(\n                action_tensor_to_operand(self.inputs[0].tensor[0]),\n                action_tensor_to_operand(self.inputs[1].tensor[0]),\n            ),\n            outputs=(action_tensor_to_operand(self.outputs[0].tensor[0]),),\n            dtype=self.get_dtype(),\n        )\n        return workload\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n\n        total_latency = 0\n        sic_id, sip_id = self.get_kernel_idx()\n        rank_num = self.topo.value[1]\n        rank = self.config.gcu_id\n        total_sip_num = (\n            self.config.inst_num.NUM_OF_CLUSTER\n            * self.config.inst_num.NUM_OF_CORE_PER_CLUSTER\n            * self.config.inst_num.NUM_OF_DIE\n        )\n\n        chip_workload = self.get_workload()\n        tiled_workloads = _tile_workload(chip_workload, rank_num * 2)\n        tile_num = len(tiled_workloads)\n        slice_per_rank = (tile_num + rank_num - 1) // rank_num\n\n        def _gemm(tile_idx, relative_ts):\n            gemm_workloads, best_shape = tile_shared_gemm_workload(self.config, tiled_workloads[tile_idx])\n            latency = yield from dsm_shared_gemm_kernel(\n                gemm_workloads,\n                best_shape,\n                self.config,\n                f\"{self.case_id}_{rank}_{tile_idx}\",\n                sic_id,\n                sip_id,\n                relative_ts,\n                self.ref,\n            )\n            return latency\n\n        def _allgather_master_mode(send_tile_idx, recv_tile_idx, trunk_idx, relative_ts):\n            channel_cnt = 2\n            if sic_id not in [0, 1] or sip_id != 0:\n                return relative_ts\n\n            def _get_channels(sic_id, sic_num):\n                num_channels = channel_cnt // sic_num\n                start_channel = sic_id * num_channels\n                end_channel = start_channel + num_channels\n                return list(range(start_channel, end_channel))\n\n            workload = tiled_workloads[send_tile_idx]\n            in_tensor = workload.inputs[0]\n            element_cnt = list_product(in_tensor.dim) * in_tensor.bpe\n            element_cnt_per_channel = element_cnt // channel_cnt\n            channel_list = _get_channels(sic_id, 2)\n            stat_ref = relative_ts\n            total_latency = 0\n            for channel_id in channel_list:\n                channel_offset = channel_id * element_cnt_per_channel\n                pre_rank, next_rank = get_peer_ranks(channel_id, rank, rank_num)\n                prims = SimpleProtoPrimitives(\n                    case_id=f\"{self.case_id}\",\n                    channel_id=channel_id,\n                    recvPeers=[pre_rank],\n                    sendPeers=[next_rank],\n                    sendBuff=in_tensor.addr,\n                    recvBuff=in_tensor.addr,\n                    rank=rank,\n                    ref=self.ref,\n                    esl_bw_factor=16,\n                )\n                send_latency = yield from prims.directSend(channel_offset, element_cnt_per_channel, stat_ref, trunk_idx)\n                recv_latency = yield from prims.directRecv(\n                    channel_offset, element_cnt_per_channel, send_latency, trunk_idx\n                )\n                total_latency = max(total_latency, recv_latency)\n            return total_latency\n\n        total_latency = 0\n        gemm_ref = 0\n        ag_ref = 0\n        gemm_latency = 0\n        allgather_latency = 0\n        trunk_ite_idx = 0\n        trunk_ready_ref = {}\n        for i in range(rank_num):\n            for j in range(slice_per_rank):\n                send_tile_idx = ((rank - i) % rank_num) * slice_per_rank + j\n                recv_tile_idx = ((rank - i - 1) % rank_num) * slice_per_rank + j\n                gemm_ref = max(gemm_ref, trunk_ready_ref.get(send_tile_idx, 0))\n                gemm_latency = yield from _gemm(send_tile_idx, gemm_ref)\n                if i < rank_num - 1:\n\n                    allgather_latency = yield from _allgather_master_mode(\n                        send_tile_idx, recv_tile_idx, trunk_ite_idx, ag_ref\n                    )\n                else:\n                    allgather_latency = ag_ref\n                allgather_latency = yield from bm.get_barrier(\n                    f\"{self.case_id}_{rank}_{trunk_ite_idx}\", total_sip_num\n                ).wait(self.ref + allgather_latency)\n                allgather_latency -= self.ref\n                trunk_ready_ref[recv_tile_idx] = allgather_latency\n                trunk_ite_idx += 1\n                gemm_ref = gemm_latency\n\n                ag_ref = allgather_latency\n        total_latency = max(gemm_latency, allgather_latency)\n        self.core_cost.latency = total_latency",
    "start_line": 95,
    "end_line": 215,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUAllGatherGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action.XPUAllGatherGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._tile_workload": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._tile_workload",
    "name": "_tile_workload",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "def _tile_workload(chip_workload: Workload, tile_num):\n    lhs = chip_workload.inputs[0]\n    rhs = chip_workload.inputs[1]\n    res = chip_workload.outputs[0]\n    B = lhs.dim[0]\n    M = lhs.dim[1]\n    K = lhs.dim[2]\n    N = rhs.dim[2]\n    m_per_tile = (M + tile_num - 1) // tile_num\n    tiled_workloads = []\n\n    for idx in range(tile_num):\n        offset_m = m_per_tile * idx\n        if offset_m >= M:\n            break\n        valid_m = m_per_tile if M >= m_per_tile + offset_m else M - offset_m\n        tiled_lhs = Operand(\n            dim=(B, valid_m, K),\n            addr=lhs.addr,\n            bpe=lhs.bpe,\n            dim_offset=(0, offset_m, 0),\n            dim_stride=lhs.dim_stride,\n        )\n        tiled_res = Operand(\n            dim=(B, valid_m, N),\n            addr=res.addr,\n            bpe=res.bpe,\n            dim_offset=(0, offset_m, 0),\n            dim_stride=res.dim_stride,\n        )\n        attr = {\"b\": B, \"m\": valid_m, \"n\": N, \"k\": K}\n        tiled_workloads.append(\n            Workload(inputs=[tiled_lhs, rhs], outputs=[tiled_res], dtype=chip_workload.dtype, attr=attr)\n        )\n    return tiled_workloads",
    "start_line": 218,
    "end_line": 252,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "chip_workload",
      "tile_num"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _tile_workload",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._tile_workload"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._get_comm_tile_num": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._get_comm_tile_num",
    "name": "_get_comm_tile_num",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allgather_gemm_action.py",
    "depends_on": [],
    "source_code": "def _get_comm_tile_num(M, N, K, bpe, ranks, config):\n    min_m = 128\n    max_tile_num = (M + min_m - 1) // min_m\n    min_tile_num = ranks\n    tile_num = max_tile_num\n    l3_bw = 128\n    while tile_num >= min_tile_num:\n        tiled_m = (M + tile_num - 1) // tile_num\n        mac_ops = 2 * tiled_m * N * K\n        mac_cycles = mac_ops / 4096\n        l3_access_bytes = bpe[0] * tiled_m * K + bpe[1] * K * N + bpe[2] * tiled_m * N\n        l3_bw_required = l3_access_bytes\n        if l3_bw_required < l3_bw:\n            break\n        tile_num = tile_num / 2",
    "start_line": 255,
    "end_line": 269,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "M",
      "N",
      "K",
      "bpe",
      "ranks",
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _get_comm_tile_num",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allgather_gemm_action._get_comm_tile_num"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    grid_dim: List[int] = field(default_factory=list)\n    block_dim: List[int] = field(default_factory=list)\n    thread_dim: List[int] = field(default_factory=list)\n    subthread_dim: List[int] = field(default_factory=list)",
    "start_line": 27,
    "end_line": 32,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    sip_workloads: list = field(default_factory=list)",
    "start_line": 36,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.action_tensor_to_operand": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.action_tensor_to_operand",
    "name": "action_tensor_to_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.Operand"
    ],
    "source_code": "def action_tensor_to_operand(tensor):\n    return Operand(\n        dim=tuple(tensor.dims),\n        addr=tensor.addr,\n        bpe=tensor.bpe,\n        dim_offset=tuple(tensor.offsets),\n        dim_stride=tuple(tensor.stride_dims),\n    )",
    "start_line": 41,
    "end_line": 48,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function action_tensor_to_operand",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.action_tensor_to_operand"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.get_peer_ranks": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.get_peer_ranks",
    "name": "get_peer_ranks",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "depends_on": [],
    "source_code": "def get_peer_ranks(rank_num, rank):\n    peer_rank = list(range(0, rank_num))\n    peer_rank = peer_rank[rank + 1:] + peer_rank[0:rank]\n    return peer_rank",
    "start_line": 51,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "rank_num",
      "rank"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_peer_ranks",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.get_peer_ranks"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.XPUAllReduceAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.XPUAllReduceAction",
    "name": "XPUAllReduceAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_allreduce_action.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_barrier.BarrierManager",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.get_peer_ranks",
      "nova-platform.nova_platform.benchmark.op_base.list_product",
      "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.action_tensor_to_operand"
    ],
    "source_code": "class XPUAllReduceAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_kernel_idx(self):\n        return self.data[0], self.data[1]\n\n    def _iter_tensor_addr(self, tensor: Operand, rw) -> Generator[DataflowActionMemoryAccess, None, None]:\n        mem_access = tensor.get_contiguous_mem_accesses()\n        for addr, size in mem_access:\n            yield DataflowActionMemoryAccess(addr, size, rw)\n\n    def _iter_addr(self, tensor: Operand, rw) -> DataflowActionMemoryAccess:\n        base_addr = tensor.get_phy_addr_by_offset(tensor.dim_offset)\n        data_size = list_product(tuple(tensor.dim)) * tensor.bpe\n        return DataflowActionMemoryAccess(base_addr, data_size, rw)\n\n    def get_sync_barriers(self, mesh_id, rank_cnt):\n        bm = BarrierManager()\n        barrier_id = f\"{self.case_id}_{mesh_id}\"\n        return bm.get_barrier(barrier_id + \"_rs\", rank_cnt)\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        if self.topo in [TOPO.FULLMESH8]:\n            yield from self.fullmesh_allreduce()\n        elif self.topo in [TOPO.SUPERNODE32]:\n            yield from self.get_memory_stat_supernode()\n        elif self.topo in [TOPO.SUPERNODE16]:\n            yield from self.test()\n        else:\n            raise NotImplementedError\n\n    def test(self):\n        data_size = 10*2**20\n        addr_l3_base = 0x51000000000\n        src_gen = self._iter_access_gen(\n            [DataflowActionMemoryAccess(\n                addr_l3_base,\n                data_size, \"r\")]\n        )\n        next(src_gen)\n        tar_gen = self._iter_access_gen(\n            [DataflowActionMemoryAccess(\n                addr_l3_base,\n                data_size, \"w\")]\n        )\n        next(tar_gen)\n        if self.config.gcu_id == 0 and self.get_die_id() == 0 and self.get_cluster_id() == 0 and self.get_local_engine_id() == 0:\n            mem_stat = DataflowActionMemoryStat(\n                total_count=data_size,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=1,\n                relative_ts=0,\n                memory_access_list=src_gen.send(data_size),\n                remote_target_mem_access_list=tar_gen.send(\n                    data_size),\n                bw_factor=1,\n                name=f\"RS l3->esl->gcu{1}_l3\",\n            )\n            yield mem_stat\n            em.get_event(\"test\").set(self.ref+mem_stat.latency)\n            total_latency = mem_stat.latency\n        elif self.config.gcu_id == 1 and self.get_die_id() == 0 and self.get_cluster_id() == 0 and self.get_local_engine_id() == 0:\n            max_time = yield from em.get_event(\"test\").wait(self.ref)\n            relative_ts = max_time-self.ref\n            mem_stat = DataflowActionMemoryStat(\n                total_count=data_size,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=0,\n                relative_ts=relative_ts,\n                memory_access_list=src_gen.send(data_size),\n                remote_target_mem_access_list=tar_gen.send(\n                    data_size),\n                bw_factor=1,\n                name=f\"RS l3->esl->gcu{0}_l3\",\n            )\n            yield mem_stat\n            total_latency = max_time + mem_stat.latency\n        else:\n            total_latency = 1e-9\n\n        self.core_cost.latency = total_latency\n\n    def get_memory_stat_supernode(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        data_size = 10*2**20\n        addr_l3_base = 0x51000000000\n\n        src_gen = self._iter_access_gen(\n            [DataflowActionMemoryAccess(\n                addr_l3_base,\n                data_size, \"r\")]\n        )\n        next(src_gen)\n        tar_gen = self._iter_access_gen(\n            [DataflowActionMemoryAccess(\n                addr_l3_base,\n                data_size, \"w\")]\n        )\n        next(tar_gen)\n        total_latency = 1e-9\n        if self.config.gcu_id in [0, 1]:\n            mem_stat = DataflowActionMemoryStat(\n                total_count=data_size,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=2,\n                relative_ts=0,\n                memory_access_list=src_gen.send(data_size),\n                remote_target_mem_access_list=tar_gen.send(\n                    data_size),\n                bw_factor=1,\n                name=f\"RS l3->esl->gcu{2}_l3\",\n            )\n            yield mem_stat\n            total_latency = mem_stat.latency\n\n        self.core_cost.latency = total_latency\n        return\n\n    def ring_allreduce(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        sic_id, sip_id = self.get_kernel_idx()\n        if sic_id not in [0, 1] or sip_id != 0:\n            return\n        gcu_cnt = 16\n        ring_num = 8\n        slice_num = 1\n        ring_group = sic_id\n        dtype = self.get_dtype()\n        bpe = dtype.get_bpe()\n        in_tensor = action_tensor_to_operand(self.inputs[0].tensor[0])\n        element_cnt = list_product(in_tensor.dim)\n        element_per_slice = element_cnt / gcu_cnt / ring_num / ring_group / slice_num\n        size_per_slice = element_per_slice * bpe\n        l3_base_addr = 0x51000000000\n        BUFFER_SIZE = 0x400000\n        esl_bw_factor = 1\n        rank = self.config.gcu_id\n\n        def get_sync_barriers(self, ring_id, pre_rank, post_rank):\n            bm = BarrierManager()\n            barrier_id = f\"{self.case_id}_ring{ring_id}_rank{pre_rank}_rank{post_rank}\"\n            return bm.get_barrier(barrier_id, 2)\n\n        def get_peer_ranks(self, rank, rank_num, ring_id, ring_num):\n            pre_rank = (rank - 1 + rank_num) % rank_num\n            post_rank = (rank + 1) % rank_num\n            return pre_rank, post_rank\n\n        stat_ref = 0\n        total_latency = 0\n        for ring_id in range(1, ring_num + 1):\n            for slice_id in range(1, slice_num + 1):\n                temp_buffer_addr = l3_base_addr + ring_id * BUFFER_SIZE\n                inout_buffer_addr = l3_base_addr + ring_num * ring_id * BUFFER_SIZE\n\n                temp_buffer_gen = self._iter_access_gen(\n                    [DataflowActionMemoryAccess(temp_buffer_addr, size_per_slice, \"w\")]\n                )\n\n                pre_rank, post_rank = get_peer_ranks(self, rank, gcu_cnt, ring_id, ring_num)\n                receive_barrier = get_sync_barriers(self, ring_id, pre_rank, rank)\n                send_barrier = get_sync_barriers(self, ring_id, rank, post_rank)\n\n                # reduce scatter\n                rs_ref = stat_ref\n                for step in range(1, gcu_cnt):\n                    rs_send = DataflowActionMemoryStat(\n                        total_count=size_per_slice,\n                        master=DataflowActionType.ESL,\n                        src=AddrDomain.L3,\n                        dst=AddrDomain.L3_REMOTE,\n                        rw=\"w\",\n                        gcu_id=post_rank,  # self.get_die_id(),  # 0..8\n                        relative_ts=rs_ref,\n                        memory_access_list=temp_buffer_gen.send(size_per_slice),\n                        remote_target_mem_access_list=temp_buffer_gen.send(size_per_slice),\n                        bw_factor=esl_bw_factor,\n                        name=f\"RS 0 l3->esl->->gcu{rank}_l3\",\n                    )\n                    yield rs_send\n                    rs_send_latency = rs_ref + rs_send.latency\n                    yield from send_barrier.wait(rs_send_latency)\n                    yield from receive_barrier.wait(rs_ref)\n\n    def fullmesh_allreduce(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        sic_id, sip_id = self.get_kernel_idx()\n        if sic_id not in [0, 1] or sip_id != 0:\n            return\n        # sic 0 for mesh 0, sic 1 for mesh 1\n        mesh_num = 2\n        esl_bw_factor = 1 / mesh_num\n        mesh_id = sic_id\n        in_tensor = action_tensor_to_operand(self.inputs[0].tensor[0])\n        rank_cnt = 8\n        element_cnt = list_product(in_tensor.dim)\n        element_cnt_per_mesh = element_cnt // 2\n        element_cnt_per_rank = element_cnt_per_mesh // rank_cnt\n        rank = self.config.gcu_id\n        peer_ranks = get_peer_ranks(rank_cnt, rank)\n        dtype = self.get_dtype()\n        bpe = dtype.get_bpe()\n        addr_l3_base = 0x51000000000 + (mesh_id + 1) * 0x00010000000\n        flag_l3_base = 0x52000000000 + (mesh_id + 1) * 0x00010000000\n        data_l3_base = in_tensor.addr + mesh_id * element_cnt_per_mesh * bpe\n        total_latency = 0\n        stat_ref = 0\n\n        rs_barrier = self.get_sync_barriers(mesh_id, rank_cnt)\n        # address broadcast\n\n        def write_flag(flag_ref):\n            total_latency = 0\n            flag_size = 128  # bytes\n            for idx, peer in enumerate(peer_ranks):\n                st_flag_gen = self._iter_access_gen(\n                    [DataflowActionMemoryAccess(\n                        flag_l3_base + rank * flag_size, flag_size, \"w\")]\n                )\n                next(st_flag_gen)\n                ld_flag_gen = self._iter_access_gen(\n                    [DataflowActionMemoryAccess(\n                        flag_l3_base + rank * flag_size, flag_size, \"r\")]\n                )\n                next(ld_flag_gen)\n                remote_flag_gen = self._iter_access_gen(\n                    [DataflowActionMemoryAccess(\n                        flag_l3_base + peer * flag_size, flag_size, \"w\")]\n                )\n                next(remote_flag_gen)\n\n                flag_st_l3 = DataflowActionMemoryStat(\n                    total_count=flag_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"w\",\n                    relative_ts=flag_ref,\n                    memory_access_list=st_flag_gen.send(flag_size),\n                    name=f\"flag l0->l3\",\n                )\n                yield flag_st_l3\n\n                flag_broadcast_ref = flag_ref + flag_st_l3.latency\n                flag_broadcast = DataflowActionMemoryStat(\n                    total_count=flag_size,\n                    master=DataflowActionType.ESL,\n                    src=AddrDomain.L3,\n                    dst=AddrDomain.L3_REMOTE,\n                    rw=\"w\",\n                    src_gcu_id=self.config.gcu_id,\n                    tar_gcu_id=peer,  # self.get_die_id(),  # 0..8\n                    relative_ts=flag_broadcast_ref,\n                    memory_access_list=ld_flag_gen.send(flag_size),\n                    remote_target_mem_access_list=remote_flag_gen.send(\n                        flag_size),\n                    bw_factor=esl_bw_factor,\n                    name=f\"flag l3->esl->->gcu{peer}_l3\",\n                )\n                yield flag_broadcast\n                total_latency = max(\n                    total_latency, flag_broadcast_ref + flag_broadcast.latency)\n            return total_latency\n\n        for idx, peer in enumerate(peer_ranks):\n            st_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(addr_l3_base + rank * 128, 128, \"w\")])\n            next(st_addr_gen)\n            ld_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(addr_l3_base + rank * 128, 128, \"r\")])\n            next(ld_addr_gen)\n            peer_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(addr_l3_base + peer * 128, 128, \"w\")])\n            next(peer_addr_gen)\n\n            addr_st_l3 = DataflowActionMemoryStat(\n                total_count=128,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=stat_ref,\n                memory_access_list=st_addr_gen.send(128),\n                name=f\"addr l0->l3\",\n            )\n            yield addr_st_l3\n\n            addr_broadcast_ref = stat_ref + addr_st_l3.latency\n            addr_broadcast = DataflowActionMemoryStat(\n                total_count=128,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=peer,\n                relative_ts=addr_broadcast_ref,\n                memory_access_list=ld_addr_gen.send(128),\n                remote_target_mem_access_list=peer_addr_gen.send(128),\n                bw_factor=esl_bw_factor,\n                name=f\"addr l3->esl->gcu{peer}_l3\",\n            )\n            yield addr_broadcast\n            total_latency = max(\n                total_latency, addr_broadcast_ref + addr_broadcast.latency)\n\n        # write flags\n        flag_ref = total_latency\n        total_latency = yield from write_flag(flag_ref)\n        # reduce scatter\n        reduce_scatter_ref = total_latency\n        data_size = element_cnt_per_rank * bpe\n        for idx, peer in enumerate(peer_ranks):\n            rs_l3_base = data_l3_base + peer * data_size\n            rs_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(rs_l3_base, data_size, \"r\")])\n            next(rs_addr_gen)\n            remote_rs_l3_base = data_l3_base + rank * data_size\n            remote_rs_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(remote_rs_l3_base, data_size, \"w\")])\n            next(remote_rs_addr_gen)\n            reduce_scatter = DataflowActionMemoryStat(\n                total_count=data_size,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=peer,\n                relative_ts=reduce_scatter_ref,\n                memory_access_list=rs_addr_gen.send(data_size),\n                remote_target_mem_access_list=remote_rs_addr_gen.send(\n                    data_size),\n                bw_factor=esl_bw_factor,\n                name=f\"RS l3->esl->gcu{peer}_l3\",\n            )\n            yield reduce_scatter\n            total_latency = max(\n                total_latency, reduce_scatter_ref + reduce_scatter.latency)\n\n        # write flags\n        flag_ref = total_latency\n        total_latency = yield from write_flag(flag_ref)\n        # all gather\n        total_latency = yield from rs_barrier.wait(self.ref + total_latency)\n        total_latency -= self.ref\n        all_gather_ref = total_latency\n        data_size = element_cnt_per_rank * bpe\n        for idx, peer in enumerate(peer_ranks):\n            al_l3_base = data_l3_base + rank * data_size\n            al_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(al_l3_base, data_size, \"r\")])\n            next(al_addr_gen)\n            remote_al_l3_base = data_l3_base + peer * data_size\n            remote_al_addr_gen = self._iter_access_gen(\n                [DataflowActionMemoryAccess(remote_al_l3_base, data_size, \"w\")])\n            next(remote_al_addr_gen)\n            all_gather = DataflowActionMemoryStat(\n                total_count=data_size,\n                master=DataflowActionType.ESL,\n                src=AddrDomain.L3,\n                dst=AddrDomain.L3_REMOTE,\n                rw=\"w\",\n                src_gcu_id=self.config.gcu_id,\n                tar_gcu_id=peer,\n                relative_ts=all_gather_ref,\n                memory_access_list=al_addr_gen.send(data_size),\n                remote_target_mem_access_list=remote_al_addr_gen.send(\n                    data_size),\n                bw_factor=esl_bw_factor,\n                name=f\"AG l3->esl->gcu{peer}_l3\",\n            )\n            yield all_gather\n            total_latency = max(\n                total_latency, all_gather_ref + all_gather.latency)\n\n        # write flags\n        flag_ref = total_latency\n        total_latency = yield from write_flag(flag_ref)\n\n        self.core_cost.latency = total_latency\n\n    def _basic_stat_info(self):\n        self.core_cost.tensor_dtype = self.get_dtype()",
    "start_line": 61,
    "end_line": 453,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUAllReduceAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_allreduce_action.XPUAllReduceAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    ld_ins_num: int = 0\n    scalar_ins_num: int = 0\n    st_ins_num: int = 0\n    compute_ins_num: int = 0\n    compute_ins_shape: int = 0",
    "start_line": 16,
    "end_line": 22,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.elementwise_vector_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.elementwise_vector_kernel",
    "name": "elementwise_vector_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.InstructionInfo"
    ],
    "source_code": "def elementwise_vector_kernel(size: int, dtype: DType = DType.FP16):\n    compute_ins_num = 0\n    st_ins_num = 0\n    scalar_ins_num = 18\n    ld_ins_num = 0\n    elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n    enroll_time = 16\n    for i in range(0, size, elements_per_OA):\n        ld_ins_num += 2\n        compute_ins_num += 1\n        scalar_ins_num += 3\n        st_ins_num += 1\n\n    return InstructionInfo(dtype, ld_ins_num, scalar_ins_num, st_ins_num, compute_ins_num, elements_per_OA)",
    "start_line": 25,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "size",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function elementwise_vector_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.elementwise_vector_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    unroll_num: int = 16\n    subthread_num: int = 4\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = None\n    elements_per_cycle: float = 0",
    "start_line": 42,
    "end_line": 52,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUElementWiseAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUElementWiseAction",
    "name": "XPUElementWiseAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.elementwise_vector_kernel",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
    ],
    "source_code": "class XPUElementWiseAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        lhs_tensor = self.inputs[0].tensor[0]\n        lhs_access = self._iter_addr(lhs_tensor, 'r')\n        rhs_tensor = self.inputs[1].tensor[0]\n        rhs_access = self._iter_addr(rhs_tensor, 'r')\n        out_tensor = self.outputs[0].tensor[0]\n        out_access = self._iter_addr(out_tensor, 'w')\n        yield from self._iter_access_gen([lhs_access, rhs_access, out_access])\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 3 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = int(\n            self.config.memory.l0.OA_SIZE / self.core_cost.oa_occupation)\n        self.core_cost.main_body_length = self.core_cost.unroll_num * \\\n            self.core_cost.subthread_num\n        self.core_cost.main_body_num = float(\n            self.core_cost.instruction_info.compute_ins_num) / self.core_cost.main_body_length\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        self.core_cost.instruction_info = elementwise_vector_kernel(\n            size, dtype=self.get_dtype())\n        self._get_l0_occupation()\n        self._get_main_body_length()\n        stat_ref = 0\n        total_latency = 0\n        total_compute_latency = 0\n\n        dtype = self.core_cost.instruction_info.dtype\n\n        throughput = self.config.compute.thread_1d_throughput[dtype]\n\n        lhs_tensor = self.inputs[0].tensor[0]\n        lhs_access = self._iter_addr(lhs_tensor, 'r')\n        rhs_tensor = self.inputs[1].tensor[0]\n        rhs_access = self._iter_addr(rhs_tensor, 'r')\n        out_tensor = self.outputs[0].tensor[0]\n        out_access = self._iter_addr(out_tensor, 'w')\n        input_gen = self._iter_access_gen([lhs_access, rhs_access])\n        next(input_gen)\n        output_gen = self._iter_access_gen([out_access])\n        next(output_gen)\n\n        for i in range(0, self.core_cost.instruction_info.compute_ins_num, self.core_cost.main_body_length):\n            inst_num = min(self.core_cost.main_body_length,\n                           self.core_cost.instruction_info.compute_ins_num - i)\n            lhs, rhs, out = [inst_num * BYPTES_PER_OA] * 3\n\n            lhs_read = DataflowActionMemoryStat(\n                total_count=lhs*2,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(lhs)\n            )\n            yield lhs_read\n            lhs_latency, lhs_leading_latency = lhs_read.latency, lhs_read.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - lhs_latency:{lhs_latency}, lhs_leading_latency:{lhs_leading_latency}\"\n            )\n\n            # rhs_ref = stat_ref+lhs_latency-lhs_leading_latency\n            # rhs_read = DataflowActionMemoryStat(\n            #     rhs, DataflowActionType.XPU, AddrDomain.L0, AddrDomain.L3, \"r\", rhs_ref)\n            # yield rhs_read\n            # rhs_latency, rhs_leading_latency = rhs_read.latency, rhs_read.leading_latency\n            # logger.debug(\n            #     f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - rhs_latency:{rhs_latency}, rhs_leading_latency:{rhs_leading_latency}\"\n            # )\n\n            # vld_latency = max(lhs_latency, rhs_latency)\n            # vld_leading_latency = max(lhs_leading_latency, rhs_leading_latency)\n\n            vld_leading_latency = lhs_leading_latency\n\n            compute_ops = inst_num * self.core_cost.instruction_info.compute_ins_shape\n            compute_cycle = (\n                compute_ops / throughput\n                + inst_num * self.core_cost.instruction_info.scalar_ins_num /\n                self.core_cost.instruction_info.compute_ins_num\n            )\n            compute_ref = stat_ref+vld_leading_latency\n            # compute_latency = compute_cycle / \\\n            #     (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n\n            compute_stat = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_ops\n                },\n                relative_ts=compute_ref,\n            )\n            yield compute_stat\n\n            compute_latency = compute_stat.latency\n            total_compute_latency += compute_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - compute_cycle: {compute_cycle}, compute latency:{compute_latency}\"\n            )\n\n            out_ref = compute_ref + compute_latency\n            out_write = DataflowActionMemoryStat(\n                total_count=out,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=out_ref,\n                memory_access_list=output_gen.send(out)\n            )\n            yield out_write\n            out_latency, out_leading_latency = out_write.latency, out_write.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - out_latency:{out_latency}, out_leading_latency:{out_leading_latency}\"\n            )\n\n            total_latency = max(\n                total_latency,\n                stat_ref + lhs_latency,\n                compute_ref + compute_latency,  # compute latency,\n                out_ref + out_latency,  # vst latency\n            )\n            stat_ref = total_latency - vld_leading_latency  # update ref\n            # stat_ref = total_latency-out_leading_latency  # update ref\n        print(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n        self.core_cost.compute_cost = total_compute_latency\n        yield  # using yield instead of return to avoid from StopIterator exception\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        dtype = self.core_cost.instruction_info.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.vector_dtype = dtype\n        self.core_cost.vector_ops = info.compute_ins_shape * info.compute_ins_num\n        self.core_cost.ld_l0_l3 = 2 * \\\n            (info.compute_ins_shape * bpe) * info.compute_ins_num\n        self.core_cost.st_l0_l3 = (info.compute_ins_shape *\n                                   bpe) * info.compute_ins_num\n        self.core_cost.elements_per_cycle = (\n            info.compute_ins_shape * info.st_ins_num /\n            (self.core_cost.latency * 1e9 * self.config.freq.CORE)\n        )",
    "start_line": 56,
    "end_line": 206,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUElementWiseAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUElementWiseAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUAddAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUAddAction",
    "name": "XPUAddAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUElementWiseAction"
    ],
    "source_code": "class XPUAddAction(XPUElementWiseAction):\n    pass",
    "start_line": 209,
    "end_line": 210,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUElementWiseAction"
    ],
    "class_name": null,
    "display_name": "class XPUAddAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUAddAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUMulAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUMulAction",
    "name": "XPUMulAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_elementwise_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUElementWiseAction"
    ],
    "source_code": "class XPUMulAction(XPUElementWiseAction):\n    pass",
    "start_line": 213,
    "end_line": 214,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUElementWiseAction"
    ],
    "class_name": null,
    "display_name": "class XPUMulAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_elementwise_action.XPUMulAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action._InvGauss": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action._InvGauss",
    "name": "_InvGauss",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [],
    "source_code": "    class _InvGauss:\n        def rvs(self, mu):\n            sigma = abs(mu) * 0.1 if mu else 1.0\n            return max(random.gauss(mu if mu else 1.0, sigma), 0.0)\n\n        def ppf(self, q, mu):\n            # simple fallback based on gaussian assumption\n            sigma = abs(mu) * 0.1 if mu else 1.0\n            return max(mu + sigma, 0.0)",
    "start_line": 12,
    "end_line": 20,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _InvGauss",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action._InvGauss"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action._Uniform": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action._Uniform",
    "name": "_Uniform",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action.uniform"
    ],
    "source_code": "    class _Uniform:\n        def __init__(self, low, width):\n            self.low = low\n            self.width = width\n\n        def rvs(self):\n            return random.uniform(self.low, self.low + self.width)",
    "start_line": 22,
    "end_line": 28,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _Uniform",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action._Uniform"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.uniform": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.uniform",
    "name": "uniform",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action._Uniform"
    ],
    "source_code": "    def uniform(a, b):\n        return _Uniform(a, b)",
    "start_line": 32,
    "end_line": 33,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "a",
      "b"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function uniform",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.uniform"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.align_data_size": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.align_data_size",
    "name": "align_data_size",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [],
    "source_code": "def align_data_size(size, align=NOC_DATALINE_SIZE):\n    return (size + align - 1) // align * align",
    "start_line": 44,
    "end_line": 45,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "size",
      "align"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function align_data_size",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.align_data_size"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    bpe: int = 2\n    ld_index_l3_ins_num: int = 0\n    ld_table_l3_ins_num: int = 0\n    st_table_l3_ins_num: int = 0\n    scalar_ins_num: int = 0",
    "start_line": 49,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.gather_instruction_mapping": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.gather_instruction_mapping",
    "name": "gather_instruction_mapping",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action.InstructionInfo"
    ],
    "source_code": "def gather_instruction_mapping(embedding_size: int, index: int, bpe: int = 2):\n    ld_index_L3_inst_cout = 0\n    ld_table_L3_inst_cout = 0\n    st_table_L3_inst_cout = 0\n    scalar_inst_count = 0\n    remain_index_size = index\n    ELEMENTS_PER_OA = int(BYPTES_PER_OA / bpe)\n    scalar_inst_count += 8\n    for i in range(0, index, ELEMENTS_PER_OA):\n        slice_index_size = ELEMENTS_PER_OA if remain_index_size > ELEMENTS_PER_OA else remain_index_size\n        remain_index_size -= slice_index_size\n        scalar_inst_count += 8\n        scalar_inst_count += 16\n        ld_index_L3_inst_cout += 1\n        for j in range(0, slice_index_size):\n            scalar_inst_count += 3\n            scalar_inst_count += 6\n            remain_length_size = embedding_size\n            for k in range(0, embedding_size, ELEMENTS_PER_OA):\n                slice_length_size = ELEMENTS_PER_OA if remain_length_size > ELEMENTS_PER_OA else remain_length_size\n                remain_length_size -= slice_length_size\n                scalar_inst_count += 3\n                scalar_inst_count += 7\n                ld_table_L3_inst_cout += 1\n                st_table_L3_inst_cout += 1\n\n    return InstructionInfo(\n        bpe=bpe,\n        ld_index_l3_ins_num=ld_index_L3_inst_cout,\n        ld_table_l3_ins_num=ld_table_L3_inst_cout,\n        st_table_l3_ins_num=st_table_L3_inst_cout,\n        scalar_ins_num=scalar_inst_count,\n    )",
    "start_line": 60,
    "end_line": 92,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "embedding_size",
      "index",
      "bpe"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function gather_instruction_mapping",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.gather_instruction_mapping"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    subthread_num: int = 8\n    unroll_num: int = 16\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    elements_per_cycle: float = 0",
    "start_line": 96,
    "end_line": 106,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gather_action.XPUGatherAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.XPUGatherAction",
    "name": "XPUGatherAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gather_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action.align_data_size",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action.gather_instruction_mapping",
      "nova-platform.nova_platform.dataflow.action.xpu_gather_action.uniform"
    ],
    "source_code": "class XPUGatherAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        out_dim = self.outputs[0].tensor[0].dims\n        return out_dim[0], out_dim[1]  # table size, index size\n\n    def _iter_random_addr(self, tensor: DiagTensor, num):\n        bpe = tensor.bpe\n        stride = tensor.stride_dims[0]*bpe\n        width = tensor.dims[0]*bpe\n        mu = self.config.gather_mu\n        width = align_data_size(width) if mu >= 0 else width\n        rows = tensor.dims[1]\n        base_addr = tensor.addr\n\n        if mu < 0:\n            # e.g. mu = -4.100  [3].[200]\n            # skip_m = 4\n            # seq_num = 200\n            skip_m = -int(mu)\n            seq_num = int((\"%.03f\" % (mu + skip_m))[3:6])\n            assert num//seq_num == num/seq_num, \"num should be divided by continuity\"\n            loop_num = num//seq_num\n            # if mu <0, use uniform distribution\n            uniform_gen = uniform(0, rows)\n            for _ in range(loop_num):\n                r = int(uniform_gen.rvs()) % (rows-(seq_num-1)*skip_m)\n                for i in range(seq_num):\n                    addr = base_addr + (r+i)*stride\n                    yield DataflowActionMemoryAccess(addr, width, 'r')\n        else:\n            lx = invgauss.ppf(0.95, mu)\n            scale = rows/lx\n            for _ in range(num):\n                # addr = base_addr + random.randint(0, rows-1)*stride\n                # r = int(invgauss.rvs(rows-1))  # 1 0.5 0.25\n                # r = int(invgauss.rvs(mu)) % rows\n                r = int(invgauss.rvs(mu)*scale) % rows\n                addr = base_addr + r*stride\n\n                yield DataflowActionMemoryAccess(addr, width, 'r')\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        embedding_size, idx_size = self.get_valid_shape()\n        bpe = self.inputs[0].tensor[0].bpe\n        self.core_cost.instruction_info = gather_instruction_mapping(\n            embedding_size, idx_size, bpe=bpe)\n        ld_table_vs_index = int(\n            self.core_cost.instruction_info.ld_table_l3_ins_num /\n            self.core_cost.instruction_info.ld_index_l3_ins_num\n        )\n\n        index_tensor = self.inputs[1].tensor[0]\n        num_lookup = index_tensor.dims[0]\n        index_access = DataflowActionMemoryAccess(\n            index_tensor.addr, index_tensor.dims[0]*index_tensor.bpe, 'r')\n        input_gen = self._iter_access_gen([index_access])\n        next(input_gen)\n\n        lookup_tensor = self.inputs[0].tensor[0]\n        mu = self.config.gather_mu\n        lookup_access = self._iter_random_addr(\n            lookup_tensor, num_lookup)\n        lookup_gen = self._iter_access_gen(lookup_access)\n        next(lookup_gen)\n\n        output_tensor = self.outputs[0].tensor[0]\n        output_access = self._iter_tensor_addr(\n            output_tensor.addr, output_tensor, 'w')\n        output_gen = self._iter_access_gen(output_access)\n        next(output_gen)\n\n        stat_ref = 0\n        total_latency = 0\n        for i in range(0, self.core_cost.instruction_info.ld_index_l3_ins_num, self.core_cost.subthread_num):\n            ld_index_num = min(self.core_cost.subthread_num,\n                               self.core_cost.instruction_info.ld_index_l3_ins_num - i)\n            index_read = DataflowActionMemoryStat(\n                total_count=ld_index_num * BYPTES_PER_OA,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(\n                    ld_index_num * BYPTES_PER_OA),\n                name=\"ld_index\",\n            )\n\n            yield index_read\n            total_ld_table_num = ld_table_vs_index * ld_index_num\n            table_ref = stat_ref + index_read.latency\n            table_latency = 0\n            offset = self.core_cost.unroll_num * self.core_cost.subthread_num\n            # if mu < 0:\n            #     offset = total_ld_table_num\n            for j in range(0, total_ld_table_num, offset):\n                mu = self.config.gather_mu\n                if mu >= 0:\n                    ld_table_num = min(\n                        self.core_cost.unroll_num * self.core_cost.subthread_num, total_ld_table_num - j)\n                    table_datalane_size = min(\n                        BYPTES_PER_OA, align_data_size(embedding_size * bpe))\n                else:\n                    # ld_table_num = total_ld_table_num\n                    # table_datalane_size = embedding_size * bpe\n                    ld_table_num = min(\n                        self.core_cost.unroll_num * self.core_cost.subthread_num, total_ld_table_num - j)\n                    table_datalane_size = min(\n                        BYPTES_PER_OA, embedding_size * bpe)\n                table_read = DataflowActionMemoryStat(\n                    total_count=ld_table_num * table_datalane_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"r\",\n                    relative_ts=table_ref,\n                    memory_access_list=lookup_gen.send(\n                        ld_table_num * table_datalane_size),\n                    name=\"ld_table\",\n                )\n                yield table_read\n                table_write = DataflowActionMemoryStat(\n                    total_count=ld_table_num * table_datalane_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"w\",\n                    relative_ts=table_ref + table_read.leading_latency,\n                    memory_access_list=output_gen.send(\n                        ld_table_num * table_datalane_size),\n                    name=\"st_table\",\n                )\n                yield table_write\n                table_latency = max(\n                    table_latency,\n                    table_ref + table_read.latency,\n                    table_ref + table_read.leading_latency + table_write.latency,\n\n                )\n                table_ref = table_latency - table_read.leading_latency\n\n            total_latency = max(\n                total_latency,\n                stat_ref + index_read.latency,  # index latency,\n                table_latency,\n            )\n            stat_ref = total_latency - index_read.leading_latency\n        print(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = 16 * 4\n\n        self.core_cost.main_body_num = float(\n            self.core_cost.instruction_info.st_table_l3_ins_num) / self.core_cost.main_body_length\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        self.core_cost.ld_l0_l3 = (info.ld_index_l3_ins_num +\n                                   info.ld_table_l3_ins_num) * BYPTES_PER_OA\n        self.core_cost.st_l0_l3 = info.st_table_l3_ins_num * BYPTES_PER_OA\n\n    def compute(self, context: BossaNovaContext) -> Generator[DataflowActionMemoryStat, any, CoreCost]:\n        self._get_main_body_length()\n        yield from self.get_memory_stat()\n        self._basic_stat_info()\n        return self.core_cost",
    "start_line": 110,
    "end_line": 279,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUGatherAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gather_action.XPUGatherAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType\n    mid_level_ins_num: int\n    mid_level_ins_shape: list\n    scalar_ins_num: int\n    st_ins_num: int",
    "start_line": 18,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    dtype: DType = DType.FP16\n    ld_smr_occupation: int = 0  # Bytes\n    ld_iv_occupation: int = 0  # Bytes\n    va_occupation: int = 0  # Bytes\n\n    ld_smr_basic_cost: int = 0\n    ld_iv_basic_cost: int = 0\n    scalar_basic_cost_per_cycle: int = 0\n    vmm_basic_cost_per_cycle: int = 0\n    vst_basic_cost: int = 0\n\n    main_body_length: int = 0\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n    compute_vs_store_ratio: float = 0\n    vst_cost: int = 0\n    leading_latency_without_reuse: int = 0\n    data_reuse_times: int = 0\n\n    instruction_info: InstructionInfo = 0\n    gemm_shape: List[int] = field(default_factory=list)\n\n    subthread_num: int = 4\n    input_src: AddrDomain = AddrDomain.LOCAL\n    output_dest: AddrDomain = AddrDomain.L3",
    "start_line": 27,
    "end_line": 52,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.gemm_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.gemm_kernel",
    "name": "gemm_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.InstructionInfo"
    ],
    "source_code": "def gemm_kernel(m: int, n: int, k: int, dtype: DType = DType.FP16):\n    computer_count = 0\n    st_count = 0\n    scalar_count = 0\n\n    if dtype == DType.FP16:\n        slice_m = 64\n        slice_n = 64\n        slice_k = 32\n        eleByte = 2\n    elif dtype == DType.INT8 or dtype == DType.FP8:\n        slice_m = 64\n        slice_n = 64\n        slice_k = 64\n        eleByte = 1\n    else:\n        raise RuntimeError(f\"unsupport gemm_kernel dtype {dtype}\")\n\n    scalar_count += 23\n    scalar_count += 2\n    for loop_m in range(0, m, slice_m):\n        scalar_count += 3\n        scalar_count += 2\n        scalar_count += 2\n        for loop_n in range(0, n, slice_n):\n            scalar_count += 3\n            scalar_count += 8\n            computer_count += 1\n            scalar_count += 2\n            for loop_k in range(0, k, slice_k):\n                scalar_count += 3\n                scalar_count += 4\n                computer_count += 1\n            scalar_count += 2\n            st_count += 1\n            scalar_count += 1\n    ins_shape_m = min(m, slice_m)\n    ins_shape_n = min(n, slice_n)\n    ins_shape_k = min(k, slice_k)\n    return InstructionInfo(dtype, computer_count, [ins_shape_m, ins_shape_n, ins_shape_k], scalar_count, st_count)",
    "start_line": 55,
    "end_line": 94,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "m",
      "n",
      "k",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function gemm_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.gemm_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.XPUGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.XPUGemmAction",
    "name": "XPUGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.gemm_kernel",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess"
    ],
    "source_code": "class XPUGemmAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        valid_m = self.data[4] & (0xFFFF)\n        valid_n = self.data[4] >> 16\n        valid_k = self.data[5]\n        return [valid_m, valid_n, valid_k]\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        gemm_shape = self.get_valid_shape()\n        self.core_cost.gemm_shape = gemm_shape\n        dtype = self.get_dtype()\n        self.core_cost.dtype = dtype\n        self.core_cost.instruction_info = gemm_kernel(\n            gemm_shape[0], gemm_shape[1], gemm_shape[2], dtype=dtype)\n        self._get_mem_src()\n        self._get_l0_occupation()\n        self._get_basic_cost()\n        self._get_main_body_length()\n        bpe = self.core_cost.instruction_info.dtype.get_bpe()\n        m = 0\n        n = 1\n        k = 2\n        dtype = self.core_cost.instruction_info.dtype\n        mac_array = self.config.compute.thread_2d_throughput[dtype]\n\n        lhs_tensor = self.inputs[0].tensor[0]\n        lhs_access = DataflowActionMemoryAccess(\n            lhs_tensor.addr, gemm_shape[m] * gemm_shape[n] * bpe, \"r\")\n        lhs_gen = self._iter_access_gen([lhs_access])\n        next(lhs_gen)\n\n        rhs_tensor = self.inputs[1].tensor[0]\n        rhs_access = DataflowActionMemoryAccess(\n            rhs_tensor.addr, gemm_shape[n] * gemm_shape[k] * bpe, \"r\")\n        rhs_gen = self._iter_access_gen([rhs_access])\n        next(rhs_gen)\n\n        if len(self.outputs) > 0:\n            out_tensor = self.outputs[0].tensor[0]\n            out_access = DataflowActionMemoryAccess(\n                out_tensor.addr, gemm_shape[m] * gemm_shape[n] * bpe, \"r\")\n            out_gen = self._iter_access_gen([out_access])\n            next(out_gen)\n\n        stat_ref = 0\n        total_latency = 0\n        mid_level_ins_shape = self.core_cost.instruction_info.mid_level_ins_shape\n        for i in range(0, self.core_cost.instruction_info.mid_level_ins_num, self.core_cost.main_body_length):\n            mid_ins_num = min(self.core_cost.main_body_length,\n                              self.core_cost.instruction_info.mid_level_ins_num - i)\n            vst_ins_num = (i + self.core_cost.main_body_length) // (self.core_cost.compute_vs_store_ratio) - i // (\n                self.core_cost.compute_vs_store_ratio\n            )\n            lhs_size = mid_ins_num * \\\n                mid_level_ins_shape[m] * mid_level_ins_shape[k] * bpe\n            lhs_read = DataflowActionMemoryStat(\n                total_count=lhs_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=self.core_cost.input_src,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=lhs_gen.send(lhs_size),\n                name=\"lhs\",\n            )\n            yield lhs_read\n            rhs_size = mid_ins_num * \\\n                mid_level_ins_shape[n] * mid_level_ins_shape[k] * bpe\n            rhs_read = DataflowActionMemoryStat(\n                total_count=rhs_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=self.core_cost.input_src,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=rhs_gen.send(rhs_size),\n                name=\"rhs\",\n            )\n            yield rhs_read\n\n            scalar_ops = self.core_cost.instruction_info.scalar_ins_num / \\\n                self.core_cost.instruction_info.mid_level_ins_num*mid_ins_num\n            compute_stat1 = DataflowActionComputeStat(\n                name=\"scalar\",\n                compute_scalar_cycle=scalar_ops,\n                relative_ts=stat_ref + max(lhs_read.latency, rhs_read.latency),\n            )\n            yield compute_stat1\n            scalar_cost = compute_stat1.latency\n\n            compute_mac = self.core_cost.vmm_basic_cost_per_cycle * mid_ins_num\n\n            VMM = 10  # KG->L0 latency\n            compute_ref = (\n                stat_ref\n                + lhs_read.leading_latency\n                # + (max(lhs_read.latency - lhs_read.leading_latency, rhs_read.latency - rhs_read.leading_latency))\n                # / self.core_cost.subthread_num\n            )\n            compute_stat2 = DataflowActionComputeStat(\n                name=\"mac\",\n                compute_2d_ops={dtype: compute_mac * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2\n\n            compute_vmm = DataflowActionComputeStat(\n                name=\"vmm\",\n                compute_nop_cycle=VMM,\n                relative_ts=compute_ref + compute_stat2.latency,\n            )\n            yield compute_vmm\n            compute_latency = compute_ref + compute_stat2.latency + compute_vmm.latency\n            # out_ref = compute_ref + compute_latency / self.core_cost.subthread_num\n            if self.core_cost.dtype == DType.FP8:\n                scaling_ref = compute_ref\n                compute_scaling = DataflowActionComputeStat(\n                    name=\"scaling\",\n                    compute_1d_ops={DType.FP32: (\n                        mid_level_ins_shape[m] * mid_level_ins_shape[n] * mid_ins_num)},\n                    relative_ts=scaling_ref,\n                )\n                yield compute_scaling\n                compute_latency = max(\n                    compute_latency, scaling_ref + compute_scaling.latency)\n            out_ref = compute_ref\n\n            out_latency, out_leading_latency = 0, 0\n            out_size = vst_ins_num * \\\n                mid_level_ins_shape[m] * mid_level_ins_shape[n] * bpe\n            if len(self.outputs) > 0 and vst_ins_num > 0:\n                out_wirte = DataflowActionMemoryStat(\n                    total_count=out_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=self.core_cost.output_dest,\n                    rw=\"w\",\n                    relative_ts=out_ref,\n                    memory_access_list=out_gen.send(out_size),\n                    name=\"out\",\n                )\n                yield out_wirte\n                out_latency, out_leading_latency = out_wirte.latency, out_wirte.leading_latency\n            total_latency = max(\n                total_latency,\n                stat_ref + max(lhs_read.latency,\n                               rhs_read.latency) + scalar_cost,\n                compute_latency,\n                out_ref + out_latency,\n            )\n            stat_ref = (\n                max(\n                    max(\n                        stat_ref + max(lhs_read.latency,\n                                       rhs_read.latency) + scalar_cost,\n                        compute_latency,\n                    ),\n                    out_ref + out_leading_latency,\n                )\n                - lhs_read.leading_latency\n            )  # update ref\n\n        self.core_cost.latency = total_latency\n\n    def _get_mem_src(self):\n        bench_gemm_op_version = self.dataflow_config.get(\n            \"bench_gemm_op_version\", 1)\n        if bench_gemm_op_version == 1 or bench_gemm_op_version == 2:\n            self.core_cost.input_src = AddrDomain.SHARED\n            self.core_cost.output_dest = AddrDomain.SHARED\n        elif bench_gemm_op_version == 3:\n            self.core_cost.input_src = AddrDomain.LOCAL\n            self.core_cost.output_dest = AddrDomain.L3\n        else:\n            raise RuntimeError(\n                f\"Unsupported bench_gemm_op_version {bench_gemm_op_version}\")\n\n    def _get_l0_occupation(self):  # step 3\n        instruction_info = self.core_cost.instruction_info\n        mid_level_ins_shape = instruction_info.mid_level_ins_shape\n        bpe = self.core_cost.dtype.get_bpe()\n        m = 0\n        n = 1\n        k = 2\n        self.core_cost.ld_smr_occupation = mid_level_ins_shape[m] * \\\n            mid_level_ins_shape[k] * bpe\n        self.core_cost.ld_iv_occupation = mid_level_ins_shape[n] * \\\n            mid_level_ins_shape[k] * bpe\n        self.core_cost.va_occupation = mid_level_ins_shape[m] * \\\n            mid_level_ins_shape[n] * bpe\n\n    def _get_basic_cost(self):  # step 4\n        m = 0\n        n = 1\n        k = 2\n        instruction_info: InstructionInfo = self.core_cost.instruction_info\n        mid_level_ins_shape = instruction_info.mid_level_ins_shape\n        # bpe = instruction_info.dtype.get_bpe()\n        # throughput = asdict(self.config.compute.thread_2d_throughput)\n        # mac_array = throughput[instruction_info.dtype.name.upper()]\n        # mac_array = self.config.compute.thread_2d_throughput[instruction_info.dtype]\n        self.core_cost.scalar_basic_cost_per_cycle = (\n            instruction_info.scalar_ins_num\n            / instruction_info.mid_level_ins_num\n            # / (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n        )\n        self.core_cost.vmm_basic_cost_per_cycle = (\n            mid_level_ins_shape[m]\n            * mid_level_ins_shape[n]\n            * mid_level_ins_shape[k]\n            # / (mac_array * self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n        )\n\n        self.core_cost.compute_vs_store_ratio = instruction_info.mid_level_ins_num // instruction_info.st_ins_num\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = min(\n            (self.config.memory.l0.IV_SIZE // self.core_cost.ld_iv_occupation),\n            (self.config.memory.l0.SMR_SIZE // self.core_cost.ld_smr_occupation),\n        )\n\n    def _basic_stat_info(self):\n        # TODO: need review\n        info = self.core_cost.instruction_info\n        m, n, k = info.mid_level_ins_shape  # m,n,k\n        dtype = self.core_cost.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.tensor_macs[dtype] = m * \\\n            n * k * info.mid_level_ins_num  # mac\n        if dtype == DType.FP8:\n            self.core_cost.vector_ops[DType.FP32] = info.scalar_ins_num + \\\n                m * n * info.mid_level_ins_num\n        else:\n            self.core_cost.vector_ops[dtype] = info.scalar_ins_num\n        self.core_cost.ld_l0_shared = (\n            m * k * bpe + n * k * bpe) * info.mid_level_ins_num\n        self.core_cost.st_l0_shared = m * n * bpe * info.st_ins_num",
    "start_line": 98,
    "end_line": 336,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action.XPUGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.op_base.list_product"
    ],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    grid_dim: List[int] = field(default_factory=list)\n    block_dim: List[int] = field(default_factory=list)\n    thread_dim: List[int] = field(default_factory=list)\n    subthread_dim: List[int] = field(default_factory=list)\n\n    def subthread_cnt_in_thread(self):\n        return list_product(tuple(self.thread_dim))",
    "start_line": 19,
    "end_line": 27,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    sip_workloads: list = field(default_factory=list)",
    "start_line": 31,
    "end_line": 33,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.reversed_dims": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.reversed_dims",
    "name": "reversed_dims",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "depends_on": [],
    "source_code": "def reversed_dims(t):\n    return tuple(reversed(t))",
    "start_line": 36,
    "end_line": 37,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "t"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function reversed_dims",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.reversed_dims"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.XPUGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.XPUGemmAction",
    "name": "XPUGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_2.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.InstructionInfo",
      "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.action_tensor_to_operand",
      "nova-platform.nova_platform.benchmark.op_base.Workload",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.benchmark.op_base.Operand",
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.reversed_dims"
    ],
    "source_code": "class XPUGemmAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        valid_m = self.data[4] & (0xFFFF)\n        valid_n = self.data[4] >> 16\n        valid_k = self.data[5]\n        return [valid_m, valid_n, valid_k]\n\n    def get_instruction_info(self):\n        return InstructionInfo(\n            dtype=self.get_dtype(),\n            grid_dim=reversed_dims(self.tile_info.cube_dim),\n            block_dim=reversed_dims(self.tile_info.grid_dim),\n            thread_dim=reversed_dims(self.tile_info.block_dim),\n            subthread_dim=self.tile_info.tile_shape,\n        )\n\n    def get_sip_workload(self):\n\n        def action_tensor_to_operand(tensor, shape):\n\n            return Operand(\n                dim=shape,\n                addr=tensor.addr,\n                bpe=tensor.bpe,\n                dim_offset=(0, 0, 0),\n                dim_stride=reversed_dims(tensor.stride_dims[:3]),\n            )\n\n        lhs = self.inputs[0].tensor[0]\n        rhs = self.inputs[1].tensor[0]\n        res = self.outputs[0].tensor[0] if len(self.outputs) else None\n        m, n, k = self.get_valid_shape()\n        return Workload(\n            inputs=[action_tensor_to_operand(\n                lhs, (1, m, k)), action_tensor_to_operand(rhs, (1, k, n))],\n            outputs=[action_tensor_to_operand(res, (1, m, n))] if res else [],\n            dtype=self.get_dtype(),\n        )\n\n    def get_minimum_iter_k(self, b, m, n):\n        throughput = self.config.compute.thread_2d_throughput[self.get_dtype()]\n        l1_latency = self.config.bw.l0.local.pre_latency\n        k = 0\n        mac_cycle = 0\n        while mac_cycle < l1_latency:\n            k += 32\n            mac_cycle = b * m * n * k / throughput\n        return k\n\n    def _iter_tensor_addr(self, tensor: Operand, rw) -> Generator[DataflowActionMemoryAccess, None, None]:\n        mem_access = tensor.get_contiguous_mem_accesses()\n        for addr, size in mem_access:\n            yield DataflowActionMemoryAccess(addr, size, rw)\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        in_m, in_n, in_k = self.get_valid_shape()\n        in_b = 1\n        self.core_cost.instruction_info = self.get_instruction_info()\n        self.core_cost.sip_workloads.append((in_b, in_m, in_n, in_k))\n        dtype = self.get_dtype()\n        self.core_cost.dtype = dtype\n        bpe = dtype.get_bpe()\n        sip_workload = self.get_sip_workload()\n\n        dtype = self.core_cost.instruction_info.dtype\n        lhs_gen = self._iter_access_gen(\n            list(self._iter_tensor_addr(sip_workload.inputs[0], \"r\")))\n        next(lhs_gen)\n\n        rhs_gen = self._iter_access_gen(\n            list(self._iter_tensor_addr(sip_workload.inputs[1], \"r\")))\n        next(rhs_gen)\n\n        if len(sip_workload.outputs) > 0:\n            out_gen = self._iter_access_gen(\n                list(self._iter_tensor_addr(sip_workload.outputs[0], \"r\")))\n            next(out_gen)\n        iter_ref = 0\n        iter_k = self.get_minimum_iter_k(in_b, in_m, in_n)\n        iter_k = min(in_k, iter_k)\n        self.core_cost.tensor_macs[dtype] = 0\n        for i in range(0, in_k, iter_k):\n            k = min(iter_k, in_k - i)\n            l0_lhs_size = in_m * k * bpe\n            l0_lhs_read = DataflowActionMemoryStat(\n                total_count=l0_lhs_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.SHARED,\n                rw=\"r\",\n                relative_ts=iter_ref,\n                memory_access_list=lhs_gen.send(l0_lhs_size),\n                name=f\"lhs:l1->l0\",\n            )\n            yield l0_lhs_read\n\n            l0_rhs_size = in_n * k * bpe\n            l0_rhs_read = DataflowActionMemoryStat(\n                total_count=l0_rhs_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.SHARED,\n                rw=\"r\",\n                relative_ts=iter_ref,\n                memory_access_list=rhs_gen.send(l0_rhs_size),\n                name=f\"rhs:l1->l0\",\n            )\n            yield l0_rhs_read\n\n            compute_ref = iter_ref + \\\n                max(l0_lhs_read.leading_latency, l0_rhs_read.leading_latency)\n            scalar_ops = 4 * self.core_cost.instruction_info.subthread_cnt_in_thread()\n\n            compute_stat1 = DataflowActionComputeStat(\n                name=f\"scalar\",\n                compute_scalar_cycle=scalar_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat1\n            scalar_cost = compute_stat1.latency\n\n            compute_mac = in_m * in_n * k\n            compute_stat2 = DataflowActionComputeStat(\n                name=f\"mac\",\n                compute_2d_ops={dtype: compute_mac * 2},\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2\n            self.core_cost.tensor_macs[dtype] += compute_mac\n            VMM = 10\n            compute_vmm = DataflowActionComputeStat(\n                name=f\"vmm\",\n                compute_nop_cycle=VMM,\n                relative_ts=compute_ref + compute_stat2.latency,\n            )\n            yield compute_vmm\n            compute_latency = max(\n                compute_ref + compute_stat2.latency + compute_vmm.latency,\n                iter_ref + max(l0_lhs_read.latency, l0_rhs_read.latency),\n            )\n            iter_ref = compute_latency - \\\n                max(l0_lhs_read.leading_latency, l0_rhs_read.leading_latency)\n            out_ref = compute_ref\n\n        out_latency = 0\n        if len(sip_workload.outputs) > 0:\n            if self.core_cost.instruction_info.dtype == DType.FP8:\n                scaling_ref = compute_ref + compute_stat2.latency + compute_vmm.latency\n                compute_scaling = DataflowActionComputeStat(\n                    name=\"scaling\",\n                    compute_1d_ops={DType.FP32: in_m * in_n},\n                    relative_ts=scaling_ref,\n                )\n                yield compute_scaling\n                compute_latency = max(\n                    compute_latency, scaling_ref + compute_scaling.latency)\n            out_size = in_m * in_n * bpe\n            out_wirte = DataflowActionMemoryStat(\n                total_count=out_size,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.SHARED,\n                rw=\"w\",\n                relative_ts=out_ref,\n                memory_access_list=out_gen.send(out_size),\n                name=\"out\",\n            )\n            yield out_wirte\n            out_latency = out_ref + out_wirte.latency\n        total_latency = max(out_latency, compute_latency)\n        self.core_cost.latency = total_latency\n\n    def _basic_stat_info(self):\n        # TODO: need review\n        pass",
    "start_line": 41,
    "end_line": 217,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_2.XPUGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType\n    mid_level_ins_num: int\n    mid_level_ins_shape: list\n    scalar_ins_num: int\n    st_ins_num: int",
    "start_line": 13,
    "end_line": 18,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    dtype: DType = DType.FP16\n    ld_smr_occupation: int = 0  # Bytes\n    ld_iv_occupation: int = 0  # Bytes\n    va_occupation: int = 0  # Bytes\n\n    ld_smr_basic_cost: int = 0\n    ld_iv_basic_cost: int = 0\n    scalar_basic_cost: int = 0\n    vmm_basic_cost: int = 0\n    vst_basic_cost: int = 0\n    ld_smr_latency: int = 0\n    ld_iv_latency: int = 0\n    main_body_length: int = 0\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n    compute_vs_store_ratio: float = 0\n    vst_cost: int = 0\n    leading_latency_without_reuse: int = 0\n    data_reuse_times: int = 0\n    single_thread_main_body_cost_without_reuse: int = 0\n    single_thread_compute_ratio_without_reuse: int = 0\n    single_thread_main_body_cost_with_reuse: int = 0\n    single_thread_compute_ratio_with_reuse: int = 0\n    multi_thread_main_body_cost_without_reuse: int = 0\n    multi_thread_compute_ratio_without_reuse: int = 0\n    multi_thread_main_body_cost_with_reuse: int = 0\n    multi_thread_compute_ratio_with_reuse: int = 0\n    total_cost: float = 0\n    instruction_info: InstructionInfo = 0",
    "start_line": 22,
    "end_line": 51,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.gemm_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.gemm_kernel",
    "name": "gemm_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.InstructionInfo"
    ],
    "source_code": "def gemm_kernel(m: int, n: int, k: int, dtype: DType = DType.FP16):\n    computer_count = 0\n    st_count = 0\n    scalar_count = 0\n\n    if dtype == DType.FP16:\n        slice_m = 64\n        slice_n = 64\n        slice_k = 32\n        eleByte = 2\n    elif dtype == DType.INT8:\n        slice_m = 64\n        slice_n = 64\n        slice_k = 64\n        eleByte = 1\n    else:\n        raise RuntimeError(f\"unsupport gemm_kernel dtype {dtype}\")\n\n    scalar_count += 23\n    scalar_count += 2\n    for loop_m in range(0, m, slice_m):\n        scalar_count += 3\n        scalar_count += 2\n        scalar_count += 2\n        for loop_n in range(0, n, slice_n):\n            scalar_count += 3\n            scalar_count += 8\n            computer_count += 1\n            scalar_count += 2\n            for loop_k in range(0, k, slice_k):\n                scalar_count += 3\n                scalar_count += 4\n                computer_count += 1\n            scalar_count += 2\n            st_count += 1\n            scalar_count += 1\n    ins_shape_m = min(m, slice_m)\n    ins_shape_n = min(n, slice_n)\n    ins_shape_k = min(k, slice_k)\n    return InstructionInfo(dtype, computer_count, [ins_shape_m, ins_shape_n, ins_shape_k], scalar_count, st_count)",
    "start_line": 54,
    "end_line": 93,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "m",
      "n",
      "k",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function gemm_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.gemm_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.XPUGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.XPUGemmAction",
    "name": "XPUGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_gemm_action_obsoleted.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.gemm_kernel",
      "nova-platform.nova_platform.utils.base_utils.hash_list",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr"
    ],
    "source_code": "class XPUGemmAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        valid_m = self.data[4] & (0xFFFF)\n        valid_n = self.data[4] >> 16\n        valid_k = self.data[5]\n        return [valid_m, valid_n, valid_k]\n\n    def get_memory_access(self) -> Generator[DataflowActionMemoryAccess, None, None]:\n        for input in self.inputs:\n            for tensor in input.tensor:\n                yield from self._iter_tensor_addr(tensor.addr, tensor, 'r')\n        for output in self.outputs:\n            for tensor in output.tensor:\n                yield from self._iter_tensor_addr(tensor.addr, tensor, 'w')\n\n    def compute(self, context: BossaNovaContext) -> Generator[None, None, BaseCoreStat]:\n        valid_shape = self.get_valid_shape()\n        dtype = self.get_dtype()\n        # dtype = DType.FP16\n        gemm_shape = hash_list(valid_shape)\n        self._compute(gemm_shape, dtype)\n        self.core_cost.latency = self.core_cost.total_cost/1e9\n        yield from ()\n        return self.core_cost\n\n    def _compute(self, gemm_shape, dtype) -> CoreCost:\n        self.core_cost.dtype = dtype\n        self.core_cost.instruction_info = gemm_kernel(\n            gemm_shape[0], gemm_shape[1], gemm_shape[2], dtype=dtype)\n\n        self._get_l0_occupation()\n        self._get_basic_cost()\n        self._get_input_latency()\n        self._get_main_body_length()\n        self._get_main_body_cost()\n        self._get_leading_latency()\n        self._get_single_thread_cost()\n        self._get_multi_thread_cost()\n        self._get_total_cost()\n        self._basic_stat_info()\n\n    def _get_l0_occupation(self):  # step 3\n        instruction_info = self.core_cost.instruction_info\n        mid_level_ins_shape = instruction_info.mid_level_ins_shape\n        bpe = self.core_cost.dtype.get_bpe()\n        m = 0\n        n = 1\n        k = 2\n        self.core_cost.ld_smr_occupation = mid_level_ins_shape[m] * \\\n            mid_level_ins_shape[k] * bpe\n        self.core_cost.ld_iv_occupation = mid_level_ins_shape[n] * \\\n            mid_level_ins_shape[k] * bpe\n        self.core_cost.va_occupation = mid_level_ins_shape[m] * \\\n            mid_level_ins_shape[n] * bpe\n\n    def _get_basic_cost(self):  # step 4\n        m = 0\n        n = 1\n        k = 2\n        instruction_info: InstructionInfo = self.core_cost.instruction_info\n        mid_level_ins_shape = instruction_info.mid_level_ins_shape\n        bpe = instruction_info.dtype.get_bpe()\n        # throughput = asdict(self.config.compute.thread_2d_throughput)\n        # mac_array = throughput[instruction_info.dtype.name.upper()]\n        mac_array = self.config.compute.thread_2d_throughput[instruction_info.dtype]\n        LOAD_SMR = self.config.bw.xpu.l0.SHARED\n        self.core_cost.ld_smr_basic_cost = (\n            mid_level_ins_shape[m]\n            * mid_level_ins_shape[k]\n            * bpe\n            / (LOAD_SMR * self.config.freq.CORE)\n        )  # 假设smr读取的是m,k\n        LOAD_IV = self.config.bw.xpu.l0.SHARED\n        self.core_cost.ld_iv_basic_cost = (\n            mid_level_ins_shape[n]\n            * mid_level_ins_shape[k]\n            * bpe\n            / (LOAD_IV * self.config.freq.CORE)\n        )  # 假设iv读取的是n,k\n        self.core_cost.scalar_basic_cost = (\n            instruction_info.scalar_ins_num / instruction_info.mid_level_ins_num /\n            self.config.freq.CORE\n        )\n        self.core_cost.vmm_basic_cost = (\n            mid_level_ins_shape[m]\n            * mid_level_ins_shape[n]\n            * mid_level_ins_shape[k]\n            / (mac_array * self.config.freq.CORE)\n        )\n        VST_BW = self.config.bw.xpu.l0.SHARED/self.config.bw.xpu.l0.SHARED_RW_RATIO\n        self.core_cost.vst_basic_cost = (\n            mid_level_ins_shape[m]\n            * mid_level_ins_shape[n]\n            * bpe\n            / (VST_BW * self.config.freq.CORE)\n        )\n\n    def _get_input_latency(self):  # step 5\n        # self.core_cost.ld_smr_latency = self.dsm_cost.dsm_local_latency\n        # self.core_cost.ld_iv_latency = self.dsm_cost.dsm_local_latency\n        self.core_cost.ld_smr_latency = self.config.latency.l0.SHARED[0] / \\\n            self.config.freq.CORE\n        self.core_cost.ld_iv_latency = self.config.latency.l0.SHARED[0] / \\\n            self.config.freq.CORE\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = min(\n            (self.config.memory.l0.IV_SIZE / self.core_cost.ld_iv_occupation),\n            (self.config.memory.l0.SMR_SIZE / self.core_cost.ld_smr_occupation),\n        )\n\n    def _get_main_body_cost(self):  # step 7\n        instruction_info = self.core_cost.instruction_info\n        ld_smr_cost = self.core_cost.ld_smr_basic_cost * self.core_cost.main_body_length\n        ld_iv_cost = self.core_cost.ld_iv_basic_cost * self.core_cost.main_body_length\n        scalar_cost = self.core_cost.scalar_basic_cost * self.core_cost.main_body_length\n        self.core_cost.data_preparation_cost = scalar_cost + \\\n            max(ld_smr_cost, ld_iv_cost)\n        VMM = 10  # KG->L0 latency\n        # VMM = self.config.latency.l0.SHARED[0]  # only core clock domain latency\n        self.core_cost.compute_cost = self.core_cost.vmm_basic_cost * self.core_cost.main_body_length + (\n            VMM / self.config.freq.CORE\n        )\n        self.core_cost.compute_vs_store_ratio = (\n            instruction_info.mid_level_ins_num / instruction_info.st_ins_num\n        )\n        # only core clock domain latency\n        VST = self.config.latency.l0.SHARED[0]\n        self.core_cost.vst_cost = (\n            self.core_cost.vst_basic_cost * self.core_cost.main_body_length /\n            self.core_cost.compute_vs_store_ratio\n            + (VST /\n               self.config.freq.CORE)\n        )\n\n    def _get_leading_latency(self):  # step 8\n        self.core_cost.leading_latency_without_reuse = max(\n            0,\n            (\n                max(self.core_cost.ld_smr_latency, self.core_cost.ld_iv_latency)\n                - (self.core_cost.compute_cost + self.core_cost.vst_cost)\n                / (self.core_cost.main_body_length / self.core_cost.compute_vs_store_ratio)\n            ),\n        )\n        self.core_cost.data_reuse_times = math.ceil(\n            max(self.core_cost.ld_smr_latency, self.core_cost.ld_iv_latency)\n            / (\n                (self.core_cost.compute_cost + self.core_cost.vst_cost)\n                / (self.core_cost.main_body_length / self.core_cost.compute_vs_store_ratio)\n            )\n        )\n\n    def _get_single_thread_cost(self):  # step 9\n        self.core_cost.single_thread_main_body_cost_without_reuse = (\n            max(self.core_cost.data_preparation_cost, self.core_cost.compute_cost)\n            + self.core_cost.vst_cost\n            + self.core_cost.leading_latency_without_reuse\n        )\n        self.core_cost.single_thread_compute_ratio_without_reuse = (\n            self.core_cost.compute_cost /\n            self.core_cost.single_thread_main_body_cost_without_reuse\n        )\n        self.core_cost.single_thread_main_body_cost_with_reuse = (\n            max(self.core_cost.data_preparation_cost,\n                self.core_cost.compute_cost) + self.core_cost.vst_cost\n        )\n        self.core_cost.single_thread_compute_ratio_with_reuse = (\n            self.core_cost.compute_cost / self.core_cost.single_thread_main_body_cost_with_reuse\n        )\n\n    def _get_multi_thread_cost(self):  # step 10\n        self.core_cost.multi_thread_main_body_cost_without_reuse = (\n            max(self.core_cost.data_preparation_cost, self.core_cost.compute_cost)\n            if self.core_cost.leading_latency_without_reuse == 0\n            else self.core_cost.single_thread_main_body_cost_without_reuse\n        )\n        self.core_cost.multi_thread_compute_ratio_without_reuse = (\n            self.core_cost.compute_cost / self.core_cost.multi_thread_main_body_cost_without_reuse\n        )\n        self.core_cost.multi_thread_main_body_cost_with_reuse = max(\n            self.core_cost.data_preparation_cost, self.core_cost.compute_cost\n        )\n        self.core_cost.multi_thread_compute_ratio_with_reuse = (\n            self.core_cost.compute_cost / self.core_cost.multi_thread_main_body_cost_with_reuse\n        )\n\n    def _basic_stat_info(self):\n        # TODO: need review\n        info = self.core_cost.instruction_info\n        m, n, k = info.mid_level_ins_shape  # m,n,k\n        dtype = self.core_cost.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.tensor_macs[dtype] = m*n*k*info.mid_level_ins_num  # mac\n        self.core_cost.vector_ops[dtype] = info.scalar_ins_num\n        self.core_cost.ld_l0_shared = (\n            m*k*bpe+n*k*bpe)*info.mid_level_ins_num\n        self.core_cost.ld_l0_shared = m*n*bpe*info.st_ins_num\n\n    def _get_total_cost(self):\n        instruction_info = self.core_cost.instruction_info\n        main_body_cost = self.core_cost.multi_thread_main_body_cost_with_reuse\n        main_body_num = float(\n            instruction_info.mid_level_ins_num) / self.core_cost.main_body_length\n        self.core_cost.total_cost = main_body_cost * main_body_num",
    "start_line": 97,
    "end_line": 302,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_gemm_action_obsoleted.XPUGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    ld_ins_num: int = 0\n    scalar_ins_num: int = 0\n    st_ins_num: int = 0\n    compute_cycle_num: int = 0\n    compute_1d_ins_num: int = 0\n    compute_sfu_ins_num: int = 0\n    compute_ins_shape: int = 0",
    "start_line": 22,
    "end_line": 30,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel_fp16": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel_fp16",
    "name": "layernorm_kernel_fp16",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.InstructionInfo"
    ],
    "source_code": "def layernorm_kernel_fp16(dim0_size: int, dim1_size: int):\n    compute_cycle_num = 0\n    compute_1d_ins_num = 0\n    st_ins_num = 0\n    scalar_ins_num = 18\n    ld_ins_num = 0\n    compute_sfu_ins_num = 0\n    elements_per_OA = int(BYPTES_PER_OA / 2)\n    unroll_num = 8\n    for i in range(dim0_size):\n        scalar_ins_num += 3\n        scalar_ins_num += 19\n        for j in range(0, dim1_size, elements_per_OA * unroll_num):\n            scalar_ins_num += 3\n            ld_ins_num += unroll_num\n            compute_1d_ins_num += unroll_num  # mop_cvt\n            compute_1d_ins_num += unroll_num * 2  # mop_mul\n            compute_1d_ins_num += unroll_num * 4  # mop_add\n            compute_cycle_num += unroll_num\n        compute_1d_ins_num += 14\n        scalar_ins_num += 1\n        for j in range(0, dim1_size, elements_per_OA * unroll_num):\n            scalar_ins_num += 3\n            ld_ins_num += unroll_num\n            compute_1d_ins_num += unroll_num  # mop_cvt\n            compute_1d_ins_num += unroll_num * 2  # mop_sub\n            compute_1d_ins_num += unroll_num * 2  # mop_mul\n            compute_1d_ins_num += unroll_num * 2  # mop_add\n            compute_1d_ins_num += unroll_num  # mop_cvt\n            st_ins_num += 1\n\n    return InstructionInfo(\n        dtype=DType.FP16,\n        ld_ins_num=ld_ins_num,\n        scalar_ins_num=scalar_ins_num,\n        st_ins_num=st_ins_num,\n        compute_cycle_num=compute_cycle_num,\n        compute_1d_ins_num=compute_1d_ins_num,\n        compute_sfu_ins_num=compute_sfu_ins_num,\n        compute_ins_shape=elements_per_OA,\n    )",
    "start_line": 33,
    "end_line": 73,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "dim0_size",
      "dim1_size"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function layernorm_kernel_fp16",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel_fp16"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel",
    "name": "layernorm_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.InstructionInfo"
    ],
    "source_code": "def layernorm_kernel(size: int, dtype: DType = DType.FP16):\n    compute_cycle_num = 0\n    compute_1d_ins_num = 0\n    st_ins_num = 0\n    scalar_ins_num = 18\n    ld_ins_num = 0\n    compute_sfu_ins_num = 0\n    elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n    for i in range(0, size, elements_per_OA):\n        ld_ins_num += 2\n        compute_1d_ins_num += 20\n        st_ins_num += 1\n        compute_cycle_num += 1\n\n    return InstructionInfo(\n        dtype=dtype,\n        ld_ins_num=ld_ins_num,\n        scalar_ins_num=scalar_ins_num,\n        st_ins_num=st_ins_num,\n        compute_cycle_num=compute_cycle_num,\n        compute_1d_ins_num=compute_1d_ins_num,\n        compute_sfu_ins_num=compute_sfu_ins_num,\n        compute_ins_shape=elements_per_OA,\n    )",
    "start_line": 76,
    "end_line": 99,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "size",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function layernorm_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    unroll_num: int = 8\n    subthread_num: int = 8\n\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)",
    "start_line": 103,
    "end_line": 113,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.XPULayernormAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.XPULayernormAction",
    "name": "XPULayernormAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_layernorm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.layernorm_kernel",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
    ],
    "source_code": "class XPULayernormAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        return self.outputs[0].tensor[0].dims\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 3 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = self.core_cost.unroll_num * \\\n            self.core_cost.subthread_num\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        self.core_cost.instruction_info = layernorm_kernel(\n            size, dtype=self.get_dtype())\n        # core_cost.instruction_info = layernorm_kernel_fp16(size[1], size[0])\n        self._get_l0_occupation()\n        self._get_main_body_length()\n        stat_ref = 0\n        total_latency = 0\n        total_compute_latency = 0\n        dtype = self.core_cost.instruction_info.dtype\n        throughput_1d = self.config.compute.thread_1d_throughput[dtype]\n        throughput_sfu = self.config.compute.thread_sfu_throughput\n\n        lhs1_tensor = self.inputs[0].tensor[0]\n        lhs1_access = self._iter_addr(lhs1_tensor, \"r\")\n        lhs2_tensor = self.inputs[0].tensor[0]\n        lhs2_access = self._iter_addr(lhs2_tensor, \"r\")\n        out_tensor = self.outputs[0].tensor[0]\n        out_access = self._iter_addr(out_tensor, \"w\")\n        input1_gen = self._iter_access_gen([lhs1_access])\n        next(input1_gen)\n        input2_gen = self._iter_access_gen([lhs2_access])\n        next(input2_gen)\n        output_gen = self._iter_access_gen([out_access])\n        next(output_gen)\n\n        for i in range(0, self.core_cost.instruction_info.compute_cycle_num, self.core_cost.main_body_length):\n            inst_num = min(self.core_cost.main_body_length,\n                           self.core_cost.instruction_info.compute_cycle_num - i)\n            lhs, out = [inst_num * BYPTES_PER_OA] * 2\n\n            lhs_read_1 = DataflowActionMemoryStat(\n                total_count=lhs,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input1_gen.send(lhs),\n            )\n            yield lhs_read_1\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - lhs_read_1.latency:{lhs_read_1.latency}, lhs_read_1.leading_latency:{lhs_read_1.leading_latency}\"\n            )\n\n            compute_1d_ops = (\n                inst_num\n                * (\n                    self.core_cost.instruction_info.compute_1d_ins_num\n                    / self.core_cost.instruction_info.compute_cycle_num\n                )\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_sfu_ops = (\n                inst_num\n                * (\n                    self.core_cost.instruction_info.compute_sfu_ins_num\n                    / self.core_cost.instruction_info.compute_cycle_num\n                )\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_scalar_cycle = (\n                inst_num\n                * self.core_cost.instruction_info.scalar_ins_num\n                / self.core_cost.instruction_info.compute_cycle_num\n            )\n            compute_cycle = (\n                compute_1d_ops / throughput_1d\n                + compute_sfu_ops / throughput_sfu\n                + compute_scalar_cycle\n            )\n            compute_ref = stat_ref + lhs_read_1.leading_latency\n            # compute_latency = compute_cycle / \\\n            #     (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n\n            # total_compute_latency += compute_latency\n\n            compute_stat1_1d = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_1d_ops/2\n                },\n                relative_ts=compute_ref,\n            )\n            yield compute_stat1_1d\n            compute_stat1_sfu = DataflowActionComputeStat(\n                compute_msf_ops=compute_sfu_ops/2,\n                relative_ts=compute_ref+compute_stat1_1d.latency,\n            )\n            yield compute_stat1_sfu\n            compute_stat1_scalar = DataflowActionComputeStat(\n                compute_scalar_cycle=compute_scalar_cycle/2,\n                relative_ts=compute_ref+compute_stat1_1d.latency+compute_stat1_sfu.latency,\n            )\n            yield compute_stat1_scalar\n            # note: compute_latency = half of the latency computed by compute_cycle / throughput / freq\n            compute_latency1 = compute_stat1_1d.latency + \\\n                compute_stat1_sfu.latency+compute_stat1_scalar.latency\n            total_compute_latency += compute_latency1\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - compute_cycle: {compute_cycle/2}, compute latency:{compute_latency1}\"\n            )\n\n            # lhs_read_2_ref = lhs_read_1.leading_latency + compute_latency / 2\n            lhs_read_2_ref = lhs_read_1.leading_latency + compute_latency1\n            lhs_read_2_ref = max(\n                lhs_read_2_ref, lhs_read_1.latency - lhs_read_1.leading_latency)\n            lhs_read_2 = DataflowActionMemoryStat(\n                total_count=lhs,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref + lhs_read_2_ref,\n                memory_access_list=input2_gen.send(lhs),\n            )\n            yield lhs_read_2\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - lhs_read_2.latency:{lhs_read_2.latency}, lhs_read_2.leading_latency:{lhs_read_2.leading_latency}\"\n            )\n\n            compute_ref = stat_ref + lhs_read_2_ref + lhs_read_2.leading_latency\n            compute_stat2_1d = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_1d_ops/2\n                },\n                relative_ts=compute_ref,\n            )\n            yield compute_stat2_1d\n            compute_stat2_sfu = DataflowActionComputeStat(\n                compute_msf_ops=compute_sfu_ops/2,\n                relative_ts=compute_ref+compute_stat2_1d.latency,\n            )\n            yield compute_stat2_sfu\n            compute_stat2_scalar = DataflowActionComputeStat(\n                compute_scalar_cycle=compute_scalar_cycle/2,\n                relative_ts=compute_ref+compute_stat2_1d.latency+compute_stat2_sfu.latency,\n            )\n            yield compute_stat2_scalar\n            compute_latency2 = (\n                compute_stat2_1d.latency\n                + compute_stat2_sfu.latency\n                + compute_stat2_scalar.latency\n            )\n            total_compute_latency += compute_latency2\n\n            # out_ref = lhs_read_2_ref + lhs_read_2.leading_latency + compute_latency / 2\n            out_ref = lhs_read_2_ref + lhs_read_2.leading_latency + compute_latency2\n            out_write = DataflowActionMemoryStat(\n                total_count=out,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=stat_ref + out_ref,\n                memory_access_list=output_gen.send(out),\n            )\n            yield out_write\n            out_latency, out_leading_latency = out_write.latency, out_write.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - out_latency:{out_latency}, out_leading_latency:{out_leading_latency}\"\n            )\n\n            total_latency = max(\n                total_latency,\n                stat_ref + lhs_read_1.latency,  # vld 1 latency\n                stat_ref + lhs_read_1.leading_latency + \\\n                # compute_latency / 2,  # compute 1 latency\n                compute_latency1,\n                stat_ref + lhs_read_2_ref + lhs_read_2.latency,  # vld 2 latency\n                stat_ref + lhs_read_2_ref + lhs_read_2.leading_latency + \\\n                # compute_latency / 2,  # compute 2 latency\n                compute_latency2,\n                stat_ref + out_ref + out_latency,  # vst latency\n            )\n            # stat_ref = total_latency - vld_leading_latency  # update ref\n            stat_ref = (\n                stat_ref\n                + out_ref\n                + out_write.leading_latency\n                + (out_write.latency - out_write.leading_latency) /\n                self.core_cost.subthread_num\n            )  # update ref\n        print(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n        self.core_cost.compute_cost = total_compute_latency\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        dtype = self.core_cost.instruction_info.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.vector_dtype = dtype\n        self.core_cost.vector_ops = info.compute_ins_shape * info.compute_1d_ins_num\n        self.core_cost.ld_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.ld_ins_num\n        self.core_cost.st_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.st_ins_num",
    "start_line": 117,
    "end_line": 328,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPULayernormAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_layernorm_action.XPULayernormAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    gemm_shape: Dict[str, int] = field(default_factory=dict)\n    grid_shape: List[int] = field(default_factory=list)\n    block_shape: List[int] = field(default_factory=list)\n    thread_shape: List[int] = field(default_factory=list)\n    subthread_shape: List[int] = field(default_factory=list)\n    grid_dim: List[int] = field(default_factory=list)\n    block_dim: List[int] = field(default_factory=list)\n    thread_dim: List[int] = field(default_factory=list)",
    "start_line": 26,
    "end_line": 35,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)",
    "start_line": 39,
    "end_line": 40,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.action_tensor_to_operand": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.action_tensor_to_operand",
    "name": "action_tensor_to_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [],
    "source_code": "def action_tensor_to_operand(tensor):\n\n    def reversed_dims(t):\n        return tuple(reversed(t))\n\n    return Operand(\n        dim=reversed_dims(tensor.dims[:3]),\n        addr=tensor.addr,\n        bpe=tensor.bpe,\n        dim_offset=reversed_dims(tensor.offsets[:3]),\n        dim_stride=reversed_dims(tensor.stride_dims[:3]),\n    )",
    "start_line": 43,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "tensor"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function action_tensor_to_operand",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.action_tensor_to_operand"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.reversed_dims": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.reversed_dims",
    "name": "reversed_dims",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [],
    "source_code": "    def reversed_dims(t):\n        return tuple(reversed(t))",
    "start_line": 45,
    "end_line": 46,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "t"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function reversed_dims",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.reversed_dims"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUBaseGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUBaseGemmAction",
    "name": "XPUBaseGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.action_tensor_to_operand",
      "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.InstructionInfo",
      "nova-platform.nova_platform.benchmark.op_base.Workload"
    ],
    "source_code": "class XPUBaseGemmAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_kernel_idx(self):\n        return self.data[0], self.data[1]\n\n    def get_chip_workload(self):\n        lhs = action_tensor_to_operand(self.inputs[0].tensor[0])\n        rhs = action_tensor_to_operand(self.inputs[1].tensor[0])\n        res = action_tensor_to_operand(self.outputs[0].tensor[0])\n        inputs = [lhs, rhs]\n        quant_type = self.dataflow_config.get(\"bench_gemm_quant_type\", QuantType.No_Quant)\n        if quant_type == QuantType.Wf4g_Af8t:\n            scaling = action_tensor_to_operand(self.inputs[2].tensor[0])\n            inputs.append(scaling)\n        attr = {\"b\": lhs.dim[0], \"m\": lhs.dim[1], \"k\": lhs.dim[2], \"n\": rhs.dim[2], \"quant_type\": quant_type}\n        return Workload(\n            inputs=inputs,\n            outputs=[res],\n            attr=attr,\n            dtype=self.get_dtype(),\n        )\n\n    def get_instruction_info(self, chip_workload: Workload, best_shape: BatchGemmGridShape):\n        def _get_shape(dim_stride):\n            return {\"b\": dim_stride[2], \"m\": dim_stride[1], \"n\": dim_stride[0], \"k\": best_shape.calc_ceil_K_l2}\n\n        return InstructionInfo(\n            dtype=self.get_dtype(),\n            gemm_shape=chip_workload.attr,\n            grid_shape=_get_shape(best_shape.grid_dims_stride()),\n            block_shape=_get_shape(best_shape.block_dims_stride()),\n            thread_shape=_get_shape(best_shape.thread_dims_stride()),\n            subthread_shape=_get_shape(best_shape.subthread_dims_stride()),\n            grid_dim=best_shape.grid_dims,\n            block_dim=best_shape.block_dims,\n            thread_dim=best_shape.thread_dims,\n        )",
    "start_line": 58,
    "end_line": 95,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUBaseGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUBaseGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPULocalGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPULocalGemmAction",
    "name": "XPULocalGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_local.dsm_local_gemm_kernel",
      "nova-platform.nova_platform.benchmark.batch_gemm_local.tile_local_gemm_workload",
      "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUBaseGemmAction"
    ],
    "source_code": "class XPULocalGemmAction(XPUBaseGemmAction):\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        sic_id, sip_id = self.get_kernel_idx()\n        total_latency = 0\n        chip_workload = self.get_chip_workload()\n        workload, best_shape = tile_local_gemm_workload(self.config, self.get_chip_workload())\n        self.core_cost.instruction_info = self.get_instruction_info(chip_workload, best_shape)\n        if sic_id not in workload or sip_id not in workload[sic_id]:\n            self.core_cost.latency = 0\n            return\n        # pprint(workload[sic_id][sip_id])\n        total_latency = yield from dsm_local_gemm_kernel(\n            workload[sic_id][sip_id], self.config, best_shape.subthread_cnt_in_thread(), 0\n        )\n        self.core_cost.latency = total_latency\n\n    def _basic_stat_info(self):\n        self.core_cost.tensor_dtype = self.get_dtype()",
    "start_line": 99,
    "end_line": 117,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUBaseGemmAction"
    ],
    "class_name": null,
    "display_name": "class XPULocalGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPULocalGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUSharedGemmAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUSharedGemmAction",
    "name": "XPUSharedGemmAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_local_gemm_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared.tile_shared_gemm_workload",
      "nova-platform.nova_platform.benchmark.batch_gemm_shared.dsm_shared_gemm_kernel",
      "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUBaseGemmAction"
    ],
    "source_code": "class XPUSharedGemmAction(XPUBaseGemmAction):\n    # 统计信息\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        sic_id, sip_id = self.get_kernel_idx()\n        total_latency = 0\n        chip_workload = self.get_chip_workload()\n        workload, best_shape = tile_shared_gemm_workload(self.config, chip_workload)\n        self.core_cost.instruction_info = self.get_instruction_info(chip_workload, best_shape)\n        if sic_id not in workload:\n            self.core_cost.latency = 0\n            return\n        # pprint(workload[sic_id])\n        total_latency = yield from dsm_shared_gemm_kernel(\n            workload, best_shape, self.config, f\"{self.case_id}_{self.config.gcu_id}\", sic_id, sip_id, 0, self.ref\n        )\n        self.core_cost.latency = total_latency\n\n    def _basic_stat_info(self):\n        self.core_cost.tensor_dtype = self.get_dtype()",
    "start_line": 121,
    "end_line": 141,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUBaseGemmAction"
    ],
    "class_name": null,
    "display_name": "class XPUSharedGemmAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_local_gemm_action.XPUSharedGemmAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    bpe: int = 2\n    ld_index_l3_ins_num: int = 0\n    ld_table_l3_ins_num: int = 0\n    st_table_l3_ins_num: int = 0\n    scalar_ins_num: int = 0",
    "start_line": 20,
    "end_line": 25,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.scatter_instruction_mapping": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.scatter_instruction_mapping",
    "name": "scatter_instruction_mapping",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.InstructionInfo"
    ],
    "source_code": "def scatter_instruction_mapping(embedding_size: int, index: int, bpe: int = 2):\n    ld_index_L3_inst_cout = 0\n    ld_table_L3_inst_cout = 0\n    st_table_L3_inst_cout = 0\n    scalar_inst_count = 0\n    remain_index_size = index\n    ELEMENTS_PER_OA = int(BYPTES_PER_OA / bpe)\n    scalar_inst_count += 8\n    for i in range(0, index, ELEMENTS_PER_OA):\n        slice_index_size = ELEMENTS_PER_OA if remain_index_size > ELEMENTS_PER_OA else remain_index_size\n        remain_index_size -= slice_index_size\n        scalar_inst_count += 8\n        scalar_inst_count += 16\n        ld_index_L3_inst_cout += 1\n        for j in range(0, slice_index_size):\n            scalar_inst_count += 3\n            scalar_inst_count += 6\n            remain_length_size = embedding_size\n            for k in range(0, embedding_size, ELEMENTS_PER_OA):\n                slice_length_size = ELEMENTS_PER_OA if remain_length_size > ELEMENTS_PER_OA else remain_length_size\n                remain_length_size -= slice_length_size\n                scalar_inst_count += 3\n                scalar_inst_count += 7\n                ld_table_L3_inst_cout += 1\n                st_table_L3_inst_cout += 1\n\n    return InstructionInfo(\n        bpe=bpe,\n        ld_index_l3_ins_num=ld_index_L3_inst_cout,\n        ld_table_l3_ins_num=ld_table_L3_inst_cout,\n        st_table_l3_ins_num=st_table_L3_inst_cout,\n        scalar_ins_num=scalar_inst_count,\n    )",
    "start_line": 32,
    "end_line": 64,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "embedding_size",
      "index",
      "bpe"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function scatter_instruction_mapping",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.scatter_instruction_mapping"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    subthread_num: int = 8\n    unroll_num: int = 16\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)\n    elements_per_cycle: float = 0",
    "start_line": 68,
    "end_line": 78,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.XPUScatterAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.XPUScatterAction",
    "name": "XPUScatterAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_scatter_action.py",
    "depends_on": [
      "nova-platform.nova_platform.benchmark.batch_gemm_shared._iter_tensor_addr",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.scatter_instruction_mapping"
    ],
    "source_code": "class XPUScatterAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        out_dim = self.inputs[2].tensor[0].dims\n        return out_dim[0], out_dim[1]  # table size, index size\n\n    def _iter_random_addr(self, tensor: DiagTensor, num, rw):\n        bpe = tensor.bpe\n        stride = tensor.stride_dims[0] * bpe\n        width = tensor.dims[0] * bpe\n        rows = tensor.dims[1]\n        base_addr = tensor.addr\n\n        for _ in range(num):\n            addr = base_addr + random.randint(0, rows - 1) * stride\n            yield DataflowActionMemoryAccess(addr, width, rw)\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        embedding_size, idx_size = self.get_valid_shape()\n        bpe = self.inputs[0].tensor[0].bpe\n        self.core_cost.instruction_info = scatter_instruction_mapping(\n            embedding_size, idx_size, bpe=bpe)\n        ld_table_vs_index = int(\n            self.core_cost.instruction_info.ld_table_l3_ins_num /\n            self.core_cost.instruction_info.ld_index_l3_ins_num\n        )\n\n        index_tensor = self.inputs[1].tensor[0]\n        num_index = index_tensor.dims[0]\n        index_access = DataflowActionMemoryAccess(\n            index_tensor.addr, index_tensor.dims[0] * index_tensor.bpe, \"r\")\n        input_gen = self._iter_access_gen([index_access])\n        next(input_gen)\n\n        update_tensor = self.inputs[2].tensor[0]\n        update_access = self._iter_tensor_addr(\n            update_tensor.addr, update_tensor, \"w\")\n        update_gen = self._iter_access_gen(update_access)\n        next(update_gen)\n\n        output_tensor = self.outputs[0].tensor[0]\n\n        output_access = self._iter_random_addr(output_tensor, num_index, \"w\")\n        output_gen = self._iter_access_gen(output_access)\n        next(output_gen)\n\n        stat_ref = 0\n        total_latency = 0\n        for i in range(0, self.core_cost.instruction_info.ld_index_l3_ins_num, self.core_cost.subthread_num):\n            ld_index_num = min(self.core_cost.subthread_num,\n                               self.core_cost.instruction_info.ld_index_l3_ins_num - i)\n            index_read = DataflowActionMemoryStat(\n                total_count=ld_index_num * BYPTES_PER_OA,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(\n                    ld_index_num * BYPTES_PER_OA),\n            )\n            yield index_read\n            total_ld_table_num = ld_table_vs_index * ld_index_num\n            table_ref = stat_ref + index_read.latency\n            table_latency = 0\n            for j in range(0, total_ld_table_num, self.core_cost.unroll_num * self.core_cost.subthread_num):\n                ld_table_num = min(\n                    self.core_cost.unroll_num * self.core_cost.subthread_num, total_ld_table_num - j)\n                table_read = DataflowActionMemoryStat(\n                    total_count=ld_table_num * BYPTES_PER_OA,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"r\",\n                    relative_ts=table_ref,\n                    memory_access_list=update_gen.send(\n                        ld_table_num * BYPTES_PER_OA),\n                )\n                yield table_read\n                table_write = DataflowActionMemoryStat(\n                    total_count=ld_table_num * BYPTES_PER_OA,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"w\",\n                    relative_ts=table_ref + table_read.leading_latency,\n                    memory_access_list=output_gen.send(\n                        ld_table_num * BYPTES_PER_OA),\n                )\n                yield table_write\n                table_ref = table_ref + table_read.latency\n                table_latency = max(\n                    table_latency,\n                    table_ref + table_read.latency,\n                    table_ref + table_read.leading_latency + table_write.latency,\n                )\n\n            total_latency = max(\n                total_latency,\n                stat_ref + index_read.latency,  # index latency,\n                table_latency,\n            )\n            stat_ref = total_latency - index_read.leading_latency\n        print(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = 16 * 4\n\n        self.core_cost.main_body_num = (\n            float(self.core_cost.instruction_info.st_table_l3_ins_num) /\n            self.core_cost.main_body_length\n        )\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        self.core_cost.ld_l0_l3 = (\n            info.ld_index_l3_ins_num + info.ld_table_l3_ins_num) * BYPTES_PER_OA\n        self.core_cost.st_l0_l3 = info.st_table_l3_ins_num * BYPTES_PER_OA\n\n    def compute(self, context: BossaNovaContext) -> Generator[DataflowActionMemoryStat, any, CoreCost]:\n        self._get_main_body_length()\n        yield from self.get_memory_stat()\n        self._basic_stat_info()\n        return self.core_cost",
    "start_line": 82,
    "end_line": 208,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUScatterAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_scatter_action.XPUScatterAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    head_dim: int = 128\n    head_num: int = 1\n    row_block: int = 1\n    col_block: int = 1\n    kv_seq: int = 1024\n    with_dropout: bool = True",
    "start_line": 28,
    "end_line": 35,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.sdpa_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.sdpa_kernel",
    "name": "sdpa_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.InstructionInfo"
    ],
    "source_code": "def sdpa_kernel(row_block, col_block, head_num, kv_seq, dtype: DType):\n    return InstructionInfo(dtype=dtype, row_block=row_block, col_block=col_block, head_num=head_num, kv_seq=kv_seq)",
    "start_line": 38,
    "end_line": 39,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "row_block",
      "col_block",
      "head_num",
      "kv_seq",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function sdpa_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.sdpa_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.cal_dropout_cycles_per_element": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.cal_dropout_cycles_per_element",
    "name": "cal_dropout_cycles_per_element",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "depends_on": [],
    "source_code": "def cal_dropout_cycles_per_element(rng_dtype, round, throughput_1D_fp32):\n    rng_num_per_iter = {\"u32\": 4, \"u8\": 16}\n    dropout_1d_cal = {\"u32\": 2, \"u8\": 4}\n    rng_cycles = 10 + round * 10\n    return (\n        rng_cycles / (rng_num_per_iter[rng_dtype] * throughput_1D_fp32) +\n        dropout_1d_cal[rng_dtype] / throughput_1D_fp32\n    )",
    "start_line": 42,
    "end_line": 49,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "rng_dtype",
      "round",
      "throughput_1D_fp32"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function cal_dropout_cycles_per_element",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.cal_dropout_cycles_per_element"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n    unroll_num: int = 8\n    subthread_num: int = 8\n\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)",
    "start_line": 53,
    "end_line": 63,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.XPUSdpaAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.XPUSdpaAction",
    "name": "XPUSdpaAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_sdpa_action.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryAccess",
      "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.InstructionInfo",
      "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.cal_dropout_cycles_per_element"
    ],
    "source_code": "class XPUSdpaAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def get_valid_shape(self):\n        col_block = self.data[0]\n        row_block = self.data[1]\n        head_num = self.data[2]\n        kv_seq = self.inputs[1].tensor[0].dims[1]\n        head_dim = self.inputs[1].tensor[0].dims[0]\n        return row_block, col_block, head_num, kv_seq, head_dim\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 3 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = self.core_cost.unroll_num * \\\n            self.core_cost.subthread_num\n\n    def _iter_addr(self, tensor: DiagTensor, rw) -> DataflowActionMemoryAccess:\n        base_addr = tensor.addr\n        data_size = tensor.dims[0] * tensor.dims[1] * \\\n            tensor.dims[2] * tensor.dims[3] * tensor.bpe\n        return DataflowActionMemoryAccess(base_addr, data_size, rw)\n\n    # parallel patten: 2d&sfu-1d, 2d-1d-sfu, 2d&1d-sfu\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        with_dropout = self.dataflow_config.get(\"bench_sdpa_with_dropout\", 1)\n        assert self.config.compute.compute_parallel_capability in [\n            0, 1, 2]  # 2d&sfu-1d, 2d-1d-sfu, 2d&1d-sfu\n        compute_parallel_capability = self.config.compute.compute_parallel_capability\n        row_block, col_block, head_num, kv_seq, head_dim = self.get_valid_shape()\n\n        dtype = self.get_dtype()\n        self._basic_stat_info()\n\n        self.core_cost.instruction_info = InstructionInfo(\n            dtype=dtype,\n            head_dim=head_dim,\n            head_num=head_num,\n            row_block=row_block,\n            col_block=col_block,\n            kv_seq=kv_seq,\n            with_dropout=with_dropout,\n        )\n        instruction_info = self.core_cost.instruction_info\n        # print(\"!!!instruction_info\", self.core_cost.instruction_info)\n        bpe = instruction_info.dtype.get_bpe()\n\n        def shape_nbypes(shape):\n            return reduce(lambda a, b: a * b, shape, 1) * bpe\n\n        q_access = self._iter_addr(self.inputs[0].tensor[0], \"r\")\n        q_gen = self._iter_access_gen([q_access])\n        next(q_gen)\n\n        k_access = self._iter_addr(self.inputs[1].tensor[0], \"r\")\n        k_gen = self._iter_access_gen([k_access])\n        next(k_gen)\n\n        v_access = self._iter_addr(self.inputs[2].tensor[0], \"r\")\n        v_gen = self._iter_access_gen([v_access])\n        next(v_gen)\n\n        out_access = self._iter_addr(self.outputs[0].tensor[0], \"w\")\n        out_gen = self._iter_access_gen([out_access])\n        next(out_gen)\n\n        stat_ref = 0\n        total_latency = 0\n        self.core_cost.tensor_macs[self.get_dtype()] = 0\n        for head_index in range(instruction_info.head_num):\n            for col_index in range(0, instruction_info.kv_seq, instruction_info.col_block):\n                col_size = min(instruction_info.col_block,\n                               instruction_info.kv_seq - col_index)\n                q_tile_shape = [instruction_info.row_block,\n                                instruction_info.head_dim]\n                k_tile_shape = [instruction_info.head_dim, col_size]\n                v_tile_shape = [col_size, instruction_info.head_dim]\n                s_tile_shape = [instruction_info.row_block, col_size]\n                o_tile_shape = [instruction_info.row_block,\n                                instruction_info.head_dim]\n                q_tile_size = shape_nbypes(q_tile_shape)\n                k_tile_size = shape_nbypes(k_tile_shape)\n                v_tile_size = shape_nbypes(v_tile_shape)\n                s_tile_size = shape_nbypes(s_tile_shape)\n                o_tile_size = shape_nbypes(o_tile_shape)\n                o_elements = instruction_info.row_block * instruction_info.head_dim\n                k_read = DataflowActionMemoryStat(\n                    total_count=k_tile_size,\n                    master=DataflowActionType.CDTE,\n                    src=AddrDomain.SHARED,\n                    dst=AddrDomain.L3,\n                    rw=\"r\",\n                    relative_ts=stat_ref,\n                    memory_access_list=k_gen.send(k_tile_size),\n                    name=\"k_tile\",\n                )\n                yield k_read\n                ref_after_k_read = stat_ref + k_read.latency\n                self.core_cost.ld_shared_l3 += k_tile_size\n                self.core_cost.ld_l0_shared += k_tile_size\n                v_read = DataflowActionMemoryStat(\n                    total_count=v_tile_size,\n                    master=DataflowActionType.CDTE,\n                    src=AddrDomain.SHARED,\n                    dst=AddrDomain.L3,\n                    rw=\"r\",\n                    relative_ts=ref_after_k_read,\n                    memory_access_list=k_gen.send(v_tile_size),\n                    name=\"v_tile\",\n                )\n                yield v_read\n                self.core_cost.ld_shared_l3 += v_tile_size\n                self.core_cost.ld_l0_shared += v_tile_size\n\n                q_read = DataflowActionMemoryStat(\n                    total_count=q_tile_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"r\",\n                    relative_ts=ref_after_k_read,\n                    memory_access_list=q_gen.send(q_tile_size),\n                    name=\"q_tile\",\n                )\n                yield q_read\n                ref_after_q_leading = ref_after_k_read + q_read.leading_latency\n\n                matmul_qk_macs = instruction_info.row_block * \\\n                    instruction_info.head_dim * col_size\n                self.core_cost.tensor_macs[dtype] += matmul_qk_macs\n                compute_matmul_qk = DataflowActionComputeStat(\n                    compute_2d_ops={\n                        instruction_info.dtype: matmul_qk_macs * 2},\n                    compute_2d_efficiency=SIP_2D_EFFICIENCY,\n                    relative_ts=ref_after_q_leading,\n                    name=\"matmul_qk\",\n                )\n                yield compute_matmul_qk\n                self.core_cost.st_l0_shared += s_tile_size\n\n                if compute_parallel_capability == 1:\n                    ref_sfu = ref_after_q_leading\n                elif compute_parallel_capability == 2:\n                    ref_sfu = ref_after_q_leading\n                else:\n                    ref_sfu = ref_after_q_leading + compute_matmul_qk.latency\n                softmax_sfu_ops = instruction_info.row_block * col_size\n                self.core_cost.sfu_ops += softmax_sfu_ops\n                compute_softmax_sfu = DataflowActionComputeStat(\n                    compute_msf_ops=softmax_sfu_ops,\n                    compute_sfu_efficiency=SIP_1D_EFFICIENCY,\n                    relative_ts=ref_sfu,\n                    name=\"softmax_sfu\",\n                )\n                yield compute_softmax_sfu\n\n                if compute_parallel_capability == 1:\n                    ref_1d = ref_after_q_leading\n                elif compute_parallel_capability == 2:\n                    ref_1d = ref_after_q_leading + compute_matmul_qk.latency\n                else:\n                    ref_1d = ref_after_q_leading\n                softmax_1d_ops = 7 * instruction_info.row_block * col_size\n                self.core_cost.vector_ops[dtype] += softmax_1d_ops\n                compute_softmax_1d = DataflowActionComputeStat(\n                    compute_1d_ops={DType.FP32: softmax_1d_ops},\n                    compute_1d_efficiency=SIP_1D_EFFICIENCY,\n                    relative_ts=ref_1d,\n                    name=\"softmax_1d\",\n                )\n                yield compute_softmax_1d\n                dropout_compute_latency = 0\n                if with_dropout:\n                    throughput_1D_fp32 = self.config.compute.thread_1d_throughput[DType.FP32]\n                    dropout_cycles_per_element = cal_dropout_cycles_per_element(\n                        \"u8\", 7, throughput_1D_fp32)\n                    dropout_1d_ops = int(\n                        instruction_info.row_block * col_size *\n                        dropout_cycles_per_element * throughput_1D_fp32\n                    )\n                    ref_dropout = ref_1d + compute_softmax_1d.latency\n                    self.core_cost.vector_ops[dtype] += dropout_1d_ops\n                    compute_dropout_1d = DataflowActionComputeStat(\n                        compute_1d_ops={DType.FP32: dropout_1d_ops},\n                        compute_1d_efficiency=SIP_1D_EFFICIENCY,\n                        relative_ts=ref_dropout,\n                        name=\"dropout_1d\",\n                    )\n                    yield compute_dropout_1d\n                    dropout_compute_latency = compute_dropout_1d.latency\n\n                if compute_parallel_capability == 1:\n                    ref_pv = ref_after_q_leading + compute_matmul_qk.latency\n                elif compute_parallel_capability == 2:\n                    ref_pv = (\n                        ref_after_q_leading\n                        + compute_matmul_qk.latency\n                        + compute_softmax_1d.latency\n                        + dropout_compute_latency\n                    )\n                else:\n                    ref_pv = ref_after_q_leading + compute_matmul_qk.latency + compute_softmax_sfu.latency\n                matmul_pv_macs = instruction_info.row_block * \\\n                    instruction_info.head_dim * col_size\n                self.core_cost.tensor_macs[dtype] += matmul_pv_macs\n                compute_matmul_pv = DataflowActionComputeStat(\n                    compute_2d_ops={\n                        instruction_info.dtype: matmul_pv_macs * 2},\n                    compute_2d_efficiency=SIP_2D_EFFICIENCY,\n                    relative_ts=ref_pv,\n                    name=\"matmul_pv\",\n                )\n                yield compute_matmul_pv\n\n                # epilog_mul_exp\n                epilog_1d_ops = o_elements\n                if col_index != 0:\n                    epilog_1d_ops += o_elements * 2\n\n                # epilog_div_exp\n                epilog_1d_ops += o_elements\n\n                if compute_parallel_capability == 2:\n                    ref_epilog = (\n                        ref_after_q_leading\n                        + compute_matmul_qk.latency\n                        + compute_softmax_1d.latency\n                        + dropout_compute_latency\n                        + compute_matmul_pv.latency\n                    )\n                else:\n                    ref_epilog = ref_after_q_leading + \\\n                        compute_softmax_1d.latency + dropout_compute_latency\n                self.core_cost.vector_ops[dtype] += epilog_1d_ops\n                compute_epilog_1d = DataflowActionComputeStat(\n                    compute_1d_ops={instruction_info.dtype: epilog_1d_ops},\n                    compute_1d_efficiency=SIP_1D_EFFICIENCY,\n                    relative_ts=ref_epilog,\n                    name=\"epilog_1d\",\n                )\n                yield compute_epilog_1d\n\n                o_write = DataflowActionMemoryStat(\n                    total_count=o_tile_size,\n                    master=DataflowActionType.XPU,\n                    src=AddrDomain.L0,\n                    dst=AddrDomain.L3,\n                    rw=\"w\",\n                    relative_ts=ref_after_q_leading,\n                    memory_access_list=out_gen.send(o_tile_size),\n                    name=\"o_tile\",\n                )\n                yield o_write\n                self.core_cost.st_shared_l3 += o_tile_size\n                read_kv_latency = stat_ref + k_read.latency + v_read.latency\n                read_q_latency = ref_after_q_leading + q_read.latency\n                write_o_latency = ref_after_q_leading + o_write.latency\n                compute_latency = 0\n                if compute_parallel_capability == 0:  # 2d&sfu-1d,\n                    compute_latency = max(\n                        ref_pv + compute_matmul_pv.latency,\n                        ref_epilog + compute_epilog_1d.latency,\n                    )\n                elif compute_parallel_capability == 1:  # 2d-1d-sfu,\n                    compute_latency = max(\n                        ref_pv + compute_matmul_pv.latency,\n                        ref_epilog + compute_epilog_1d.latency,\n                        ref_sfu + compute_softmax_sfu.latency,\n                    )\n                else:  # 2d&1d-sfu\n                    compute_latency = max(\n                        ref_sfu + compute_softmax_sfu.latency,\n                        ref_epilog + compute_epilog_1d.latency,\n                    )\n                total_latency = max(\n                    total_latency,\n                    read_kv_latency,\n                    compute_latency,\n                    read_q_latency,\n                    write_o_latency,\n                )\n                stat_ref = max(total_latency - k_read.latency,\n                               read_kv_latency - k_read.leading_latency)\n        self.core_cost.latency = total_latency\n        return\n\n    def _basic_stat_info(self):\n        dtype = self.core_cost.instruction_info.dtype\n        self.core_cost.vector_ops[dtype] = 0\n        self.core_cost.tensor_macs[dtype] = 0",
    "start_line": 67,
    "end_line": 357,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUSdpaAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_sdpa_action.XPUSdpaAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    ld_ins_num: int = 0\n    scalar_ins_num: int = 0\n    st_ins_num: int = 0\n    compute_1d_ins_num: int = 0\n    compute_sfu_ins_num: int = 0\n    compute_ins_shape: int = 0",
    "start_line": 17,
    "end_line": 24,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.softmax_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.softmax_kernel",
    "name": "softmax_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.InstructionInfo"
    ],
    "source_code": "def softmax_kernel(size: int, dtype: DType = DType.FP16):\n    compute_1d_ins_num = 0\n    compute_sfu_ins_num = 0\n    st_ins_num = 0\n    scalar_ins_num = 12\n    ld_ins_num = 0\n    elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n\n    for i in range(0, size, elements_per_OA):\n        ld_ins_num += 1\n        compute_1d_ins_num += 5\n        compute_sfu_ins_num += 1\n        scalar_ins_num += 1\n        st_ins_num += 1\n\n    return InstructionInfo(\n        dtype=dtype,\n        ld_ins_num=ld_ins_num,\n        scalar_ins_num=scalar_ins_num,\n        st_ins_num=st_ins_num,\n        compute_1d_ins_num=compute_1d_ins_num,\n        compute_sfu_ins_num=compute_sfu_ins_num,\n        compute_ins_shape=elements_per_OA,\n    )",
    "start_line": 27,
    "end_line": 50,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "size",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function softmax_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.softmax_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)",
    "start_line": 54,
    "end_line": 62,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.XPUSoftmaxAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.XPUSoftmaxAction",
    "name": "XPUSoftmaxAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.softmax_kernel",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
    ],
    "source_code": "class XPUSoftmaxAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 2 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = int(\n            self.config.memory.l0.OA_SIZE / self.core_cost.oa_occupation)\n        # 64 = 4 thread * 16 per thread\n        self.core_cost.main_body_length = 64\n        self.main_body_num = float(\n            self.core_cost.instruction_info.ld_ins_num) / self.core_cost.main_body_length\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        self.core_cost.instruction_info = softmax_kernel(\n            size, dtype=self.get_dtype())\n        self._get_l0_occupation()\n        self._get_main_body_length()\n        stat_ref = 0\n        total_latency = 0\n        total_compute_latency = 0\n        dtype = self.core_cost.instruction_info.dtype\n\n        throughput_1d = self.config.compute.thread_1d_throughput[dtype]\n        throughput_sfu = self.config.compute.thread_sfu_throughput\n\n        input_tensor = self.inputs[0].tensor[0]\n        input_access = self._iter_addr(input_tensor, 'r')\n        output_tensor = self.outputs[0].tensor[0]\n        output_access = self._iter_addr(output_tensor, 'w')\n        input_gen = self._iter_access_gen([input_access])\n        next(input_gen)\n        output_gen = self._iter_access_gen([output_access])\n        next(output_gen)\n\n        for i in range(0, self.core_cost.instruction_info.ld_ins_num, self.core_cost.main_body_length):\n            inst_num = min(self.core_cost.main_body_length,\n                           self.core_cost.instruction_info.ld_ins_num - i)\n            lhs, out = [inst_num * BYPTES_PER_OA] * 2\n\n            lhs_read = DataflowActionMemoryStat(\n                total_count=lhs,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(lhs)\n            )\n            yield lhs_read\n            lhs_latency, lhs_leading_latency = lhs_read.latency, lhs_read.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - lhs_latency:{lhs_latency}, lhs_leading_latency:{lhs_leading_latency}\"\n            )\n\n            vld_latency = lhs_latency\n            vld_leading_latency = lhs_leading_latency\n\n            compute_1d_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_1d_ins_num / self.core_cost.instruction_info.ld_ins_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_sfu_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_sfu_ins_num / self.core_cost.instruction_info.ld_ins_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            # 0.8 means 1D || SFU\n            compute_cycle = (\n                max(compute_1d_ops / throughput_1d, compute_sfu_ops / throughput_sfu) / 0.8 +\n                + inst_num * self.core_cost.instruction_info.scalar_ins_num /\n                self.core_cost.instruction_info.ld_ins_num\n            )\n            compute_ref = stat_ref+lhs_leading_latency\n            # compute_latency = compute_cycle / \\\n            #     (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n            # print(\"comp[ute_latency:\", compute_1d_ops / throughput_1d, compute_sfu_ops / throughput_sfu, inst_num * core_cost.instruction_info.scalar_ins_num / core_cost.instruction_info.ld_ins_num, compute_latency)\n\n            compute_stat = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_1d_ops\n                },\n                compute_msf_ops=compute_sfu_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat\n\n            compute_latency = compute_stat.latency\n\n            total_compute_latency += compute_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - compute_cycle: {compute_cycle}, compute latency:{compute_latency}\"\n            )\n\n            out_write = DataflowActionMemoryStat(\n                total_count=out,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=stat_ref + vld_leading_latency+compute_latency/4,\n                memory_access_list=output_gen.send(out))\n            yield out_write\n            out_latency, out_leading_latency = out_write.latency, out_write.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - out_latency:{out_latency}, out_leading_latency:{out_leading_latency}\"\n            )\n\n            total_latency = max(\n                total_latency,\n                stat_ref + vld_leading_latency + compute_latency,  # compute latency,\n                stat_ref + lhs_latency,\n                stat_ref + vld_leading_latency+compute_latency / 4 + out_latency,\n            )\n            stat_ref = total_latency - vld_leading_latency  # update ref\n        logger.debug(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()}  - total_latency:{total_latency}\")\n        self.core_cost.latency = total_latency\n        self.core_cost.compute_cost = total_compute_latency\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        dtype = self.core_cost.instruction_info.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.vector_dtype = dtype\n        self.core_cost.vector_ops = info.compute_ins_shape * info.compute_1d_ins_num\n        self.core_cost.ld_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.ld_ins_num\n        self.core_cost.st_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.st_ins_num",
    "start_line": 66,
    "end_line": 199,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUSoftmaxAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_action.XPUSoftmaxAction"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.InstructionInfo": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.InstructionInfo",
    "name": "InstructionInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "depends_on": [],
    "source_code": "class InstructionInfo:\n    dtype: DType = DType.FP16\n    ld_ins_num: int = 0\n    scalar_ins_num: int = 0\n    st_ins_num: int = 0\n    compute_1d_ins_num: int = 0\n    compute_sfu_ins_num: int = 0\n    compute_ins_shape: int = 0\n    loop_num: int = 0",
    "start_line": 17,
    "end_line": 25,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class InstructionInfo",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.InstructionInfo"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.softmax_backward_kernel": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.softmax_backward_kernel",
    "name": "softmax_backward_kernel",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.InstructionInfo"
    ],
    "source_code": "def softmax_backward_kernel(size: int, dtype: DType = DType.FP16):\n    compute_1d_ins_num = 0\n    compute_sfu_ins_num = 0\n    st_ins_num = 0\n    scalar_ins_num = 18\n    ld_ins_num = 0\n    elements_per_OA = int(BYPTES_PER_OA / dtype.get_bpe())\n    loop_num = 0\n\n    for i in range(0, size, elements_per_OA):\n        ld_ins_num += 2\n        compute_1d_ins_num += 4  # dx = dy * y - sum(dy*y)*y\n        compute_sfu_ins_num += 0\n        scalar_ins_num += 3\n        st_ins_num += 1\n        loop_num += 1\n\n    return InstructionInfo(\n        dtype=dtype,\n        ld_ins_num=ld_ins_num,\n        scalar_ins_num=scalar_ins_num,\n        st_ins_num=st_ins_num,\n        compute_1d_ins_num=compute_1d_ins_num,\n        compute_sfu_ins_num=compute_sfu_ins_num,\n        compute_ins_shape=elements_per_OA,\n        loop_num=loop_num,\n    )",
    "start_line": 28,
    "end_line": 54,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "size",
      "dtype"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function softmax_backward_kernel",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.softmax_backward_kernel"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.CoreCost": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.CoreCost",
    "name": "CoreCost",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "depends_on": [],
    "source_code": "class CoreCost(BaseCoreStat):\n    oa_occupation: int = 0  # Bytes\n    main_body_length: int = 0\n    main_body_num: int = 0\n\n    data_preparation_cost: int = 0\n    compute_cost: int = 0\n\n    instruction_info: InstructionInfo = field(default_factory=InstructionInfo)",
    "start_line": 58,
    "end_line": 66,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseCoreStat"
    ],
    "class_name": null,
    "display_name": "class CoreCost",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.CoreCost"
  },
  "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.XPUSoftmaxBackwardAction": {
    "id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.XPUSoftmaxBackwardAction",
    "name": "XPUSoftmaxBackwardAction",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "relative_path": "nova-platform/nova_platform/dataflow/action/xpu_softmax_backward_action.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.softmax_backward_kernel",
      "nova-platform.nova_platform.benchmark.utils._iter_access_gen",
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.base_model.DataflowActionComputeStat"
    ],
    "source_code": "class XPUSoftmaxBackwardAction(XPUAction):\n    core_cost: CoreCost = field(default_factory=CoreCost)\n\n    def _get_l0_occupation(self):  # step 3\n        self.core_cost.oa_occupation = 2 * BYPTES_PER_OA  # 2 for inputs, 1 for outputs\n\n    def _get_main_body_length(self):  # step 6\n        self.core_cost.main_body_length = int(\n            self.config.memory.l0.OA_SIZE / self.core_cost.oa_occupation)\n        # 64 = 4 thread * 16 per thread\n        self.core_cost.main_body_length = 64\n        self.main_body_num = float(\n            self.core_cost.instruction_info.ld_ins_num) / self.core_cost.main_body_length\n\n    def get_memory_stat(self) -> Generator[DataflowActionMemoryStat, None, None]:\n        size = self.get_valid_shape()\n        size = reduce(lambda a, b: a * b, size, 1)\n        self.core_cost.instruction_info = softmax_backward_kernel(\n            size, dtype=self.get_dtype())\n        self._get_l0_occupation()\n        self._get_main_body_length()\n        stat_ref = 0\n        total_latency = 0\n        total_compute_latency = 0\n\n        dtype = self.core_cost.instruction_info.dtype\n        throughput_1d = self.config.compute.thread_1d_throughput[dtype]\n        throughput_sfu = self.config.compute.thread_sfu_throughput\n\n        lhs_tensor = self.inputs[0].tensor[0]\n        lhs_access = self._iter_addr(lhs_tensor, 'r')\n        rhs_tensor = self.inputs[1].tensor[0]\n        rhs_access = self._iter_addr(rhs_tensor, 'r')\n        out_tensor = self.outputs[0].tensor[0]\n        out_access = self._iter_addr(out_tensor, 'w')\n        input_gen = self._iter_access_gen([lhs_access, rhs_access])\n        next(input_gen)\n        output_gen = self._iter_access_gen([out_access])\n        next(output_gen)\n\n        for i in range(0, self.core_cost.instruction_info.loop_num, self.core_cost.main_body_length):\n            inst_num = min(self.core_cost.main_body_length,\n                           self.core_cost.instruction_info.loop_num - i)\n            lhs, rhs, out = [inst_num * BYPTES_PER_OA] * 3\n            # lhs and rhs the same use this, if different use different stat_ref\n            lhs_rhs_read = DataflowActionMemoryStat(\n                total_count=lhs * 2,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"r\",\n                relative_ts=stat_ref,\n                memory_access_list=input_gen.send(lhs*2))\n            yield lhs_rhs_read\n            lhs_rhs_latency, lhs_rhs_leading_latency = lhs_rhs_read.latency, lhs_rhs_read.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - lhs_rhs_latency:{lhs_rhs_latency}, lhs_rhs_leading_latency:{lhs_rhs_leading_latency}\"\n            )\n\n            vld_latency = lhs_rhs_latency\n            vld_leading_latency = lhs_rhs_leading_latency\n\n            compute_1d_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_1d_ins_num / self.core_cost.instruction_info.loop_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            compute_sfu_ops = (\n                inst_num\n                * (self.core_cost.instruction_info.compute_sfu_ins_num / self.core_cost.instruction_info.ld_ins_num)\n                * self.core_cost.instruction_info.compute_ins_shape\n            )\n            # 0.8 means 1D || SFU\n            compute_cycle = (\n                compute_1d_ops / throughput_1d +\n                + inst_num * self.core_cost.instruction_info.scalar_ins_num /\n                self.core_cost.instruction_info.ld_ins_num\n            )\n            compute_ref = stat_ref + lhs_rhs_leading_latency\n            # compute_latency = compute_cycle / \\\n            #     (self.config.freq.CORE_CLOCK_DOMAIN * 1e9)\n\n            compute_stat = DataflowActionComputeStat(\n                compute_1d_ops={\n                    dtype: compute_1d_ops\n                },\n                compute_msf_ops=compute_sfu_ops,\n                relative_ts=compute_ref,\n            )\n            yield compute_stat\n            compute_latency = compute_stat.latency\n\n            total_compute_latency += compute_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - compute_cycle: {compute_cycle}, compute latency:{compute_latency}\"\n            )\n\n            out_ref = stat_ref + vld_leading_latency+compute_latency / \\\n                self.core_cost.instruction_info.st_ins_num\n            out_write = DataflowActionMemoryStat(\n                total_count=out,\n                master=DataflowActionType.XPU,\n                src=AddrDomain.L0,\n                dst=AddrDomain.L3,\n                rw=\"w\",\n                relative_ts=out_ref,\n                memory_access_list=output_gen.send(out)\n            )\n            yield out_write\n            out_latency, out_leading_latency = out_write.latency, out_write.leading_latency\n            logger.debug(\n                f\"{self.get_engine_id()}:{self.get_engine_sub_id()} {i} - out_latency:{out_latency}, out_leading_latency:{out_leading_latency}\"\n            )\n\n            total_latency = max(\n                total_latency,\n                stat_ref + vld_leading_latency + compute_latency,  # compute latency,\n                stat_ref + lhs_rhs_latency,\n                out_ref + out_latency,  # vst latency, 4 means 4 thread\n            )\n            stat_ref = total_latency - vld_leading_latency  # update ref\n        logger.debug(\n            f\"{self.get_engine_id()}:{self.get_engine_sub_id()}  - total_latency:{total_latency}\")\n\n        # print(\"vld_leading_latency,lhs_latency,out_latency, compute_latency\", vld_leading_latency,lhs_rhs_latency,out_latency, total_compute_latency)\n        self.core_cost.latency = total_latency\n        self.core_cost.compute_cost = total_compute_latency\n\n    def _basic_stat_info(self):\n        info = self.core_cost.instruction_info\n        dtype = self.core_cost.instruction_info.dtype\n        bpe = dtype.get_bpe()\n        self.core_cost.vector_dtype = dtype\n        self.core_cost.vector_ops = info.compute_ins_shape * info.compute_1d_ins_num\n        # 2 means lhs and rhs\n        self.core_cost.ld_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.ld_ins_num * 2\n        self.core_cost.st_l0_l3 = (\n            info.compute_ins_shape * bpe) * info.st_ins_num",
    "start_line": 70,
    "end_line": 208,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "XPUAction"
    ],
    "class_name": null,
    "display_name": "class XPUSoftmaxBackwardAction",
    "component_id": "nova-platform.nova_platform.dataflow.action.xpu_softmax_backward_action.XPUSoftmaxBackwardAction"
  },
  "nova-platform.nova_platform.dataflow.dataflow.Dataflow": {
    "id": "nova-platform.nova_platform.dataflow.dataflow.Dataflow",
    "name": "Dataflow",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/dataflow/dataflow.py",
    "relative_path": "nova-platform/nova_platform/dataflow/dataflow.py",
    "depends_on": [],
    "source_code": "class Dataflow():\n    #构建action的图依赖关系，将list转换为action的dag\n    def _build_dag(self, action_list: List[DataflowAction]):\n        dag = nx.DiGraph()\n        roots = []\n        action_map = {}\n        for action in action_list:\n            action_id = action.get_action_id()\n            action_map[action_id] = action\n            if not action.get_parent_ids():\n                roots.append(action_id)\n                dag.add_node(action_id)\n            if action.get_child_ids():\n                for child_action_id in action.get_child_ids():\n                    dag.add_edge(action_id, child_action_id)\n\n        self.dag = dag\n        self._action_map = action_map\n        self._roots = roots\n        self.queue = PriorityQueue()\n\n    def execute_dataflow(self) -> Generator[DataflowAction | BARRIER, Tuple[float, bool, DataflowAction | BARRIER], None]:\n        visited = set()\n\n        # queue = self._roots.copy()\n        for action in self._roots:\n            self.queue.put((0, action))\n        # bfs\n        while True:\n            if not self.queue.empty():\n                priority, action_id = self.queue.get()\n                if action_id in visited:\n                    continue\n                if not all(p in visited for p in self.dag.predecessors(action_id)):\n                    # queue.put((priority, action_id))\n                    # TODO: need review\n                    continue\n                # logger.debug('Executing:%s', self._action_map[vertex])\n                next_ref, is_done, stat = yield action_id, self._action_map[action_id]\n            else:\n                next_ref, is_done, stat = yield None, None\n\n            if isinstance(stat, BARRIER) or isinstance(stat, BossaNovaEvent):\n                pass\n            elif not is_done:\n                # if action is not done, put it back to queue\n                self.queue.put((next_ref, action_id))\n            elif is_done:\n                # if action is done, mark it as visited and add its children to queue\n                visited.add(action_id)\n                children = self.dag.successors(action_id)\n                # queue.extend(children)\n                for _action_id in children:\n                    if (next_ref, _action_id) not in self.queue.queue:\n                        self.queue.put((next_ref, _action_id))\n                    else:\n                        pass\n            elif (next_ref, is_done, stat) == (None, None, None):\n                break\n            else:\n                raise RuntimeError\n                # if (next_ref, is_done, stat) == (None, None, None):\n                #     break\n        # check visited no of actions match dag size\n        assert (len(visited) == len(self.dag.nodes),\n                \"visited node size doesn't match dag node size\")",
    "start_line": 16,
    "end_line": 81,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Dataflow",
    "component_id": "nova-platform.nova_platform.dataflow.dataflow.Dataflow"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.MemoryManager": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.MemoryManager",
    "name": "MemoryManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [],
    "source_code": "class MemoryManager:\n    def __init__(self, total_bytes: int) -> None:\n        self.mem_name = \"gcu_l3\"\n        self.total_bytes = total_bytes\n        self.base_addr = 0x50000000000\n        self.byes_used = 0\n        self.mem_list = {}\n\n    def _align(self, bytes):\n        return (bytes + 128 - 1) // 128 * 128\n\n    def alloc_mem(self, bytes: int) -> int:\n        bytes = self._align(bytes)\n        assert bytes + self.byes_used <= self.total_bytes, f\"Out of allocatable {self.mem_name} memory\"\n        addr = self.base_addr + self.byes_used\n        self.byes_used += bytes\n        self.mem_list[addr] = bytes\n        return addr\n\n    def alloc_diag_tensor(self, tensor: DiagTensor):\n        elements = reduce(operator.mul, tensor.stride_dims, 1)\n        bypes = elements * tensor.bpe\n        tensor.addr = self.alloc_mem(bypes)\n        return tensor",
    "start_line": 38,
    "end_line": 61,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryManager",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.MemoryManager"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator",
    "name": "DataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.diag_action.TileInfo",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID",
      "nova-platform.nova_platform.executor.dataflow_gen.MemoryManager",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensor",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTensorContainer",
      "nova-platform.nova_platform.base_model.DType",
      "nova-platform.nova_platform.dataflow.action.diag_action.BufCnt"
    ],
    "source_code": "class  DataflowGenerator:\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        self.config = config\n        self.dataflow_config = dataflow_config\n        self.kwargs = kwargs\n        self.op_type = dataflow_config.get(\"bench_op_type\", \"\")\n        self.dtype = dataflow_config.get(\"bench_basic_data_type\", \"fp16\")\n        self.bpe = DType(self.dtype).get_bpe()\n        self.inputs = []\n        self.outputs = []\n        self.action_cls = DiagDataflowAction\n        self.l3_mm = MemoryManager(self.config.memory.l3.TOTAL_SIZE)\n        self.tensor_id = 0\n        self.tile_info = TileInfo(\n            cube_dim=[1, 1, 1],\n            grid_dim=[1, 1, 1],\n            block_dim=[1, 1, 1],\n            tile_shape=[1, 1, 1],\n            l2_buf_cnt=BufCnt(),\n            l1_buf_cnt=BufCnt(),\n        )\n\n    def add_input_tensor(self, dim: List[int], bpe: float = 0, level: int = 3):\n        self.inputs.append(DiagTensorContainer(self.tensor_id, [self._add_tensor(dim, bpe, level)]))\n        self.tensor_id += 1\n\n    def add_output_tensor(self, dim: List[int], bpe: float = 0, level: int = 3):\n        self.outputs.append(DiagTensorContainer(self.tensor_id, [self._add_tensor(dim, bpe, level)]))\n        self.tensor_id += 1\n\n    def _add_tensor(self, dim: List[int], bpe: float = 0, level: int = 3):\n        assert isinstance(dim, (tuple, list)) and all(isinstance(i, int) and i != 0 for i in dim)\n        bpe = self.bpe if bpe == 0 else bpe\n        tensor = DiagTensor(0, dims=list(dim), offsets=[0 for _ in dim], stride_dims=[i for i in dim], bpe=bpe)\n        self.l3_mm.alloc_diag_tensor(tensor)\n        return tensor\n\n    def _generate(self):\n        raise NotImplementedError()\n\n    def generate_dataflow(self):\n        self._generate()  # 调用子类的generate方法生成输入输出张量\n        action_list = []\n        sic_num = self.config.inst_num.NUM_OF_CLUSTER * self.config.inst_num.NUM_OF_DIE\n        sip_num = self.config.inst_num.NUM_OF_CORE_PER_CLUSTER\n        action_id = 0\n        xpu_code = f\"{self.op_type}_{self.dtype}\"\n        for sic_idx in range(sic_num):\n            for sip_idx in range(sip_num):\n                engine_id = sip_idx + sic_idx * sip_num #the global id of the sip\n                data = [sic_idx, sip_idx]\n                action_list.append(\n                    self.action_cls(\n                        code=xpu_code,\n                        config=self.config,\n                        action_id=action_id,\n                        action_type=DataflowActionType.XPU,\n                        engine_id=engine_id,\n                        engine_sub_id=0,\n                        inputs=self.inputs,\n                        outputs=self.outputs,\n                        dataflow_config=self.dataflow_config,\n                        child_action_ids=[],\n                        parent_action_ids=[],\n                        depth=0,\n                        setup_parent_action_id=-1,\n                        setup_child_action_id=-1,\n                        exe_sem_id=0,\n                        setup_sem_id=-1,\n                        trigger_id=DiagTriggerID(0, []),\n                        input_hints=[],\n                        die_id=0,\n                        tile_info=self.tile_info,\n                        data=data,\n                        **self.kwargs,\n                    )\n                )\n                action_id += 1\n        return DiagDataflow(\n            dataflow_name=f\"{self.op_type}_{self.dtype}\",\n            dataflow_id=0,\n            odte_total_bytes=0,\n            cdte_total_bytes=0,\n            sdte_total_bytes=0,\n            action_list=action_list,\n        )",
    "start_line": 64,
    "end_line": 149,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class DataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.GemmSharedDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.GemmSharedDataflowGenerator",
    "name": "GemmSharedDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.diag_action.TileInfo",
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator",
      "nova-platform.nova_platform.benchmark.batch_gemm.QuantType",
      "nova-platform.nova_platform.dataflow.action.diag_action.BufCnt"
    ],
    "source_code": "class GemmSharedDataflowGenerator(DataflowGenerator):\n    #初始化 调用父类初始化\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUSharedGemmAction\n    #生成gemm数据流\n    def _generate(self):\n        #从config中获得shape\n        gemm_shape = self.dataflow_config.get(\"bench_gemm_shape_b_m_k_n\", [])\n        assert len(gemm_shape) == 4, \"wrong input of bench_gemm_shape_b_m_k_n\"\n        b, m, k, n = gemm_shape\n        #获取量化类型\n        quant_type = QuantType(self.dataflow_config.get(\"bench_gemm_quant_type\", \"No_Quant\"))\n        if quant_type == QuantType.Wf4g_Af8t:\n            group_size = self.dataflow_config.get(\"bench_gemm_quant_group_size\", 32)\n            group_num = (k + group_size - 1) // group_size\n            lhs_bpe, rhs_bpe, scaling_bpe, res_bpe, bias_bpe = 1, 0.5, 2, 2, 4\n        elif quant_type == QuantType.Wf8t_Af8t:\n            lhs_bpe, rhs_bpe, res_bpe, bias_bpe = 1, 1, 2, 4\n        elif quant_type == QuantType.No_Quant:\n            lhs_bpe, rhs_bpe, res_bpe, bias_bpe = [self.bpe] * 4\n        else:\n            raise RuntimeError(f\"Unsupported quant type: {quant_type}\")\n\n        # 添加输入输出张量\n        # TODO: reversed dims in shape to be compatible with diag tool, fix later\n        self.add_input_tensor([k, m, b], lhs_bpe)  # [b, m, k]\n        self.add_input_tensor([n, k, b], rhs_bpe)  # [b, k, n]\n        if quant_type == QuantType.Wf4g_Af8t:\n            self.add_input_tensor([n, group_num, b], scaling_bpe)  # [b, group_num, n]\n        self.add_output_tensor([n, m, b], res_bpe)  # [b, m, n]\n\n        # 设定tile info【TODO：理解cube grid block tile的作用】\n        self.tile_info = TileInfo(\n            cube_dim=[b, m, n],\n            grid_dim=[b, m, n],\n            block_dim=[1, 1, 1],\n            tile_shape=[1, 1, 1],\n            l2_buf_cnt=BufCnt(),\n            l1_buf_cnt=BufCnt(),\n        )",
    "start_line": 152,
    "end_line": 192,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class GemmSharedDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.GemmSharedDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.GemmLocalDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.GemmLocalDataflowGenerator",
    "name": "GemmLocalDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.GemmSharedDataflowGenerator"
    ],
    "source_code": "class GemmLocalDataflowGenerator(GemmSharedDataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPULocalGemmAction",
    "start_line": 195,
    "end_line": 198,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "GemmSharedDataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class GemmLocalDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.GemmLocalDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.TPUGemmDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.TPUGemmDataflowGenerator",
    "name": "TPUGemmDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagTriggerID",
      "nova-platform.nova_platform.dataflow.action.diag_action.DiagDataflow",
      "nova-platform.nova_platform.executor.dataflow_gen.GemmSharedDataflowGenerator"
    ],
    "source_code": "class TPUGemmDataflowGenerator(GemmSharedDataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = TpuGemmAction\n\n    def generate_dataflow(self):\n        # TPU: 没有 sic/sip 概念，按阵列数生成动作（默认单阵列）\n        self._generate()\n        action_list = []\n        array_num = getattr(getattr(self.config.compute, \"tpu\", None), \"ARRAY_NUM\", 1) or 1\n        xpu_code = f\"{self.op_type}_{self.dtype}\"\n        action_id = 0\n        for array_idx in range(array_num):\n            action_list.append(\n                self.action_cls(\n                    code=xpu_code,\n                    config=self.config,\n                    action_id=action_id,\n                    action_type=DataflowActionType.XPU,\n                    engine_id=array_idx,\n                    engine_sub_id=0,\n                    inputs=self.inputs,\n                    outputs=self.outputs,\n                    dataflow_config=self.dataflow_config,\n                    child_action_ids=[],\n                    parent_action_ids=[],\n                    depth=0,\n                    setup_parent_action_id=-1,\n                    setup_child_action_id=-1,\n                    exe_sem_id=0,\n                    setup_sem_id=-1,\n                    trigger_id=DiagTriggerID(0, []),\n                    input_hints=[],\n                    die_id=0,\n                    tile_info=self.tile_info,\n                    data=[array_idx],\n                    **self.kwargs,\n                )\n            )\n            action_id += 1\n\n        return DiagDataflow(\n            dataflow_name=f\"{self.op_type}_{self.dtype}\",\n            dataflow_id=0,\n            odte_total_bytes=0,\n            cdte_total_bytes=0,\n            sdte_total_bytes=0,\n            action_list=action_list,\n        )",
    "start_line": 201,
    "end_line": 249,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "GemmSharedDataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class TPUGemmDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.TPUGemmDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.ElementwiseDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.ElementwiseDataflowGenerator",
    "name": "ElementwiseDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class ElementwiseDataflowGenerator(DataflowGenerator):\n    _BINARY_OPS = {\"add\", \"mul\"}\n    _ACTION_MAP = {\n        \"add\": XPUAddAction,\n        \"mul\": XPUMulAction,\n        \"gelu\": XPUGeluAction,\n        \"relu\": XPUReluAction,\n        \"silu\": XPUSiluAction,\n        \"sigmoid\": XPUSigmoidAction,\n    }\n\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = self._ACTION_MAP[self.op_type]\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_elementwise_shape\")\n        assert shape, \"bench_elementwise_shape must be provided for elementwise ops\"\n        operand_count = 2 if self.op_type in self._BINARY_OPS else 1\n        for _ in range(operand_count):\n            self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 251,
    "end_line": 272,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class ElementwiseDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.ElementwiseDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.LayerNormDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.LayerNormDataflowGenerator",
    "name": "LayerNormDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class LayerNormDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPULayernormAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_layernorm_shape\")\n        assert shape, \"bench_layernorm_shape must be provided for layernorm\"\n        self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 275,
    "end_line": 284,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class LayerNormDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.LayerNormDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.SoftmaxDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.SoftmaxDataflowGenerator",
    "name": "SoftmaxDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class SoftmaxDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        if self.op_type == \"softmaxbackward\":\n            self.action_cls = XPUSoftmaxBackwardAction\n        else:\n            self.action_cls = XPUSoftmaxAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_softmax_shape_b_c\")\n        assert shape, \"bench_softmax_shape_b_c must be provided for softmax\"\n        self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 287,
    "end_line": 299,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class SoftmaxDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.SoftmaxDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.GatherDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.GatherDataflowGenerator",
    "name": "GatherDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class GatherDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUGatherAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_gather_shape\")\n        assert shape, \"bench_gather_shape must be provided for gather\"\n        self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 302,
    "end_line": 311,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class GatherDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.GatherDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.ScatterDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.ScatterDataflowGenerator",
    "name": "ScatterDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class ScatterDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUScatterAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_scatter_shape\")\n        assert shape, \"bench_scatter_shape must be provided for scatter\"\n        self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 314,
    "end_line": 323,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class ScatterDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.ScatterDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.TransposeDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.TransposeDataflowGenerator",
    "name": "TransposeDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class TransposeDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUNopAction\n\n    def _generate(self):\n        tensor_shape = self.dataflow_config.get(\"bench_transpose_tensor_b_h_w_c\")\n        assert tensor_shape, \"bench_transpose_tensor_b_h_w_c must be provided for transpose\"\n        self.add_input_tensor(tensor_shape)\n        self.add_output_tensor(tensor_shape)",
    "start_line": 326,
    "end_line": 335,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class TransposeDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.TransposeDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.SdpaDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.SdpaDataflowGenerator",
    "name": "SdpaDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class SdpaDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUSdpaAction\n\n    def _generate(self):\n        q_shape = self.dataflow_config.get(\"bench_sdpa_q_shape\")\n        kv_shape = self.dataflow_config.get(\"bench_sdpa_kv_shape\")\n        assert q_shape and kv_shape, \"sdpa requires q/kv shapes\"\n        self.add_input_tensor(q_shape)\n        self.add_input_tensor(kv_shape)\n        self.add_output_tensor(q_shape)",
    "start_line": 338,
    "end_line": 349,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class SdpaDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.SdpaDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.AllReduceDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.AllReduceDataflowGenerator",
    "name": "AllReduceDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class AllReduceDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUAllReduceAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_all_reduce_shape\")\n        assert shape, \"bench_all_reduce_shape must be provided for allreduce\"\n        self.add_input_tensor(shape)\n        self.add_output_tensor(shape)",
    "start_line": 352,
    "end_line": 361,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class AllReduceDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.AllReduceDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.AllGatherDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.AllGatherDataflowGenerator",
    "name": "AllGatherDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class AllGatherDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUAllGatherAction\n\n    def _generate(self):\n        out_shape = self.dataflow_config.get(\"bench_allgather_out_shape\")\n        in_shape = self.dataflow_config.get(\"bench_allgather_in_shape\")\n        assert in_shape and out_shape, \"allgather requires in/out shapes\"\n        self.add_input_tensor(in_shape)\n        self.add_output_tensor(out_shape)",
    "start_line": 364,
    "end_line": 374,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class AllGatherDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.AllGatherDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.AllGatherGemmDataflowGenerator": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.AllGatherGemmDataflowGenerator",
    "name": "AllGatherGemmDataflowGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.DataflowGenerator"
    ],
    "source_code": "class AllGatherGemmDataflowGenerator(DataflowGenerator):\n    def __init__(self, config: BossaNovaConfig, dataflow_config: Dict[str, Any], **kwargs) -> None:\n        super().__init__(config, dataflow_config, **kwargs)\n        self.action_cls = XPUAllGatherGemmAction\n\n    def _generate(self):\n        shape = self.dataflow_config.get(\"bench_allgather_gemm_shape_b_m_k_n\")\n        assert shape and len(shape) == 4, \"bench_allgather_gemm_shape_b_m_k_n must be [B,M,K,N]\"\n        b, m, k, n = shape\n        self.add_input_tensor([k, m, b])\n        self.add_input_tensor([n, k, b])\n        self.add_output_tensor([n, m, b])",
    "start_line": 377,
    "end_line": 388,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DataflowGenerator"
    ],
    "class_name": null,
    "display_name": "class AllGatherGemmDataflowGenerator",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.AllGatherGemmDataflowGenerator"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.Backend": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.Backend",
    "name": "Backend",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [],
    "source_code": "class Backend:\n    def get_generator_cls(self, op_type: str):\n        raise NotImplementedError\n\n    def has_generator(self, op_type: str) -> bool:\n        raise NotImplementedError",
    "start_line": 390,
    "end_line": 395,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Backend",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.Backend"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.XPUBackend": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.XPUBackend",
    "name": "XPUBackend",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.Backend"
    ],
    "source_code": "class XPUBackend(Backend):\n    def __init__(self):\n        self._generator_mapping = {\n            \"gemm\": GemmSharedDataflowGenerator,\n            \"gemm.shared\": GemmSharedDataflowGenerator,\n            \"gemm.local\": GemmLocalDataflowGenerator,\n            \"add\": ElementwiseDataflowGenerator,\n            \"mul\": ElementwiseDataflowGenerator,\n            \"gelu\": ElementwiseDataflowGenerator,\n            \"relu\": ElementwiseDataflowGenerator,\n            \"silu\": ElementwiseDataflowGenerator,\n            \"sigmoid\": ElementwiseDataflowGenerator,\n            \"layernorm\": LayerNormDataflowGenerator,\n            \"softmax\": SoftmaxDataflowGenerator,\n            \"softmaxbackward\": SoftmaxDataflowGenerator,\n            \"gather\": GatherDataflowGenerator,\n            \"scatter\": ScatterDataflowGenerator,\n            \"transpose\": TransposeDataflowGenerator,\n            \"sdpa\": SdpaDataflowGenerator,\n            \"allreduce\": AllReduceDataflowGenerator,\n            \"allgather\": AllGatherDataflowGenerator,\n            \"allgather_gemm\": AllGatherGemmDataflowGenerator,\n        }\n\n    def get_generator_cls(self, op_type: str):\n        return self._generator_mapping[op_type]\n    \n    def has_generator(self, op_type: str) -> bool:\n        return op_type in self._generator_mapping",
    "start_line": 397,
    "end_line": 425,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Backend"
    ],
    "class_name": null,
    "display_name": "class XPUBackend",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.XPUBackend"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.TPUBackend": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.TPUBackend",
    "name": "TPUBackend",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.Backend"
    ],
    "source_code": "class TPUBackend(Backend):\n    def __init__(self):\n        self._generator_mapping = {\n            \"gemm\": TPUGemmDataflowGenerator,\n            \"gemm.shared\": TPUGemmDataflowGenerator,\n            \"gemm.local\": TPUGemmDataflowGenerator,\n        }\n\n    def get_generator_cls(self, op_type: str):\n        return self._generator_mapping[op_type]\n\n    def has_generator(self, op_type: str) -> bool:\n        return op_type in self._generator_mapping",
    "start_line": 427,
    "end_line": 439,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Backend"
    ],
    "class_name": null,
    "display_name": "class TPUBackend",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.TPUBackend"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.GPUBackend": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.GPUBackend",
    "name": "GPUBackend",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.Backend"
    ],
    "source_code": "class GPUBackend(Backend):\n    def __init__(self):\n        self._generator_mapping = {\n            # 以后再填 GPU 自己的一套 op -> Generator\n        }\n\n    def get_generator_cls(self, op_type: str):\n        return self._generator_mapping[op_type]\n\n    def has_generator(self, op_type: str) -> bool:\n        return op_type in self._generator_mapping",
    "start_line": 441,
    "end_line": 451,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Backend"
    ],
    "class_name": null,
    "display_name": "class GPUBackend",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.GPUBackend"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.get_backend": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.get_backend",
    "name": "get_backend",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.TPUBackend",
      "nova-platform.nova_platform.executor.dataflow_gen.GPUBackend",
      "nova-platform.nova_platform.executor.dataflow_gen.XPUBackend"
    ],
    "source_code": "def get_backend(config: BossaNovaConfig) -> Backend:\n    arch = getattr(config, \"arch\", \"xpu\")\n    if arch == 'xpu':\n        return XPUBackend()\n    elif arch == 'tpu':\n        return TPUBackend()\n    elif arch == 'gpu':\n        return GPUBackend()\n    else:\n        raise RuntimeError(f\"Unsupported backend arch: {arch}\")",
    "start_line": 477,
    "end_line": 486,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_backend",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.get_backend"
  },
  "nova-platform.nova_platform.executor.dataflow_gen.generate_dataflow": {
    "id": "nova-platform.nova_platform.executor.dataflow_gen.generate_dataflow",
    "name": "generate_dataflow",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/dataflow_gen.py",
    "relative_path": "nova-platform/nova_platform/executor/dataflow_gen.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.dataflow_gen.get_backend",
      "nova-platform.nova_platform.executor.nova_platform_executor.generate_dataflow"
    ],
    "source_code": "def generate_dataflow(config: BossaNovaConfig, dataflow_config: Dict[str, Any], topo: TOPO, case_id: int):\n    bench_op_type = dataflow_config.get(\"bench_op_type\", \"\")\n    backend = get_backend(config) \n    generator_cls = backend.get_generator_cls(bench_op_type)\n    generator = generator_cls(config, dataflow_config, topo=topo, case_id=case_id)\n    return generator.generate_dataflow()",
    "start_line": 489,
    "end_line": 494,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config",
      "dataflow_config",
      "topo",
      "case_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function generate_dataflow",
    "component_id": "nova-platform.nova_platform.executor.dataflow_gen.generate_dataflow"
  },
  "nova-platform.nova_platform.executor.nova_platform_barrier.BARRIER": {
    "id": "nova-platform.nova_platform.executor.nova_platform_barrier.BARRIER",
    "name": "BARRIER",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "depends_on": [],
    "source_code": "class BARRIER():\n    def __init__(self, count):\n        self.count = count\n        self.barrier = [i for i in range(count)]\n        self.barrier_map = {b: False for b in self.barrier}\n        self.max_t = 0\n        self.is_done = False\n        self.lock = Lock()\n\n    def get_barrier(self):\n        b = self.barrier.pop()\n        self.barrier_map[b] = False\n        return partial(self.__wait, b)\n\n    def __wait(self, barrier_obj, ref):\n        self.lock.acquire()\n        self.max_t = max(self.max_t, ref)\n        del self.barrier_map[barrier_obj]\n        self.lock.release()\n        while True:\n            if self.barrier_map:\n                yield self\n            else:\n                self.is_done = True\n                break\n        return self.max_t\n\n    def wait(self, ref):\n        b = self.get_barrier()\n        max_t = yield from b(ref)\n        return max_t",
    "start_line": 12,
    "end_line": 42,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BARRIER",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_barrier.BARRIER"
  },
  "nova-platform.nova_platform.executor.nova_platform_barrier.BarrierManager": {
    "id": "nova-platform.nova_platform.executor.nova_platform_barrier.BarrierManager",
    "name": "BarrierManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_barrier.BARRIER"
    ],
    "source_code": "class BarrierManager(metaclass=SingletonMeta):\n    def __init__(self):\n        self.lock = Lock()\n\n    def get_barrier(self, uuid, count) -> BARRIER:\n        if uuid not in _inst_map:\n            self.lock.acquire()\n            if uuid not in _inst_map:\n                _inst_map[uuid] = BARRIER(count)\n            self.lock.release()\n        return _inst_map[uuid]",
    "start_line": 48,
    "end_line": 58,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BarrierManager",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_barrier.BarrierManager"
  },
  "nova-platform.nova_platform.executor.nova_platform_barrier.check": {
    "id": "nova-platform.nova_platform.executor.nova_platform_barrier.check",
    "name": "check",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "depends_on": [],
    "source_code": "def check():\n    inflight_count = 0\n    done_count = 0\n    if done_count == 0 and inflight_count == 0:\n        return\n    for uuid in list(_inst_map.keys()):\n        barrier = _inst_map[uuid]\n        if not barrier.is_done:\n            logger.warning(\"barrier: %s, wait %d out of %d\", uuid, len(\n                barrier.barrier_map), barrier.count)\n            inflight_count += 1\n        else:\n            done_count += 1\n\n    logger.info(\"barrier: inflgiht: %d, done: %d\", inflight_count, done_count)",
    "start_line": 61,
    "end_line": 75,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_barrier.check"
  },
  "nova-platform.nova_platform.executor.nova_platform_barrier.threaded_function": {
    "id": "nova-platform.nova_platform.executor.nova_platform_barrier.threaded_function",
    "name": "threaded_function",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_barrier.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_barrier.check"
    ],
    "source_code": "def threaded_function():\n    logger.info(\"barrier: check thread started\")\n    while True:\n        check()\n        sleep(2)",
    "start_line": 78,
    "end_line": 82,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function threaded_function",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_barrier.threaded_function"
  },
  "nova-platform.nova_platform.executor.nova_platform_event.BossaNovaEvent": {
    "id": "nova-platform.nova_platform.executor.nova_platform_event.BossaNovaEvent",
    "name": "BossaNovaEvent",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_event.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_event.py",
    "depends_on": [],
    "source_code": "class BossaNovaEvent():\n\n    def __init__(self, uuid):\n        super().__init__()\n        self.is_done = False\n        self.max_t = 0\n        self.uuid = uuid\n        self.lock = Lock()\n\n    def set(self, ref):\n        assert self.is_done == False\n        self.max_t = max(ref, self.max_t)\n        self.is_done = True\n\n    def wait(self, ref):\n        self.max_t = ref\n        while True:\n            if not self.is_done:\n                yield self\n            else:\n                break\n        return self.max_t",
    "start_line": 10,
    "end_line": 31,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BossaNovaEvent",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_event.BossaNovaEvent"
  },
  "nova-platform.nova_platform.executor.nova_platform_event.EventManager": {
    "id": "nova-platform.nova_platform.executor.nova_platform_event.EventManager",
    "name": "EventManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_event.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_event.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_event.BossaNovaEvent"
    ],
    "source_code": "class EventManager(metaclass=SingletonMeta):\n    def __init__(self):\n        self.Lock = Lock()\n\n    def get_event(self, uuid) -> BossaNovaEvent:\n        if uuid not in _inst_map:\n            self.Lock.acquire()\n            if uuid not in _inst_map:\n                _inst_map[uuid] = BossaNovaEvent(uuid)\n            self.Lock.release()\n\n        return _inst_map[uuid]",
    "start_line": 37,
    "end_line": 48,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EventManager",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_event.EventManager"
  },
  "nova-platform.nova_platform.executor.nova_platform_event.check": {
    "id": "nova-platform.nova_platform.executor.nova_platform_event.check",
    "name": "check",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_event.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_event.py",
    "depends_on": [],
    "source_code": "def check():\n    inflight_count = 0\n    done_count = 0\n    if done_count == 0 and inflight_count == 0:\n        return\n    for uuid in list(_inst_map.keys()):\n        event = _inst_map[uuid]\n        if not event.is_done:\n            logger.warning(\"event: wait %s\", uuid)\n            inflight_count += 1\n        else:\n            done_count += 1\n\n    logger.info(\"event: inflgiht: %d, done: %d\", inflight_count, done_count)",
    "start_line": 51,
    "end_line": 64,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_event.check"
  },
  "nova-platform.nova_platform.executor.nova_platform_event.threaded_function": {
    "id": "nova-platform.nova_platform.executor.nova_platform_event.threaded_function",
    "name": "threaded_function",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_event.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_event.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_event.check"
    ],
    "source_code": "def threaded_function():\n    logger.info(\"event: check thread started\")\n    while True:\n        check()\n        sleep(2)",
    "start_line": 67,
    "end_line": 71,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function threaded_function",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_event.threaded_function"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor._DummyTqdm": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor._DummyTqdm",
    "name": "_DummyTqdm",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "    class _DummyTqdm:\n        def __init__(self, iterable=None, **kwargs):\n            self.iterable = iterable\n\n        def update(self, n=1):\n            pass\n\n        def close(self):\n            pass\n\n        def __iter__(self):\n            if self.iterable is None:\n                return iter([])\n            for item in self.iterable:\n                yield item\n\n        def __enter__(self):\n            return self\n\n        def __exit__(self, exc_type, exc_val, exc_tb):\n            pass",
    "start_line": 6,
    "end_line": 26,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _DummyTqdm",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor._DummyTqdm"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.tqdm": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.tqdm",
    "name": "tqdm",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_executor._DummyTqdm"
    ],
    "source_code": "    def tqdm(iterable=None, **kwargs):\n        return _DummyTqdm(iterable=iterable, **kwargs)",
    "start_line": 28,
    "end_line": 29,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "iterable"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tqdm",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.tqdm"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.get_data_flow_gen_cmd": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.get_data_flow_gen_cmd",
    "name": "get_data_flow_gen_cmd",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "def get_data_flow_gen_cmd(dataflow_config: dict, arch_config: BossaNovaConfig):\n    if dataflow_config[\"bench_op_type\"] not in SUPPORTED_OP_TYPES:\n        raise RuntimeError(\n            f'Not supported op type {dataflow_config[\"bench_op_type\"]}')\n    binary_path = Path(__file__).resolve().parents[2] / \"build\" / \"dataflow_gen\"\n    sic_num = arch_config.inst_num.NUM_OF_CLUSTER * arch_config.inst_num.NUM_OF_DIE\n    l3_mc_bw = arch_config.bw.mc.l3.bw * arch_config.freq.MC * \\\n        arch_config.inst_num.NUM_OF_DIE * 1e9\n    args = [\n        str(binary_path),\n        f\"--arch.sic_num={sic_num}\",\n        f\"--arch.xpu_num_per_sic={arch_config.inst_num.NUM_OF_CORE_PER_CLUSTER}\",\n        f\"--arch.l2_bytes_per_sic={arch_config.memory.l2.SIZE_PER_SIC}\",\n        f\"--arch.l1_bytes_per_xpu={arch_config.memory.l1.SIZE_PER_CORE}\",\n        f\"--arch.l3_mc_bandwidth={l3_mc_bw}\",\n        f\"--arch.l3_mc_cnt=1\",\n    ]\n\n    for dtype, v in arch_config.compute.thread_2d_throughput.items():\n        dtype: DType\n        # TODO: need review as now using libra default value when v is not set\n        if not v or dtype == DType.FP4:\n            continue\n        args.append(f\"--arch.xpu_2d_macs.{dtype.name.lower()}={v}\")\n\n    for dtype, v in arch_config.compute.thread_1d_throughput.items():\n        dtype: DType\n        # TODO: need review as now using libra default value when v is not set\n        if not v or dtype == DType.FP4:\n            continue\n        args.append(f\"--arch.xpu_1d_throughput.{dtype.name.lower()}={v}\")\n    args.append(\n        f\"--arch.xpu_sfu_throughput={arch_config.compute.thread_sfu_throughput}\")\n\n    for k, v in dataflow_config.items():\n        if isinstance(v, (list, tuple)):\n            v = \",\".join(str(i) for i in v)\n        args.append(f\"--{k}={v}\")\n\n    cmd = \" \".join(args)\n    return cmd",
    "start_line": 97,
    "end_line": 137,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "dataflow_config",
      "arch_config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_data_flow_gen_cmd",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.get_data_flow_gen_cmd"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.check_cuda": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.check_cuda",
    "name": "check_cuda",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "def check_cuda():\n    try:\n        import pycuda\n        return True\n    except Exception as e:\n        return False",
    "start_line": 140,
    "end_line": 145,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_cuda",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.check_cuda"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.customize_loader": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.customize_loader",
    "name": "customize_loader",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "def customize_loader(d, config, dataflow_config, topo, case_id):\n    if \"action_type\" not in d:\n        raise Exception(\"action_type is required\")\n    action_type = d[\"action_type\"]\n    d[\"config\"] = config\n    d[\"topo\"] = topo\n    d[\"dataflow_config\"] = dataflow_config\n    d[\"case_id\"] = case_id\n    if action_type == \"xpu\":\n        code = d[\"code\"]\n        optype = code.rsplit(\"_\", 1)[0]\n        if optype in action_map[\"xpu\"]:\n            cls = action_map[\"xpu\"][optype]\n            return cls\n        else:\n            raise Exception(f\"{action_type}:{optype} not supported\")\n    elif action_type == \"cdte\":\n        op = d[\"op\"]\n        if op in action_map[\"cdte\"]:\n            cls = action_map[\"cdte\"][op]\n            return cls\n        else:\n            raise Exception(f\"{action_type}:{op} not supported\")\n    else:\n        raise Exception(f\"action_type={d['action_type']} is not supported\")",
    "start_line": 178,
    "end_line": 202,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "d",
      "config",
      "dataflow_config",
      "topo",
      "case_id"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function customize_loader",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.customize_loader"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.BossaNovaExecutor": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.BossaNovaExecutor",
    "name": "BossaNovaExecutor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.cache.parallel_cache_cost_service.ParallelCacheCostService",
      "nova-platform.nova_platform.utils.cuda_utils.get_gpu_count",
      "nova-platform.nova_platform.cost_service.cache.cache_cost_service.CacheCostService"
    ],
    "source_code": "class BossaNovaExecutor():\n\n    def __init__(\n        self,\n        config: BossaNovaConfig,\n        outdir: str,\n        tgen: TraceGenerator,\n        op_shape,\n        optype,\n        dtype,\n        input_addr,\n        output_addr,\n        dataflow_config,\n        mu=1.0,\n        device=\"auto\",\n        device_id=0,\n        enable_cache=False,\n        cache_svc=None,\n        enable_power_svc=False,\n        enable_dump_addr=False,\n        initial_ref=0,\n        esl_switch: BaseESLSwitch = None,\n        case_idx=None,\n    ):\n        self.config = config\n        self.cache_svc = None\n        self.outdir = outdir\n        self.input_addr = input_addr\n        self.output_addr = output_addr\n        self.dataflow_config = dataflow_config\n        self.esl_switch = esl_switch\n        self.case_idx = case_idx\n        if not enable_cache:\n            logger.info(\"cache model disabled\")\n\n        elif cache_svc:\n            self.cache_svc = cache_svc\n        else:\n            if device == 'cpu':\n                self.cache_svc = CacheCostService(config)\n                logger.info(\"device=cpu, default cache svc inited\")\n            elif device == 'gpu':\n                device_count = get_gpu_count()\n                if device_count:\n                    from nova_platform.cost_service.cache.parallel_cache_cost_service import ParallelCacheCostService\n                    self.cache_svc = ParallelCacheCostService(\n                        config, device_id=device_id)\n                    logger.info(\"device=auto, parallel cache svc inited\")\n                else:\n                    msg = \"device=gpu, BossaNova failed to init parallel cache svc\"\n                    logger.error(msg)\n                    raise Exception(msg)\n            elif device == 'auto':\n                device_count = get_gpu_count()\n                if device_count:\n                    from nova_platform.cost_service.cache.parallel_cache_cost_service import ParallelCacheCostService\n                    self.cache_svc = ParallelCacheCostService(\n                        config, device_id=device_id)\n                    logger.info(\"device=auto, parallel cache svc inited\")\n                else:\n                    self.cache_svc = CacheCostService(config)\n                    logger.info(\"device=auto, default cache svc inited\")\n            else:\n                raise Exception(\"device=%s is not supported\", device)\n            logger.info(\"cache model enabled\")\n\n        # make an empty power_gen\n        class EmptyPowerSvc:\n            def process(self, action, context, ref):\n                while True:\n                    stat: DataflowActionMemoryStat = yield\n                    if not stat:\n                        break\n                yield\n\n            def get_edc_freq(self, ref, context):\n                base_freq_cfg = context.bw_resource_context.config.freq\n                return base_freq_cfg\n\n            def init_context(self):\n                pass\n\n            def post_process(self, context: BossaNovaContext):\n                pass\n\n            def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n                return {}\n\n        self.power_svc = PowerCostService(\n            config) if enable_power_svc else EmptyPowerSvc()\n\n        def get_dump_addr():\n            self.f = open(f\"{self.outdir}/addr_dump.txt\", \"w\")\n            self.f.write(\n                \"start,end,master,die_id,cluster_id,engine_id,address,size,rw\\n\")\n\n            def dump_addr(action: DiagDataflowAction, stat, ref):\n                master = action.get_action_type()\n                die_id = action.get_die_id()\n                cluster_id = action.get_cluster_id()\n                engine_id = action.get_local_engine_id()\n                for access in stat.memory_access_list:\n                    if access.base_addr >= 2**40*5:\n                        s = ref+stat.relative_ts\n                        e = ref+stat.relative_ts+stat.latency\n                        self.f.write(\n                            f\"{s:0.9f},{e:0.9f},{master}, {die_id}, {cluster_id}, {engine_id}, {hex(access.base_addr)}, {hex(access.size)}, {access.rw}\\n\")\n            return dump_addr\n\n        self.compute_svc = ComputeCostService(\n            config,\n            power_svc=self.power_svc,\n            cache_svc=self.cache_svc if self.cache_svc else None,\n            esl_switch=esl_switch,\n            dump_addr=get_dump_addr() if enable_dump_addr else lambda action, stat, ref: None\n        )\n        # if not tgen:\n        #     self.tgen = TraceGenerator(\n        #         f\"{outdir}/trace.perfetto-trace\", self.config.gcu_id)\n        # else:\n        #     self.tgen = tgen\n\n        self.tgen = tgen\n        self.post_processor = PostProcessor(outdir=outdir, tgen=self.tgen)\n        self.context = BossaNovaContext(\n            initial_ref=initial_ref,\n            bw_resource_context=BWResourceContext(\n                config=self.config, esl_switch=self.esl_switch),\n            power_context=self.power_svc.init_context(),\n            tgen=self.tgen,\n        )\n        self.dataflow = self.generate_dataflow(op_shape, optype, dtype, mu)\n\n    def get_service_list(self) -> Generator[BaseCostService, None, None]:\n        if self.cache_svc:\n            yield self.cache_svc\n        yield self.compute_svc\n        if self.power_svc:\n            yield self.power_svc\n\n    def get_ref(self, dataflow: Dataflow, action: DataflowAction, visited):\n        '''\n        caculate the start time at the last action in a dataflow\n        '''\n        dag = dataflow.dag\n        if action.get_action_id() in visited:\n            return visited[action.get_action_id()]\n        ref = 0\n        for parent_id in dag.predecessors(action.get_action_id()):\n            parent = dataflow._action_map[parent_id]\n            latency = self.get_ref(dataflow, parent, visited)\n            latency += self.context.cost_dict[parent_id].latency\n            if latency > ref:\n                ref = latency\n        visited[action.get_action_id()] = ref\n        return ref\n\n    def _process_action(self, action: DataflowAction, ref):\n        cid = action.get_cluster_id()\n        engine_id = action.get_local_engine_id()\n        action_type = action.get_action_type()\n        trace_label = getattr(action, \"get_trace_label\", lambda: action_type.name.lower())()\n        action.ref = ref\n        track = self.context.get_cluster_tgen(action.get_die_id(), cid).create_track(\n            f\"{trace_label}:{engine_id}\", tid=engine_id)\n        yield from self.compute_svc.process(action, self.context, ref, trace_label=trace_label)\n        # yield from self.power_svc.process(action, self.context, ref)\n        self.compute_svc.post_process(self.context)\n        if self.power_svc:\n            self.power_svc.post_process(self.context)\n\n        # core_stat = action.get_core_stat()\n\n        cost_book = self.context.get_cost_book(action)\n\n        track.duration(ref, cost_book.latency, action_type, cost_book)\n        return cost_book.latency\n\n    def execute(self) -> Generator[float, None, None]:\n        visited = {}\n        action_gen = self.dataflow.execute_dataflow()\n        action_id, action = next(action_gen)\n        action_map: Dict[int, Generator] = {}\n\n        initial_ref = self.context.initial_ref\n        max_ref: float = 0\n        action_len = len(self.dataflow._action_map)\n\n        worker_id = os.getenv(\"PYTEST_XDIST_WORKER\", \"master\")\n\n        position = int(worker_id.replace(\"gw\", \"\")\n                       ) if worker_id != \"master\" else 0  # 根据 worker ID 计算位置\n\n        pbar = tqdm(total=action_len,\n                    desc=f\"Worker {worker_id}\", position=position, leave=True, unit=\"act\")\n        next_ref = 0\n        event_barrier_map: Dict[int, BARRIER | BossaNovaEvent] = {}\n\n        def _check_barrier():\n            for _action_id in list(event_barrier_map.keys()):\n                _event_or_barrier = event_barrier_map[_action_id]\n                if _event_or_barrier.is_done:\n                    self.dataflow.queue.put(\n                        (_event_or_barrier.max_t, _action_id))\n                    event_barrier_map.pop(_action_id)\n\n        while True:\n            try:\n                if action:\n                    local_latency = self.get_ref(\n                        self.dataflow, action, visited)\n                    ref = local_latency+initial_ref\n                    is_done = False\n                    stat = None\n                    if id(action) in action_map:\n                        action_process = action_map[id(action)]\n                        try:\n                            next_ref, stat = next(action_process)\n                            if isinstance(stat, BARRIER) or isinstance(stat, BossaNovaEvent):\n                                event_barrier_map[action_id] = stat\n                        except StopIteration as e:\n                            next_ref = ref + e.value\n                            action_map.pop(id(action))\n                            pbar.update(1)\n                            is_done = True\n                        finally:\n                            yield next_ref, 'running'\n                    else:\n                        action_process = self._process_action(action, ref)\n                        assert isinstance(\n                            action_process, GeneratorType), \"process must be an generator\"\n                        action_map[id(action)] = action_process\n                        next_ref = ref\n                        # continue\n\n                    action_latency = self.context.cost_dict[action.get_action_id(\n                    )].latency\n                    max_ref = max(max_ref, ref + action_latency)\n\n                    _check_barrier()\n                    # send back current action status and get the next\n                    action_id, action = action_gen.send(\n                        (next_ref, is_done, stat))\n                    pass\n                elif len(event_barrier_map) > 0:\n                    # print(\"!!!!!event_barrier_map\")\n                    next_ref = MAX_TIME\n                    for b in event_barrier_map.values():\n                        next_ref = min(next_ref, b.max_t)\n                    # wait for barrier\n                    yield next_ref, 'wait'\n                    _check_barrier()\n                    action_id, action = action_gen.send(\n                        (next_ref, False, None))\n\n                else:\n                    action_id, action = action_gen.send(\n                        (None, None, None))\n                    # 当action和barrier_map 均无待处理工作时表示完成\n                    break\n\n            except StopIteration:\n                break\n        self.end_ref = max_ref\n        pbar.close()\n\n    def generate_dataflow(self, op_shape, optype, dtype, mu) -> Dataflow:\n        dataflow_config = {\n            \"bench_out_dir\": self.outdir,\n            \"bench_op_type\": optype,\n            \"bench_basic_data_type\": dtype.name.lower(),\n        }\n\n        def _resolve_gemm_op_type(version: int) -> str:\n            if version in (4, 6):\n                return \"gemm.local\"\n            if version == 5:\n                return \"gemm.shared\"\n            return \"gemm\"\n\n        if optype == 'linear':\n            op_shape = [1, op_shape[0] * op_shape[1],\n                        op_shape[2], op_shape[3]]\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == 'matmul':\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm\":\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype in [\"add\", \"mul\", \"gelu\", \"relu\", \"silu\", \"sigmoid\"]:\n            dataflow_config[\"bench_elementwise_shape\"] = op_shape\n        elif optype == \"layernorm\":\n            dataflow_config[\"bench_layernorm_shape\"] = op_shape\n        elif optype in [\"softmax\", \"softmaxbackward\"]:\n            dataflow_config[\"bench_softmax_shape_b_c\"] = op_shape\n        elif optype == \"gather\":\n            dataflow_config[\"bench_gather_shape\"] = op_shape\n            self.config.gather_mu = mu\n        elif optype == \"scatter\":\n            dataflow_config[\"bench_scatter_shape\"] = op_shape\n        elif optype == \"transpose\":\n            dataflow_config[\"bench_transpose_tensor_b_h_w_c\"] = op_shape[0]\n            dataflow_config[\"bench_transpose_dim_map\"] = op_shape[1]\n        elif optype == \"sdpa\":\n            dataflow_config[\"bench_sdpa_q_shape\"] = op_shape[0]\n            dataflow_config[\"bench_sdpa_kv_shape\"] = op_shape[1]\n            if len(op_shape) > 2:\n                assert isinstance(op_shape[2], (bool, int))\n                dataflow_config[\"bench_sdpa_with_dropout\"] = 1 if op_shape[2] else 0\n        elif optype == \"allreduce\":\n            dataflow_config[\"bench_all_reduce_shape\"] = op_shape\n        elif optype == \"allgather\":\n            dataflow_config[\"bench_allgather_in_shape\"] = op_shape[1]\n            dataflow_config[\"bench_allgather_out_shape\"] = op_shape[0]\n        elif optype == \"allgather_gemm\":\n            dataflow_config[\"bench_allgather_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm.shared\":\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm.local\":\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        else:\n            raise Exception(\"check if introducing new optype, %s\", optype)\n\n        addr_input_list, addr_output_list = self.input_addr, self.output_addr\n        if addr_input_list or addr_output_list:\n            dataflow_config[\"bench_input_tensor_addr\"] = addr_input_list\n            dataflow_config[\"bench_output_tensor_addr\"] = addr_output_list\n        else:\n            print(\"-------------------------------------------\")\n            logger.info(\"No input/output addresses provided for this case\")\n\n        dataflow_config = {**dataflow_config, **self.dataflow_config}\n        bench_op_type = dataflow_config[\"bench_op_type\"]\n        backend = get_backend(self.config)\n        python_supported = backend.has_generator(bench_op_type)\n\n        if python_supported:\n            return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n\n        repo_root = Path(__file__).resolve().parents[2]\n        dataflow_binary = repo_root / \"build\" / \"dataflow_gen\"\n\n        if not dataflow_binary.exists():\n            if python_supported:\n                logger.warning(\n                    \"dataflow_gen binary not found at %s; falling back to in-process generator for bench_op_type=%s\",\n                    dataflow_binary, bench_op_type\n                )\n                return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n            raise FileNotFoundError(\n                f\"Required dataflow generator binary not found at {dataflow_binary}. \"\n                f\"Bench op type '{bench_op_type}' has no built-in fallback.\"\n            )\n        dataflow_path = dataflow_config[\"bench_out_dir\"]\n        cmd = get_data_flow_gen_cmd(dataflow_config, self.config)\n        process = subprocess.run(cmd.encode(\n            \"utf-8\"), shell=True, encoding=\"utf-8\", capture_output=True)\n        ret_code = process.returncode\n        logger.info(\"\\n\" + process.stdout)\n        if ret_code != 0:\n            logger.error(process.stderr)\n            if python_supported:\n                logger.warning(\n                    \"External dataflow generation failed; retrying with in-process generator for bench_op_type=%s\",\n                    bench_op_type\n                )\n                return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n            raise Exception(\n                f\"Failed to generate dataflow: {process.stderr}\")\n\n        df_files = glob(f\"{dataflow_path}/*dataflow.yaml\")\n        assert len(df_files) > 0, \"Can't find generated dataflow yaml file!\"\n        logger.info(f\"Found dataflow files: {df_files}\")  # print the files\n        assert len(\n            df_files) == 1, \"Found more than one dataflow yaml which is not supported now!\"\n\n        p = Path(df_files[0])\n        if not p.exists():\n            raise Exception(f\"{df_files[0]} not exists!\")\n        with open(p) as f:\n            data_dict = yaml.safe_load(f)\n\n        dataflow = dict_to_dataclass(\n            data_dict,\n            DiagDataflow,\n            {\n                DiagDataflowAction: functools.partial(\n                    customize_loader,\n                    config=self.config,\n                    topo=self.esl_switch.topo,\n                    dataflow_config=dataflow_config,\n                    case_id=self.case_idx\n                )\n            },\n            restrict_mode=False,\n        )\n        if dataflow_config[\"bench_op_type\"] in [\n            \"transpose\",\n            \"sdpa\",\n            \"allreduce\",\n            \"allgather\",\n            \"allgather_gemm\",\n        ] or dataflow_config.get(\"bench_gemm_op_version\", 1) in [4, 5]:\n            return dataflow\n        shape_files = glob(f\"{dataflow_path}/*shape_list.txt\")\n        assert len(\n            shape_files) > 0, \"Can't find generated grid_shape_list file!\"\n        assert len(\n            shape_files) == 1, \"Found more than one grid_shape_list file which is not supported now!\"\n        tile_info = parse_shape_tile_info(shape_files[0])\n        for act in dataflow.action_list:\n            act.tile_info = tile_info\n        return dataflow\n\n    def generate_report(self):\n        report = self.post_processor.generate_report(\n            self.context, self.dataflow, self.config, self.get_service_list())\n        # self.post_processor.tgen.flush()\n        # self.tgen.block_until_all_tasks_done()\n        return report",
    "start_line": 205,
    "end_line": 629,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BossaNovaExecutor",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.BossaNovaExecutor"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.EmptyPowerSvc": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.EmptyPowerSvc",
    "name": "EmptyPowerSvc",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "        class EmptyPowerSvc:\n            def process(self, action, context, ref):\n                while True:\n                    stat: DataflowActionMemoryStat = yield\n                    if not stat:\n                        break\n                yield\n\n            def get_edc_freq(self, ref, context):\n                base_freq_cfg = context.bw_resource_context.config.freq\n                return base_freq_cfg\n\n            def init_context(self):\n                pass\n\n            def post_process(self, context: BossaNovaContext):\n                pass\n\n            def post_stat(self, context: BossaNovaContext, dataflow: Dataflow):\n                return {}",
    "start_line": 272,
    "end_line": 291,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class EmptyPowerSvc",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.EmptyPowerSvc"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.get_dump_addr": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.get_dump_addr",
    "name": "get_dump_addr",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "        def get_dump_addr():\n            self.f = open(f\"{self.outdir}/addr_dump.txt\", \"w\")\n            self.f.write(\n                \"start,end,master,die_id,cluster_id,engine_id,address,size,rw\\n\")\n\n            def dump_addr(action: DiagDataflowAction, stat, ref):\n                master = action.get_action_type()\n                die_id = action.get_die_id()\n                cluster_id = action.get_cluster_id()\n                engine_id = action.get_local_engine_id()\n                for access in stat.memory_access_list:\n                    if access.base_addr >= 2**40*5:\n                        s = ref+stat.relative_ts\n                        e = ref+stat.relative_ts+stat.latency\n                        self.f.write(\n                            f\"{s:0.9f},{e:0.9f},{master}, {die_id}, {cluster_id}, {engine_id}, {hex(access.base_addr)}, {hex(access.size)}, {access.rw}\\n\")\n            return dump_addr",
    "start_line": 296,
    "end_line": 312,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_dump_addr",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.get_dump_addr"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.dump_addr": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.dump_addr",
    "name": "dump_addr",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "            def dump_addr(action: DiagDataflowAction, stat, ref):\n                master = action.get_action_type()\n                die_id = action.get_die_id()\n                cluster_id = action.get_cluster_id()\n                engine_id = action.get_local_engine_id()\n                for access in stat.memory_access_list:\n                    if access.base_addr >= 2**40*5:\n                        s = ref+stat.relative_ts\n                        e = ref+stat.relative_ts+stat.latency\n                        self.f.write(\n                            f\"{s:0.9f},{e:0.9f},{master}, {die_id}, {cluster_id}, {engine_id}, {hex(access.base_addr)}, {hex(access.size)}, {access.rw}\\n\")",
    "start_line": 301,
    "end_line": 311,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "action",
      "stat",
      "ref"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dump_addr",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.dump_addr"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.get_service_list": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.get_service_list",
    "name": "get_service_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "    def get_service_list(self) -> Generator[BaseCostService, None, None]:\n        if self.cache_svc:\n            yield self.cache_svc\n        yield self.compute_svc\n        if self.power_svc:\n            yield self.power_svc",
    "start_line": 338,
    "end_line": 343,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_service_list",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.get_service_list"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.get_ref": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.get_ref",
    "name": "get_ref",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_executor.get_ref"
    ],
    "source_code": "    def get_ref(self, dataflow: Dataflow, action: DataflowAction, visited):\n        '''\n        caculate the start time at the last action in a dataflow\n        '''\n        dag = dataflow.dag\n        if action.get_action_id() in visited:\n            return visited[action.get_action_id()]\n        ref = 0\n        for parent_id in dag.predecessors(action.get_action_id()):\n            parent = dataflow._action_map[parent_id]\n            latency = self.get_ref(dataflow, parent, visited)\n            latency += self.context.cost_dict[parent_id].latency\n            if latency > ref:\n                ref = latency\n        visited[action.get_action_id()] = ref\n        return ref",
    "start_line": 345,
    "end_line": 360,
    "has_docstring": true,
    "docstring": "caculate the start time at the last action in a dataflow",
    "parameters": [
      "self",
      "dataflow",
      "action",
      "visited"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_ref",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.get_ref"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor._process_action": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor._process_action",
    "name": "_process_action",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.cost_service.power.power_cost_service.duration",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process"
    ],
    "source_code": "    def _process_action(self, action: DataflowAction, ref):\n        cid = action.get_cluster_id()\n        engine_id = action.get_local_engine_id()\n        action_type = action.get_action_type()\n        trace_label = getattr(action, \"get_trace_label\", lambda: action_type.name.lower())()\n        action.ref = ref\n        track = self.context.get_cluster_tgen(action.get_die_id(), cid).create_track(\n            f\"{trace_label}:{engine_id}\", tid=engine_id)\n        yield from self.compute_svc.process(action, self.context, ref, trace_label=trace_label)\n        # yield from self.power_svc.process(action, self.context, ref)\n        self.compute_svc.post_process(self.context)\n        if self.power_svc:\n            self.power_svc.post_process(self.context)\n\n        # core_stat = action.get_core_stat()\n\n        cost_book = self.context.get_cost_book(action)\n\n        track.duration(ref, cost_book.latency, action_type, cost_book)\n        return cost_book.latency",
    "start_line": 362,
    "end_line": 381,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "action",
      "ref"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _process_action",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor._process_action"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.execute": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.execute",
    "name": "execute",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_executor.tqdm"
    ],
    "source_code": "    def execute(self) -> Generator[float, None, None]:\n        visited = {}\n        action_gen = self.dataflow.execute_dataflow()\n        action_id, action = next(action_gen)\n        action_map: Dict[int, Generator] = {}\n\n        initial_ref = self.context.initial_ref\n        max_ref: float = 0\n        action_len = len(self.dataflow._action_map)\n\n        worker_id = os.getenv(\"PYTEST_XDIST_WORKER\", \"master\")\n\n        position = int(worker_id.replace(\"gw\", \"\")\n                       ) if worker_id != \"master\" else 0  # 根据 worker ID 计算位置\n\n        pbar = tqdm(total=action_len,\n                    desc=f\"Worker {worker_id}\", position=position, leave=True, unit=\"act\")\n        next_ref = 0\n        event_barrier_map: Dict[int, BARRIER | BossaNovaEvent] = {}\n\n        def _check_barrier():\n            for _action_id in list(event_barrier_map.keys()):\n                _event_or_barrier = event_barrier_map[_action_id]\n                if _event_or_barrier.is_done:\n                    self.dataflow.queue.put(\n                        (_event_or_barrier.max_t, _action_id))\n                    event_barrier_map.pop(_action_id)\n\n        while True:\n            try:\n                if action:\n                    local_latency = self.get_ref(\n                        self.dataflow, action, visited)\n                    ref = local_latency+initial_ref\n                    is_done = False\n                    stat = None\n                    if id(action) in action_map:\n                        action_process = action_map[id(action)]\n                        try:\n                            next_ref, stat = next(action_process)\n                            if isinstance(stat, BARRIER) or isinstance(stat, BossaNovaEvent):\n                                event_barrier_map[action_id] = stat\n                        except StopIteration as e:\n                            next_ref = ref + e.value\n                            action_map.pop(id(action))\n                            pbar.update(1)\n                            is_done = True\n                        finally:\n                            yield next_ref, 'running'\n                    else:\n                        action_process = self._process_action(action, ref)\n                        assert isinstance(\n                            action_process, GeneratorType), \"process must be an generator\"\n                        action_map[id(action)] = action_process\n                        next_ref = ref\n                        # continue\n\n                    action_latency = self.context.cost_dict[action.get_action_id(\n                    )].latency\n                    max_ref = max(max_ref, ref + action_latency)\n\n                    _check_barrier()\n                    # send back current action status and get the next\n                    action_id, action = action_gen.send(\n                        (next_ref, is_done, stat))\n                    pass\n                elif len(event_barrier_map) > 0:\n                    # print(\"!!!!!event_barrier_map\")\n                    next_ref = MAX_TIME\n                    for b in event_barrier_map.values():\n                        next_ref = min(next_ref, b.max_t)\n                    # wait for barrier\n                    yield next_ref, 'wait'\n                    _check_barrier()\n                    action_id, action = action_gen.send(\n                        (next_ref, False, None))\n\n                else:\n                    action_id, action = action_gen.send(\n                        (None, None, None))\n                    # 当action和barrier_map 均无待处理工作时表示完成\n                    break\n\n            except StopIteration:\n                break\n        self.end_ref = max_ref\n        pbar.close()",
    "start_line": 383,
    "end_line": 469,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function execute",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.execute"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor._check_barrier": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor._check_barrier",
    "name": "_check_barrier",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "        def _check_barrier():\n            for _action_id in list(event_barrier_map.keys()):\n                _event_or_barrier = event_barrier_map[_action_id]\n                if _event_or_barrier.is_done:\n                    self.dataflow.queue.put(\n                        (_event_or_barrier.max_t, _action_id))\n                    event_barrier_map.pop(_action_id)",
    "start_line": 403,
    "end_line": 409,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _check_barrier",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor._check_barrier"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.generate_dataflow": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.generate_dataflow",
    "name": "generate_dataflow",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "    def generate_dataflow(self, op_shape, optype, dtype, mu) -> Dataflow:\n        dataflow_config = {\n            \"bench_out_dir\": self.outdir,\n            \"bench_op_type\": optype,\n            \"bench_basic_data_type\": dtype.name.lower(),\n        }\n\n        def _resolve_gemm_op_type(version: int) -> str:\n            if version in (4, 6):\n                return \"gemm.local\"\n            if version == 5:\n                return \"gemm.shared\"\n            return \"gemm\"\n\n        if optype == 'linear':\n            op_shape = [1, op_shape[0] * op_shape[1],\n                        op_shape[2], op_shape[3]]\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == 'matmul':\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm\":\n            op_version = self.dataflow_config.get(\"bench_gemm_op_version\", 2)\n            dataflow_config[\"bench_op_type\"] = _resolve_gemm_op_type(op_version)\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype in [\"add\", \"mul\", \"gelu\", \"relu\", \"silu\", \"sigmoid\"]:\n            dataflow_config[\"bench_elementwise_shape\"] = op_shape\n        elif optype == \"layernorm\":\n            dataflow_config[\"bench_layernorm_shape\"] = op_shape\n        elif optype in [\"softmax\", \"softmaxbackward\"]:\n            dataflow_config[\"bench_softmax_shape_b_c\"] = op_shape\n        elif optype == \"gather\":\n            dataflow_config[\"bench_gather_shape\"] = op_shape\n            self.config.gather_mu = mu\n        elif optype == \"scatter\":\n            dataflow_config[\"bench_scatter_shape\"] = op_shape\n        elif optype == \"transpose\":\n            dataflow_config[\"bench_transpose_tensor_b_h_w_c\"] = op_shape[0]\n            dataflow_config[\"bench_transpose_dim_map\"] = op_shape[1]\n        elif optype == \"sdpa\":\n            dataflow_config[\"bench_sdpa_q_shape\"] = op_shape[0]\n            dataflow_config[\"bench_sdpa_kv_shape\"] = op_shape[1]\n            if len(op_shape) > 2:\n                assert isinstance(op_shape[2], (bool, int))\n                dataflow_config[\"bench_sdpa_with_dropout\"] = 1 if op_shape[2] else 0\n        elif optype == \"allreduce\":\n            dataflow_config[\"bench_all_reduce_shape\"] = op_shape\n        elif optype == \"allgather\":\n            dataflow_config[\"bench_allgather_in_shape\"] = op_shape[1]\n            dataflow_config[\"bench_allgather_out_shape\"] = op_shape[0]\n        elif optype == \"allgather_gemm\":\n            dataflow_config[\"bench_allgather_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm.shared\":\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        elif optype == \"gemm.local\":\n            dataflow_config[\"bench_gemm_shape_b_m_k_n\"] = op_shape\n        else:\n            raise Exception(\"check if introducing new optype, %s\", optype)\n\n        addr_input_list, addr_output_list = self.input_addr, self.output_addr\n        if addr_input_list or addr_output_list:\n            dataflow_config[\"bench_input_tensor_addr\"] = addr_input_list\n            dataflow_config[\"bench_output_tensor_addr\"] = addr_output_list\n        else:\n            print(\"-------------------------------------------\")\n            logger.info(\"No input/output addresses provided for this case\")\n\n        dataflow_config = {**dataflow_config, **self.dataflow_config}\n        bench_op_type = dataflow_config[\"bench_op_type\"]\n        backend = get_backend(self.config)\n        python_supported = backend.has_generator(bench_op_type)\n\n        if python_supported:\n            return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n\n        repo_root = Path(__file__).resolve().parents[2]\n        dataflow_binary = repo_root / \"build\" / \"dataflow_gen\"\n\n        if not dataflow_binary.exists():\n            if python_supported:\n                logger.warning(\n                    \"dataflow_gen binary not found at %s; falling back to in-process generator for bench_op_type=%s\",\n                    dataflow_binary, bench_op_type\n                )\n                return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n            raise FileNotFoundError(\n                f\"Required dataflow generator binary not found at {dataflow_binary}. \"\n                f\"Bench op type '{bench_op_type}' has no built-in fallback.\"\n            )\n        dataflow_path = dataflow_config[\"bench_out_dir\"]\n        cmd = get_data_flow_gen_cmd(dataflow_config, self.config)\n        process = subprocess.run(cmd.encode(\n            \"utf-8\"), shell=True, encoding=\"utf-8\", capture_output=True)\n        ret_code = process.returncode\n        logger.info(\"\\n\" + process.stdout)\n        if ret_code != 0:\n            logger.error(process.stderr)\n            if python_supported:\n                logger.warning(\n                    \"External dataflow generation failed; retrying with in-process generator for bench_op_type=%s\",\n                    bench_op_type\n                )\n                return generate_dataflow(self.config, dataflow_config, self.esl_switch.topo, self.case_idx)\n            raise Exception(\n                f\"Failed to generate dataflow: {process.stderr}\")\n\n        df_files = glob(f\"{dataflow_path}/*dataflow.yaml\")\n        assert len(df_files) > 0, \"Can't find generated dataflow yaml file!\"\n        logger.info(f\"Found dataflow files: {df_files}\")  # print the files\n        assert len(\n            df_files) == 1, \"Found more than one dataflow yaml which is not supported now!\"\n\n        p = Path(df_files[0])\n        if not p.exists():\n            raise Exception(f\"{df_files[0]} not exists!\")\n        with open(p) as f:\n            data_dict = yaml.safe_load(f)\n\n        dataflow = dict_to_dataclass(\n            data_dict,\n            DiagDataflow,\n            {\n                DiagDataflowAction: functools.partial(\n                    customize_loader,\n                    config=self.config,\n                    topo=self.esl_switch.topo,\n                    dataflow_config=dataflow_config,\n                    case_id=self.case_idx\n                )\n            },\n            restrict_mode=False,\n        )\n        if dataflow_config[\"bench_op_type\"] in [\n            \"transpose\",\n            \"sdpa\",\n            \"allreduce\",\n            \"allgather\",\n            \"allgather_gemm\",\n        ] or dataflow_config.get(\"bench_gemm_op_version\", 1) in [4, 5]:\n            return dataflow\n        shape_files = glob(f\"{dataflow_path}/*shape_list.txt\")\n        assert len(\n            shape_files) > 0, \"Can't find generated grid_shape_list file!\"\n        assert len(\n            shape_files) == 1, \"Found more than one grid_shape_list file which is not supported now!\"\n        tile_info = parse_shape_tile_info(shape_files[0])\n        for act in dataflow.action_list:\n            act.tile_info = tile_info\n        return dataflow",
    "start_line": 471,
    "end_line": 622,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self",
      "op_shape",
      "optype",
      "dtype",
      "mu"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function generate_dataflow",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.generate_dataflow"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor._resolve_gemm_op_type": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor._resolve_gemm_op_type",
    "name": "_resolve_gemm_op_type",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [],
    "source_code": "        def _resolve_gemm_op_type(version: int) -> str:\n            if version in (4, 6):\n                return \"gemm.local\"\n            if version == 5:\n                return \"gemm.shared\"\n            return \"gemm\"",
    "start_line": 478,
    "end_line": 483,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "version"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _resolve_gemm_op_type",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor._resolve_gemm_op_type"
  },
  "nova-platform.nova_platform.executor.nova_platform_executor.generate_report": {
    "id": "nova-platform.nova_platform.executor.nova_platform_executor.generate_report",
    "name": "generate_report",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_executor.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_executor.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_executor.generate_report",
      "nova-platform.nova_platform.executor.nova_platform_executor.get_service_list"
    ],
    "source_code": "    def generate_report(self):\n        report = self.post_processor.generate_report(\n            self.context, self.dataflow, self.config, self.get_service_list())\n        # self.post_processor.tgen.flush()\n        # self.tgen.block_until_all_tasks_done()\n        return report",
    "start_line": 624,
    "end_line": 629,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "self"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function generate_report",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_executor.generate_report"
  },
  "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch": {
    "id": "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch",
    "name": "DefaultESLSwitch",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_switch.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_switch.py",
    "depends_on": [
      "nova-platform.nova_platform.base_model.DataflowActionMemoryStat",
      "nova-platform.nova_platform.cost_service.compute.compute_cost_service.process_memory_stat"
    ],
    "source_code": "class DefaultESLSwitch(BaseESLSwitch):\n    def __init__(self, config: BossaNovaConfig, topo):\n        super().__init__(config, topo)\n\n    def calculate_esl_esl_latency():\n        raise NotImplementedError\n\n    def add_gcu(self, gcu_id, executor):\n        self.gcu_map[gcu_id] = executor\n\n    def send(self, ref, src_gcu_id, tar_gcu_id, rw, data_size, memory_list=None):\n        tar_executor = self.gcu_map[tar_gcu_id]\n        tar_compute_svc = tar_executor.compute_svc\n        tar_cache_svc = tar_executor.compute_svc.cache_svc\n\n        context = tar_executor.context\n        sw_lat = self.calculate_esl_esl_latency(src_gcu_id, tar_gcu_id)\n        edc_freq_cfg = tar_executor.power_svc.get_edc_freq(\n            ref+sw_lat, context) if tar_executor.power_svc else None\n        mem_stat = DataflowActionMemoryStat(\n            total_count=data_size,\n            master=DataflowActionType.ESL,\n            src=AddrDomain.ESL,\n            dst=AddrDomain.L3,\n            rw=rw,\n            relative_ts=sw_lat,\n            src_gcu_id=src_gcu_id,\n            tar_gcu_id=tar_gcu_id,\n            memory_access_list=memory_list,\n            name=\"esl_remote\",\n        )\n\n        def get_cache_gen(action, context, ref):\n            while True:\n                mem_stat = yield\n                if not mem_stat:\n                    break\n                if hasattr(tar_cache_svc, 'get_cache_stat_dict'):\n                    memory_access_list: List[DataflowActionMemoryAccess] = mem_stat.memory_access_list\n                    start_stat = tar_cache_svc.get_cache_stat_dict()\n                    tar_cache_svc._process_access(action, memory_access_list)\n                    end_stat = tar_cache_svc.get_cache_stat_dict()\n                    cache_stat = {}\n                    for k, v in end_stat.items():\n                        cache_stat[k] = v-start_stat[k]\n                else:\n                    cache_stat = {}\n\n                mem_stat.cache_stat = cache_stat\n            pass\n\n        cache_gen = get_cache_gen(None, context, ref)\n        next(cache_gen)\n        die_id = tar_gcu_id // self.config.inst_num.NUM_OF_ESL_PER_DIE\n        latency_stat_list = yield from tar_compute_svc.process_memory_stat(\n            mem_stat, context, edc_freq_cfg, cache_gen, die_id, None, None, f'esl gcu {src_gcu_id:02d}->{tar_gcu_id:02d}', ref)\n\n        return mem_stat.latency+sw_lat, {\n            \"sw_latency\": sw_lat,\n            \"mem_stat_latency\": mem_stat.latency,\n            \"mem_stat_leading\": mem_stat.leading_latency,\n            \"stat_list\": latency_stat_list\n        }",
    "start_line": 10,
    "end_line": 72,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseESLSwitch"
    ],
    "class_name": null,
    "display_name": "class DefaultESLSwitch",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch"
  },
  "nova-platform.nova_platform.executor.nova_platform_switch.FullmeshESLSwitch": {
    "id": "nova-platform.nova_platform.executor.nova_platform_switch.FullmeshESLSwitch",
    "name": "FullmeshESLSwitch",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_switch.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_switch.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource"
    ],
    "source_code": "class FullmeshESLSwitch(DefaultESLSwitch):\n    def __init__(self, config: BossaNovaConfig):\n        super().__init__(config, TOPO.FULLMESH8)\n        self.logic_port_num = TOPO.FULLMESH8.value[1]\n        self.logic_port_per_die_num = self.logic_port_num//self.config.inst_num.NUM_OF_DIE\n        # src gcu_id, port_id => tar gcu_id,port_id\n        # self.port_map[(0, n)] = (n, 0)\n        self.resource_map = {}\n        for gcu_id in range(8):\n            for port_id in range(8):\n                if gcu_id == port_id:\n                    continue\n                die_id, local_port_id = self.__get_id(port_id)\n                self.resource_map[(gcu_id, die_id, local_port_id, 'tx')] = BWResource(f\"esl tx gcu {gcu_id} die_id {die_id} port {local_port_id}\", BWMode.BANDWIDTH,\n                                                                                      self.config.bw.esl.noc.bw_per_second*self.config.freq.ESL/1e9)\n                self.resource_map[(gcu_id, die_id, local_port_id, 'rx')] = BWResource(f\"esl rx gcu {gcu_id} die_id {die_id} port {local_port_id}\", BWMode.BANDWIDTH,\n                                                                                      self.config.bw.esl.noc.bw_per_second*self.config.freq.ESL/1e9)\n\n    def calculate_esl_esl_latency(self, src_gcu_id, dst_gcu_id):\n        # no sw\n        return 0.7e-6\n\n    def __get_id(self, tar_gcu_id):\n        die_id = tar_gcu_id//self.logic_port_per_die_num\n        port_id = tar_gcu_id % self. logic_port_per_die_num\n        return (die_id, port_id)\n\n    def build_bw_resource(self, bw_res_context: BWResourceContext):\n        for port_id in range(self.logic_port_num):\n            if port_id == bw_res_context.config.gcu_id:\n                continue\n            die_id, local_port_id = self.__get_id(port_id)\n            bw_res_context.esl_dict[(die_id, port_id, 'tx')] = self.resource_map[(\n                bw_res_context.config.gcu_id, die_id, local_port_id, 'tx')]\n            bw_res_context.esl_dict[(die_id, port_id, 'rx')] = self.resource_map[(\n                bw_res_context.config.gcu_id, die_id, local_port_id, 'rx')]\n\n    def get_bw_resource(self, local_gpu_id: int, src_gcu_id: int, tar_gcu_id: int, rw):\n        if local_gpu_id == src_gcu_id:\n            die_id, local_port_id = self.__get_id(tar_gcu_id)\n            key = (local_gpu_id, die_id, local_port_id,\n                   'tx' if rw == 'r' else 'rx')\n        elif local_gpu_id == tar_gcu_id:\n            die_id, local_port_id = self.__get_id(src_gcu_id)\n            key = (local_gpu_id, die_id, local_port_id,\n                   'tx' if rw == 'r' else 'rx')\n        return self.resource_map[key]\n\n    def get_unique_bw_resource(self, context: BWResourceContext):\n        for i in range(self.logic_port_num):\n            die_id, port_id = self.__get_id(i)\n            if (die_id, port_id, 'tx') in context.esl_dict:\n                yield (f\"esl_tx\", \"r\", die_id, None, port_id, context.esl_dict[die_id, port_id, 'tx'])\n            if (die_id, port_id, 'rx') in context.esl_dict:\n                yield (f\"esl_rx\", \"w\", die_id, None, port_id, context.esl_dict[die_id, port_id, 'rx'])",
    "start_line": 75,
    "end_line": 129,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DefaultESLSwitch"
    ],
    "class_name": null,
    "display_name": "class FullmeshESLSwitch",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_switch.FullmeshESLSwitch"
  },
  "nova-platform.nova_platform.executor.nova_platform_switch.SupernodeESLSwitch": {
    "id": "nova-platform.nova_platform.executor.nova_platform_switch.SupernodeESLSwitch",
    "name": "SupernodeESLSwitch",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_switch.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_switch.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch",
      "nova-platform.nova_platform.cost_service.compute.base_compute_model.BWResource"
    ],
    "source_code": "class SupernodeESLSwitch(DefaultESLSwitch):\n    node_num_per_rank = 16\n\n    def __init__(self, config: BossaNovaConfig, topo):\n        super().__init__(config, topo)\n        self.node_num = topo.value[1]\n        self.resource_map = {}\n        node_port_num = self.config.inst_num.NUM_OF_ESL_PER_DIE * \\\n            self.config.inst_num.NUM_OF_DIE\n        # 25GB/s * 16ports = 400GB/s\n        node_bw = self.config.bw.esl.noc.bw_per_second * \\\n            self.config.freq.ESL*node_port_num/1e9\n        self.rank_num = self.node_num//self.node_num_per_rank\n\n        for gcu_id in range(self.node_num):\n            key = (gcu_id, 'tx')\n            tx_bw = BWResource(f\"esl tx supernode\", BWMode.BANDWIDTH, node_bw)\n            self.resource_map[key] = tx_bw\n            key = (gcu_id, 'rx')\n            rx_bw = BWResource(f\"esl rx supernode\", BWMode.BANDWIDTH, node_bw)\n            self.resource_map[key] = rx_bw\n\n    def get_parted_id(self, global_id):\n        rank_id = global_id // self.node_num_per_rank\n        local_id = global_id % self.node_num_per_rank\n        return (rank_id, local_id)\n\n    def calculate_esl_esl_latency(self, src_global_id, tar_global_id):\n        src_rank_id, _ = self.get_parted_id(src_global_id)\n        tar_rank_id, _ = self.get_parted_id(tar_global_id)\n        if src_rank_id == tar_rank_id:\n            return 5e-6\n        else:\n            return 5e-6*2\n\n    def build_bw_resource(self, bw_res_context: BWResourceContext):\n        global_gcu_id = bw_res_context.config.gcu_id\n        bw_res_context.esl_dict[('tx')] = self.resource_map.get(\n            global_gcu_id, 'tx')\n        bw_res_context.esl_dict[('rx')] = self.resource_map.get(\n            global_gcu_id, 'rx')\n\n    def get_bw_resource(self, local_gpu_id: int, src_gcu_id: int, tar_gcu_id: int, rw):\n        if local_gpu_id == src_gcu_id:\n            key = (local_gpu_id, 'tx' if rw == 'r' else 'rx')\n        elif local_gpu_id == tar_gcu_id:\n            key = (tar_gcu_id, 'tx' if rw == 'r' else 'rx')\n        return self.resource_map[key]\n\n    def get_unique_bw_resource(self, context: BWResourceContext):\n        global_gcu_id = context.config.gcu_id\n        tx = self.resource_map.get((global_gcu_id, 'tx'))\n        rx = self.resource_map.get((global_gcu_id, 'rx'))\n        yield (f\"esl_tx\", \"r\", None, None, 0, tx)\n        yield (f\"esl_rx\", \"w\", None, None, 0, rx)",
    "start_line": 132,
    "end_line": 186,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DefaultESLSwitch"
    ],
    "class_name": null,
    "display_name": "class SupernodeESLSwitch",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_switch.SupernodeESLSwitch"
  },
  "nova-platform.nova_platform.executor.nova_platform_switch.StandaloneSwitch": {
    "id": "nova-platform.nova_platform.executor.nova_platform_switch.StandaloneSwitch",
    "name": "StandaloneSwitch",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_switch.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_switch.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_switch.DefaultESLSwitch"
    ],
    "source_code": "class StandaloneSwitch(DefaultESLSwitch):\n    def __init__(self, config: BossaNovaConfig):\n        super().__init__(config, TOPO.STANDALONE)\n\n    def build_bw_resource(self, context):\n        pass\n\n    def get_unique_bw_resource(self, context: BWResourceContext):\n        if False:\n            yield\n        return",
    "start_line": 189,
    "end_line": 199,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "DefaultESLSwitch"
    ],
    "class_name": null,
    "display_name": "class StandaloneSwitch",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_switch.StandaloneSwitch"
  },
  "nova-platform.nova_platform.executor.nova_platform_switch.ESLSwitchManager": {
    "id": "nova-platform.nova_platform.executor.nova_platform_switch.ESLSwitchManager",
    "name": "ESLSwitchManager",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/executor/nova_platform_switch.py",
    "relative_path": "nova-platform/nova_platform/executor/nova_platform_switch.py",
    "depends_on": [
      "nova-platform.nova_platform.executor.nova_platform_switch.StandaloneSwitch",
      "nova-platform.nova_platform.executor.nova_platform_switch.SupernodeESLSwitch",
      "nova-platform.nova_platform.executor.nova_platform_switch.FullmeshESLSwitch"
    ],
    "source_code": "class ESLSwitchManager():\n    @classmethod\n    def build_switch(cls, config: BossaNovaConfig, topo: TOPO) -> BaseESLSwitch:\n        switch_map = {\n            TOPO.STANDALONE: lambda: StandaloneSwitch(config),\n            TOPO.FULLMESH8: lambda: FullmeshESLSwitch(config),\n            TOPO.SUPERNODE4: lambda: SupernodeESLSwitch(config, TOPO.SUPERNODE4),\n            TOPO.SUPERNODE8: lambda: SupernodeESLSwitch(config, TOPO.SUPERNODE8),\n            TOPO.SUPERNODE16: lambda: SupernodeESLSwitch(config, TOPO.SUPERNODE16),\n            TOPO.SUPERNODE32: lambda: SupernodeESLSwitch(config, TOPO.SUPERNODE32),\n        }\n        if topo not in switch_map:\n            raise NotImplementedError\n        fun = switch_map[topo]\n        return fun()",
    "start_line": 202,
    "end_line": 216,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ESLSwitchManager",
    "component_id": "nova-platform.nova_platform.executor.nova_platform_switch.ESLSwitchManager"
  },
  "nova-platform.nova_platform.perfetto_protobuf._core._BaseTraceGenerator": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._core._BaseTraceGenerator",
    "name": "_BaseTraceGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_core.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_core.py",
    "depends_on": [],
    "source_code": "class _BaseTraceGenerator:\n    def __init__(self):\n        \"\"\" Create a trace \"\"\"\n        self.__uuid__ = 1234567\n        self.interned_data = {}\n        self.interned_source = {}\n        self.flush_threshold = 10000\n        self.list_max_size = 256\n\n        # self.trace = pb2.Trace()\n        # self.file = open(filename, \"wb\")\n\n        # self.flush(pkts)\n\n    def _header_packet(self):\n        pkts = []\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n        pkts.append(pkt)\n        pkt.trusted_packet_sequence_id = 1\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 1\n        clk.timestamp = 0\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 2\n        clk.timestamp = 0\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 3\n        clk.timestamp = 0\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 4\n        clk.timestamp = 0\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 5\n        clk.timestamp = 0\n        clk = pkt.clock_snapshot.clocks.add()\n        clk.clock_id = 6\n        clk.timestamp = 0\n        pkt.clock_snapshot.primary_trace_clock = pb2.BUILTIN_CLOCK_BOOTTIME\n\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n        pkts.append(pkt)\n        pkt.trusted_packet_sequence_id = 1\n        pkt.trace_config.buffers.add().size_kb = 1024\n        pkt.trace_config.data_sources.add().config.name = \"track_event\"\n\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n        pkts.append(pkt)\n        pkt.trusted_packet_sequence_id = 2\n        pkt.trace_packet_defaults.track_event_defaults.track_uuid = 1\n        pkt.trace_packet_defaults.timestamp_clock_id = 1\n        pkt.sequence_flags = 1\n\n        return pkts\n\n    # def flush(self, pkts: List):\n    #     \"\"\" Flush trace.  This creates a perfetto trace packet and writes to disk. \"\"\"\n    #     trace = pb2.Trace()\n    #     trace.packet.extend(pkts)\n    #     s = trace.SerializeToString()\n    #     self.lock.acquire()\n    #     self.file.write(s)\n    #     # self.file.write(self.trace.SerializeToString())\n    #     # self.file.flush()\n    #     self.lock.release()\n\n    #     # self.trace = pb2.Trace()\n\n    # def __del__(self):\n    #     # self.flush()\n    #     self.file.flush()\n    #     self.file.close()\n\n    def _pid_packet(self, uuid, pid, process_name: str, track_name: str = None):\n        \"\"\" Create a group.  Each \"group\" comes with a default normal track (named track_name).\"\"\"\n        # uuid = self.__uuid__\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n        pkt.timestamp = 0\n        pkt.track_descriptor.uuid = uuid\n        pkt.track_descriptor.process.pid = pid\n        pkt.track_descriptor.process.process_name = process_name\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        if track_name is None:\n            pkt.track_descriptor.name = process_name\n        else:\n            pkt.track_descriptor.name = track_name\n\n        # self.__uuid__ += 1\n\n        # funnily enough, declaring a process and a track at the same time will get rid of the default track\n        # if there is no trace in the track.  Unfortunately this changes the process track's name to \"Process XXX\"\n        # so it shouldn't be applicable.\n        # Instead, the only thing we can do is to assume a group to also accompany a track.\n\n        # tid = self.__pid__\n        # pkt.track_descriptor.thread.pid = pid\n        # pkt.track_descriptor.thread.tid = tid\n        # pkt.track_descriptor.thread.thread_name = process_name\n        # self.__pid__ += 1\n\n        # self._flush_if_necessary()\n        # self.flush([pkt])\n\n        # return uuid\n        return pkt\n\n    def _make_trace(self, pkts: List):\n        trace = pb2.Trace()\n        trace.packet.extend(pkts)\n        return trace\n\n    # def _flush_if_necessary(self):\n    #     if len(self.trace.packet) > self.flush_threshold:\n    #         self.flush()\n\n    def _tid_packet(self, uuid, tid, parent_uuid, track_name, ttype):\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n\n        pkt.timestamp = 0\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        # uuid = self.__uuid__\n\n        pkt.track_descriptor.uuid = uuid\n        # self.__uuid__ += 1\n        pkt.track_descriptor.name = track_name\n\n        if parent_uuid != 0:\n            pkt.track_descriptor.parent_uuid = parent_uuid\n\n        if ttype == 1:\n            pkt.track_descriptor.counter.categories.append(\"dummy\")\n        # self._flush_if_necessary()\n        # self.flush([pkt])\n\n        # return uuid\n        return pkt\n\n    def _get_iid_for(self, pkt, name):\n        if name in self.interned_data:\n            return self.interned_data[name]\n\n        ev = pkt.interned_data.event_names.add()\n        ev.name = name\n        ev.iid = len(self.interned_data) + 1\n\n        self.interned_data[name] = ev.iid\n        return ev.iid\n\n    def set_single(self, x, v):\n        if isinstance(v, str):\n            x.string_value = v\n        elif isinstance(v, bool):\n            x.bool_value = v\n        elif isinstance(v, int):\n            x.int_value = v\n        elif isinstance(v, float):\n            x.double_value = v\n        elif isinstance(v, BaseEnum):\n            x.string_value = str(v)\n        elif isinstance(v, dict):\n            if len(v) == 0:\n                x.string_value = \"[empty]\"\n            else:\n                self._add_debug_annotation(x.dict_entries, v)\n        elif isinstance(v, list) or isinstance(v, tuple):\n            if len(v) == 0:\n                x.string_value = \"[empty]\"\n            else:\n                for i, vv in zip(range(len(v)), v):\n                    if i == self.list_max_size:\n                        self.set_single(x.array_values.add(\n                        ), \"... ({} more items)\".format(len(v) - i))\n                        break\n                    # for some reason, perfetto ui crashes on nested lists.\n                    # add a dummy dictionary here\n                    if isinstance(vv, list) or isinstance(vv, tuple):\n                        vv = {\"array\": vv}\n                    self.set_single(x.array_values.add(), vv)\n        else:\n            x.string_value = str(type(v))\n\n    def _add_debug_annotation(self, d, kwargs):\n        if isinstance(kwargs, list):\n            self._add_debug_annotation(d, {\"data_list\": kwargs})\n            return\n        cnt = 0\n        for k, v in kwargs.items():\n            cnt += 1\n            x = d.add()\n            if cnt == self.list_max_size:\n                x.name = \"...\"\n                x.string_value = \"({} more items)\".format(len(kwargs) - cnt)\n                break\n\n            x.name = str(k)\n\n            self.set_single(x, v)\n\n    def _track_instant(self, uuid, ts, annotation, kwargs, flow=[], caller=None, category_list=None):\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n\n        pkt.timestamp = ts\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        # pkt.track_event.category_iids.append(1)\n        if category_list:\n            for cat in category_list:\n                pkt.track_event.categories.append(cat)\n        pkt.track_event.type = pb2.TrackEvent.TYPE_INSTANT\n        pkt.track_event.track_uuid = uuid\n        pkt.track_event.name = annotation\n\n        if kwargs is not None:\n            self._add_debug_annotation(\n                pkt.track_event.debug_annotations, kwargs)\n\n        for x in flow:\n            pkt.track_event.flow_ids.append(x)\n\n        if caller is not None:\n            file, line, name = caller\n            iid = self._get_source_iid_for(pkt, file, name, line)\n            pkt.track_event.source_location_iid = iid\n\n        # self._flush_if_necessary()\n        return pkt\n\n    def _get_source_iid_for(self, pkt, file, name, line):\n        if (file, name, line) in self.interned_source:\n            return self.interned_source[(file, name, line)]\n        ev = pkt.interned_data.source_locations.add()\n        ev.file_name = file\n        ev.function_name = name\n        ev.line_number = line\n        ev.iid = len(self.interned_source) + 1\n        self.interned_source[(file, name, line)] = ev.iid\n\n        return ev.iid\n\n    def _track_open(self, uuid, ts, annotation, kwargs, flow=[], caller=None, category_list=None):\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n        pkt.timestamp = ts\n        pkt.track_event.name_iid = self._get_iid_for(pkt, annotation)\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        # pkt.track_event.category_iids.append(1)\n        if category_list:\n            for cat in category_list:\n                pkt.track_event.categories.append(cat)\n        pkt.track_event.type = pb2.TrackEvent.TYPE_SLICE_BEGIN\n        pkt.track_event.track_uuid = uuid\n\n        if kwargs is not None:\n            self._add_debug_annotation(\n                pkt.track_event.debug_annotations, kwargs)\n        for x in flow:\n            pkt.track_event.flow_ids.append(x)\n\n        if caller is not None:\n            file, line, name = caller\n            iid = self._get_source_iid_for(pkt, file, name, line)\n            pkt.track_event.source_location_iid = iid\n\n        # self._flush_if_necessary()\n        return pkt\n\n    def _track_close(self, uuid, ts, flow=[]):\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        pkt.timestamp = ts\n        pkt.track_event.track_uuid = uuid\n        pkt.track_event.type = pb2.TrackEvent.TYPE_SLICE_END\n        for x in flow:\n            pkt.track_event.flow_ids.append(x)\n\n        # self._flush_if_necessary()\n        return pkt\n\n    def _track_count(self, uuid, ts, value, category_list=None):\n        # pkt = self.trace.packet.add()\n        pkt = pb2.TracePacket()\n\n        pkt.timestamp = ts\n        pkt.trusted_packet_sequence_id = 2\n        pkt.sequence_flags = 2\n        pkt.track_event.type = pb2.TrackEvent.TYPE_COUNTER\n        pkt.track_event.track_uuid = uuid\n        pkt.track_event.double_counter_value = value\n\n        if category_list:\n            for cat in category_list:\n                pkt.track_event.categories.append(cat)\n\n        # self._flush_if_necessary()\n        return pkt",
    "start_line": 9,
    "end_line": 314,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _BaseTraceGenerator",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._core._BaseTraceGenerator"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile._create_counter_track_if_necessary": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile._create_counter_track_if_necessary",
    "name": "_create_counter_track_if_necessary",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "def _create_counter_track_if_necessary(name):\n    global _master_uuid, _counter_tracks, _tracefile\n    if name not in _counter_tracks:\n        # note that TID here is a dummy value (not really used)\n        uuid = _tracefile._tid_packet(2**32 + len(_counter_tracks), _master_uuid, name, 1)\n        _counter_tracks[name] = uuid\n    return _counter_tracks[name]",
    "start_line": 19,
    "end_line": 25,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "name"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _create_counter_track_if_necessary",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile._create_counter_track_if_necessary"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile._trace": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile._trace",
    "name": "_trace",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "class _trace:\n    def __init__(self, uuid, params, *kargs, **kwargs):\n        self._params = params\n        self._kargs = kargs\n        self._kwargs = kwargs\n        self._incoming_flow_ids = []\n        self._outgoing_flow_ids = []\n        self._uuid = uuid\n        self._caller = None\n    def set_caller(self, caller):\n        if _tracefile is not None:\n            if isinstance(caller, tuple):\n                self._caller = caller\n            elif isinstance(caller, typing.Callable):\n                self._caller = (caller.__code__.co_filename, caller.__code__.co_firstlineno, caller.__code__.co_name)\n        return self\n\n    def __enter__(self):\n        global _tracefile,_tlock\n\n        if _tracefile is not None:\n            if self._caller is None:\n                import inspect\n                frame = inspect.currentframe().f_back\n                self._caller = frame.f_code.co_filename, frame.f_lineno, frame.f_code.co_name\n            if self._uuid is not None:\n                with _tlock:\n                    _tracefile._track_open(self._uuid, time.time_ns(), self._params, {\"kargs\":self._kargs, \"kwargs\":self._kwargs}, self._incoming_flow_ids, self._caller)\n        try:\n            return self._outgoing_flow_ids\n        finally:\n            self._incoming_flow_ids = None\n\n    def set_incoming_flow_ids(self, incoming_flow_ids):\n        self._incoming_flow_ids += incoming_flow_ids\n        return self\n\n    def get_outgoing_flow_ids(self, num_outgoing_flow_ids):\n        global _flow_id, _tlock\n        with _tlock:\n            self._outgoing_flow_ids = [x for x in range(_flow_id, _flow_id + num_outgoing_flow_ids)]\n            _flow_id += num_outgoing_flow_ids\n\n        return self\n\n    def __exit__(self, type, value, traceback):\n        global _flow_id, _tlock\n        if _tracefile is not None:\n            if self._uuid is not None:\n                with _tlock:\n                    _tracefile._track_close(self._uuid, time.time_ns(), self._outgoing_flow_ids)\n        self._flow_ids = None",
    "start_line": 27,
    "end_line": 78,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _trace",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile._trace"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.track": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.track",
    "name": "track",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile._trace"
    ],
    "source_code": "class track:\n    def __init__(self, name):\n        self._name = name\n        self._uuid = None\n\n    def trace(self, param, *kargs, **kwargs):\n        global _master_uuid\n\n        if _tracefile is not None:\n            with _tlock:\n                # tid isn't really used here\n                self._uuid = _tracefile._tid_packet(0, _master_uuid, self._name, 0)\n\n        ret = _trace(self._uuid, param, *kargs, **kwargs)\n        return ret\n        \n    def instant(self, name, description : dict = None, **kwargs):\n        return self._instant(name, description, **kwargs)\n\n    def _instant(self, name, description : dict = None, **kwargs):\n        num_outgoing_flow_ids = kwargs.get(\"num_outgoing_flow_ids\", 0)\n        incoming_flow_ids = kwargs.get(\"incoming_flow_ids\", [])\n        global _tracefile, _tlock, _flow_id, _master_uuid\n        with _tlock:\n            flow_ids = [x for x in range(_flow_id, _flow_id + num_outgoing_flow_ids)]\n            _flow_id += num_outgoing_flow_ids\n            if _tracefile is not None:\n                import inspect\n\n                frame = inspect.currentframe().f_back.f_back\n                caller = frame.f_code.co_filename, frame.f_lineno, frame.f_code.co_name\n                if self._uuid is None:\n                    self._uuid = _tracefile._tid_packet(0, _master_uuid, self._name, 0)\n                _tracefile._track_instant(self._uuid, time.time_ns(), name, description, incoming_flow_ids + flow_ids, caller)\n    \n        return flow_ids",
    "start_line": 81,
    "end_line": 116,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class track",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.track"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.count": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.count",
    "name": "count",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile._create_counter_track_if_necessary"
    ],
    "source_code": "class count:\n    def __init__(self, name):\n        self._name = name\n        self._value = 0\n\n\n    def count(self, value):\n        global _tracefile\n\n        with _tlock:\n            self._value = value\n            if _tracefile is not None:\n                uuid = _create_counter_track_if_necessary(self._name)\n                _tracefile._track_count(uuid, time.time_ns(), self.value)\n\n    def increment(self, value):\n        global _tracefile\n\n        with _tlock:\n            self._value += value\n            if _tracefile is not None:\n                uuid = _create_counter_track_if_necessary(self._name)\n                _tracefile._track_count(uuid, time.time_ns(), self._value)",
    "start_line": 118,
    "end_line": 140,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class count",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.count"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.trace": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace",
    "name": "trace",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile.trace",
      "nova-platform.nova_platform.perfetto_protobuf._profile.track"
    ],
    "source_code": "def trace(params, *kargs, **kwargs):\n    if not hasattr(_tls, \"default_track\"):\n        _tls.default_track = track(threading.current_thread().name)\n    return _tls.default_track.trace(params, *kargs, **kwargs)",
    "start_line": 142,
    "end_line": 145,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "params"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function trace",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.instant": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.instant",
    "name": "instant",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile.track"
    ],
    "source_code": "def instant(name : str, description : dict = None, **kwargs):\n    if not hasattr(_tls, \"default_track\"):\n        _tls.default_track = track(threading.current_thread().name)\n    return _tls.default_track._instant(name, description, **kwargs)",
    "start_line": 147,
    "end_line": 150,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "name",
      "description"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function instant",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.instant"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func",
    "name": "trace_func",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "def trace_func(x):\n    if isinstance(x, typing.Callable):\n        func = x\n        @functools.wraps(func)\n        def f(*kargs, **kwargs):\n            with trace(func.__name__).set_caller(func) as _:\n                return func(*kargs, **kwargs)\n        return f\n    elif isinstance(x, track):\n        tobj = x\n        def trace_func_wrapper(func):\n            nonlocal tobj\n            @functools.wraps(func)\n            def f(*kargs, **kwargs):\n                nonlocal tobj\n                with tobj.trace(func.__name__).set_caller(func) as _:\n                    return func(*kargs, **kwargs)\n            return f\n        return trace_func_wrapper\n    else:\n        assert False",
    "start_line": 152,
    "end_line": 172,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "x"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function trace_func",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.f": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.f",
    "name": "f",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile.trace"
    ],
    "source_code": "            def f(*kargs, **kwargs):\n                nonlocal tobj\n                with tobj.trace(func.__name__, *kargs, **kwargs).set_caller(func) as _:\n                    return func(*kargs, **kwargs)",
    "start_line": 187,
    "end_line": 190,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function f",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.f"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_wrapper": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_wrapper",
    "name": "trace_func_wrapper",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "        def trace_func_wrapper(func):\n            nonlocal tobj\n            @functools.wraps(func)\n            def f(*kargs, **kwargs):\n                nonlocal tobj\n                with tobj.trace(func.__name__).set_caller(func) as _:\n                    return func(*kargs, **kwargs)\n            return f",
    "start_line": 162,
    "end_line": 169,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "func"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function trace_func_wrapper",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_wrapper"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_args": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_args",
    "name": "trace_func_args",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "def trace_func_args(x):\n    if isinstance(x, typing.Callable):\n        func = x\n        @functools.wraps(func)\n        def f(*kargs, **kwargs):\n            with trace(func.__name__, *kargs, **kwargs).set_caller(func) as _:\n                return func(*kargs, **kwargs)\n        return f\n    elif isinstance(x, track):\n        tobj = x\n        def tarce_func_wrapper(func):\n            nonlocal tobj\n            @functools.wraps(func)\n            def f(*kargs, **kwargs):\n                nonlocal tobj\n                with tobj.trace(func.__name__, *kargs, **kwargs).set_caller(func) as _:\n                    return func(*kargs, **kwargs)\n            return f\n        return trace_func_wrapper",
    "start_line": 174,
    "end_line": 192,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "x"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function trace_func_args",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.trace_func_args"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.tarce_func_wrapper": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.tarce_func_wrapper",
    "name": "tarce_func_wrapper",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "        def tarce_func_wrapper(func):\n            nonlocal tobj\n            @functools.wraps(func)\n            def f(*kargs, **kwargs):\n                nonlocal tobj\n                with tobj.trace(func.__name__, *kargs, **kwargs).set_caller(func) as _:\n                    return func(*kargs, **kwargs)\n            return f",
    "start_line": 184,
    "end_line": 191,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "func"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tarce_func_wrapper",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.tarce_func_wrapper"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.open": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.open",
    "name": "open",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "def open(filename): \n    global _tracefile, _master_uuid\n\n    class X:\n        def __init__(self):\n            pass\n        def __enter__(self):\n            global _tracefile, _master_uuid\n            if _master_uuid is not None:\n                raise AssertError(\"Nested trace opening not allowed\")\n\n            _tracefile = _BaseTraceGenerator(filename)\n            pid = os.getpid()\n            tid = threading.get_ident()\n            uuid = _tracefile._pid_packet(pid, sys.argv[0], threading.current_thread().name)\n            _master_uuid = uuid\n        def __exit__(self, type, value, traceback):\n            global _tracefile, _master_uuid\n            _tracefile = None\n            _master_uuid = None\n\n    return X()",
    "start_line": 194,
    "end_line": 215,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "filename"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function open",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.open"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.X": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.X",
    "name": "X",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._core._BaseTraceGenerator"
    ],
    "source_code": "    class X:\n        def __init__(self):\n            pass\n        def __enter__(self):\n            global _tracefile, _master_uuid\n            if _master_uuid is not None:\n                raise AssertError(\"Nested trace opening not allowed\")\n\n            _tracefile = _BaseTraceGenerator(filename)\n            pid = os.getpid()\n            tid = threading.get_ident()\n            uuid = _tracefile._pid_packet(pid, sys.argv[0], threading.current_thread().name)\n            _master_uuid = uuid\n        def __exit__(self, type, value, traceback):\n            global _tracefile, _master_uuid\n            _tracefile = None\n            _master_uuid = None",
    "start_line": 197,
    "end_line": 213,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class X",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.X"
  },
  "nova-platform.nova_platform.perfetto_protobuf._profile.stop": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._profile.stop",
    "name": "stop",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_profile.py",
    "depends_on": [],
    "source_code": "def stop():\n    _tracefile = None",
    "start_line": 217,
    "end_line": 218,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function stop",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._profile.stop"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.BaseTrack": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.BaseTrack",
    "name": "BaseTrack",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "class BaseTrack:\n    def __init__(self, name, parent, uuid):\n        self._parent: TraceGenerator = parent\n        self._uuid = uuid\n        self._name = name\n\n    def submit(self, task_type, task_param):\n        self._parent.submit(task_type, task_param)",
    "start_line": 16,
    "end_line": 23,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BaseTrack",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.BaseTrack"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.CounterTrack": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.CounterTrack",
    "name": "CounterTrack",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._tgen.BaseTrack"
    ],
    "source_code": "class CounterTrack(BaseTrack):\n    def count(self, ts, value):\n        # return\n        \"\"\" Add a count value on the track. \"\"\"\n        ts = int(ts*1e9)\n        # pkt = self._parent._track_count(self._uuid, ts, value)\n        self.submit('count', {\n            \"uuid\": self._uuid,\n            \"ts\": ts,\n            \"value\": value\n        })",
    "start_line": 26,
    "end_line": 36,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseTrack"
    ],
    "class_name": null,
    "display_name": "class CounterTrack",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.CounterTrack"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.NormalTrack": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.NormalTrack",
    "name": "NormalTrack",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._tgen.BaseTrack"
    ],
    "source_code": "class NormalTrack(BaseTrack):\n    def __init__(self, name, parent, uuid):\n        self._parent: TraceGenerator = parent\n        self._uuid = uuid\n        self._name = name\n\n    def duration(self, ts, dur, annotation, kwargs=None, flow=[], category_list=[]):\n        # kwargs = None\n        if kwargs and hasattr(type(kwargs), '__dataclass_fields__'):\n            kwargs = asdict(kwargs)\n\n        ts = int(ts*1e9)\n        # pkt1 = self._parent._track_open(\n        #     self._uuid, ts, annotation, kwargs, flow, category_list=category_list)\n\n        dur = ts+int(dur*1e9)\n        # pkt2 = self._parent._track_close(self._uuid, ts, flow)\n        self.submit('duration', {\n            \"uuid\": self._uuid,\n            \"ts\": ts,\n            \"dur\": dur,\n            \"annotation\": annotation,\n            \"kwargs\": kwargs,\n        })\n\n    def instant(self, ts, annotation, kwargs=None, flow=[], category_list=[]):\n        \"\"\" Record an instant event. \"\"\"\n        if kwargs and hasattr(type(kwargs), '__dataclass_fields__'):\n            kwargs = asdict(kwargs)\n        ts = int(ts*1e9)\n        # pkt = self._parent._track_instant(\n        #     self._uuid, ts, annotation, kwargs, flow, category_list=category_list)\n        self.submit(\"instant\", {\n            \"uuid\": self._uuid,\n            \"ts\": ts,\n            \"annotation\": annotation,\n            \"kwargs\": kwargs,\n        })",
    "start_line": 39,
    "end_line": 76,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "BaseTrack"
    ],
    "class_name": null,
    "display_name": "class NormalTrack",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.NormalTrack"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.GroupTrack": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.GroupTrack",
    "name": "GroupTrack",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "class GroupTrack:\n    def __init__(self, name, parent, uuid):\n        self._parent: TraceGenerator = parent\n        self._uuid = uuid\n        self._name = name\n\n    def create_track(self) -> NormalTrack:\n        \"\"\" Create a child track for this track.\"\"\"\n        return self._parent._create_track(self._uuid, self._name, 0)",
    "start_line": 79,
    "end_line": 87,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GroupTrack",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.GroupTrack"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.Group": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.Group",
    "name": "Group",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "class Group:\n    def __init__(self, name, parent, uuid):\n        self._parent: TraceGenerator = parent\n        self._uuid = uuid\n\n    def create_track(self, track_name: str, tid) -> NormalTrack:\n        \"\"\" Create a normal track for this track.\"\"\"\n        return self._parent._create_track(self._uuid, track_name, 0)\n\n    def create_counter_track(self, track_name: str) -> CounterTrack:\n        \"\"\" Create a counter track.  Counter tracks can be used for recording int values.\"\"\"\n        return self._parent._create_counter_track(self._uuid, track_name, 1)\n\n    def create_group(self, track_name: str) -> GroupTrack:\n        \"\"\" Create a group track.  Group tracks can be used for grouping normal tracks.\"\"\"\n        return self._parent._create_track(self._uuid, track_name, 2)",
    "start_line": 90,
    "end_line": 105,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Group",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.Group"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.tg_worker": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.tg_worker",
    "name": "tg_worker",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._core._BaseTraceGenerator"
    ],
    "source_code": "def tg_worker(queue: Queue, filename):\n    tg = _BaseTraceGenerator()\n    packet_list = []\n\n    def _duration(uuid, ts, dur, annotation, kwargs):\n        # _track_open(self, uuid, ts, annotation, kwargs, flow, caller=None, category_list=None):\n        pkt1 = tg._track_open(uuid, ts, annotation, kwargs)\n        # _track_close(self, uuid, ts, flow):\n        pkt2 = tg._track_close(uuid, dur)\n        return [pkt1, pkt2]\n\n    class TGExit(Exception):\n        pass\n\n    def _exit():\n        raise TGExit()\n\n    task_handler_map = {\n        'create_header': tg._header_packet,\n        'duration': _duration,\n        'count': lambda **kwargs: [tg._track_count(**kwargs)],\n        'instant': lambda **kwargs: [tg._track_instant(**kwargs)],\n        'create_group': lambda **kwargs: [tg._pid_packet(**kwargs)],\n        'create_track': lambda **kwargs: [tg._tid_packet(**kwargs)],\n        'exit': _exit,\n    }\n\n    def flush_packet_list():\n        nonlocal packet_list\n        trace = tg._make_trace(packet_list)\n        f.write(trace.SerializeToString())\n        packet_list = []\n\n    with open(filename, 'ab') as f:\n        try:\n            while True:\n                # count, duration, instant\n                task_name, param = queue.get()\n                handler = task_handler_map[task_name]\n                pkts = handler(**param)\n                packet_list.extend(pkts)\n\n                if len(packet_list) > 1000:\n                    flush_packet_list()\n        except TGExit as e:\n            if packet_list:\n                flush_packet_list()\n            logger.info('tg exit')",
    "start_line": 108,
    "end_line": 155,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "queue",
      "filename"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function tg_worker",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.tg_worker"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen._duration": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen._duration",
    "name": "_duration",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "    def _duration(uuid, ts, dur, annotation, kwargs):\n        # _track_open(self, uuid, ts, annotation, kwargs, flow, caller=None, category_list=None):\n        pkt1 = tg._track_open(uuid, ts, annotation, kwargs)\n        # _track_close(self, uuid, ts, flow):\n        pkt2 = tg._track_close(uuid, dur)\n        return [pkt1, pkt2]",
    "start_line": 112,
    "end_line": 117,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "uuid",
      "ts",
      "dur",
      "annotation",
      "kwargs"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _duration",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen._duration"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.TGExit": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.TGExit",
    "name": "TGExit",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "    class TGExit(Exception):\n        pass",
    "start_line": 119,
    "end_line": 120,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Exception"
    ],
    "class_name": null,
    "display_name": "class TGExit",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.TGExit"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen._exit": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen._exit",
    "name": "_exit",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._tgen.TGExit"
    ],
    "source_code": "    def _exit():\n        raise TGExit()",
    "start_line": 122,
    "end_line": 123,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _exit",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen._exit"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.flush_packet_list": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.flush_packet_list",
    "name": "flush_packet_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [],
    "source_code": "    def flush_packet_list():\n        nonlocal packet_list\n        trace = tg._make_trace(packet_list)\n        f.write(trace.SerializeToString())\n        packet_list = []",
    "start_line": 135,
    "end_line": 139,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function flush_packet_list",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.flush_packet_list"
  },
  "nova-platform.nova_platform.perfetto_protobuf._tgen.TraceGenerator": {
    "id": "nova-platform.nova_platform.perfetto_protobuf._tgen.TraceGenerator",
    "name": "TraceGenerator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "relative_path": "nova-platform/nova_platform/perfetto_protobuf/_tgen.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._tgen.GroupTrack",
      "nova-platform.nova_platform.perfetto_protobuf._tgen.CounterTrack",
      "nova-platform.nova_platform.perfetto_protobuf._tgen.Group",
      "nova-platform.nova_platform.perfetto_protobuf._tgen.NormalTrack"
    ],
    "source_code": "class TraceGenerator(_BaseTraceGenerator):\n    def __init__(self, filename: str, gcu_id):\n        \"\"\" Create a trace \"\"\"\n        super().__init__()\n        self.__pid__ = 1\n        self.tgen_cluster: Dict[Tuple[int, int], Group] = {}\n        self.track_map = {}\n\n        # if check_gil_enabled():  # GIL=1\n        use_MP_env = os.getenv('BOSSANOVA_PROTOBUF_MP', '1') == '1'\n        logger.info(f\"use_MP: {use_MP_env}\")\n        self.manager = None\n        use_mp_runtime = use_MP_env\n        if use_mp_runtime:\n            try:\n                self.manager = Manager()\n                self.queue = self.manager.Queue()\n                self.worker = Process(target=tg_worker, args=(\n                    self.queue, filename), name=f\"TG-{gcu_id}\")\n\n                def cleanup():\n                    try:\n                        self.worker.terminate()\n                    except Exception as e:\n                        logger.error(e)\n                    if self.manager:\n                        try:\n                            self.manager.shutdown()\n                        except Exception as e:\n                            logger.error(e)\n\n                atexit.register(cleanup)\n                self.worker.start()\n            except Exception as exc:\n                logger.warning(\n                    \"Falling back to thread-based TraceGenerator: %s\", exc)\n                self.manager = None\n                use_mp_runtime = False\n\n        if not use_mp_runtime:\n            self.queue = Queue()\n            self.worker = Thread(target=tg_worker, args=(\n                self.queue, filename), name=f\"TG-{gcu_id}\", daemon=True)\n            self.worker.start()\n        self.submit('create_header', {})\n\n    def __deepcopy__(self, memo):\n        # deepcopy逻辑，None表示跳过\n        return None\n    \n    def block_until_all_tasks_done(self):\n        self.submit('exit', {})\n        self.worker.join()\n        # return\n\n    def submit(self, task_type, task_param):\n        self.queue.put((task_type, task_param))\n\n    def _create_group(self, process_name: str, track_name: str = None):\n        \"\"\" Create a group.  Each \"group\" comes with a default normal track (named track_name).\"\"\"\n        pid = self.__pid__\n        self.__pid__ += 1\n\n        uuid = self.__uuid__\n        self.__uuid__ += 1\n\n        # pkt = self._pid_packet(uuid, pid, process_name, track_name)\n        self.submit('create_group', {\n            \"uuid\": uuid,\n            \"pid\": pid,\n            \"process_name\": process_name,\n            \"track_name\": track_name\n        })\n\n        return Group(process_name, self, uuid)\n\n    def _create_track(self, parent_uuid, track_name, ttype):\n        tid = self.__pid__\n        self.__pid__ += 1\n\n        uuid = self.__uuid__\n        self.__uuid__ += 1\n\n        # uuid = 0\n        # uuid, pkt = self._tid_packet(uuid, tid, parent_uuid, track_name, ttype)\n        self.submit(\"create_track\", {\n            \"uuid\": uuid,\n            \"tid\": tid,\n            \"parent_uuid\": parent_uuid,\n            \"track_name\": track_name,\n            \"ttype\": ttype\n        })\n        if ttype == 0:\n            return NormalTrack(track_name, self, uuid)\n        elif ttype == 1:\n            return CounterTrack(track_name, self, uuid)\n        elif ttype == 2:\n            return GroupTrack(track_name, self, uuid)\n        else:\n            assert False\n\n    def _create_counter_track(self, parent_uuid, track_name, ttype) -> CounterTrack:\n        k = (parent_uuid, track_name, ttype)\n        if k not in self.track_map:\n            self.track_map[k] = self._create_track(\n                parent_uuid, track_name, ttype)\n        return self.track_map[k]\n\n    def get_cluster_tgen(self, die_id: int, cid: int) -> Group:\n        if (die_id, cid) not in self.tgen_cluster:\n            if die_id == None and cid == None:\n                group_name = f\"Global\"\n            elif cid == None:\n                group_name = f\"Die:{die_id}, Global\"\n            else:\n                group_name = f\"Die:{die_id}, Cluster:{cid}\"\n            self.tgen_cluster[(die_id, cid)] = self._create_group(\n                group_name)\n        return self.tgen_cluster[(die_id, cid)]",
    "start_line": 163,
    "end_line": 281,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "_BaseTraceGenerator"
    ],
    "class_name": null,
    "display_name": "class TraceGenerator",
    "component_id": "nova-platform.nova_platform.perfetto_protobuf._tgen.TraceGenerator"
  },
  "nova-platform.nova_platform.pytest_nova_lite.pytest_addoption": {
    "id": "nova-platform.nova_platform.pytest_nova_lite.pytest_addoption",
    "name": "pytest_addoption",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/pytest_nova_lite.py",
    "relative_path": "nova-platform/nova_platform/pytest_nova_lite.py",
    "depends_on": [],
    "source_code": "def pytest_addoption(parser: pytest.Parser) -> None:\n    group = parser.getgroup(\"nova-lite\")\n    group.addoption(\n        \"--run-nova-lite\",\n        action=\"store_true\",\n        default=False,\n        help=\"Run nova-lite simulation pipeline instead of collecting tests.\",\n    )\n    group.addoption(\n        \"--nova-lite-config\",\n        action=\"store\",\n        default=\"config/libra_1DIE_3.2TB_24SIP_256OST.yaml\",\n        help=\"Config template path passed to SimulationPipeline.\",\n    )\n    group.addoption(\n        \"--nova-lite-shape\",\n        action=\"store\",\n        default=\"1,512,256,256\",\n        help=\"Comma-separated GEMM tensor shape in B,M,K,N order.\",\n    )\n    group.addoption(\n        \"--nova-lite-dtype\",\n        action=\"store\",\n        default=\"fp16\",\n        help=\"Datatype enum name (matches nova_platform.base_model.DType).\",\n    )\n    group.addoption(\n        \"--nova-lite-bench-version\",\n        action=\"store\",\n        type=int,\n        default=5,\n        help=\"bench_gemm_op_version passed into the pipeline.\",\n    )\n    group.addoption(\n        \"--nova-lite-tags\",\n        action=\"store\",\n        default=\"nova-lite-cli\",\n        help=\"Comma separated metadata tags carried with the run.\",\n    )\n    group.addoption(\n        \"--nova-lite-topo\",\n        action=\"store\",\n        default=\"standalone\",\n        help=\"Topology enum name (nova_platform.config.TOPO).\",\n    )\n    group.addoption(\n        \"--nova-lite-output\",\n        action=\"store\",\n        default=None,\n        help=\"Optional output directory for simulation artifacts.\",\n    )\n    group.addoption(\n        \"--nova-lite-force-rerun\",\n        action=\"store_true\",\n        default=True,\n        help=\"Force pipeline rerun even if cached artifacts exist.\",\n    )",
    "start_line": 20,
    "end_line": 76,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "parser"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function pytest_addoption",
    "component_id": "nova-platform.nova_platform.pytest_nova_lite.pytest_addoption"
  },
  "nova-platform.nova_platform.pytest_nova_lite.pytest_cmdline_main": {
    "id": "nova-platform.nova_platform.pytest_nova_lite.pytest_cmdline_main",
    "name": "pytest_cmdline_main",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/pytest_nova_lite.py",
    "relative_path": "nova-platform/nova_platform/pytest_nova_lite.py",
    "depends_on": [],
    "source_code": "def pytest_cmdline_main(config: pytest.Config) -> Optional[int]:\n    if not logging.getLogger().handlers:\n        logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(name)s:%(message)s\")\n    if not config.getoption(\"--run-nova-lite\"):\n        return None\n\n    from nova_lite.pipeline import SimulationPipeline\n    from nova_platform.base_model import DType\n    from nova_platform.config import TOPO\n\n    reporter = config.pluginmanager.get_plugin(\"terminalreporter\")\n\n    def write_line(message: str) -> None:\n        if reporter:\n            reporter.write_line(message)\n        else:\n            print(message)\n\n    try:\n        shape = _parse_shape(config.getoption(\"--nova-lite-shape\"))\n    except ValueError as exc:  # pragma: no cover - surfaced to CLI\n        raise pytest.UsageError(str(exc)) from exc\n\n    dtype_name = config.getoption(\"--nova-lite-dtype\").upper()\n    try:\n        dtype = DType[dtype_name]\n    except KeyError as exc:  # pragma: no cover - surfaced to CLI\n        valid = \", \".join(member.name.lower() for member in DType)\n        raise pytest.UsageError(f\"Unknown dtype '{dtype_name}'. Valid options: {valid}\") from exc\n\n    topo_name = config.getoption(\"--nova-lite-topo\").upper()\n    try:\n        topo = TOPO[topo_name]\n    except KeyError as exc:  # pragma: no cover - surfaced to CLI\n        valid = \", \".join(member.name.lower() for member in TOPO)\n        raise pytest.UsageError(f\"Unknown topology '{topo_name}'. Valid options: {valid}\") from exc\n\n    config_template = Path(config.getoption(\"--nova-lite-config\"))\n    output_root = config.getoption(\"--nova-lite-output\")\n    if output_root:\n        output_root = Path(output_root)\n    tags = _split_tags(config.getoption(\"--nova-lite-tags\"))\n    bench_version = config.getoption(\"--nova-lite-bench-version\")\n    force_rerun = config.getoption(\"--nova-lite-force-rerun\")\n\n    pipeline = SimulationPipeline(config_template, output_root=output_root)\n    write_line(#打印信息到终端\n        \"Running nova-lite pipeline with \"\n        f\"config={config_template}, \"\n        f\"shape={shape}, \"\n        f\"dtype={dtype.name.lower()}, \"\n        f\"bench_version={bench_version}, \"\n        f\"topo={topo.name.lower()}\"\n    )\n\n    try:\n        result = pipeline.run_gemm(\n            shape=shape,\n            dtype=dtype,\n            bench_version=bench_version,\n            topo=topo,\n            force_rerun=force_rerun,\n            tags=tags,\n        )\n    except Exception as exc:  # pragma: no cover - surfaced to CLI\n        write_line(f\"Nova-lite pipeline failed: {exc}\")\n        return 1\n\n    write_line(f\"Nova-lite report: {result.report_path}\")\n    write_line(f\"Nova-lite perfetto trace: {result.trace_path}\")\n    write_line(f\"Nova-lite output dir: {result.output_dir}\")\n    return 0",
    "start_line": 79,
    "end_line": 150,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function pytest_cmdline_main",
    "component_id": "nova-platform.nova_platform.pytest_nova_lite.pytest_cmdline_main"
  },
  "nova-platform.nova_platform.pytest_nova_lite.write_line": {
    "id": "nova-platform.nova_platform.pytest_nova_lite.write_line",
    "name": "write_line",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/pytest_nova_lite.py",
    "relative_path": "nova-platform/nova_platform/pytest_nova_lite.py",
    "depends_on": [
      "nova-platform.nova_platform.pytest_nova_lite.write_line"
    ],
    "source_code": "    def write_line(message: str) -> None:\n        if reporter:\n            reporter.write_line(message)\n        else:\n            print(message)",
    "start_line": 91,
    "end_line": 95,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "message"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function write_line",
    "component_id": "nova-platform.nova_platform.pytest_nova_lite.write_line"
  },
  "nova-platform.nova_platform.pytest_nova_lite._parse_shape": {
    "id": "nova-platform.nova_platform.pytest_nova_lite._parse_shape",
    "name": "_parse_shape",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/pytest_nova_lite.py",
    "relative_path": "nova-platform/nova_platform/pytest_nova_lite.py",
    "depends_on": [],
    "source_code": "def _parse_shape(raw: str) -> List[int]:\n    components = [segment.strip() for segment in raw.split(\",\") if segment.strip()]\n    if len(components) != 4:\n        raise ValueError(f\"Expected 4 shape components (B,M,K,N). Received: {raw!r}\")\n    try:\n        return [int(value) for value in components]\n    except ValueError as exc:\n        raise ValueError(f\"Shape components must be integers. Received: {raw!r}\") from exc",
    "start_line": 153,
    "end_line": 160,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "raw"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _parse_shape",
    "component_id": "nova-platform.nova_platform.pytest_nova_lite._parse_shape"
  },
  "nova-platform.nova_platform.pytest_nova_lite._split_tags": {
    "id": "nova-platform.nova_platform.pytest_nova_lite._split_tags",
    "name": "_split_tags",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/pytest_nova_lite.py",
    "relative_path": "nova-platform/nova_platform/pytest_nova_lite.py",
    "depends_on": [],
    "source_code": "def _split_tags(raw: str) -> Optional[List[str]]:\n    if not raw:\n        return None\n    if isinstance(raw, (list, tuple)):  # pragma: no cover - defensive\n        return [str(tag) for tag in raw]\n    if \",\" in raw:\n        tags = [segment.strip() for segment in raw.split(\",\") if segment.strip()]\n    else:\n        tags = [segment.strip() for segment in shlex.split(raw) if segment.strip()]\n    return tags or None",
    "start_line": 163,
    "end_line": 172,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "raw"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _split_tags",
    "component_id": "nova-platform.nova_platform.pytest_nova_lite._split_tags"
  },
  "nova-platform.nova_platform.simulator.case.custom_asdict_factory": {
    "id": "nova-platform.nova_platform.simulator.case.custom_asdict_factory",
    "name": "custom_asdict_factory",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/simulator/case.py",
    "relative_path": "nova-platform/nova_platform/simulator/case.py",
    "depends_on": [],
    "source_code": "def custom_asdict_factory(data):\n    def convert(obj):\n        obj_type = type(obj)\n        if issubclass(obj_type, BaseEnum):\n            return obj.name\n        if obj_type == dict:\n            return dict((convert(k), convert(v)) for k, v in obj.items())\n        if hasattr(obj, \"__dataclass_fields__\"):\n            return asdict(obj, dict_factory=custom_asdict_factory)\n        if obj_type == tuple:\n            return list(obj)\n        return obj\n\n    res = {}\n    for k, v in data:\n        if v is None:\n            continue\n        res[convert(k)] = convert(v)\n    return res",
    "start_line": 35,
    "end_line": 53,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "data"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function custom_asdict_factory",
    "component_id": "nova-platform.nova_platform.simulator.case.custom_asdict_factory"
  },
  "nova-platform.nova_platform.simulator.case.convert": {
    "id": "nova-platform.nova_platform.simulator.case.convert",
    "name": "convert",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/simulator/case.py",
    "relative_path": "nova-platform/nova_platform/simulator/case.py",
    "depends_on": [
      "nova-platform.nova_platform.simulator.case.convert"
    ],
    "source_code": "    def convert(obj):\n        obj_type = type(obj)\n        if issubclass(obj_type, BaseEnum):\n            return obj.name\n        if obj_type == dict:\n            return dict((convert(k), convert(v)) for k, v in obj.items())\n        if hasattr(obj, \"__dataclass_fields__\"):\n            return asdict(obj, dict_factory=custom_asdict_factory)\n        if obj_type == tuple:\n            return list(obj)\n        return obj",
    "start_line": 36,
    "end_line": 46,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "obj"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function convert",
    "component_id": "nova-platform.nova_platform.simulator.case.convert"
  },
  "nova-platform.nova_platform.simulator.case.CaseInfo": {
    "id": "nova-platform.nova_platform.simulator.case.CaseInfo",
    "name": "CaseInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/simulator/case.py",
    "relative_path": "nova-platform/nova_platform/simulator/case.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._tgen.TraceGenerator",
      "nova-platform.nova_platform.utils.cuda_utils.get_gpu_count",
      "nova-platform.nova_platform.simulator.case.custom_asdict_factory",
      "nova-platform.nova_platform.executor.nova_platform_executor.execute",
      "nova-platform.nova_platform.executor.nova_platform_executor.generate_report",
      "nova-platform.nova_platform.utils.gcu_utils.GCUData",
      "nova-platform.nova_platform.executor.nova_platform_executor.BossaNovaExecutor",
      "nova-platform.nova_platform.utils.config_utils.load_config",
      "nova-platform.nova_platform.utils.base_utils.check_gil_enabled"
    ],
    "source_code": "class CaseInfo:\n    optype: str\n    shape: List[int]\n    expected_res: float = None\n    fun: Optional[Callable] = None\n    model_name: str = \"UNKNOWN\"\n    tag: List[str] = field(default_factory=lambda: [\"UNKNOWN\"])\n    dtype: DType = DType.FP16\n    config: str | os.PathLike[str] = \"libra_1DIE_3.2TB_24SIP_256OST.yaml\"\n    enable_cache: bool = False\n    outdir: str = \"./\"\n    dataflow_config: dict = field(default_factory=dict)\n    mu: int = None\n\n    input_addr: List[Any] = None\n    output_addr: List[Any] = None\n    start_ref: float = 0\n    end_ref: float = 0\n    kernel_launch_latency: float = 0\n    _sum: float = 0\n    cache_svc = None\n    tgen = None\n    post_stat = None\n    topo: TOPO = TOPO.STANDALONE\n    gcu_data: Dict[int, GCUData] = field(default_factory=lambda: defaultdict(GCUData))\n    is_last_case: bool = True\n\n    def __repr__(self):\n        name = f\"{self.optype}_\" if self.optype else \"\"\n        name += \"-\".join(str(e) for e in self.shape)\n\n        if self.topo != TOPO.STANDALONE:\n            name += f\"_{self.topo.name}\"\n\n        if self.mu:\n            name += f\"_mu_{self.mu}\"\n\n        if self.enable_cache:\n            name += \"_c\"\n\n        cfg = Path(self.config)\n        cfg_repr = cfg.name if cfg.suffix else str(cfg)\n        return f\"{cfg_repr}/{name}\"\n\n    def __post_init__(self):\n        self.gcu_data = defaultdict(GCUData)\n        num_gcus = self.topo.value[1]\n        for i in range(num_gcus):\n            self.gcu_data[i] = GCUData()\n\n    # ---- utility helpers -------------------------------------------------\n\n    def _resolve_config_path(self) -> Path:\n        cfg = Path(self.config)\n        repo_root = Path(__file__).resolve().parents[2]\n\n        candidates: List[Path] = []\n        seen: set[Path] = set()\n\n        def enqueue(path: Path) -> None:\n            path = Path(path)\n            if path in seen:\n                return\n            seen.add(path)\n            candidates.append(path)\n\n        enqueue(cfg)\n\n        if cfg.is_absolute():\n            try:\n                rel_cfg = cfg.relative_to(repo_root)\n            except ValueError:\n                rel_cfg = None\n        else:\n            rel_cfg = cfg\n\n        if rel_cfg is not None:\n            suffixes = {rel_cfg}\n            if rel_cfg.parts and rel_cfg.parts[0] == \"config\":\n                suffixes.add(Path(*rel_cfg.parts[1:]))\n            else:\n                suffixes.add(Path(\"config\") / rel_cfg)\n\n            search_roots = (\n                Path.cwd(),\n                repo_root,\n                repo_root / \"nova_lite\",\n            )\n            for root in search_roots:\n                for suffix in suffixes:\n                    enqueue(root / suffix)\n\n        for candidate in candidates:\n            if candidate.exists():\n                return candidate.resolve()\n\n        fallback = repo_root / \"config\" / \"libra_1DIE_3.2TB_24SIP_256OST.yaml\"\n        if fallback.exists():\n            logger.warning(\n                \"Config template %s not found, falling back to %s\",\n                self.config,\n                fallback,\n            )\n            return fallback.resolve()\n\n        raise FileNotFoundError(f\"Cannot locate config template for {self.config}\")\n\n    # ---- trace helpers ---------------------------------------------------\n\n    def gcu_tgen_init(self):\n        for gcu_id in range(self.topo.value[1]):\n            outdir = Path(self.outdir) / f\"gcu{gcu_id:02d}\"\n            outdir.mkdir(parents=True, exist_ok=True)\n            if gcu_id not in self.gcu_data:\n                self.gcu_data[gcu_id] = GCUData()\n            if self.gcu_data[gcu_id].tgen is None:\n                trace_path = outdir / \"trace.perfetto-trace\"\n                self.gcu_data[gcu_id].tgen = TraceGenerator(str(trace_path), gcu_id)\n\n    def gcu_tgen_exit(self, idx):\n        if self.gcu_data[idx].tgen is not None:\n            self.gcu_data[idx].tgen.block_until_all_tasks_done()\n\n    def _write_stub_artifacts(self, case_outdir: Path) -> None:\n        gcu_dir = case_outdir / \"gcu00\"\n        gcu_dir.mkdir(parents=True, exist_ok=True)\n        report_path = gcu_dir / \"report.yaml\"\n        if not report_path.exists():\n            report_path.write_text(\"status: skipped\\ntotal_latency: 0\\n\")\n\n    # ---- simulation core -------------------------------------------------\n\n    def do_sim(self, force_rerun: bool = False, rank_id: int = 0, device: str = \"auto\"):\n        try:\n            self.gcu_tgen_init()\n            self.do_sim_inner(force_rerun, rank_id, device)\n        except Exception as exc:  # pragma: no cover - pass-through for test assertions\n            logger.error(\"case %s failed: %s\\ndetail: %s\", self, exc, traceback.format_exc())\n            raise\n\n    def do_sim_inner(self, force_rerun: bool = False, rank_id: int = 0, device: str = \"auto\"):\n        repo_root = Path(__file__).resolve().parents[1]\n        case_outdir = Path(self.outdir)\n        if not case_outdir.is_absolute():\n            case_outdir = repo_root / case_outdir\n        case_outdir = case_outdir.resolve()\n        self.outdir = str(case_outdir)\n\n        report_exists = (case_outdir / \"gcu00\" / \"report.yaml\").exists()\n        if not force_rerun and report_exists:\n            logger.info(\"case %s exists, skipped\", self.outdir)\n            return\n\n        dataflow_binary = repo_root / \"build\" / \"dataflow_gen\"\n        if not dataflow_binary.exists() and os.getenv(\"NOVA_SIM_FORCE_REAL\") != \"1\":\n            logger.warning(\"Skipping simulation for %s because dataflow generator binary is missing\", self)\n            self._write_stub_artifacts(case_outdir)\n            return\n\n        case_outdir.mkdir(parents=True, exist_ok=True)\n\n        config_path = self._resolve_config_path()\n        config_basename = config_path.name\n\n        serializable_items = []\n        for dataclass_field in fields(self):\n            if dataclass_field.name in {\"gcu_data\", \"fun\"}:\n                continue\n            serializable_items.append((dataclass_field.name, getattr(self, dataclass_field.name)))\n        metadata_dict = custom_asdict_factory(serializable_items)\n\n        with open(case_outdir / \"metadata.yaml\", \"w\") as f:\n            yaml.dump(metadata_dict, f)\n\n        config = load_config(str(config_path), BossaNovaConfig)\n        with open(case_outdir / config_basename, \"w\") as f:\n            config_dict = asdict(config, dict_factory=custom_asdict_factory)\n            yaml.dump(config_dict, f)\n\n        logger.info(\"config: %s\", config)\n\n        device_count = get_gpu_count()\n        device_id = rank_id % device_count if device_count else 0\n\n        initial_refs = [\n            self.gcu_data[i].start_ref + self.kernel_launch_latency for i in range(len(self.gcu_data))\n        ]\n        executor_list: List[BossaNovaExecutor] = []\n\n        esl_switch = ESLSwitchManager.build_switch(config=config, topo=self.topo)\n        enable_power_svc = os.getenv(\"BOSSANOVA_PW\", \"0\") == \"1\"\n\n        for gcu_id in range(self.topo.value[1]):\n            gcu_outdir = case_outdir / f\"gcu{gcu_id:02d}\"\n            gcu_outdir.mkdir(parents=True, exist_ok=True)\n\n            with open(gcu_outdir / \"metadata.yaml\", \"w\") as f:\n                gcu_metadata = metadata_dict.copy()\n                gcu_metadata[\"gcu_id\"] = gcu_id\n                yaml.dump(gcu_metadata, f)\n\n            gcu_config = load_config(str(config_path), BossaNovaConfig)\n            gcu_config.gcu_id = gcu_id\n            with open(gcu_outdir / config_basename, \"w\") as f:\n                config_dict = asdict(gcu_config, dict_factory=custom_asdict_factory)\n                config_dict[\"enable_power_svc\"] = enable_power_svc\n                yaml.dump(config_dict, f)\n\n            executor = BossaNovaExecutor(\n                gcu_config,\n                outdir=str(gcu_outdir),\n                tgen=self.gcu_data[gcu_id].tgen,\n                enable_cache=self.enable_cache,\n                enable_power_svc=enable_power_svc,\n                device_id=device_id,\n                device=device,\n                cache_svc=self.gcu_data[gcu_id].cache_svc,\n                initial_ref=initial_refs[gcu_id],\n                op_shape=self.shape,\n                optype=self.optype,\n                dtype=self.dtype,\n                mu=self.mu,\n                input_addr=self.input_addr,\n                output_addr=self.output_addr,\n                dataflow_config=self.dataflow_config,\n                esl_switch=esl_switch,\n                case_idx=id(self),\n                enable_dump_addr=os.getenv(\"BOSSANOVA_DUMP_ADDR\", \"0\") == \"1\",\n            )\n            esl_switch.add_gcu(gcu_id, executor)\n            executor_list.append(executor)\n\n        def _p_execute(executor_list: List[BossaNovaExecutor]):\n            barrier = threading.Barrier(len(executor_list))\n\n            def _p_exe(executor: BossaNovaExecutor):\n                exe_gen = executor.execute()\n                while True:\n                    try:\n                        next(exe_gen)\n                    except StopIteration:\n                        break\n                barrier.wait()\n                report = executor.generate_report()\n                idx = executor.config.gcu_id\n                if self.is_last_case:\n                    self.gcu_tgen_exit(idx)\n                self.gcu_data[idx].end_ref = max(self.gcu_data[idx].end_ref, executor.end_ref)\n                self.gcu_data[idx].last_ref = max(self.gcu_data[idx].last_ref, report[\"total_latency\"])\n                self.gcu_data[idx].cache_svc = executor.cache_svc\n                self.gcu_data[idx].tgen = executor.tgen\n                self.gcu_data[idx].post_stat = executor.post_processor.post_stat\n\n            with concurrent.futures.ThreadPoolExecutor(thread_name_prefix=\"gcu\") as executor:\n                list(executor.map(_p_exe, executor_list))\n\n        def _execute(executor_list: List[BossaNovaExecutor]):\n            queue: PriorityQueue[Tuple[int, float, int, Any]] = PriorityQueue()\n            priority = 0\n            for i, exe in enumerate(executor_list):\n                exe_gen = exe.execute()\n                priority += 1\n                queue.put((priority, initial_refs[i], i, exe_gen))\n\n            while not queue.empty():\n                _, curr_ref, i, exe_gen = queue.get()\n                try:\n                    next_ref, state = next(exe_gen)\n                    if state == \"wait\":\n                        priority += 1\n                    queue.put((priority, next_ref, i, exe_gen))\n                except StopIteration:\n                    pass\n\n            for idx, exe in enumerate(executor_list):\n                report = exe.generate_report()\n                if self.is_last_case:\n                    self.gcu_tgen_exit(idx)\n                self.gcu_data[idx].end_ref = max(self.gcu_data[idx].end_ref, exe.end_ref)\n                self._sum = max(self._sum, report[\"total_latency\"])\n                self.gcu_data[idx].cache_svc = exe.cache_svc\n                self.gcu_data[idx].tgen = exe.tgen\n                self.gcu_data[idx].post_stat = exe.post_processor.post_stat\n\n        if check_gil_enabled():\n            logger.info(\"gil enabled\")\n            _execute(executor_list)\n        else:\n            logger.info(\"gil disabled\")\n            _p_execute(executor_list)\n\n        if self.expected_res:\n            assert abs(self._sum - self.expected_res) / self._sum <= 0.1, \"error > 10%\"\n        logger.info(\"done\")",
    "start_line": 57,
    "end_line": 350,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class CaseInfo",
    "component_id": "nova-platform.nova_platform.simulator.case.CaseInfo"
  },
  "nova-platform.nova_platform.simulator.case.FusionCaseInfo": {
    "id": "nova-platform.nova_platform.simulator.case.FusionCaseInfo",
    "name": "FusionCaseInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/simulator/case.py",
    "relative_path": "nova-platform/nova_platform/simulator/case.py",
    "depends_on": [
      "nova-platform.nova_platform.simulator.case.CaseInfo",
      "nova-platform.nova_platform.data_visual.post_processor.FusionPostProcessor",
      "nova-platform.nova_platform.utils.gcu_utils.GCUData"
    ],
    "source_code": "class FusionCaseInfo(CaseInfo):\n    def __init__(self, optype, shape, cases: List[CaseInfo] = None, **kwargs):\n        super().__init__(optype, shape, **kwargs)\n        self.cases = cases or []\n        self.outdir = \"./\"\n        self.config = \"libra_v1.0_24SIP_3.2TB_7MB.yaml\"\n        self.kernel_launch_latency = 2.5e-6\n        self.gcu_data: Dict[int, GCUData] = defaultdict(GCUData)\n\n    def init_gcu_context(self):\n        if self.gcu_data:\n            return\n        num_gcus = self.topo.value[1]\n        for i in range(num_gcus):\n            self.gcu_data[i] = GCUData()\n\n    def add_case(self, cases=CaseInfo):\n        self.cases.append(cases)\n\n    def get_k_launch_latency(self):\n        return self.kernel_launch_latency\n\n    def do_sim(self, force_rerun: bool = False, rank_id: int = 0, device: str = \"auto\"):\n        fus_pp = FusionPostProcessor(outdir=self.outdir)\n        total_start_time = time.time()\n        time_lines = []\n        for index, caseinfo in enumerate(self.cases):\n            fus_post_stats = [gcu.post_stat for gcu in self.gcu_data.values()]\n            self._prepare_case(caseinfo, index)\n            start_time = time.time()\n\n            caseinfo.do_sim(force_rerun=force_rerun, rank_id=rank_id, device=device)\n            caseinfo_post_stats = [gcu.post_stat for gcu in caseinfo.gcu_data.values() if gcu is not None]\n\n            fus_post_stats = fus_pp.fusion_post_stats_list(fus_post_stats, caseinfo_post_stats)\n\n            end_time = time.time()\n            elapsed_time = end_time - start_time\n            time_lines.append(\n                f\"Index: {index}, Name: {caseinfo.optype}, Runtime: {elapsed_time:.2f} seconds\"\n            )\n            self._update_fusion_state(caseinfo)\n\n        total_end_time = time.time()\n        total_elapsed_time = total_end_time - total_start_time\n        time_lines.append(f\"Total Runtime: {total_elapsed_time:.2f} seconds\")\n\n        log_file_path = f\"{self.outdir}/runtime_log.txt\"\n        with open(log_file_path, \"w\") as log_file:\n            log_file.write(\"\\n\".join(time_lines))\n\n        fus_pp.generate_fus_reports(fus_post_stats)\n\n    def _prepare_case(self, caseinfo: CaseInfo, index: int):\n        shape_tokens = [\n            \"-\".join(map(str, token)) if isinstance(token, list) else str(token) for token in caseinfo.shape\n        ]\n        caseinfo.outdir = f\"{self.outdir}/{index}_{caseinfo.optype}_{'_'.join(shape_tokens)}\"\n        self.init_gcu_context()\n        caseinfo.gcu_data = self.gcu_data\n        if index == self.topo.value[1] - 1:\n            caseinfo.is_last_case = True\n        else:\n            caseinfo.is_last_case = False\n\n        for i in range(self.topo.value[1]):\n            caseinfo.gcu_data[i].start_ref = self.gcu_data[i].last_ref\n        caseinfo.kernel_launch_latency = self.get_k_launch_latency()\n\n    def _update_fusion_state(self, caseinfo: CaseInfo):\n        self.gcu_data = caseinfo.gcu_data\n        if caseinfo.optype in [\"allreduce\", \"allgather\", \"allgather_gemm\"]:\n            max_end_ref = max(gcu.end_ref for gcu in caseinfo.gcu_data.values() if gcu is not None)\n            for i in range(self.topo.value[1]):\n                self.gcu_data[i].last_ref = max_end_ref\n        else:\n            for i in range(self.topo.value[1]):\n                self.gcu_data[i].last_ref = caseinfo.gcu_data[i].end_ref",
    "start_line": 353,
    "end_line": 430,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "CaseInfo"
    ],
    "class_name": null,
    "display_name": "class FusionCaseInfo",
    "component_id": "nova-platform.nova_platform.simulator.case.FusionCaseInfo"
  },
  "nova-platform.nova_platform.utils.base_utils.BaseDataclass": {
    "id": "nova-platform.nova_platform.utils.base_utils.BaseDataclass",
    "name": "BaseDataclass",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/base_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/base_utils.py",
    "depends_on": [],
    "source_code": "class BaseDataclass:\n    def __str__(self):\n        return str(asdict(self))",
    "start_line": 6,
    "end_line": 8,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class BaseDataclass",
    "component_id": "nova-platform.nova_platform.utils.base_utils.BaseDataclass"
  },
  "nova-platform.nova_platform.utils.base_utils.hash_list": {
    "id": "nova-platform.nova_platform.utils.base_utils.hash_list",
    "name": "hash_list",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/base_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/base_utils.py",
    "depends_on": [],
    "source_code": "class hash_list(list):\n    def __init__(self, *args):\n        if len(args) == 1 and isinstance(args[0], Iterable):\n            args = args[0]\n        super().__init__(args)\n\n    def __hash__(self):\n        return hash(tuple(self))",
    "start_line": 11,
    "end_line": 18,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "list"
    ],
    "class_name": null,
    "display_name": "class hash_list",
    "component_id": "nova-platform.nova_platform.utils.base_utils.hash_list"
  },
  "nova-platform.nova_platform.utils.base_utils.SingletonMeta": {
    "id": "nova-platform.nova_platform.utils.base_utils.SingletonMeta",
    "name": "SingletonMeta",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/base_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/base_utils.py",
    "depends_on": [],
    "source_code": "class SingletonMeta(type):\n    _instances = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls not in cls._instances:\n            cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]",
    "start_line": 21,
    "end_line": 27,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "type"
    ],
    "class_name": null,
    "display_name": "class SingletonMeta",
    "component_id": "nova-platform.nova_platform.utils.base_utils.SingletonMeta"
  },
  "nova-platform.nova_platform.utils.base_utils.check_gil_enabled": {
    "id": "nova-platform.nova_platform.utils.base_utils.check_gil_enabled",
    "name": "check_gil_enabled",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/base_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/base_utils.py",
    "depends_on": [],
    "source_code": "def check_gil_enabled():\n    if not hasattr(sys, '_is_gil_enabled') or sys._is_gil_enabled():\n        return True\n    else:\n        return False",
    "start_line": 30,
    "end_line": 34,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function check_gil_enabled",
    "component_id": "nova-platform.nova_platform.utils.base_utils.check_gil_enabled"
  },
  "nova-platform.nova_platform.utils.config_utils.ConfigLoader": {
    "id": "nova-platform.nova_platform.utils.config_utils.ConfigLoader",
    "name": "ConfigLoader",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/config_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/config_utils.py",
    "depends_on": [],
    "source_code": "class ConfigLoader(yaml.SafeLoader):\n    # https://stackoverflow.com/questions/528281/how-can-i-include-a-yaml-file-inside-another\n    def __init__(self, stream):\n        self._root = os.path.split(stream.name)[0]\n        super(ConfigLoader, self).__init__(stream)\n\n    def include(self, node):\n        filename = os.path.join(self._root, self.construct_scalar(node))\n        with open(filename, \"r\") as f:\n            ret = yaml.load(f, ConfigLoader)\n        assert ret is not None, \"included file is empty? file: %s\" % filename\n        return ret\n\n    def eval(self, node):\n        expr = self.construct_scalar(node)\n        return eval(expr)",
    "start_line": 11,
    "end_line": 26,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "yaml.SafeLoader"
    ],
    "class_name": null,
    "display_name": "class ConfigLoader",
    "component_id": "nova-platform.nova_platform.utils.config_utils.ConfigLoader"
  },
  "nova-platform.nova_platform.utils.config_utils.Deserializable": {
    "id": "nova-platform.nova_platform.utils.config_utils.Deserializable",
    "name": "Deserializable",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/config_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/config_utils.py",
    "depends_on": [],
    "source_code": "class Deserializable:\n    def deserialize(o: any):\n        raise NotImplementedError()",
    "start_line": 33,
    "end_line": 35,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class Deserializable",
    "component_id": "nova-platform.nova_platform.utils.config_utils.Deserializable"
  },
  "nova-platform.nova_platform.utils.config_utils.dict_to_dataclass": {
    "id": "nova-platform.nova_platform.utils.config_utils.dict_to_dataclass",
    "name": "dict_to_dataclass",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/config_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/config_utils.py",
    "depends_on": [
      "nova-platform.nova_platform.utils.config_utils.dict_to_dataclass"
    ],
    "source_code": "def dict_to_dataclass(d: Dict[str, Any], cls: T, custom_loader_map={}, *, restrict_mode=True) -> T:\n    if cls in custom_loader_map:\n        cls = custom_loader_map[cls](d)\n\n    if isinstance(cls, type) and issubclass(cls, Deserializable):\n        return cls.deserialize(d)\n\n    if not hasattr(cls, \"__dataclass_fields__\"):\n        if isinstance(cls, _GenericAlias) and cls.__origin__ is list:\n            cls: List\n            inner_cls = cls.__args__[0]\n            return [dict_to_dataclass(x, inner_cls, custom_loader_map, restrict_mode=restrict_mode) for x in d]\n        elif isinstance(cls, _GenericAlias) and cls.__origin__ is tuple:\n            return tuple([dict_to_dataclass(x,  cls.__args__[i], custom_loader_map, restrict_mode=restrict_mode) for i, x in enumerate(d)])\n        elif isinstance(cls, _GenericAlias) and cls.__origin__ is dict:\n            key_type = cls.__args__[0]\n            val_type = cls.__args__[1]\n            res = dict({\n                key_type(k): dict_to_dataclass(v, val_type, custom_loader_map, restrict_mode=restrict_mode)\n                for k, v in d.items()\n            })\n            return res\n        return cls(d)\n\n    fields = cls.__dataclass_fields__\n    kwargs = {}\n    for field_name, field_type in fields.items():\n        # TODO: need review\n        if isinstance(field_type, Field):\n            field_value = d.get(field_name)\n            if field_value is not None:\n                if field_type.type is None or type(field_value) == field_type.type:\n                    kwargs[field_name] = field_value\n                else:\n                    kwargs[field_name] = dict_to_dataclass(\n                        field_value, field_type.type, custom_loader_map, restrict_mode=restrict_mode)\n            else:\n                if not isinstance(field_type.default_factory, _MISSING_TYPE):\n                    kwargs[field_name] = field_type.default_factory()\n                elif not isinstance(field_type.default, _MISSING_TYPE):\n                    kwargs[field_name] = field_type.default\n                else:\n                    if restrict_mode:\n                        raise Exception(\n                            f\"required {field_name} is not provided\")\n                    else:\n                        kwargs[field_name] = None\n                        # logger.warn(f\"required {field_name} is not provided\")\n\n        else:\n            kwargs[field_name] = field_type.type(field_value)\n    return cls(**kwargs)",
    "start_line": 41,
    "end_line": 92,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "d",
      "cls",
      "custom_loader_map"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function dict_to_dataclass",
    "component_id": "nova-platform.nova_platform.utils.config_utils.dict_to_dataclass"
  },
  "nova-platform.nova_platform.utils.config_utils.load_config": {
    "id": "nova-platform.nova_platform.utils.config_utils.load_config",
    "name": "load_config",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/config_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/config_utils.py",
    "depends_on": [
      "nova-platform.nova_platform.utils.config_utils.dict_to_dataclass"
    ],
    "source_code": "def load_config(config_path: str, cls: T) -> T:\n    with open(config_path) as f:\n        data = yaml.load(f, ConfigLoader)\n        config = dict_to_dataclass(data, cls)\n    return config",
    "start_line": 95,
    "end_line": 99,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "config_path",
      "cls"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function load_config",
    "component_id": "nova-platform.nova_platform.utils.config_utils.load_config"
  },
  "nova-platform.nova_platform.utils.config_utils.BaseEnum": {
    "id": "nova-platform.nova_platform.utils.config_utils.BaseEnum",
    "name": "BaseEnum",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/config_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/config_utils.py",
    "depends_on": [],
    "source_code": "class BaseEnum(Enum):\n    @classmethod\n    def _missing_(cls, value):\n        value = value.lower()\n        for member in cls:\n            if member.name.lower() == value:\n                return member\n        return None\n\n    def __repr__(self):\n        return self.name",
    "start_line": 102,
    "end_line": 112,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "Enum"
    ],
    "class_name": null,
    "display_name": "class BaseEnum",
    "component_id": "nova-platform.nova_platform.utils.config_utils.BaseEnum"
  },
  "nova-platform.nova_platform.utils.cuda_utils.get_gpu_count": {
    "id": "nova-platform.nova_platform.utils.cuda_utils.get_gpu_count",
    "name": "get_gpu_count",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/cuda_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/cuda_utils.py",
    "depends_on": [
      "nova-platform.nova_platform.perfetto_protobuf._profile.count"
    ],
    "source_code": "def get_gpu_count():\n    try:\n        import pycuda.driver as cuda\n        # Initialize CUDA\n        cuda.init()\n        device_count = cuda.Device.count()\n        logger.info(\"found %s GPUs\", device_count)\n        return device_count\n    except:\n        return 0",
    "start_line": 5,
    "end_line": 14,
    "has_docstring": false,
    "docstring": "",
    "parameters": [],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_gpu_count",
    "component_id": "nova-platform.nova_platform.utils.cuda_utils.get_gpu_count"
  },
  "nova-platform.nova_platform.utils.fusion_utils.MemoryAllocator": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.MemoryAllocator",
    "name": "MemoryAllocator",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [],
    "source_code": "class MemoryAllocator:\n    def __init__(self):\n        self.addr_ref = 5 * 2**40 # 5TB\n        self.addr_table = {}\n    \n    #input operand_id_list, operand_shape_list and result_id_list, output_shape_list and dtype_list\n    #output: input address list and output address list\n    def allocate(self, ssa_ids, shapes, dtype):\n        if not isinstance(ssa_ids, list):\n            ssa_ids = [ssa_ids]\n            shapes = [shapes]\n        \n        addr_list = []\n        for ssa_id, shape in zip(ssa_ids, shapes):\n            if (ssa_id in self.addr_table):\n                addr_list.append(self.addr_table[ssa_id])\n            else:\n                offset = self.get_shape_offset(shape, dtype)\n                address = self.addr_ref\n                self.addr_ref += offset\n                self.addr_table[ssa_id] = address\n                addr_list.append(address)\n        return addr_list\n\n    def get_shape_offset(self, shape, dtype:DType) -> int:\n        bpe = dtype.get_bpe()\n        total_elem = 1\n        for dim in shape:\n            total_elem *= dim\n        offset = total_elem * bpe\n        return offset",
    "start_line": 11,
    "end_line": 41,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class MemoryAllocator",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.MemoryAllocator"
  },
  "nova-platform.nova_platform.utils.fusion_utils.TensorInfo": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.TensorInfo",
    "name": "TensorInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [],
    "source_code": "class TensorInfo:\n    def __init__(self, tensor_id, shape, data_type):\n        self.tensor_id = tensor_id\n        self.shape = shape\n        self.data_type = data_type\n\n    @classmethod\n    def from_tensor(cls, tensor):\n        tensor_id = tensor.getTensorID()\n        shape = tensor.getDims()\n        dtype_str = tensor.getDataType().name\n\n        # TODO: Add more data types as needed\n        dtype_map={\n            'F8':DType.FP8,\n            'F16':DType.FP16,\n            'F32':DType.FP32,\n        }\n        dtype=dtype_map[dtype_str]\n        \n        return cls(tensor_id, shape, dtype)",
    "start_line": 43,
    "end_line": 63,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TensorInfo",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.TensorInfo"
  },
  "nova-platform.nova_platform.utils.fusion_utils.ScalarInfo": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.ScalarInfo",
    "name": "ScalarInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [],
    "source_code": "class ScalarInfo:\n    def __init__(self, value):\n        self.value = value\n    @classmethod\n    def from_scalar(cls, scalar):\n        value = scalar.getValue()\n        return cls(value)",
    "start_line": 65,
    "end_line": 71,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class ScalarInfo",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.ScalarInfo"
  },
  "nova-platform.nova_platform.utils.fusion_utils.OperationInfo": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.OperationInfo",
    "name": "OperationInfo",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [
      "nova-platform.nova_platform.simulator.case.CaseInfo"
    ],
    "source_code": "class OperationInfo:\n    result_ids: List[int]\n    result_shapes: List[int]\n    op_ids: List[int]\n    op_shapes: List[List[int]]\n    #不支持混合精度量化\n    dtype: DType\n    caseinfo: CaseInfo\n\n    def __str__(self):\n        caseinfo_str = (\n            f\"OperationInfo(\\n\"\n            f\"  op_name: {self.caseinfo.optype},\\n\"\n            \"\\n\"\n            f\"  dtype: {self.dtype},\\n\"\n            f\"  op_ids: {self.op_ids},\\n\"\n            f\"  op_shapes: {self.op_shapes},\\n\"\n            \"\\n\"\n            f\"  result_ids: {self.result_ids},\\n\"\n            f\"  result_shapes: {self.result_shapes},\\n\"\n            \"\\n\"\n            f\"  case_shape: {self.caseinfo.shape},\\n\"\n            f\"  cache_enable: {self.caseinfo.enable_cache},\\n\"\n            \"\\n\"\n            f\"  topo: {self.caseinfo.topo},\\n\"\n            f\")\"\n        )\n        return caseinfo_str\n        \n    # generate dim_map for transpose\n    @staticmethod\n    def get_dim_map(dim_list:List[int]):\n        dim_map = [0, 1, 2, 3]\n        dim_map[3-dim_list[0]], dim_map[3-dim_list[1]] =  dim_map[3-dim_list[1]], dim_map[3-dim_list[0]]\n        return dim_map\n\n    @classmethod\n    def from_instruction(cls, instr: optrace.instruct) -> \"OperationInfo\":\n        optrace_name = instr.getOpname()\n        operands = instr.getOperands()\n\n        # Parse operand details using TensorInfo\n        tensor_info = [TensorInfo.from_tensor(op) for op in operands if op.getValueType().name == \"TENSOR\"]\n        scalar_info = [op.getDataI32() for op in operands if op.getValueType().name == \"SCALAR\"]\n        \n\n        # Reflect to caseinfo optype and shape\n        caseinfo_shape, optype = cls._caseinfo_shape(optrace_name, [op.shape for op in tensor_info], scalar_info)\n\n        # Get result info using TensorInfo\n        results = instr.getResults()\n        result_info = [TensorInfo.from_tensor(r) for r in results]\n\n        # Create CaseInfo instance\n        caseinfo = CaseInfo(\n            optype=optype, \n            shape=caseinfo_shape, \n            expected_res=None,\n            fun=None,\n            tag=[\"Fusion_DecodeLayer\"], \n            dtype=result_info[0].data_type,  \n        )\n        \n        return cls(\n            result_ids=[r.tensor_id for r in result_info],\n            result_shapes=[r.shape for r in result_info],\n            op_ids=[op.tensor_id for op in tensor_info],\n            op_shapes=[op.shape for op in tensor_info],\n\n            caseinfo=caseinfo,\n            dtype=result_info[0].data_type\n        )\n    \n    @staticmethod\n    def _caseinfo_shape(optrace_name: str, op_shape: List[List[int]], op_dim: List[int]) -> List[int]:\n        \"\"\"Determine the shape for the caseinfo based on the operation name.\"\"\"\n        #TODO: Add more cases as needed\n        #caseinfo name is not same as optrace_name\n        if optrace_name in [\"dot\"]:\n            optype = \"gemm\"\n            caseinfo_shape = op_shape[0][:]+ op_shape[2]\n        elif optrace_name in [\"mbedding\", \"embedding\"]:\n            optype = \"gather\"\n            caseinfo_shape = op_shape[0][:]+ op_shape[1][:]\n        elif optrace_name in [\"flash_attention_fusion\"]:\n            optype = \"sdpa\"\n            caseinfo_shape = [op_shape[0][:]]+ [op_shape[1][:]]+[True]\n        elif optrace_name in [\"layer_norm\"]:\n            optype = \"layernorm\"\n            caseinfo_shape = op_shape[0][1:]\n        else:#caseinfo name is same as optrace_name\n            if optrace_name in [\"add\", \"gelu\"]:\n                caseinfo_shape = [1, op_shape[0][1], 1, op_shape[0][2]]\n                optype = optrace_name\n            elif optrace_name in [\"transpose\"]:\n                dim_map = OperationInfo.get_dim_map(op_dim)\n                caseinfo_shape = [op_shape[0][:]] + [dim_map[:]]\n                optype = optrace_name\n            elif optrace_name in [\"allreduce\"]:\n                optype = \"allreduce\"\n                caseinfo_shape = op_shape[0][:]\n            elif optrace_name in [\"allgather\"]:\n                optype = \"allgather\"\n                #TODO：need to review ‘32’ \n                caseinfo_shape = [[op_shape[0][0], op_shape[0][1]*32, op_shape[0][2], op_shape[0][3]]] + [op_shape[0][:]]\n            elif optrace_name in [\"allgather_gemm\"]:\n                optype = \"allgather_gemm\"\n                caseinfo_shape = op_shape[0][:]+ op_shape[2]\n        return caseinfo_shape, optype",
    "start_line": 76,
    "end_line": 184,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class OperationInfo",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.OperationInfo"
  },
  "nova-platform.nova_platform.utils.fusion_utils.get_caseinfo_list": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.get_caseinfo_list",
    "name": "get_caseinfo_list",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [
      "nova-platform.nova_platform.utils.fusion_utils.MemoryAllocator",
      "optrace_benchmark.optrace"
    ],
    "source_code": "def get_caseinfo_list(op_trace_path: str, topo=TOPO.STANDALONE, enable_cache=True) -> Tuple[List[CaseInfo], List[OperationInfo]]:\n    opt = optrace.optrace(op_trace_path)\n    mod = opt.getModule()\n    instruct_list = mod.getInstructs()\n\n    alloc = MemoryAllocator()\n    caseinfo_list = []\n    op_info_list = []\n    for instr_dump in instruct_list:\n        case_in_trace = OperationInfo.from_instruction(instr_dump)\n        case_in_trace.caseinfo.topo = topo\n        case_in_trace.caseinfo.enable_cache = enable_cache\n        if case_in_trace.caseinfo.optype == \"gather\":\n           case_in_trace.caseinfo.mu = 1e-9\n        case_in_trace.caseinfo.input_addr = alloc.allocate(case_in_trace.op_ids, case_in_trace.op_shapes, case_in_trace.dtype)\n\n        case_in_trace.caseinfo.output_addr = alloc.allocate(case_in_trace.result_ids, case_in_trace.result_shapes, case_in_trace.dtype)\n        caseinfo_list.append(case_in_trace.caseinfo)\n        op_info_list.append(case_in_trace)\n    return caseinfo_list, op_info_list",
    "start_line": 191,
    "end_line": 210,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "op_trace_path",
      "topo",
      "enable_cache"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function get_caseinfo_list",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.get_caseinfo_list"
  },
  "nova-platform.nova_platform.utils.fusion_utils.display_fusion_info": {
    "id": "nova-platform.nova_platform.utils.fusion_utils.display_fusion_info",
    "name": "display_fusion_info",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/fusion_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/fusion_utils.py",
    "depends_on": [],
    "source_code": "def display_fusion_info(outdir, op_info_list: List[OperationInfo]):\n    with open(f\"{outdir}/optrace_file_output.txt\", \"w\") as f:\n        for op_info in op_info_list:\n            try:\n                f.write(f\"{op_info}\\n\")\n                f.write(f\"mu: {op_info.caseinfo.mu}\\n\")\n                f.write(f\"Input Address: {op_info.caseinfo.input_addr}\\n\")            \n                f.write(f\"Output Address: {op_info.caseinfo.output_addr}\\n\")   \n\n                f.write(f\"Input Address Hex: {[hex(addr) for addr in op_info.caseinfo.input_addr]}\\n\")\n                f.write(f\"Output Address Hex: {[hex(addr) for addr in op_info.caseinfo.output_addr]}\\n\")       \n                f.write(\"-\" * 40 + \"\\n\") \n            except Exception as e:\n                f.write(f\"Error processing case: {op_info}\\n\")\n                f.write(f\"Exception: {e}\\n\")\n                f.write(\"-\" * 40 + \"\\n\")",
    "start_line": 217,
    "end_line": 232,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "outdir",
      "op_info_list"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function display_fusion_info",
    "component_id": "nova-platform.nova_platform.utils.fusion_utils.display_fusion_info"
  },
  "nova-platform.nova_platform.utils.gcu_utils.GCUData": {
    "id": "nova-platform.nova_platform.utils.gcu_utils.GCUData",
    "name": "GCUData",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/gcu_utils.py",
    "relative_path": "nova-platform/nova_platform/utils/gcu_utils.py",
    "depends_on": [],
    "source_code": "class GCUData:  \n    tgen: Any = None\n    cache_svc: Any = None\n    post_stat: Any = None\n    start_ref: float = 0.0\n    end_ref: float = 0.0\n    last_ref: float = 0.0",
    "start_line": 5,
    "end_line": 11,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class GCUData",
    "component_id": "nova-platform.nova_platform.utils.gcu_utils.GCUData"
  },
  "nova-platform.nova_platform.utils.test.parse_string": {
    "id": "nova-platform.nova_platform.utils.test.parse_string",
    "name": "parse_string",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/nova-platform/nova_platform/utils/test.py",
    "relative_path": "nova-platform/nova_platform/utils/test.py",
    "depends_on": [],
    "source_code": "def parse_string(string: str) -> Tuple[int, List[int], DType]:\n        match = re.match(r\"%(\\d+):<([\\dx]+)xf(\\d+)>\", string)\n        if match:\n            ssa_id = int(match.group(1))\n            shape_str = match.group(2)\n            shape = [int(dim) for dim in shape_str.split('x')]\n            dtype_str = match.group(3)\n            if dtype_str == \"16\":\n                dtype = DType.FP16\n            elif dtype_str == \"32\":\n                dtype = DType.FP32\n            return ssa_id, shape, dtype\n        else:\n            raise ValueError(f\"Invalid result string format: {string}\")",
    "start_line": 9,
    "end_line": 22,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "string"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function parse_string",
    "component_id": "nova-platform.nova_platform.utils.test.parse_string"
  },
  "optrace_benchmark._ValueType": {
    "id": "optrace_benchmark._ValueType",
    "name": "_ValueType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "class _ValueType:\n    name: str",
    "start_line": 9,
    "end_line": 10,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _ValueType",
    "component_id": "optrace_benchmark._ValueType"
  },
  "optrace_benchmark._DataType": {
    "id": "optrace_benchmark._DataType",
    "name": "_DataType",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "class _DataType:\n    name: str",
    "start_line": 14,
    "end_line": 15,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _DataType",
    "component_id": "optrace_benchmark._DataType"
  },
  "optrace_benchmark._Operand": {
    "id": "optrace_benchmark._Operand",
    "name": "_Operand",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [
      "optrace_benchmark._DataType"
    ],
    "source_code": "class _Operand:\n    def __init__(self, tensor_id: int, dims: List[int], dtype: str):\n        self._tensor_id = tensor_id\n        self._dims = dims\n        self._dtype = _DataType(dtype)\n\n    def getValueType(self):\n        return _TENSOR\n\n    def getTensorID(self):\n        return self._tensor_id\n\n    def getDims(self):\n        return list(self._dims)\n\n    def getDataType(self):\n        return self._dtype",
    "start_line": 22,
    "end_line": 38,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _Operand",
    "component_id": "optrace_benchmark._Operand"
  },
  "optrace_benchmark._ScalarOperand": {
    "id": "optrace_benchmark._ScalarOperand",
    "name": "_ScalarOperand",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "class _ScalarOperand:\n    def __init__(self, value: int):\n        self._value = value\n\n    def getValueType(self):\n        return _SCALAR\n\n    def getDataI32(self):\n        return self._value",
    "start_line": 41,
    "end_line": 49,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _ScalarOperand",
    "component_id": "optrace_benchmark._ScalarOperand"
  },
  "optrace_benchmark._Result": {
    "id": "optrace_benchmark._Result",
    "name": "_Result",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [
      "optrace_benchmark._Operand"
    ],
    "source_code": "class _Result(_Operand):\n    pass",
    "start_line": 52,
    "end_line": 53,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": [
      "_Operand"
    ],
    "class_name": null,
    "display_name": "class _Result",
    "component_id": "optrace_benchmark._Result"
  },
  "optrace_benchmark.instruct": {
    "id": "optrace_benchmark.instruct",
    "name": "instruct",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "class instruct:\n    def __init__(self, opname: str, operands: List[object], results: List[_Result]):\n        self._opname = opname\n        self._operands = operands\n        self._results = results\n\n    def getOpname(self):\n        return self._opname\n\n    def getOperands(self):\n        return list(self._operands)\n\n    def getResults(self):\n        return list(self._results)",
    "start_line": 56,
    "end_line": 69,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class instruct",
    "component_id": "optrace_benchmark.instruct"
  },
  "optrace_benchmark._Module": {
    "id": "optrace_benchmark._Module",
    "name": "_Module",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "class _Module:\n    def __init__(self, instructs: List[instruct]):\n        self._instructs = instructs\n\n    def getInstructs(self):\n        return list(self._instructs)",
    "start_line": 72,
    "end_line": 77,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class _Module",
    "component_id": "optrace_benchmark._Module"
  },
  "optrace_benchmark.optrace": {
    "id": "optrace_benchmark.optrace",
    "name": "optrace",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [
      "optrace_benchmark._parse_trace",
      "optrace_benchmark._Module"
    ],
    "source_code": "class optrace:\n    \"\"\"\n    Bare-bones parser for the text-based optrace format used in the tests.\n    The real optrace_benchmark package contains a full protobuf parser; we only\n    need enough to drive the unit tests with the simplified traces under\n    ``tests/resources``.\n    \"\"\"\n\n    def __init__(self, trace_path: str):\n        trace = Path(trace_path)\n        if not trace.exists():\n            raise FileNotFoundError(trace_path)\n        self._module = _Module(_parse_trace(trace.read_text().splitlines()))\n\n    def getModule(self):\n        return self._module",
    "start_line": 80,
    "end_line": 95,
    "has_docstring": true,
    "docstring": "Bare-bones parser for the text-based optrace format used in the tests.\nThe real optrace_benchmark package contains a full protobuf parser; we only\nneed enough to drive the unit tests with the simplified traces under\n``tests/resources``.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class optrace",
    "component_id": "optrace_benchmark.optrace"
  },
  "optrace_benchmark._parse_trace": {
    "id": "optrace_benchmark._parse_trace",
    "name": "_parse_trace",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [
      "optrace_benchmark._parse_operand",
      "optrace_benchmark._split_args",
      "optrace_benchmark.instruct",
      "optrace_benchmark._ScalarOperand"
    ],
    "source_code": "def _parse_trace(lines: List[str]) -> List[instruct]:\n    instructions: List[instruct] = []\n    for raw in lines:\n        line = raw.strip()\n        if not line or line.startswith(\"#\"):\n            continue\n        if \"=\" not in line:\n            continue\n        lhs, rhs = line.split(\"=\", 1)\n        lhs = lhs.split(\"%\", 1)[-1]  # drop timeline prefix\n        result_tokens = [tok.strip() for tok in lhs.split(\",\") if tok.strip()]\n        results = [_parse_operand(tok, as_result=True) for tok in result_tokens]\n\n        op_spec, args = rhs.strip().split(\"(\", 1)\n        op_name = op_spec.strip().split(\".\")[-1]\n        args = args.rsplit(\")\", 1)[0]\n        operands = []\n        for token in _split_args(args):\n            token = token.strip()\n            if not token:\n                continue\n            if token.startswith(\"%\"):\n                operands.append(_parse_operand(token, as_result=False))\n            else:\n                try:\n                    operands.append(_ScalarOperand(int(token)))\n                except ValueError:\n                    continue\n        instructions.append(instruct(op_name, operands, results))\n    return instructions",
    "start_line": 98,
    "end_line": 127,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "lines"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _parse_trace",
    "component_id": "optrace_benchmark._parse_trace"
  },
  "optrace_benchmark._split_args": {
    "id": "optrace_benchmark._split_args",
    "name": "_split_args",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "def _split_args(arg_str: str) -> List[str]:\n    chunks = []\n    depth = 0\n    current = []\n    for ch in arg_str:\n        if ch == \",\" and depth == 0:\n            chunk = \"\".join(current).strip()\n            if chunk:\n                chunks.append(chunk)\n            current = []\n            continue\n        if ch == \"(\":\n            depth += 1\n        elif ch == \")\":\n            depth = max(0, depth - 1)\n        current.append(ch)\n    tail = \"\".join(current).strip()\n    if tail:\n        chunks.append(tail)\n    return chunks",
    "start_line": 130,
    "end_line": 149,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "arg_str"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _split_args",
    "component_id": "optrace_benchmark._split_args"
  },
  "optrace_benchmark._parse_operand": {
    "id": "optrace_benchmark._parse_operand",
    "name": "_parse_operand",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [
      "optrace_benchmark._parse_shape"
    ],
    "source_code": "def _parse_operand(token: str, as_result: bool) -> object:\n    # token format: %5:<1x384x12288xf16>\n    token = token.strip()\n    if token.startswith(\"%\"):\n        token = token[1:]\n    tensor_id, _, remainder = token.partition(\":\")\n    tensor_id = int(tensor_id)\n    dims, dtype = _parse_shape(remainder)\n    operand_cls = _Result if as_result else _Operand\n    return operand_cls(tensor_id, dims, dtype)",
    "start_line": 152,
    "end_line": 161,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "token",
      "as_result"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _parse_operand",
    "component_id": "optrace_benchmark._parse_operand"
  },
  "optrace_benchmark._parse_shape": {
    "id": "optrace_benchmark._parse_shape",
    "name": "_parse_shape",
    "component_type": "function",
    "file_path": "/home/zhw/nova-sim/optrace_benchmark.py",
    "relative_path": "optrace_benchmark.py",
    "depends_on": [],
    "source_code": "def _parse_shape(payload: str):\n    payload = payload.strip()\n    if payload.startswith(\"<\") and payload.endswith(\">\"):\n        payload = payload[1:-1]\n    parts = [p for p in payload.split(\"x\") if p]\n    if not parts:\n        return [], \"F16\"\n    dtype = parts[-1].upper()\n    dims = []\n    for part in parts[:-1]:\n        try:\n            dims.append(int(part))\n        except ValueError:\n            dims.append(1)\n    return dims, dtype",
    "start_line": 164,
    "end_line": 178,
    "has_docstring": false,
    "docstring": "",
    "parameters": [
      "payload"
    ],
    "node_type": "function",
    "base_classes": null,
    "class_name": null,
    "display_name": "function _parse_shape",
    "component_id": "optrace_benchmark._parse_shape"
  },
  "perfetto.trace_processor.TraceProcessorConfig": {
    "id": "perfetto.trace_processor.TraceProcessorConfig",
    "name": "TraceProcessorConfig",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/perfetto/trace_processor.py",
    "relative_path": "perfetto/trace_processor.py",
    "depends_on": [],
    "source_code": "class TraceProcessorConfig:\n    def __init__(self, **kwargs):\n        self.options = kwargs",
    "start_line": 6,
    "end_line": 8,
    "has_docstring": false,
    "docstring": "",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TraceProcessorConfig",
    "component_id": "perfetto.trace_processor.TraceProcessorConfig"
  },
  "perfetto.trace_processor.TraceProcessor": {
    "id": "perfetto.trace_processor.TraceProcessor",
    "name": "TraceProcessor",
    "component_type": "class",
    "file_path": "/home/zhw/nova-sim/perfetto/trace_processor.py",
    "relative_path": "perfetto/trace_processor.py",
    "depends_on": [],
    "source_code": "class TraceProcessor:\n    \"\"\"\n    Very small stub that mimics the public API used by the tests.\n\n    The real perfetto TraceProcessor streams SQL queries from trace files.\n    For the purposes of the unit tests we only need query() to be iterable, so\n    the stub simply returns an empty iterator.\n    \"\"\"\n\n    def __init__(self, trace: str | None = None, config: TraceProcessorConfig | None = None):\n        self.trace = trace\n        self.config = config\n\n    def query(self, sql: str) -> Iterator[dict]:\n        return iter(())\n\n    def close(self):\n        pass",
    "start_line": 11,
    "end_line": 28,
    "has_docstring": true,
    "docstring": "Very small stub that mimics the public API used by the tests.\n\nThe real perfetto TraceProcessor streams SQL queries from trace files.\nFor the purposes of the unit tests we only need query() to be iterable, so\nthe stub simply returns an empty iterator.",
    "parameters": null,
    "node_type": "class",
    "base_classes": null,
    "class_name": null,
    "display_name": "class TraceProcessor",
    "component_id": "perfetto.trace_processor.TraceProcessor"
  }
}