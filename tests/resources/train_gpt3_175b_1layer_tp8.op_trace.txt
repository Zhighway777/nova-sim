58388:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58393:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58392:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58387:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58390:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58389:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58387:0:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58394:None:0 %0:<f32> = torch.2_1_0.aten::lift_fresh(%0:<f32>) 
58391:4:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %1:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%1:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %2:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%2:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58387:0:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58390:3:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58392:5:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58389:2:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58388:1:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58394:7:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %3:<6144xf16> = torch.2_1_0.aten::rand(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58389:2:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58392:5:0 %1:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58387:0:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58390:3:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58388:1:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58394:7:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58389:2:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58392:5:0 %4:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %1:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58393:6:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58393:6:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58393:6:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58393:6:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58393:6:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58387:0:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58388:1:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58390:3:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58390:3:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58391:4:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58390:3:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58390:3:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58394:7:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58389:2:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58393:6:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58393:6:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58393:6:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58392:5:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%4:<2048x1x6144xf16>, 0.5:f32) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%4:<2048x1x6144xf16>, 0.79788456:f32) 
58390:3:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58390:3:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58390:3:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%4:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58390:3:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %4:<2048x1x6144xf16>) 
58388:1:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58392:5:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::add(%8:<2048x1x6144xf16>, 1:i32, 1:i32) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %9:<2048x1x6144xf16>) 
58388:1:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58388:1:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58388:1:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58391:4:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58392:5:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%8:<2048x1x6144xf16>) 
58392:5:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%10:<2048x1x6144xf16>) 
58391:4:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58392:5:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::add(%10:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58394:7:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58389:2:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58392:5:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, %12:<2048x1x6144xf16>) 
58391:4:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58387:0:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58391:4:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58394:7:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58389:2:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58388:1:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58388:1:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58394:7:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58394:7:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58394:7:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58394:7:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58387:0:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58387:0:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58391:4:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58391:4:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58391:4:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58387:0:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58387:0:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58394:7:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58394:7:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58394:7:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58394:7:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58389:2:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%8:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58389:2:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::add(%9:<2048x1x6144xf16>, 1:i32, 1:i32) 
58389:2:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%7:<2048x1x6144xf16>, %10:<2048x1x6144xf16>) 
58387:0:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58387:0:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58387:0:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58389:2:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%11:<2048x1x6144xf16>) 
58389:2:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58389:2:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58389:2:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %14:<2048x1x6144xf16>) 
58393:6:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58393:6:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58393:6:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58393:6:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.79788456:f32) 
58393:6:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.044715:f32) 
58393:6:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%22:<2048x1x6144xf16>, %20:<2048x1x6144xf16>) 
58390:3:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58390:3:0 %12:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%12:<6144xf16>) 
58390:3:0 %19:<2048x1x6144xf16> = torch.2_1_0.aten::add(%18:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%19:<2048x1x6144xf16>, 0.5:f32) 
58388:1:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58388:1:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58390:3:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%19:<2048x1x6144xf16>, 0.79788456:f32) 
58388:1:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58393:6:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::add(%23:<2048x1x6144xf16>, 1:i32, 1:i32) 
58393:6:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %24:<2048x1x6144xf16>) 
58388:1:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.5:f32) 
58388:1:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58391:4:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58391:4:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.044715:f32) 
58391:4:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58388:1:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%22:<2048x1x6144xf16>, %20:<2048x1x6144xf16>) 
58390:3:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%19:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::add(%23:<2048x1x6144xf16>, 1:i32, 1:i32) 
58388:1:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %24:<2048x1x6144xf16>) 
58390:3:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %19:<2048x1x6144xf16>) 
58388:1:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%25:<2048x1x6144xf16>) 
58388:1:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::add(%26:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%13:<2048x1x6144xf16>) 
58392:5:0 %13:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %14:<6144xf16> = torch.2_1_0.aten::detach(%13:<6144xf16>) 
58391:4:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58388:1:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %27:<2048x1x6144xf16>) 
58390:3:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::add(%22:<2048x1x6144xf16>, 1:i32, 1:i32) 
58390:3:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %23:<2048x1x6144xf16>) 
58388:1:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58388:1:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58388:1:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58388:1:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58387:0:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58387:0:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58388:1:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58387:0:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58387:0:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58388:1:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58388:1:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %5:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::add(%14:<6144xf16>, %8:<2048x1x6144xf16>, alpha=1:i32) 
58391:4:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.5:f32) 
58390:3:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%24:<2048x1x6144xf16>) 
58390:3:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::add(%25:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58394:7:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58394:7:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58389:2:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58389:2:0 %18:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %19:<6144xf16> = torch.2_1_0.aten::detach(%18:<6144xf16>) 
58391:4:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.79788456:f32) 
58387:0:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.5:f32) 
58391:4:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.044715:f32) 
58389:2:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58387:0:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%22:<2048x1x6144xf16>, %20:<2048x1x6144xf16>) 
58387:0:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, 0.044715:f32) 
58393:6:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%25:<2048x1x6144xf16>) 
58393:6:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::add(%26:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%13:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %27:<2048x1x6144xf16>) 
58393:6:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58393:6:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58393:6:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58393:6:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58393:6:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58393:6:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%13:<2048x1x6144xf16>, 0.79788456:f32) 
58393:6:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58393:6:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58393:6:0 %5:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %13:<6144xf16> = torch.2_1_0.aten::detach(%5:<6144xf16>) 
58392:5:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%13:<2048x1x6144xf16>, 0.044715:f32) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%16:<2048x1x6144xf16>, %13:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::add(%17:<2048x1x6144xf16>, 1:i32, 1:i32) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%15:<2048x1x6144xf16>, %8:<2048x1x6144xf16>) 
58392:5:0 %18:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%17:<2048x1x6144xf16>) 
58392:5:0 %19:<2048x1x6144xf16> = torch.2_1_0.aten::add(%18:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%14:<2048x1x6144xf16>, %19:<2048x1x6144xf16>) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58392:5:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%13:<2048x1x6144xf16>) 
58392:5:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%8:<2048x1x6144xf16>) 
58392:5:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%18:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%14:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58392:5:0 %7:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %9:<6144xf16> = torch.2_1_0.aten::detach(%7:<6144xf16>) 
58390:3:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, %26:<2048x1x6144xf16>) 
58390:3:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58390:3:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58390:3:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58390:3:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%23:<2048x1x6144xf16>) 
58390:3:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58390:3:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58390:3:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58390:3:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58390:3:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %6:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58394:7:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::add(%19:<6144xf16>, %17:<2048x1x6144xf16>, alpha=1:i32) 
58394:7:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.5:f32) 
58394:7:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.79788456:f32) 
58394:7:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.044715:f32) 
58394:7:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %11:<2048x1x6144xf16>) 
58389:2:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.5:f32) 
58389:2:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.79788456:f32) 
58389:2:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.044715:f32) 
58389:2:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %11:<2048x1x6144xf16>) 
58389:2:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::add(%22:<2048x1x6144xf16>, 1:i32, 1:i32) 
58389:2:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %23:<2048x1x6144xf16>) 
58389:2:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%24:<2048x1x6144xf16>) 
58389:2:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::add(%25:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58389:2:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, %26:<2048x1x6144xf16>) 
58389:2:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58389:2:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58389:2:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58389:2:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%23:<2048x1x6144xf16>) 
58389:2:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58389:2:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58389:2:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58389:2:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58389:2:0 %5:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %13:<6144xf16> = torch.2_1_0.aten::detach(%5:<6144xf16>) 
58391:4:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::add(%23:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %24:<2048x1x6144xf16>) 
58391:4:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%25:<2048x1x6144xf16>) 
58394:7:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::add(%22:<2048x1x6144xf16>, 1:i32, 1:i32) 
58394:7:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %23:<2048x1x6144xf16>) 
58391:4:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::add(%26:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%22:<2048x1x6144xf16>, %20:<2048x1x6144xf16>) 
58391:4:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %27:<2048x1x6144xf16>) 
58391:4:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58391:4:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58391:4:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::add(%23:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58391:4:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58391:4:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58391:4:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58387:0:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %24:<2048x1x6144xf16>) 
58391:4:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58391:4:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58391:4:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58391:4:0 %6:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58387:0:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%25:<2048x1x6144xf16>) 
58387:0:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::add(%26:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %27:<2048x1x6144xf16>) 
58387:0:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58387:0:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58387:0:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58387:0:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58394:7:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%24:<2048x1x6144xf16>) 
58387:0:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58387:0:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58394:7:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::add(%25:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58387:0:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58387:0:0 %6:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58394:7:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, %26:<2048x1x6144xf16>) 
58394:7:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58394:7:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58394:7:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58394:7:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%23:<2048x1x6144xf16>) 
58394:7:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58394:7:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58394:7:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58394:7:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %10:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58394:7:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %6:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58392:5:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%5:<2048x1x6144xf16>) 
58392:5:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%18:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58392:5:0 %22:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %26:<6144xf16> = torch.2_1_0.aten::detach(%22:<6144xf16>) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58392:5:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58392:5:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58392:5:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%5:<2048x1x6144xf16>) 
58392:5:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%8:<2048x1x6144xf16>) 
58392:5:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %11:<6144xf16> = torch.2_1_0.aten::detach(%8:<6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58392:5:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%14:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58392:5:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58392:5:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58392:5:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58392:5:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %1:<2048x1x6144xf16>, alpha=1:i32) 
58392:5:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%16:<2048x1x6144xf16>, 0.5:f32) 
58392:5:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%16:<2048x1x6144xf16>, 0.79788456:f32) 
58392:5:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%16:<2048x1x6144xf16>, 0.044715:f32) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, %16:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::add(%25:<2048x1x6144xf16>, 1:i32, 1:i32) 
58392:5:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%21:<2048x1x6144xf16>, %23:<2048x1x6144xf16>) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%20:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::add(%17:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%22:<2048x1x6144xf16>, %25:<2048x1x6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %15:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %12:<6144xf16> = torch.2_1_0.aten::detach(%15:<6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::add(%12:<6144xf16>, %24:<2048x1x6144xf16>, alpha=1:i32) 
58392:5:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%15:<2048x1x6144xf16>, 0.5:f32) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%15:<2048x1x6144xf16>, 0.79788456:f32) 
58392:5:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%15:<2048x1x6144xf16>, 0.044715:f32) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, %15:<2048x1x6144xf16>) 
58392:5:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::add(%8:<2048x1x6144xf16>, 1:i32, 1:i32) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%6:<2048x1x6144xf16>, %12:<2048x1x6144xf16>) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%8:<2048x1x6144xf16>) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::add(%17:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%24:<2048x1x6144xf16>, %7:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%5:<2048x1x6144xf16>) 
58392:5:0 %30:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58392:5:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58392:5:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58392:5:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58392:5:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58392:5:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58392:5:0 %26:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %11:<6144xf16> = torch.2_1_0.aten::detach(%26:<6144xf16>) 
58388:1:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58388:1:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58388:1:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58388:1:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58388:1:0 %39:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58388:1:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58388:1:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58388:1:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%34:<2048x1x6144xf16>) 
58388:1:0 %28:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %31:<6144xf16> = torch.2_1_0.aten::detach(%28:<6144xf16>) 
58388:1:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%31:<2048x1x6144xf16>) 
58388:1:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58388:1:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58388:1:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58388:1:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58388:1:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%52:<2048x1x6144xf16>) 
58388:1:0 %39:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58388:1:0 %43:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %41:<6144xf16> = torch.2_1_0.aten::detach(%43:<6144xf16>) 
58388:1:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%54:<2048x1x6144xf16>) 
58388:1:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58388:1:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58388:1:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%31:<2048x1x6144xf16>) 
58388:1:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%58:<2048x1x6144xf16>) 
58388:1:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%60:<2048x1x6144xf16>) 
58388:1:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58388:1:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58388:1:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.5:f32) 
58388:1:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.79788456:f32) 
58388:1:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%49:<2048x1x6144xf16>, %29:<2048x1x6144xf16>) 
58388:1:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::add(%47:<2048x1x6144xf16>, 1:i32, 1:i32) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%51:<2048x1x6144xf16>, %53:<2048x1x6144xf16>) 
58388:1:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%47:<2048x1x6144xf16>) 
58388:1:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58388:1:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::add(%62:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58388:1:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%55:<2048x1x6144xf16>, %64:<2048x1x6144xf16>) 
58392:5:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58388:1:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58392:5:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58392:5:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %13:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58392:5:0 %25:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %8:<6144xf16> = torch.2_1_0.aten::detach(%25:<6144xf16>) 
58392:5:0 %15:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%8:<2048x1x6144xf16>) 
58392:5:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%12:<2048x1x6144xf16>) 
58392:5:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%14:<2048x1x6144xf16>) 
58392:5:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%13:<2048x1x6144xf16>) 
58392:5:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58392:5:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%1:<2048x1x6144xf16>) 
58392:5:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58392:5:0 %22:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58392:5:0 %5:<6144xf16> = torch.2_1_0.aten::detach(%22:<6144xf16>) 
58392:5:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%5:<2048x1x6144xf16>) 
58392:5:0 %7:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%13:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58392:5:0 %12:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%8:<2048x1x6144xf16>) 
58392:5:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58388:1:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58392:5:0 %27:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58388:1:0 %40:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %57:<6144xf16> = torch.2_1_0.aten::detach(%40:<6144xf16>) 
58392:5:0 %20:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58392:5:0 %12:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58392:5:0 %1:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%12:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::add(%57:<6144xf16>, %63:<2048x1x6144xf16>, alpha=1:i32) 
58388:1:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.5:f32) 
58388:1:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.79788456:f32) 
58388:1:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.044715:f32) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%52:<2048x1x6144xf16>, %61:<2048x1x6144xf16>) 
58388:1:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::add(%47:<2048x1x6144xf16>, 1:i32, 1:i32) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%59:<2048x1x6144xf16>, %57:<2048x1x6144xf16>) 
58388:1:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%47:<2048x1x6144xf16>) 
58388:1:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::add(%60:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58388:1:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%63:<2048x1x6144xf16>, %48:<2048x1x6144xf16>) 
58388:1:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%52:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::add(%27:<2048x1x12288xf16>, %1:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58388:1:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%59:<2048x1x6144xf16>) 
58388:1:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58388:1:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%60:<2048x1x6144xf16>) 
58388:1:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58388:1:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58388:1:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58388:1:0 %62:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %53:<6144xf16> = torch.2_1_0.aten::detach(%62:<6144xf16>) 
58393:6:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58393:6:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%14:<2048x1x6144xf16>) 
58393:6:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58393:6:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58393:6:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58393:6:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58393:6:0 %42:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58393:6:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%34:<2048x1x6144xf16>) 
58393:6:0 %28:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %31:<6144xf16> = torch.2_1_0.aten::detach(%28:<6144xf16>) 
58392:5:0 %3, %23:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%14:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%31:<2048x1x6144xf16>) 
58393:6:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::add(%20:<2048x1x12288xf16>, %3:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58393:6:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%45:<2048x1x6144xf16>) 
58393:6:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%47:<2048x1x6144xf16>) 
58393:6:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%49:<2048x1x6144xf16>) 
58393:6:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58393:6:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58393:6:0 %42:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %22:<6144xf16> = torch.2_1_0.aten::detach(%42:<6144xf16>) 
58393:6:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%32:<2048x1x6144xf16>) 
58393:6:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58393:6:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58393:6:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%31:<2048x1x6144xf16>) 
58393:6:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58393:6:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%54:<2048x1x6144xf16>) 
58393:6:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58393:6:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58393:6:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.79788456:f32) 
58393:6:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, 0.044715:f32) 
58393:6:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%52:<2048x1x6144xf16>, %29:<2048x1x6144xf16>) 
58393:6:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::add(%50:<2048x1x6144xf16>, 1:i32, 1:i32) 
58393:6:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%43:<2048x1x6144xf16>, %54:<2048x1x6144xf16>) 
58393:6:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%48:<2048x1x6144xf16>) 
58393:6:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58393:6:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::add(%57:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58393:6:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%41:<2048x1x6144xf16>, %46:<2048x1x6144xf16>) 
58390:3:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58390:3:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58390:3:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%6:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58390:3:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58392:5:0 %6:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58390:3:0 %42:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::add(%6:<2048x1x12288xf16>, %14:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %6:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%6:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58392:5:0 %6:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %3, %7:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%6:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58388:1:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58388:1:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58388:1:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%60:<2048x1x6144xf16>) 
58390:3:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58390:3:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%32:<2048x1x6144xf16>) 
58388:1:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%71:<2048x1x6144xf16>) 
58390:3:0 %33:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%73:<2048x1x6144xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::add(%14:<2048x1x12288xf16>, %3:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %31:<6144xf16> = torch.2_1_0.aten::detach(%33:<6144xf16>) 
58388:1:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%75:<2048x1x6144xf16>) 
58388:1:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%68:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %69:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58388:1:0 %66:<6144xf16> = torch.2_1_0.aten::detach(%69:<6144xf16>) 
58392:5:0 %23:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%26:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %56:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%31:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58393:6:0 %40:<6144xf16> = torch.2_1_0.aten::detach(%56:<6144xf16>) 
58390:3:0 %25:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58390:3:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58388:1:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%77:<2048x1x6144xf16>) 
58390:3:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58388:1:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%47:<2048x1x6144xf16>) 
58390:3:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58388:1:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%71:<2048x1x6144xf16>) 
58390:3:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58388:1:0 %78:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58390:3:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58388:1:0 %79:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58388:1:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58390:3:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58388:1:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58390:3:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58388:1:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %42:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58390:3:0 %23:<6144xf16> = torch.2_1_0.aten::detach(%42:<6144xf16>) 
58388:1:0 %74:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58388:1:0 %57:<6144xf16> = torch.2_1_0.aten::detach(%74:<6144xf16>) 
58390:3:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%23:<2048x1x6144xf16>) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::add(%26:<2048x1x12288xf16>, %23:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58390:3:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58388:1:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58390:3:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%52:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58388:1:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58390:3:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58388:1:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58392:5:0 %23:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%26:<2048x1x12288xf16>) 
58390:3:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58388:1:0 %86:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%85:<2048x1x6144xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58390:3:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58388:1:0 %88:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%87:<2048x1x6144xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58390:3:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%59:<2048x1x6144xf16>) 
58388:1:0 %89:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%77:<2048x1x6144xf16>) 
58390:3:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58393:6:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::add(%40:<6144xf16>, %57:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%28:<2048x1x6144xf16>, 0.5:f32) 
58393:6:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%55:<2048x1x6144xf16>, 0.5:f32) 
58390:3:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%28:<2048x1x6144xf16>, 0.79788456:f32) 
58393:6:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%55:<2048x1x6144xf16>, 0.79788456:f32) 
58390:3:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%28:<2048x1x6144xf16>, 0.044715:f32) 
58393:6:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%55:<2048x1x6144xf16>, 0.044715:f32) 
58390:3:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%49:<2048x1x6144xf16>, %28:<2048x1x6144xf16>) 
58393:6:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%53:<2048x1x6144xf16>, %55:<2048x1x6144xf16>) 
58390:3:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::add(%43:<2048x1x6144xf16>, 1:i32, 1:i32) 
58393:6:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::add(%60:<2048x1x6144xf16>, 1:i32, 1:i32) 
58393:6:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%57:<2048x1x6144xf16>, %61:<2048x1x6144xf16>) 
58390:3:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%51:<2048x1x6144xf16>, %53:<2048x1x6144xf16>) 
58390:3:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%47:<2048x1x6144xf16>) 
58393:6:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%56:<2048x1x6144xf16>) 
58390:3:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58388:1:0 %56:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::add(%62:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58390:3:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::add(%44:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58393:6:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%47:<2048x1x6144xf16>, %63:<2048x1x6144xf16>) 
58390:3:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%40:<2048x1x6144xf16>, %57:<2048x1x6144xf16>) 
58393:6:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58393:6:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58393:6:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58393:6:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58388:1:0 %90:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58393:6:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%47:<2048x1x6144xf16>) 
58393:6:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58388:1:0 %63:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%59:<2048x1x6144xf16>) 
58393:6:0 %59:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %58:<6144xf16> = torch.2_1_0.aten::detach(%59:<6144xf16>) 
58388:1:0 %86:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%63:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %88:<2048x1x12288xf16> = torch.2_1_0.aten::add(%56:<2048x1x12288xf16>, %86:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%58:<2048x1x6144xf16>) 
58390:3:0 %60:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %46:<6144xf16> = torch.2_1_0.aten::detach(%60:<6144xf16>) 
58390:3:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::add(%46:<6144xf16>, %56:<2048x1x6144xf16>, alpha=1:i32) 
58390:3:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%63:<2048x1x6144xf16>, 0.5:f32) 
58390:3:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%63:<2048x1x6144xf16>, 0.79788456:f32) 
58390:3:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%63:<2048x1x6144xf16>, 0.044715:f32) 
58390:3:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%65:<2048x1x6144xf16>, %63:<2048x1x6144xf16>) 
58390:3:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::add(%66:<2048x1x6144xf16>, 1:i32, 1:i32) 
58390:3:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%46:<2048x1x6144xf16>, %67:<2048x1x6144xf16>) 
58390:3:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%54:<2048x1x6144xf16>) 
58390:3:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::add(%68:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58390:3:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%64:<2048x1x6144xf16>, %69:<2048x1x6144xf16>) 
58388:1:0 %64, %52:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%88:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%65:<2048x1x6144xf16>) 
58390:3:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58390:3:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58390:3:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%67:<2048x1x6144xf16>) 
58388:1:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::add(%90:<2048x1x12288xf16>, %64:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%68:<2048x1x6144xf16>) 
58390:3:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58390:3:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%69:<2048x1x6144xf16>) 
58390:3:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%49:<2048x1x6144xf16>) 
58390:3:0 %62:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %61:<6144xf16> = torch.2_1_0.aten::detach(%62:<6144xf16>) 
58388:1:0 %81:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %82:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%81:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58388:1:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::add(%79:<2048x1x12288xf16>, %82:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%65:<2048x1x12288xf16>) 
58388:1:0 %77:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>) 
58388:1:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%77:<2048x1x12288xf16>) 
58393:6:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58393:6:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58393:6:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%72:<2048x1x6144xf16>) 
58393:6:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58393:6:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%73:<2048x1x6144xf16>) 
58393:6:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%75:<2048x1x6144xf16>) 
58393:6:0 %78:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%77:<2048x1x6144xf16>) 
58388:1:0 %61, %51:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%87:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%70:<2048x1x6144xf16>) 
58393:6:0 %67:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %71:<6144xf16> = torch.2_1_0.aten::detach(%67:<6144xf16>) 
58388:1:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::add(%79:<2048x1x12288xf16>, %61:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58393:6:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%69:<2048x1x6144xf16>) 
58393:6:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%77:<2048x1x6144xf16>) 
58388:1:0 %83:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58393:6:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%79:<2048x1x6144xf16>) 
58388:1:0 %79:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58388:1:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58393:6:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58388:1:0 %48:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58393:6:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58393:6:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58393:6:0 %52:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58393:6:0 %78:<6144xf16> = torch.2_1_0.aten::detach(%52:<6144xf16>) 
58393:6:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%78:<2048x1x6144xf16>) 
58393:6:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58393:6:0 %82:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58393:6:0 %79:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58393:6:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%69:<2048x1x6144xf16>) 
58388:1:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::add(%48:<2048x1x12288xf16>, %79:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58393:6:0 %85:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58388:1:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %52:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%92:<2048x1x12288xf16>) 
58388:1:0 %70:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%75:<2048x1x12288xf16>) 
58388:1:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%70:<2048x1x12288xf16>) 
58393:6:0 %41:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %86:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %47:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %55:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%47:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58393:6:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::add(%41:<2048x1x12288xf16>, %55:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58390:3:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%78:<2048x1x6144xf16>) 
58390:3:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58390:3:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%79:<2048x1x6144xf16>) 
58390:3:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58390:3:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58390:3:0 %85:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58390:3:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%77:<2048x1x6144xf16>) 
58393:6:0 %82, %79:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%85:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %51:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %74:<6144xf16> = torch.2_1_0.aten::detach(%51:<6144xf16>) 
58393:6:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::add(%86:<2048x1x12288xf16>, %82:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58390:3:0 %82:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58390:3:0 %79:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58390:3:0 %87:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%86:<2048x1x6144xf16>) 
58390:3:0 %89:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%88:<2048x1x6144xf16>) 
58390:3:0 %91:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%90:<2048x1x6144xf16>) 
58390:3:0 %93:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%92:<2048x1x6144xf16>) 
58390:3:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58390:3:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%83:<2048x1x6144xf16>) 
58390:3:0 %67:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58390:3:0 %63:<6144xf16> = torch.2_1_0.aten::detach(%67:<6144xf16>) 
58390:3:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58390:3:0 %88:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%90:<2048x1x6144xf16>) 
58390:3:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58390:3:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%94:<2048x1x6144xf16>) 
58390:3:0 %96:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%95:<2048x1x6144xf16>) 
58390:3:0 %98:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%97:<2048x1x6144xf16>) 
58390:3:0 %100:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%99:<2048x1x6144xf16>) 
58390:3:0 %86:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58390:3:0 %4:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %62:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %63:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%62:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %100:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58393:6:0 %66:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%66:<2048x1x12288xf16>) 
58390:3:0 %98:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%100:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58393:6:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::add(%75:<2048x1x12288xf16>, %63:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58393:6:0 %57:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>) 
58393:6:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%57:<2048x1x12288xf16>) 
58393:6:0 %70, %78:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%80:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::add(%86:<2048x1x12288xf16>, %98:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %46:<2048x1x12288xf16> = torch.2_1_0.aten::add(%75:<2048x1x12288xf16>, %70:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %83:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %70:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %64:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%75:<2048x1x12288xf16>) 
58393:6:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::add(%64:<2048x1x12288xf16>, %70:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %52:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %57:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%52:<2048x1x12288xf16>) 
58393:6:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%79:<2048x1x12288xf16>) 
58393:6:0 %88:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%87:<2048x1x12288xf16>) 
58390:3:0 %74, %51:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%99:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %53:<2048x1x12288xf16> = torch.2_1_0.aten::add(%4:<2048x1x12288xf16>, %74:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %89:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %87:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%89:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %72:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%82:<2048x1x12288xf16>) 
58390:3:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::add(%72:<2048x1x12288xf16>, %87:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%94:<2048x1x12288xf16>) 
58390:3:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%79:<2048x1x12288xf16>) 
58390:3:0 %90:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%95:<2048x1x12288xf16>) 
58390:3:0 %63, %101:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%90:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %72:<2048x1x12288xf16> = torch.2_1_0.aten::add(%93:<2048x1x12288xf16>, %63:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %79:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %93:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%79:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::add(%103:<2048x1x12288xf16>, %93:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%104:<2048x1x12288xf16>) 
58390:3:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58390:3:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58394:7:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58394:7:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%34:<2048x1x6144xf16>) 
58394:7:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58394:7:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58394:7:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58394:7:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58394:7:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%32:<2048x1x6144xf16>) 
58394:7:0 %16:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %29:<6144xf16> = torch.2_1_0.aten::detach(%16:<6144xf16>) 
58394:7:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%29:<2048x1x6144xf16>) 
58394:7:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%34:<2048x1x6144xf16>) 
58394:7:0 %42:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58394:7:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58394:7:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58394:7:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58394:7:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58394:7:0 %40:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %38:<6144xf16> = torch.2_1_0.aten::detach(%40:<6144xf16>) 
58394:7:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58391:4:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58391:4:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%9:<2048x1x6144xf16>) 
58394:7:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%28:<2048x1x6144xf16>) 
58391:4:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58394:7:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58391:4:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%22:<2048x1x6144xf16>) 
58394:7:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58391:4:0 %39:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58394:7:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58391:4:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58394:7:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58391:4:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58394:7:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58394:7:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.5:f32) 
58394:7:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.79788456:f32) 
58394:7:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%11:<2048x1x6144xf16>, 0.044715:f32) 
58394:7:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%49:<2048x1x6144xf16>, %11:<2048x1x6144xf16>) 
58391:4:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::add(%47:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%33:<2048x1x6144xf16>) 
58391:4:0 %35:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%41:<2048x1x6144xf16>, %43:<2048x1x6144xf16>) 
58391:4:0 %30:<6144xf16> = torch.2_1_0.aten::detach(%35:<6144xf16>) 
58394:7:0 %42:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%53:<2048x1x6144xf16>) 
58394:7:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58394:7:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::add(%42:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58391:4:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%30:<2048x1x6144xf16>) 
58394:7:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%37:<2048x1x6144xf16>, %55:<2048x1x6144xf16>) 
58391:4:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%9:<2048x1x6144xf16>) 
58391:4:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%38:<2048x1x6144xf16>) 
58391:4:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58391:4:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58391:4:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58391:4:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%52:<2048x1x6144xf16>) 
58391:4:0 %39:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58391:4:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58391:4:0 %43:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58391:4:0 %21:<6144xf16> = torch.2_1_0.aten::detach(%43:<6144xf16>) 
58391:4:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%21:<2048x1x6144xf16>) 
58391:4:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58391:4:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%43:<2048x1x6144xf16>) 
58391:4:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%54:<2048x1x6144xf16>) 
58391:4:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58391:4:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%58:<2048x1x6144xf16>) 
58391:4:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%60:<2048x1x6144xf16>) 
58391:4:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58391:4:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%43:<2048x1x6144xf16>, 0.5:f32) 
58391:4:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%43:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%43:<2048x1x6144xf16>, 0.044715:f32) 
58391:4:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%51:<2048x1x6144xf16>, %43:<2048x1x6144xf16>) 
58391:4:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::add(%45:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%53:<2048x1x6144xf16>, %54:<2048x1x6144xf16>) 
58391:4:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%49:<2048x1x6144xf16>) 
58391:4:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58391:4:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::add(%61:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58391:4:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%41:<2048x1x6144xf16>, %58:<2048x1x6144xf16>) 
58394:7:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %20:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %35:<6144xf16> = torch.2_1_0.aten::detach(%20:<6144xf16>) 
58394:7:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::add(%35:<6144xf16>, %52:<2048x1x6144xf16>, alpha=1:i32) 
58394:7:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%50:<2048x1x6144xf16>, 0.5:f32) 
58394:7:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%50:<2048x1x6144xf16>, 0.79788456:f32) 
58394:7:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%50:<2048x1x6144xf16>, 0.044715:f32) 
58394:7:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%44:<2048x1x6144xf16>, %50:<2048x1x6144xf16>) 
58394:7:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::add(%54:<2048x1x6144xf16>, 1:i32, 1:i32) 
58394:7:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%46:<2048x1x6144xf16>, %51:<2048x1x6144xf16>) 
58394:7:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%56:<2048x1x6144xf16>) 
58394:7:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::add(%53:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58394:7:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%52:<2048x1x6144xf16>, %54:<2048x1x6144xf16>) 
58394:7:0 %42:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58391:4:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58391:4:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%59:<2048x1x6144xf16>) 
58394:7:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58391:4:0 %48:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58391:4:0 %55:<6144xf16> = torch.2_1_0.aten::detach(%48:<6144xf16>) 
58394:7:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58394:7:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%52:<2048x1x6144xf16>) 
58394:7:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%54:<2048x1x6144xf16>) 
58394:7:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %37:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %35:<6144xf16> = torch.2_1_0.aten::detach(%37:<6144xf16>) 
58391:4:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::add(%55:<6144xf16>, %57:<2048x1x6144xf16>, alpha=1:i32) 
58391:4:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%46:<2048x1x6144xf16>, 0.5:f32) 
58391:4:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%46:<2048x1x6144xf16>, 0.79788456:f32) 
58391:4:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%46:<2048x1x6144xf16>, 0.044715:f32) 
58391:4:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%65:<2048x1x6144xf16>, %46:<2048x1x6144xf16>) 
58391:4:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::add(%66:<2048x1x6144xf16>, 1:i32, 1:i32) 
58391:4:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%55:<2048x1x6144xf16>, %67:<2048x1x6144xf16>) 
58391:4:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%48:<2048x1x6144xf16>) 
58391:4:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::add(%68:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58391:4:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%64:<2048x1x6144xf16>, %69:<2048x1x6144xf16>) 
58391:4:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%65:<2048x1x6144xf16>) 
58391:4:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58391:4:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58391:4:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%67:<2048x1x6144xf16>) 
58391:4:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%68:<2048x1x6144xf16>) 
58391:4:0 %76:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58391:4:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%69:<2048x1x6144xf16>) 
58391:4:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58391:4:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58391:4:0 %43:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58391:4:0 %54:<6144xf16> = torch.2_1_0.aten::detach(%43:<6144xf16>) 
58394:7:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58394:7:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58394:7:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58394:7:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58394:7:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58394:7:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58394:7:0 %16:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58394:7:0 %37:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %13:<6144xf16> = torch.2_1_0.aten::detach(%37:<6144xf16>) 
58394:7:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%13:<2048x1x6144xf16>) 
58394:7:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%16:<2048x1x6144xf16>) 
58394:7:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58394:7:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%65:<2048x1x6144xf16>) 
58394:7:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%66:<2048x1x6144xf16>) 
58394:7:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%68:<2048x1x6144xf16>) 
58394:7:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%64:<2048x1x6144xf16>) 
58394:7:0 %43:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58394:7:0 %64:<6144xf16> = torch.2_1_0.aten::detach(%43:<6144xf16>) 
58394:7:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58394:7:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%45:<2048x1x6144xf16>) 
58394:7:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%66:<2048x1x6144xf16>) 
58394:7:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%70:<2048x1x6144xf16>) 
58394:7:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%72:<2048x1x6144xf16>) 
58394:7:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58394:7:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%76:<2048x1x6144xf16>) 
58391:4:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%43:<2048x1x6144xf16>) 
58391:4:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%58:<2048x1x6144xf16>) 
58394:7:0 %11:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%68:<2048x1x6144xf16>) 
58391:4:0 %53:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%78:<2048x1x6144xf16>) 
58391:4:0 %79:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%65:<2048x1x6144xf16>) 
58391:4:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58391:4:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58391:4:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58394:7:0 %16:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%75:<2048x1x6144xf16>) 
58391:4:0 %76:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58391:4:0 %73:<6144xf16> = torch.2_1_0.aten::detach(%76:<6144xf16>) 
58394:7:0 %77:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58391:4:0 %82:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58391:4:0 %80:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58391:4:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%78:<2048x1x6144xf16>) 
58394:7:0 %73:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%77:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%85:<2048x1x6144xf16>) 
58391:4:0 %86:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58391:4:0 %88:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%87:<2048x1x6144xf16>) 
58391:4:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58391:4:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%81:<2048x1x6144xf16>) 
58391:4:0 %43:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58391:4:0 %53:<6144xf16> = torch.2_1_0.aten::detach(%43:<6144xf16>) 
58391:4:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%40:<2048x1x6144xf16>) 
58391:4:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%43:<2048x1x6144xf16>) 
58391:4:0 %85:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58391:4:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58391:4:0 %90:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%89:<2048x1x6144xf16>) 
58391:4:0 %92:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%91:<2048x1x6144xf16>) 
58391:4:0 %94:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%93:<2048x1x6144xf16>) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::add(%11:<2048x1x12288xf16>, %73:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %40:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %43:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %94:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58391:4:0 %90:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%94:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::add(%40:<2048x1x12288xf16>, %90:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %71, %63:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%65:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58394:7:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::add(%16:<2048x1x12288xf16>, %71:<2048x1x12288xf16>, alpha=1:i32) 
58391:4:0 %69, %85:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%92:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::add(%43:<2048x1x12288xf16>, %69:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %69:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %71:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%65:<2048x1x12288xf16>) 
58394:7:0 %50:<2048x1x12288xf16> = torch.2_1_0.aten::add(%69:<2048x1x12288xf16>, %71:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %71:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%50:<2048x1x12288xf16>) 
58394:7:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58394:7:0 %59, %69:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%49:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %73:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %68:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%82:<2048x1x12288xf16>) 
58394:7:0 %60:<2048x1x12288xf16> = torch.2_1_0.aten::add(%71:<2048x1x12288xf16>, %59:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %63:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %67:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%63:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %50:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %63:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%50:<2048x1x12288xf16>) 
58391:4:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::add(%91:<2048x1x12288xf16>, %68:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %72:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %88:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%72:<2048x1x12288xf16>) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>) 
58391:4:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58394:7:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::add(%63:<2048x1x12288xf16>, %67:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %63:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%75:<2048x1x12288xf16>) 
58394:7:0 %59:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58394:7:0 %75:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%59:<2048x1x12288xf16>) 
58391:4:0 %46, %53:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%74:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::add(%88:<2048x1x12288xf16>, %46:<2048x1x12288xf16>, alpha=1:i32) 
58391:4:0 %80:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %88:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58391:4:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::add(%95:<2048x1x12288xf16>, %88:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%97:<2048x1x12288xf16>) 
58391:4:0 %98:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58391:4:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>) 
58387:0:0 %27:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%6:<2048x1x6144xf16>) 
58387:0:0 %24:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58387:0:0 %22:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%20:<2048x1x6144xf16>) 
58387:0:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58387:0:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58387:0:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58387:0:0 %34:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %33:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%34:<2048x1x6144xf16>) 
58387:0:0 %28:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58387:0:0 %31:<6144xf16> = torch.2_1_0.aten::detach(%28:<6144xf16>) 
58387:0:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%41:<2048x1x6144xf16>) 
58387:0:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58387:0:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58387:0:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58387:0:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58387:0:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58387:0:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %8:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58387:0:0 %40:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58387:0:0 %24:<6144xf16> = torch.2_1_0.aten::detach(%40:<6144xf16>) 
58387:0:0 %11:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%24:<2048x1x6144xf16>) 
58387:0:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%7:<2048x1x6144xf16>) 
58387:0:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%49:<2048x1x6144xf16>) 
58387:0:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58387:0:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58387:0:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%27:<2048x1x6144xf16>) 
58387:0:0 %5:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58387:0:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.5:f32) 
58387:0:0 %20:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.79788456:f32) 
58387:0:0 %17:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%5:<2048x1x6144xf16>, 0.044715:f32) 
58387:0:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%17:<2048x1x6144xf16>, %5:<2048x1x6144xf16>) 
58387:0:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::add(%47:<2048x1x6144xf16>, 1:i32, 1:i32) 
58387:0:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%20:<2048x1x6144xf16>, %51:<2048x1x6144xf16>) 
58387:0:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%45:<2048x1x6144xf16>) 
58387:0:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58387:0:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::add(%44:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58387:0:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%28:<2048x1x6144xf16>, %43:<2048x1x6144xf16>) 
58387:0:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %6:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%19:<2048x1x6144xf16>) 
58389:2:0 %26:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%14:<2048x1x6144xf16>) 
58387:0:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58389:2:0 %23:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58387:0:0 %55:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%11:<2048x1x6144xf16>) 
58387:0:0 %54:<6144xf16> = torch.2_1_0.aten::detach(%55:<6144xf16>) 
58389:2:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58389:2:0 %38:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%37:<2048x1x6144xf16>) 
58389:2:0 %40:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%39:<2048x1x6144xf16>) 
58389:2:0 %32:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %31:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%32:<2048x1x6144xf16>) 
58389:2:0 %34:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %29:<6144xf16> = torch.2_1_0.aten::detach(%34:<6144xf16>) 
58389:2:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%29:<2048x1x6144xf16>) 
58389:2:0 %9:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58387:0:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::add(%54:<6144xf16>, %52:<2048x1x6144xf16>, alpha=1:i32) 
58389:2:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%35:<2048x1x6144xf16>) 
58389:2:0 %43:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%42:<2048x1x6144xf16>) 
58387:0:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%48:<2048x1x6144xf16>, 0.5:f32) 
58389:2:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58389:2:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%46:<2048x1x6144xf16>) 
58387:0:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%48:<2048x1x6144xf16>, 0.79788456:f32) 
58389:2:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58387:0:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%48:<2048x1x6144xf16>, 0.044715:f32) 
58389:2:0 %36:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %21:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%36:<2048x1x6144xf16>) 
58387:0:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%59:<2048x1x6144xf16>, %48:<2048x1x6144xf16>) 
58389:2:0 %40:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %26:<6144xf16> = torch.2_1_0.aten::detach(%40:<6144xf16>) 
58387:0:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::add(%60:<2048x1x6144xf16>, 1:i32, 1:i32) 
58387:0:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%54:<2048x1x6144xf16>, %61:<2048x1x6144xf16>) 
58387:0:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%50:<2048x1x6144xf16>) 
58389:2:0 %35:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%26:<2048x1x6144xf16>) 
58387:0:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::add(%62:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58389:2:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%29:<2048x1x6144xf16>) 
58389:2:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%25:<2048x1x6144xf16>) 
58387:0:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%58:<2048x1x6144xf16>, %63:<2048x1x6144xf16>) 
58389:2:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%51:<2048x1x6144xf16>) 
58389:2:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%53:<2048x1x6144xf16>) 
58387:0:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%59:<2048x1x6144xf16>) 
58389:2:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%55:<2048x1x6144xf16>) 
58387:0:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58389:2:0 %58:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%57:<2048x1x6144xf16>) 
58387:0:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%54:<2048x1x6144xf16>) 
58387:0:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58387:0:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58387:0:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%58:<2048x1x6144xf16>) 
58389:2:0 %37:<2048x1x6144xf16> = torch.2_1_0.aten::add(%3:<6144xf16>, %4:<2048x1x6144xf16>, alpha=1:i32) 
58387:0:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58389:2:0 %29:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%37:<2048x1x6144xf16>, 0.5:f32) 
58387:0:0 %28:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%28:<2048x1x6144xf16>) 
58387:0:0 %20:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%37:<2048x1x6144xf16>, 0.79788456:f32) 
58387:0:0 %56:<6144xf16> = torch.2_1_0.aten::detach(%20:<6144xf16>) 
58389:2:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%37:<2048x1x6144xf16>, 0.044715:f32) 
58389:2:0 %41:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%47:<2048x1x6144xf16>, %37:<2048x1x6144xf16>) 
58389:2:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::add(%41:<2048x1x6144xf16>, 1:i32, 1:i32) 
58389:2:0 %45:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%49:<2048x1x6144xf16>, %51:<2048x1x6144xf16>) 
58389:2:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%45:<2048x1x6144xf16>) 
58389:2:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%44:<2048x1x6144xf16>) 
58389:2:0 %55:<2048x1x6144xf16> = torch.2_1_0.aten::add(%44:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58389:2:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%29:<2048x1x6144xf16>, %55:<2048x1x6144xf16>) 
58389:2:0 %56:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %54:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%56:<2048x1x6144xf16>) 
58389:2:0 %58:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %50:<6144xf16> = torch.2_1_0.aten::detach(%58:<6144xf16>) 
58389:2:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::add(%50:<6144xf16>, %54:<2048x1x6144xf16>, alpha=1:i32) 
58389:2:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.5:f32) 
58389:2:0 %50:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.79788456:f32) 
58389:2:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%61:<2048x1x6144xf16>, 0.044715:f32) 
58389:2:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%63:<2048x1x6144xf16>, %61:<2048x1x6144xf16>) 
58389:2:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::add(%64:<2048x1x6144xf16>, 1:i32, 1:i32) 
58389:2:0 %52:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%50:<2048x1x6144xf16>, %65:<2048x1x6144xf16>) 
58389:2:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::tanh(%52:<2048x1x6144xf16>) 
58389:2:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::add(%66:<2048x1x6144xf16>, 1.0:f32, 1:i32) 
58389:2:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::mul(%62:<2048x1x6144xf16>, %67:<2048x1x6144xf16>) 
58389:2:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58389:2:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%61:<2048x1x6144xf16>) 
58389:2:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%50:<2048x1x6144xf16>) 
58389:2:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%65:<2048x1x6144xf16>) 
58389:2:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%66:<2048x1x6144xf16>) 
58389:2:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58389:2:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%67:<2048x1x6144xf16>) 
58389:2:0 %60:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%60:<2048x1x6144xf16>) 
58389:2:0 %60:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %51:<6144xf16> = torch.2_1_0.aten::detach(%60:<6144xf16>) 
58387:0:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%5:<2048x1x6144xf16>) 
58387:0:0 %61:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58387:0:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58387:0:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%72:<2048x1x6144xf16>) 
58387:0:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58387:0:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%76:<2048x1x6144xf16>) 
58387:0:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %64:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%70:<2048x1x6144xf16>) 
58387:0:0 %71:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58387:0:0 %68:<6144xf16> = torch.2_1_0.aten::detach(%71:<6144xf16>) 
58387:0:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%17:<2048x1x6144xf16>) 
58387:0:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%66:<2048x1x6144xf16>) 
58387:0:0 %72:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%74:<2048x1x6144xf16>) 
58387:0:0 %78:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58387:0:0 %80:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%79:<2048x1x6144xf16>) 
58387:0:0 %82:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%81:<2048x1x6144xf16>) 
58387:0:0 %84:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%83:<2048x1x6144xf16>) 
58387:0:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58387:0:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%73:<2048x1x6144xf16>) 
58387:0:0 %77:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58387:0:0 %63:<6144xf16> = torch.2_1_0.aten::detach(%77:<6144xf16>) 
58387:0:0 %83:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58387:0:0 %62:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%79:<2048x1x6144xf16>) 
58387:0:0 %85:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%66:<2048x1x6144xf16>) 
58387:0:0 %86:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%48:<2048x1x6144xf16>) 
58387:0:0 %88:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%87:<2048x1x6144xf16>) 
58387:0:0 %90:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%89:<2048x1x6144xf16>) 
58387:0:0 %92:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%91:<2048x1x6144xf16>) 
58387:0:0 %5:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58387:0:0 %17:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58387:0:0 %51:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58387:0:0 %90:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%51:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58389:2:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%76:<2048x1x6144xf16>) 
58389:2:0 %59:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%30:<2048x1x6144xf16>) 
58389:2:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58389:2:0 %65:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%49:<2048x1x6144xf16>) 
58389:2:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%63:<2048x1x6144xf16>) 
58389:2:0 %79:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%78:<2048x1x6144xf16>) 
58389:2:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%80:<2048x1x6144xf16>) 
58389:2:0 %75:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %68:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%75:<2048x1x6144xf16>) 
58389:2:0 %75:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %72:<6144xf16> = torch.2_1_0.aten::detach(%75:<6144xf16>) 
58389:2:0 %73:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%30:<2048x1x6144xf16>) 
58389:2:0 %70:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%71:<2048x1x6144xf16>) 
58389:2:0 %69:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58389:2:0 %74:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%83:<2048x1x6144xf16>) 
58389:2:0 %63:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%62:<2048x1x6144xf16>) 
58389:2:0 %67:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%84:<2048x1x6144xf16>) 
58389:2:0 %86:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%85:<2048x1x6144xf16>) 
58389:2:0 %81:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%4:<2048x1x6144xf16>) 
58389:2:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%81:<2048x1x6144xf16>) 
58389:2:0 %76:<6144xf16> = torch.2_1_0.aten::detach(%3:<6144xf16>) 
58389:2:0 %81:<6144xf16> = torch.2_1_0.aten::detach(%76:<6144xf16>) 
58389:2:0 %77:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%76:<2048x1x6144xf16>) 
58389:2:0 %66:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%87:<2048x1x6144xf16>) 
58389:2:0 %85:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%47:<2048x1x6144xf16>) 
58389:2:0 %71:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%82:<2048x1x6144xf16>) 
58389:2:0 %49:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%88:<2048x1x6144xf16>) 
58389:2:0 %90:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%89:<2048x1x6144xf16>) 
58389:2:0 %91:<2048x1x6144xf16> = torch.2_1_0.aten::detach(%15:<2048x1x6144xf16>) 
58389:2:0 %4:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58389:2:0 %3:<2048x1x12288xf16> = torch.2_1_0.aten::rand(list{2048:i32, 1:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58389:2:0 %30:<12288xf16> = torch.2_1_0.aten::rand(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:device, pin_memory=False:pred) 
58389:2:0 %15:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::expand(%30:<12288xf16>, list{2048:i32, 1:i32, 12288:i32}, implicit=False:pred) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::add(%4:<2048x1x12288xf16>, %15:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::add(%5:<2048x1x12288xf16>, %90:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %87, %90:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%68:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58389:2:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::add(%3:<2048x1x12288xf16>, %87:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %85, %86:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%92:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::add(%17:<2048x1x12288xf16>, %85:<2048x1x12288xf16>, alpha=1:i32) 
58389:2:0 %84:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %71:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%84:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::add(%68:<2048x1x12288xf16>, %71:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %71:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58387:0:0 %80:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %84:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %72:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%72:<2048x1x12288xf16>) 
58387:0:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::add(%65:<2048x1x12288xf16>, %84:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%87:<2048x1x12288xf16>) 
58387:0:0 %48:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58387:0:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%48:<2048x1x12288xf16>) 
58389:2:0 %83, %49:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%68:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58387:0:0 %66, %93:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%82:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58389:2:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::add(%71:<2048x1x12288xf16>, %83:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %89:<2048x1x12288xf16> = torch.2_1_0.aten::add(%65:<2048x1x12288xf16>, %66:<2048x1x12288xf16>, alpha=1:i32) 
58389:2:0 %84:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %78:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %63:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %85:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%84:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58389:2:0 %90:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58389:2:0 %90:<2048x1x12288xf16> = torch.2_1_0.aten::add(%68:<2048x1x12288xf16>, %85:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%65:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58387:0:0 %86:<2048x1x12288xf16> = torch.2_1_0.aten::add(%83:<2048x1x12288xf16>, %63:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%94:<2048x1x12288xf16>) 
58387:0:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58387:0:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%95:<2048x1x12288xf16>) 
58388:1:0 %52:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %70:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%52:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58388:1:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::add(%94:<2048x1x12288xf16>, %70:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %70:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%94:<2048x1x12288xf16>) 
58388:1:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58388:1:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%95:<2048x1x12288xf16>) 
58388:1:0 %70:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %95:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%70:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %48:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%48:<2048x1x12288xf16>) 
58388:1:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::add(%96:<2048x1x12288xf16>, %95:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %98:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>) 
58388:1:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%97:<2048x1x12288xf16>) 
58388:1:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%99:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::add(%56:<2048x1x12288xf16>, %86:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58388:1:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58388:1:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%100:<2048x1x12288xf16>) 
58388:1:0 %99:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %100:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%99:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%101:<2048x1x12288xf16>) 
58388:1:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::add(%102:<2048x1x12288xf16>, %100:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%104:<2048x1x12288xf16>) 
58388:1:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>) 
58388:1:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %106:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58388:1:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::add(%96:<2048x1x12288xf16>, %106:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58388:1:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58388:1:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58388:1:0 %106:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %107:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%106:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58388:1:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::add(%102:<2048x1x12288xf16>, %107:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58388:1:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%109:<2048x1x12288xf16>) 
58388:1:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58388:1:0 %108:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %105:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>{0,12288,1}) 
58388:1:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%56:<2048x1x12288xf16>) 
58388:1:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58388:1:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::add(%108:<2048x1x12288xf16>, %105:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58388:1:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58388:1:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58388:1:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58388:1:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58388:1:0 %56:<1xf64> = torch.2_1_0.aten::lift_fresh(%56:<1xf64>) 
58388:None:0 list{%56:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%56:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %7:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %8:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::add(%7:<2048x1x12288xf16>, %8:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %14:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%8:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::add(%8:<2048x1x12288xf16>, %14:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x1x12288xf16>) 
58392:5:0 %24:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%24:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::add(%27:<2048x1x12288xf16>, %1:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %14:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%8:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::add(%8:<2048x1x12288xf16>, %14:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x1x12288xf16>) 
58392:5:0 %25:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %8:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%25:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %7:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::add(%15:<2048x1x12288xf16>, %7:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %23:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%23:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%26:<2048x1x12288xf16>) 
58392:5:0 %23:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %23:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::add(%7:<2048x1x12288xf16>, %23:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %7:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>) 
58392:5:0 %23:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%7:<2048x1x12288xf16>) 
58392:5:0 %26:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%1:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %15:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%26:<2048x1x12288xf16>{0,12288,1}) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%27:<2048x1x12288xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58392:5:0 %14:<2048x1x12288xf16> = torch.2_1_0.aten::add(%26:<2048x1x12288xf16>, %15:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%20:<2048x1x12288xf16>) 
58392:5:0 %15:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%26:<2048x1x12288xf16>) 
58392:5:0 %23:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%14:<2048x1x12288xf16>) 
58392:5:0 %26:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%23:<2048x1x12288xf16>) 
58393:6:0 %57:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %78:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%57:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %89:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%89:<2048x1x12288xf16>) 
58393:6:0 %88:<2048x1x12288xf16> = torch.2_1_0.aten::add(%87:<2048x1x12288xf16>, %78:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %90:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %87:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>) 
58393:6:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%88:<2048x1x12288xf16>) 
58393:6:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58393:6:0 %92:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %87:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%92:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%93:<2048x1x12288xf16>) 
58393:6:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::add(%94:<2048x1x12288xf16>, %87:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%95:<2048x1x12288xf16>) 
58393:6:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58393:6:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58393:6:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::add(%41:<2048x1x12288xf16>, %55:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %98:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>) 
58393:6:0 %64:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%97:<2048x1x12288xf16>) 
58393:6:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%64:<2048x1x12288xf16>) 
58393:6:0 %99:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %94:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%99:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58393:6:0 %64:<2048x1x12288xf16> = torch.2_1_0.aten::add(%100:<2048x1x12288xf16>, %94:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%101:<2048x1x12288xf16>) 
58393:6:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%64:<2048x1x12288xf16>) 
58393:6:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58393:6:0 %103:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %100:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%104:<2048x1x12288xf16>) 
58393:6:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::add(%105:<2048x1x12288xf16>, %100:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58393:6:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%106:<2048x1x12288xf16>) 
58393:6:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58393:6:0 %102:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %100:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %57:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%57:<2048x1x12288xf16>) 
58393:6:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::add(%105:<2048x1x12288xf16>, %100:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58393:6:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58393:6:0 %57:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58393:6:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%57:<2048x1x12288xf16>) 
58393:6:0 %105:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %110:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%41:<2048x1x12288xf16>) 
58393:6:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%111:<2048x1x12288xf16>) 
58393:6:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::add(%74:<2048x1x12288xf16>, %110:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58392:5:0 %27:<1xf64> = torch.2_1_0.aten::lift_fresh(%27:<1xf64>) 
58393:6:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58393:6:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%112:<2048x1x12288xf16>) 
58393:6:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%109:<2048x1x12288xf16>) 
58393:6:0 %114:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%113:<2048x1x12288xf16>) 
58392:None:0 list{%27:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%27:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %103:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %101:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58390:3:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::add(%106:<2048x1x12288xf16>, %101:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58390:3:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>) 
58390:3:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58390:3:0 %78:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %108:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %18:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58390:3:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::add(%18:<2048x1x12288xf16>, %108:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>) 
58390:3:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58390:3:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58390:3:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::add(%86:<2048x1x12288xf16>, %98:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58390:3:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>) 
58390:3:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58390:3:0 %78:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %22:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %18:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%18:<2048x1x12288xf16>) 
58390:3:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::add(%112:<2048x1x12288xf16>, %22:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58390:3:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58390:3:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58390:3:0 %108:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58393:6:0 %109:<1xf64> = torch.2_1_0.aten::lift_fresh(%109:<1xf64>) 
58390:3:0 %18:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58393:None:0 list{%109:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%109:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::add(%113:<2048x1x12288xf16>, %18:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%112:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58390:3:0 %112:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %22:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%112:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58390:3:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::add(%113:<2048x1x12288xf16>, %22:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %114:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58390:3:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58390:3:0 %108:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %18:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>{0,12288,1}) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58390:3:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::add(%113:<2048x1x12288xf16>, %18:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58390:3:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58390:3:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58390:3:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%112:<2048x1x12288xf16>) 
58390:3:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58390:3:0 %86:<1xf64> = torch.2_1_0.aten::lift_fresh(%86:<1xf64>) 
58390:None:0 list{%86:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%86:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %85:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %100:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%85:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %98:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::add(%101:<2048x1x12288xf16>, %100:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %53:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58391:4:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>) 
58391:4:0 %104:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %105:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%104:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%101:<2048x1x12288xf16>) 
58391:4:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::add(%106:<2048x1x12288xf16>, %105:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58391:4:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>) 
58391:4:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::add(%40:<2048x1x12288xf16>, %90:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%111:<2048x1x12288xf16>) 
58391:4:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%113:<2048x1x12288xf16>) 
58391:4:0 %111:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %114:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%111:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %115:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %116:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%115:<2048x1x12288xf16>) 
58391:4:0 %115:<2048x1x12288xf16> = torch.2_1_0.aten::add(%116:<2048x1x12288xf16>, %114:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %117:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %118:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x1x12288xf16>) 
58391:4:0 %119:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%115:<2048x1x12288xf16>) 
58391:4:0 %120:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%119:<2048x1x12288xf16>) 
58391:4:0 %121:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %115:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%121:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %122:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::add(%122:<2048x1x12288xf16>, %115:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %122:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %120:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%122:<2048x1x12288xf16>) 
58391:4:0 %122:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %123:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%122:<2048x1x12288xf16>) 
58391:4:0 %124:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %73:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%124:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %121:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %123:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%121:<2048x1x12288xf16>) 
58391:4:0 %115:<2048x1x12288xf16> = torch.2_1_0.aten::add(%123:<2048x1x12288xf16>, %73:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %125:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%125:<2048x1x12288xf16>) 
58391:4:0 %121:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%115:<2048x1x12288xf16>) 
58391:4:0 %122:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%121:<2048x1x12288xf16>) 
58391:4:0 %124:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %121:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%124:<2048x1x12288xf16>{0,12288,1}) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%40:<2048x1x12288xf16>) 
58391:4:0 %123:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %118:<2048x1x12288xf16> = torch.2_1_0.aten::add(%123:<2048x1x12288xf16>, %121:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58391:4:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%43:<2048x1x12288xf16>) 
58391:4:0 %122:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58391:4:0 %126:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x1x12288xf16>) 
58391:4:0 %123:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%126:<2048x1x12288xf16>) 
58391:4:0 %43:<1xf64> = torch.2_1_0.aten::lift_fresh(%43:<1xf64>) 
58391:None:0 list{%43:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%43:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %49:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %75:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %51:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %50:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%51:<2048x1x12288xf16>) 
58394:7:0 %59:<2048x1x12288xf16> = torch.2_1_0.aten::add(%50:<2048x1x12288xf16>, %75:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %67:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%67:<2048x1x12288xf16>) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%59:<2048x1x12288xf16>) 
58394:7:0 %62:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58394:7:0 %62:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %65:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%62:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %70:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58394:7:0 %76:<2048x1x12288xf16> = torch.2_1_0.aten::add(%70:<2048x1x12288xf16>, %65:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>) 
58394:7:0 %70:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%76:<2048x1x12288xf16>) 
58394:7:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%70:<2048x1x12288xf16>) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::add(%11:<2048x1x12288xf16>, %73:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %72:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %62:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%72:<2048x1x12288xf16>) 
58394:7:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%65:<2048x1x12288xf16>) 
58394:7:0 %66:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>) 
58394:7:0 %66:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %47:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%66:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %66:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58394:7:0 %45:<2048x1x12288xf16> = torch.2_1_0.aten::add(%66:<2048x1x12288xf16>, %47:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %76:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%76:<2048x1x12288xf16>) 
58394:7:0 %66:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%45:<2048x1x12288xf16>) 
58394:7:0 %76:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%66:<2048x1x12288xf16>) 
58394:7:0 %47:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %66:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58394:7:0 %50:<2048x1x12288xf16> = torch.2_1_0.aten::add(%69:<2048x1x12288xf16>, %66:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %45:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%44:<2048x1x12288xf16>) 
58394:7:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%50:<2048x1x12288xf16>) 
58394:7:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%79:<2048x1x12288xf16>) 
58394:7:0 %80:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %79:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%44:<2048x1x12288xf16>) 
58394:7:0 %81:<2048x1x12288xf16> = torch.2_1_0.aten::add(%80:<2048x1x12288xf16>, %79:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %79:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58394:7:0 %80:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%81:<2048x1x12288xf16>) 
58394:7:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%80:<2048x1x12288xf16>) 
58394:7:0 %79:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %80:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%79:<2048x1x12288xf16>{0,12288,1}) 
58394:7:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%11:<2048x1x12288xf16>) 
58394:7:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%82:<2048x1x12288xf16>) 
58394:7:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::add(%44:<2048x1x12288xf16>, %80:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58394:7:0 %84:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%16:<2048x1x12288xf16>) 
58394:7:0 %81:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%84:<2048x1x12288xf16>) 
58394:7:0 %85:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>) 
58394:7:0 %86:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%85:<2048x1x12288xf16>) 
58394:7:0 %11:<1xf64> = torch.2_1_0.aten::lift_fresh(%11:<1xf64>) 
58394:None:0 list{%11:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%11:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %83:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %96:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%95:<2048x1x12288xf16>) 
58387:0:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::add(%93:<2048x1x12288xf16>, %96:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>) 
58387:0:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58387:0:0 %98:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%97:<2048x1x12288xf16>) 
58387:0:0 %98:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %93:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%98:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %99:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%99:<2048x1x12288xf16>) 
58387:0:0 %97:<2048x1x12288xf16> = torch.2_1_0.aten::add(%100:<2048x1x12288xf16>, %93:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>) 
58387:0:0 %77:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%97:<2048x1x12288xf16>) 
58387:0:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%77:<2048x1x12288xf16>) 
58387:0:0 %101:<2048x1x12288xf16> = torch.2_1_0.aten::add(%5:<2048x1x12288xf16>, %90:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %96:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%96:<2048x1x12288xf16>) 
58387:0:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%101:<2048x1x12288xf16>) 
58387:0:0 %77:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58387:0:0 %77:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %100:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%77:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %103:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%103:<2048x1x12288xf16>) 
58387:0:0 %102:<2048x1x12288xf16> = torch.2_1_0.aten::add(%104:<2048x1x12288xf16>, %100:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %104:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x1x12288xf16>) 
58387:0:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%102:<2048x1x12288xf16>) 
58387:0:0 %107:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%106:<2048x1x12288xf16>) 
58387:0:0 %107:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %104:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%107:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%100:<2048x1x12288xf16>) 
58387:0:0 %106:<2048x1x12288xf16> = torch.2_1_0.aten::add(%83:<2048x1x12288xf16>, %104:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %108:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %109:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%108:<2048x1x12288xf16>) 
58387:0:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%106:<2048x1x12288xf16>) 
58387:0:0 %55:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58387:0:0 %55:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %106:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %78:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%78:<2048x1x12288xf16>) 
58387:0:0 %55:<2048x1x12288xf16> = torch.2_1_0.aten::add(%83:<2048x1x12288xf16>, %106:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %100:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%100:<2048x1x12288xf16>) 
58387:0:0 %111:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%55:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%111:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %91:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%83:<2048x1x12288xf16>{0,12288,1}) 
58387:0:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%5:<2048x1x12288xf16>) 
58387:0:0 %112:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58387:0:0 %110:<2048x1x12288xf16> = torch.2_1_0.aten::add(%112:<2048x1x12288xf16>, %91:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58387:0:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%17:<2048x1x12288xf16>) 
58387:0:0 %83:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58387:0:0 %89:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%110:<2048x1x12288xf16>) 
58387:0:0 %113:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%89:<2048x1x12288xf16>) 
58387:0:0 %5:<1xf64> = torch.2_1_0.aten::lift_fresh(%5:<1xf64>) 
58387:None:0 list{%5:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%5:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %90:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %68:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%90:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %86:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%86:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::add(%91:<2048x1x12288xf16>, %68:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %71:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%71:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58389:2:0 %63:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %63:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::add(%91:<2048x1x12288xf16>, %63:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %63:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58389:2:0 %69:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%69:<2048x1x12288xf16>) 
58389:2:0 %63:<2048x1x12288xf16> = torch.2_1_0.aten::add(%4:<2048x1x12288xf16>, %15:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %66:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%66:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%63:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %68:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::add(%91:<2048x1x12288xf16>, %68:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%91:<2048x1x12288xf16>) 
58389:2:0 %89:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58389:2:0 %91:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%89:<2048x1x12288xf16>) 
58389:2:0 %68:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %89:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%68:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>) 
58389:2:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::add(%73:<2048x1x12288xf16>, %89:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %88:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%88:<2048x1x12288xf16>) 
58389:2:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%74:<2048x1x12288xf16>) 
58389:2:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%82:<2048x1x12288xf16>) 
58389:2:0 %73:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %49:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>) 
58389:2:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::add(%73:<2048x1x12288xf16>, %49:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%73:<2048x1x12288xf16>) 
58389:2:0 %82:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>) 
58389:2:0 %73:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%82:<2048x1x12288xf16>) 
58389:2:0 %49:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%15:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %82:<2048x1x12288xf16>{0,12288,1} = torch.2_1_0.aten::detach(%49:<2048x1x12288xf16>{0,12288,1}) 
58389:2:0 %81:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%4:<2048x1x12288xf16>) 
58389:2:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%81:<2048x1x12288xf16>) 
58389:2:0 %47:<2048x1x12288xf16> = torch.2_1_0.aten::add(%92:<2048x1x12288xf16>, %82:<2048x1x12288xf16>{0,12288,1}, alpha=1:i32) 
58389:2:0 %93:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%3:<2048x1x12288xf16>) 
58389:2:0 %92:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%93:<2048x1x12288xf16>) 
58389:2:0 %94:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%47:<2048x1x12288xf16>) 
58389:2:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%94:<2048x1x12288xf16>) 
58389:2:0 %30:<1xf64> = torch.2_1_0.aten::lift_fresh(%30:<1xf64>) 
58389:None:0 list{%30:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%30:<1xf64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%86:<1xf64>) 
58387:0:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%5:<1xf64>) 
58392:5:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%27:<1xf64>) 
58394:7:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%11:<1xf64>) 
58393:6:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%109:<1xf64>) 
58388:1:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%56:<1xf64>) 
58389:2:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%30:<1xf64>) 
58390:3:0 %4:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %113:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %26:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %81:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %74:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %96:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%4:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%113:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 %63:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%26:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%74:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%81:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 1720757894.23714:f32: = torch.2_1_0.aten::_local_scalar_dense(%43:<1xf64>) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%63:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %115:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%115:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%96:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %108:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %14:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %114:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %86:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %22:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %112:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %77:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%108:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%14:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%114:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%86:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%22:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%112:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%77:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %118:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%118:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %115:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %115:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %25:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %37:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %115:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %116:<6400x12288xf16> = torch.2_1_0.aten::normal_(%116:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58390:3:0 %116:<6400x12288xf16> = torch.2_1_0.aten::normal_(%116:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:5:0 %24:<6400x12288xf16> = torch.2_1_0.aten::normal_(%24:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58393:None:0 %117:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:None:0 %117:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %12:<6400x12288xf16> = torch.2_1_0.aten::normal_(%12:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58387:0:0 %116:<6400x12288xf16> = torch.2_1_0.aten::normal_(%116:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:None:0 %7:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:None:0 %37:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:None:0 %40:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %97:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %98:<6400x12288xf16> = torch.2_1_0.aten::normal_(%98:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58391:4:0 %3:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:None:0 %12:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %40:<6400x12288xf16> = torch.2_1_0.aten::empty(list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %106:<6400x12288xf16> = torch.2_1_0.aten::normal_(%106:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58391:None:0 %1:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %113:<6400x12288xf16> = torch.2_1_0.aten::normal_(%113:<6400x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:None:0 %42:<2048x12288xf32> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:None:0 %114:<2048x12288xf32> = torch.2_1_0.aten::normal_(%114:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58392:None:0 %36:<2048x12288xf32> = torch.2_1_0.aten::normal_(%36:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58394:None:0 %8:<2048x12288xf32> = torch.2_1_0.aten::normal_(%8:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58393:None:0 %118:<2048x12288xf32> = torch.2_1_0.aten::normal_(%118:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58387:None:0 %117:<2048x12288xf32> = torch.2_1_0.aten::normal_(%117:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58390:None:0 %118:<2048x12288xf32> = torch.2_1_0.aten::normal_(%118:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58389:None:0 %34:<2048x12288xf32> = torch.2_1_0.aten::normal_(%34:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58391:None:0 %127:<2048x12288xf32> = torch.2_1_0.aten::normal_(%127:<2048x12288xf32>, 0.0:f32, 1.0:f32, generator=None:NoneType) 
58388:None:0 %114:<2048x12288xf32> = torch.2_1_0.aten::normal_(%114:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:None:0 %115:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:None:0 %116:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:None:0 %117:<12288xf32> = torch.2_1_0.aten::fill_(%117:<12288xf32>, 1.0:f32) 
58388:None:0 %118:<12288xf32> = torch.2_1_0.aten::zero_(%118:<12288xf32>) 
58388:1:0 %41:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %23:<12288x1536xf16> = torch.2_1_0.aten::normal_(%23:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58388:1:0 %9:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %115:<12288xf16> = torch.2_1_0.aten::zero_(%115:<12288xf16>) 
58388:1:0 %107:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %119:<4608x12288xf16> = torch.2_1_0.aten::normal_(%119:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %63:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %12:<4608xf16> = torch.2_1_0.aten::zero_(%12:<4608xf16>) 
58388:None:0 %72:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:None:0 %87:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:None:0 %63:<12288xf32> = torch.2_1_0.aten::fill_(%63:<12288xf32>, 1.0:f32) 
58388:None:0 %104:<12288xf32> = torch.2_1_0.aten::zero_(%104:<12288xf32>) 
58388:1:0 %83:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %120:<6144x12288xf16> = torch.2_1_0.aten::normal_(%120:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %83:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %107:<6144xf16> = torch.2_1_0.aten::zero_(%107:<6144xf16>) 
58388:1:0 %87:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %65:<12288x6144xf16> = torch.2_1_0.aten::normal_(%65:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58388:1:0 %51:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %48:<12288xf16> = torch.2_1_0.aten::zero_(%48:<12288xf16>) 
58388:1:0 %121:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %51:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %122:<12288xf16> = torch.2_1_0.aten::fill_(%122:<12288xf16>, 1.0:f32) 
58388:1:0 %123:<12288xf16> = torch.2_1_0.aten::zero_(%123:<12288xf16>) 
58392:None:0 %36:<2048x12288xf32> = torch.2_1_0.aten::normal_(%36:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:None:0 %11:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:None:0 %13:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:None:0 %7:<12288xf32> = torch.2_1_0.aten::fill_(%7:<12288xf32>, 1.0:f32) 
58392:None:0 %11:<12288xf32> = torch.2_1_0.aten::zero_(%11:<12288xf32>) 
58392:5:0 %13:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %17:<12288x1536xf16> = torch.2_1_0.aten::normal_(%17:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58392:5:0 %6:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %13:<12288xf16> = torch.2_1_0.aten::zero_(%13:<12288xf16>) 
58392:5:0 %8:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %37:<4608x12288xf16> = torch.2_1_0.aten::normal_(%37:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:5:0 %21:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %8:<4608xf16> = torch.2_1_0.aten::zero_(%8:<4608xf16>) 
58392:None:0 %16:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:None:0 %33:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:None:0 %21:<12288xf32> = torch.2_1_0.aten::fill_(%21:<12288xf32>, 1.0:f32) 
58393:None:0 %118:<2048x12288xf32> = torch.2_1_0.aten::normal_(%118:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:None:0 %16:<12288xf32> = torch.2_1_0.aten::zero_(%16:<12288xf32>) 
58388:1:0 %51:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%114:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:None:0 %34:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %35:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:None:0 %119:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %30:<6144x12288xf16> = torch.2_1_0.aten::normal_(%30:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58393:None:0 %2:<12288xf32> = torch.2_1_0.aten::fill_(%2:<12288xf32>, 1.0:f32) 
58392:5:0 %35:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:None:0 %45:<12288xf32> = torch.2_1_0.aten::zero_(%45:<12288xf32>) 
58392:5:0 %6:<6144xf16> = torch.2_1_0.aten::zero_(%6:<6144xf16>) 
58392:5:0 %33:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %31:<12288x6144xf16> = torch.2_1_0.aten::normal_(%31:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58393:6:0 %19:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %32:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %33:<12288xf16> = torch.2_1_0.aten::zero_(%33:<12288xf16>) 
58393:6:0 %120:<12288x1536xf16> = torch.2_1_0.aten::normal_(%120:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58393:6:0 %67:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %32:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %29:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %38:<12288xf16> = torch.2_1_0.aten::fill_(%38:<12288xf16>, 1.0:f32) 
58392:5:0 %34:<12288xf16> = torch.2_1_0.aten::zero_(%34:<12288xf16>) 
58393:6:0 %121:<12288xf16> = torch.2_1_0.aten::zero_(%121:<12288xf16>) 
58393:6:0 %73:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %80:<4608x12288xf16> = torch.2_1_0.aten::normal_(%80:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %95:<12288xf32> = torch.2_1_0.aten::_to_copy(%117:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %100:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %73:<4608xf16> = torch.2_1_0.aten::zero_(%73:<4608xf16>) 
58388:1:0 %124:<12288xf32> = torch.2_1_0.aten::_to_copy(%118:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:None:0 %122:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %95:<12288xf32> = torch.2_1_0.aten::_to_copy(%63:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:None:0 %91:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %125:<12288xf32> = torch.2_1_0.aten::_to_copy(%104:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:None:0 %123:<12288xf32> = torch.2_1_0.aten::fill_(%123:<12288xf32>, 1.0:f32) 
58393:None:0 %124:<12288xf32> = torch.2_1_0.aten::zero_(%124:<12288xf32>) 
58393:6:0 %125:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %63:<6144x12288xf16> = torch.2_1_0.aten::normal_(%63:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58393:6:0 %112:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %98:<6144xf16> = torch.2_1_0.aten::zero_(%98:<6144xf16>) 
58393:6:0 %83:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %47:<12288x6144xf16> = torch.2_1_0.aten::normal_(%47:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58393:6:0 %114:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %77:<12288xf16> = torch.2_1_0.aten::zero_(%77:<12288xf16>) 
58393:6:0 %102:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %113:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %126:<12288xf16> = torch.2_1_0.aten::fill_(%126:<12288xf16>, 1.0:f32) 
58393:6:0 %94:<12288xf16> = torch.2_1_0.aten::zero_(%94:<12288xf16>) 
58392:5:0 %39:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%36:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:None:0 %8:<2048x12288xf32> = torch.2_1_0.aten::normal_(%8:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %126:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%114:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %125:<12288xf16> = torch.2_1_0.aten::_to_copy(%117:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:None:0 %117:<2048x12288xf32> = torch.2_1_0.aten::normal_(%117:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %127:<12288xf16> = torch.2_1_0.aten::_to_copy(%118:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:None:0 %5:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %125:<12288xf16> = torch.2_1_0.aten::_to_copy(%63:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %126:<12288xf16> = torch.2_1_0.aten::_to_copy(%104:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:None:0 %78:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:None:0 %2:<12288xf32> = torch.2_1_0.aten::fill_(%2:<12288xf32>, 1.0:f32) 
58387:None:0 %118:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:None:0 %87:<12288xf32> = torch.2_1_0.aten::zero_(%87:<12288xf32>) 
58388:1:0 %35:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58387:None:0 %119:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %128:<12288xf16> = torch.2_1_0.aten::detach(%122:<12288xf16>) 
58388:1:0 %128:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58388:1:0 %95:<12288x6144xf16> = torch.2_1_0.aten::detach(%65:<12288x6144xf16>) 
58388:1:0 %95:<6144xf16> = torch.2_1_0.aten::detach(%107:<6144xf16>) 
58387:None:0 %120:<12288xf32> = torch.2_1_0.aten::fill_(%120:<12288xf32>, 1.0:f32) 
58388:1:0 %125:<6144x12288xf16> = torch.2_1_0.aten::detach(%120:<6144x12288xf16>) 
58388:1:0 %127:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58387:None:0 %121:<12288xf32> = torch.2_1_0.aten::zero_(%121:<12288xf32>) 
58388:1:0 %95:<12288xf16> = torch.2_1_0.aten::detach(%63:<12288xf16>) 
58388:1:0 %95:<4608xf16> = torch.2_1_0.aten::detach(%12:<4608xf16>) 
58388:1:0 %126:<4608x12288xf16> = torch.2_1_0.aten::detach(%119:<4608x12288xf16>) 
58388:1:0 %127:<12288xf16> = torch.2_1_0.aten::detach(%115:<12288xf16>) 
58388:1:0 %124:<12288x1536xf16> = torch.2_1_0.aten::detach(%23:<12288x1536xf16>) 
58388:1:0 %94:<12288xf16> = torch.2_1_0.aten::detach(%118:<12288xf16>) 
58388:1:0 %127:<12288xf16> = torch.2_1_0.aten::detach(%117:<12288xf16>) 
58388:1:0 %94:<2048x12288xf16> = torch.2_1_0.aten::detach(%114:<2048x12288xf16>) 
58388:1:0 %129:<6400x12288xf16> = torch.2_1_0.aten::detach(%113:<6400x12288xf16>) 
58394:7:0 %74:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %50:<12288x1536xf16> = torch.2_1_0.aten::normal_(%50:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58394:7:0 %74:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %122:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %40:<12288xf32> = torch.2_1_0.aten::_to_copy(%7:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %127:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred) 
58387:0:0 %1:<12288x1536xf16> = torch.2_1_0.aten::normal_(%1:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58392:5:0 %41:<12288xf32> = torch.2_1_0.aten::_to_copy(%11:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %130:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58390:None:0 %118:<2048x12288xf32> = torch.2_1_0.aten::normal_(%118:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58387:0:0 %35:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %131:<12288xf16> = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58392:5:0 %42:<12288xf32> = torch.2_1_0.aten::_to_copy(%21:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %132:<12288xf16> = torch.2_1_0.aten::view(%131:<12288xf16>, list{12288:i32}) 
58392:5:0 %43:<12288xf32> = torch.2_1_0.aten::_to_copy(%16:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %133:<12288xf16> = torch.2_1_0.aten::detach(%122:<12288xf16>) 
58388:1:0 %134:<12288xf16>+12288 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58388:1:0 %75:<12288xf16>+12288 = torch.2_1_0.aten::view(%134:<12288xf16>+12288, list{12288:i32}) 
58388:1:0 %135:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58388:1:0 %136:<12288xf16>+24576 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58390:None:0 %56:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %137:<12288xf16>+24576 = torch.2_1_0.aten::view(%136:<12288xf16>+24576, list{12288:i32}) 
58388:1:0 %138:<12288x6144xf16> = torch.2_1_0.aten::detach(%65:<12288x6144xf16>) 
58390:None:0 %73:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %139:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58388:1:0 %140:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%139:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58388:1:0 %141:<6144xf16> = torch.2_1_0.aten::detach(%107:<6144xf16>) 
58390:None:0 %59:<12288xf32> = torch.2_1_0.aten::fill_(%59:<12288xf32>, 1.0:f32) 
58388:1:0 %142:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58388:1:0 %143:<6144xf16>+75534336 = torch.2_1_0.aten::view(%142:<6144xf16>+75534336, list{6144:i32}) 
58390:None:0 %119:<12288xf32> = torch.2_1_0.aten::zero_(%119:<12288xf32>) 
58388:1:0 %144:<6144x12288xf16> = torch.2_1_0.aten::detach(%120:<6144x12288xf16>) 
58388:1:0 %145:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58388:1:0 %146:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%145:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58388:1:0 %147:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58388:1:0 %148:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58388:1:0 %145:<12288xf16>+151037952 = torch.2_1_0.aten::view(%148:<12288xf16>+151037952, list{12288:i32}) 
58388:1:0 %149:<12288xf16> = torch.2_1_0.aten::detach(%63:<12288xf16>) 
58388:1:0 %150:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58388:1:0 %151:<12288xf16>+151050240 = torch.2_1_0.aten::view(%150:<12288xf16>+151050240, list{12288:i32}) 
58388:1:0 %152:<4608xf16> = torch.2_1_0.aten::detach(%12:<4608xf16>) 
58388:1:0 %153:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58388:1:0 %154:<4608xf16>+151062528 = torch.2_1_0.aten::view(%153:<4608xf16>+151062528, list{4608:i32}) 
58388:1:0 %155:<4608x12288xf16> = torch.2_1_0.aten::detach(%119:<4608x12288xf16>) 
58388:1:0 %156:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58394:7:0 %32:<12288xf16> = torch.2_1_0.aten::zero_(%32:<12288xf16>) 
58388:1:0 %157:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%156:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58390:3:0 %61:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %158:<12288xf16> = torch.2_1_0.aten::detach(%115:<12288xf16>) 
58388:1:0 %159:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58388:1:0 %160:<12288xf16>+207690240 = torch.2_1_0.aten::view(%159:<12288xf16>+207690240, list{12288:i32}) 
58388:1:0 %161:<12288x1536xf16> = torch.2_1_0.aten::detach(%23:<12288x1536xf16>) 
58390:3:0 %120:<12288x1536xf16> = torch.2_1_0.aten::normal_(%120:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58388:1:0 %156:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58388:1:0 %159:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%156:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58390:3:0 %48:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %162:<12288xf16> = torch.2_1_0.aten::detach(%118:<12288xf16>) 
58388:1:0 %163:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58388:1:0 %164:<12288xf16>+226576896 = torch.2_1_0.aten::view(%163:<12288xf16>+226576896, list{12288:i32}) 
58387:0:0 %119:<12288xf16> = torch.2_1_0.aten::zero_(%119:<12288xf16>) 
58388:1:0 %165:<12288xf16> = torch.2_1_0.aten::detach(%117:<12288xf16>) 
58388:1:0 %166:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58388:1:0 %167:<12288xf16>+226589184 = torch.2_1_0.aten::view(%166:<12288xf16>+226589184, list{12288:i32}) 
58388:1:0 %168:<2048x12288xf16> = torch.2_1_0.aten::detach(%114:<2048x12288xf16>) 
58394:7:0 %85:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %166:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58388:1:0 %165:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%166:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58388:1:0 %169:<6400x12288xf16> = torch.2_1_0.aten::detach(%113:<6400x12288xf16>) 
58394:7:0 %88:<4608x12288xf16> = torch.2_1_0.aten::normal_(%88:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %166:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58388:1:0 %168:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%166:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58394:7:0 %89:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %170:<330410496xf16> = torch.2_1_0.aten::slice(%127:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58388:1:0 %166:<330410496xf16> = torch.2_1_0.aten::view(%170:<330410496xf16>, list{330410496:i32}) 
58394:7:0 %47:<4608xf16> = torch.2_1_0.aten::zero_(%47:<4608xf16>) 
58387:0:0 %87:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %171:<6400x12288xf16> = torch.2_1_0.aten::expand(%113:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %77:<4608x12288xf16> = torch.2_1_0.aten::normal_(%77:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58394:None:0 %83:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %104:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:None:0 %90:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %96:<4608xf16> = torch.2_1_0.aten::zero_(%96:<4608xf16>) 
58388:1:0 %172:<2048x12288xf16> = torch.2_1_0.aten::expand(%114:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58394:None:0 %31:<12288xf32> = torch.2_1_0.aten::fill_(%31:<12288xf32>, 1.0:f32) 
58394:None:0 %89:<12288xf32> = torch.2_1_0.aten::zero_(%89:<12288xf32>) 
58388:1:0 %173:<12288xf16> = torch.2_1_0.aten::expand(%117:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %172:<12288xf16> = torch.2_1_0.aten::expand(%118:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:None:0 %103:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %171:<12288x1536xf16> = torch.2_1_0.aten::expand(%23:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58388:1:0 %174:<12288xf16> = torch.2_1_0.aten::expand(%115:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:None:0 %106:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %171:<4608x12288xf16> = torch.2_1_0.aten::expand(%119:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58387:None:0 %87:<12288xf32> = torch.2_1_0.aten::fill_(%87:<12288xf32>, 1.0:f32) 
58393:6:0 %127:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%118:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %175:<4608xf16> = torch.2_1_0.aten::expand(%12:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58387:None:0 %101:<12288xf32> = torch.2_1_0.aten::zero_(%101:<12288xf32>) 
58388:1:0 %171:<12288xf16> = torch.2_1_0.aten::expand(%63:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %175:<12288xf16> = torch.2_1_0.aten::expand(%104:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %172:<6144x12288xf16> = torch.2_1_0.aten::expand(%120:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %176:<6144xf16> = torch.2_1_0.aten::expand(%107:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58388:1:0 %172:<12288x6144xf16> = torch.2_1_0.aten::expand(%65:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58388:1:0 %170:<12288xf16> = torch.2_1_0.aten::expand(%48:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %61:<12288xf16> = torch.2_1_0.aten::zero_(%61:<12288xf16>) 
58394:7:0 %58:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %95:<12288xf16> = torch.2_1_0.aten::expand(%122:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %94:<12288xf16> = torch.2_1_0.aten::expand(%123:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58394:7:0 %91:<6144x12288xf16> = torch.2_1_0.aten::normal_(%91:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58394:7:0 %92:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %93:<6144xf16> = torch.2_1_0.aten::zero_(%93:<6144xf16>) 
58387:0:0 %123:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %52:<6144x12288xf16> = torch.2_1_0.aten::normal_(%52:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58388:1:0 %177:<1xi32> = torch.2_1_0.aten::lift_fresh(%177:<1xi32>) 
58387:0:0 %86:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %82:<6144xf16> = torch.2_1_0.aten::zero_(%82:<6144xf16>) 
58390:3:0 %56:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %121:<4608x12288xf16> = torch.2_1_0.aten::normal_(%121:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58394:7:0 %55:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %122:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %74:<4608xf16> = torch.2_1_0.aten::zero_(%74:<4608xf16>) 
58388:1:0 %29:<1xf32> = torch.2_1_0.aten::lift_fresh(%29:<1xf32>) 
58394:7:0 %94:<12288x6144xf16> = torch.2_1_0.aten::normal_(%94:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58388:1:0 %44:<1xf32> = torch.2_1_0.aten::lift_fresh(%44:<1xf32>) 
58394:7:0 %95:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %125:<1xf32> = torch.2_1_0.aten::lift_fresh(%125:<1xf32>) 
58390:None:0 %97:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %178:<1xf32> = torch.2_1_0.aten::lift_fresh(%178:<1xf32>) 
58390:None:0 %123:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %96:<12288xf16> = torch.2_1_0.aten::zero_(%96:<12288xf16>) 
58390:None:0 %124:<12288xf32> = torch.2_1_0.aten::fill_(%124:<12288xf32>, 1.0:f32) 
58388:1:0 %124:<1xf32> = torch.2_1_0.aten::lift_fresh(%124:<1xf32>) 
58390:None:0 %48:<12288xf32> = torch.2_1_0.aten::zero_(%48:<12288xf32>) 
58387:0:0 %103:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %179:<1xi32> = torch.2_1_0.aten::lift_fresh(%179:<1xi32>) 
58388:1:0 %180:<6400x12288xf16> = torch.2_1_0.aten::detach(%113:<6400x12288xf16>) 
58388:1:0 %181:<6400x12288xf16> = torch.2_1_0.aten::detach(%180:<6400x12288xf16>) 
58387:0:0 %53:<12288x6144xf16> = torch.2_1_0.aten::normal_(%53:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58394:7:0 %55:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %124:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %97:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %46:<12288xf16> = torch.2_1_0.aten::zero_(%46:<12288xf16>) 
58394:7:0 %98:<12288xf16> = torch.2_1_0.aten::fill_(%98:<12288xf16>, 1.0:f32) 
58394:7:0 %56:<12288xf16> = torch.2_1_0.aten::zero_(%56:<12288xf16>) 
58390:3:0 %56:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %95:<6144x12288xf16> = torch.2_1_0.aten::normal_(%95:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58390:3:0 %84:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %125:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %64:<6144xf16> = torch.2_1_0.aten::zero_(%64:<6144xf16>) 
58387:0:0 %43:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %42:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%36:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %48:<12288xf16> = torch.2_1_0.aten::fill_(%48:<12288xf16>, 1.0:f32) 
58387:0:0 %126:<12288xf16> = torch.2_1_0.aten::zero_(%126:<12288xf16>) 
58388:1:0 %182:<6400x12288xf16> = torch.2_1_0.aten::clone(%181:<6400x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %44:<12288xf16> = torch.2_1_0.aten::_to_copy(%7:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::_to_copy(%11:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %46:<12288xf16> = torch.2_1_0.aten::_to_copy(%21:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %47:<12288xf16> = torch.2_1_0.aten::_to_copy(%16:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %180:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%182:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %105:<2048x12288xf16> = torch.2_1_0.aten::detach(%114:<2048x12288xf16>) 
58388:1:0 %183:<2048x12288xf16> = torch.2_1_0.aten::detach(%105:<2048x12288xf16>) 
58388:1:0 %184:<2048x12288xf16> = torch.2_1_0.aten::clone(%183:<2048x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %39:<12288xf16> = torch.2_1_0.aten::detach(%34:<12288xf16>) 
58388:1:0 %183:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%184:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %47:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58388:1:0 %181:<12288x1536xf16> = torch.2_1_0.aten::detach(%23:<12288x1536xf16>) 
58388:1:0 %185:<12288x1536xf16> = torch.2_1_0.aten::detach(%181:<12288x1536xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%33:<12288xf16>) 
58388:1:0 %184:<12288x1536xf16> = torch.2_1_0.aten::clone(%185:<12288x1536xf16>, memory_format=None:NoneType) 
58392:5:0 %46:<12288x6144xf16> = torch.2_1_0.aten::detach(%31:<12288x6144xf16>) 
58392:5:0 %45:<6144xf16> = torch.2_1_0.aten::detach(%6:<6144xf16>) 
58388:1:0 %105:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%184:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %47:<6144x12288xf16> = torch.2_1_0.aten::detach(%30:<6144x12288xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%16:<12288xf16>) 
58393:6:0 %128:<12288xf32> = torch.2_1_0.aten::_to_copy(%2:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %186:<4608x12288xf16> = torch.2_1_0.aten::detach(%119:<4608x12288xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%21:<12288xf16>) 
58388:1:0 %187:<4608x12288xf16> = torch.2_1_0.aten::detach(%186:<4608x12288xf16>) 
58392:5:0 %45:<4608xf16> = torch.2_1_0.aten::detach(%8:<4608xf16>) 
58392:5:0 %48:<4608x12288xf16> = torch.2_1_0.aten::detach(%37:<4608x12288xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%13:<12288xf16>) 
58393:6:0 %129:<12288xf32> = torch.2_1_0.aten::_to_copy(%45:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %49:<12288x1536xf16> = torch.2_1_0.aten::detach(%17:<12288x1536xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%11:<12288xf16>) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%7:<12288xf16>) 
58392:5:0 %50:<2048x12288xf16> = torch.2_1_0.aten::detach(%36:<2048x12288xf16>) 
58393:6:0 %96:<12288xf32> = torch.2_1_0.aten::_to_copy(%123:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %51:<6400x12288xf16> = torch.2_1_0.aten::detach(%24:<6400x12288xf16>) 
58393:6:0 %128:<12288xf32> = torch.2_1_0.aten::_to_copy(%124:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %184:<4608x12288xf16> = torch.2_1_0.aten::clone(%187:<4608x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %52:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%34:<12288xf16>) 
58388:1:0 %188:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%184:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %189:<6144x12288xf16> = torch.2_1_0.aten::detach(%120:<6144x12288xf16>) 
58392:5:0 %53:<12288xf16> = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58388:1:0 %182:<6144x12288xf16> = torch.2_1_0.aten::detach(%189:<6144x12288xf16>) 
58390:3:0 %104:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %44:<12288xf16> = torch.2_1_0.aten::view(%53:<12288xf16>, list{12288:i32}) 
58392:5:0 %54:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58392:5:0 %55:<12288xf16>+12288 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58392:5:0 %56:<12288xf16>+12288 = torch.2_1_0.aten::view(%55:<12288xf16>+12288, list{12288:i32}) 
58392:5:0 %45:<12288xf16> = torch.2_1_0.aten::detach(%33:<12288xf16>) 
58390:3:0 %105:<12288x6144xf16> = torch.2_1_0.aten::normal_(%105:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58388:1:0 %189:<6144x12288xf16> = torch.2_1_0.aten::clone(%182:<6144x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %57:<12288xf16>+24576 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58392:5:0 %45:<12288xf16>+24576 = torch.2_1_0.aten::view(%57:<12288xf16>+24576, list{12288:i32}) 
58390:3:0 %123:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %58:<12288x6144xf16> = torch.2_1_0.aten::detach(%31:<12288x6144xf16>) 
58392:5:0 %59:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58390:3:0 %104:<12288xf16> = torch.2_1_0.aten::zero_(%104:<12288xf16>) 
58392:5:0 %60:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%59:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58392:5:0 %61:<6144xf16> = torch.2_1_0.aten::detach(%6:<6144xf16>) 
58392:5:0 %62:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58392:5:0 %63:<6144xf16>+75534336 = torch.2_1_0.aten::view(%62:<6144xf16>+75534336, list{6144:i32}) 
58392:5:0 %64:<6144x12288xf16> = torch.2_1_0.aten::detach(%30:<6144x12288xf16>) 
58392:5:0 %65:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58392:5:0 %66:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%65:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58390:3:0 %31:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %67:<12288xf16> = torch.2_1_0.aten::detach(%16:<12288xf16>) 
58388:1:0 %190:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%189:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %68:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58390:3:0 %125:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %59:<12288xf16>+151037952 = torch.2_1_0.aten::view(%68:<12288xf16>+151037952, list{12288:i32}) 
58388:1:0 %189:<12288x6144xf16> = torch.2_1_0.aten::detach(%65:<12288x6144xf16>) 
58388:1:0 %185:<12288x6144xf16> = torch.2_1_0.aten::detach(%189:<12288x6144xf16>) 
58390:3:0 %123:<12288xf16> = torch.2_1_0.aten::fill_(%123:<12288xf16>, 1.0:f32) 
58392:5:0 %69:<12288xf16> = torch.2_1_0.aten::detach(%21:<12288xf16>) 
58392:5:0 %70:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58390:3:0 %126:<12288xf16> = torch.2_1_0.aten::zero_(%126:<12288xf16>) 
58388:1:0 %191:<12288x6144xf16> = torch.2_1_0.aten::clone(%185:<12288x6144xf16>, memory_format=None:NoneType) 
58392:5:0 %65:<12288xf16>+151050240 = torch.2_1_0.aten::view(%70:<12288xf16>+151050240, list{12288:i32}) 
58392:5:0 %71:<4608xf16> = torch.2_1_0.aten::detach(%8:<4608xf16>) 
58392:5:0 %72:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58392:5:0 %68:<4608xf16>+151062528 = torch.2_1_0.aten::view(%72:<4608xf16>+151062528, list{4608:i32}) 
58392:5:0 %70:<4608x12288xf16> = torch.2_1_0.aten::detach(%37:<4608x12288xf16>) 
58392:5:0 %73:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58388:1:0 %192:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%191:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %72:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%73:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%117:<12288xf16>) 
58392:5:0 %73:<12288xf16> = torch.2_1_0.aten::detach(%13:<12288xf16>) 
58388:1:0 %189:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58392:5:0 %74:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::clone(%189:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %75:<12288xf16>+207690240 = torch.2_1_0.aten::view(%74:<12288xf16>+207690240, list{12288:i32}) 
58392:5:0 %76:<12288x1536xf16> = torch.2_1_0.aten::detach(%17:<12288x1536xf16>) 
58388:1:0 %189:<12288xf32> = torch.2_1_0.aten::_to_copy(%191:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %77:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%118:<12288xf16>) 
58392:5:0 %78:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%77:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58388:1:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58392:5:0 %77:<12288xf16> = torch.2_1_0.aten::detach(%11:<12288xf16>) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::clone(%187:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %79:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58388:1:0 %187:<12288xf32> = torch.2_1_0.aten::_to_copy(%191:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %80:<12288xf16>+226576896 = torch.2_1_0.aten::view(%79:<12288xf16>+226576896, list{12288:i32}) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%115:<12288xf16>) 
58392:5:0 %81:<12288xf16> = torch.2_1_0.aten::detach(%7:<12288xf16>) 
58388:1:0 %193:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58392:5:0 %82:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58388:1:0 %191:<12288xf16> = torch.2_1_0.aten::clone(%193:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %83:<12288xf16>+226589184 = torch.2_1_0.aten::view(%82:<12288xf16>+226589184, list{12288:i32}) 
58388:1:0 %193:<12288xf32> = torch.2_1_0.aten::_to_copy(%191:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %84:<2048x12288xf16> = torch.2_1_0.aten::detach(%36:<2048x12288xf16>) 
58388:1:0 %185:<4608xf16> = torch.2_1_0.aten::detach(%12:<4608xf16>) 
58392:5:0 %79:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58388:1:0 %191:<4608xf16> = torch.2_1_0.aten::detach(%185:<4608xf16>) 
58392:5:0 %50:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%79:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58388:1:0 %182:<4608xf16> = torch.2_1_0.aten::clone(%191:<4608xf16>, memory_format=None:NoneType) 
58392:5:0 %82:<6400x12288xf16> = torch.2_1_0.aten::detach(%24:<6400x12288xf16>) 
58388:1:0 %191:<4608xf32> = torch.2_1_0.aten::_to_copy(%182:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %85:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58388:1:0 %185:<12288xf16> = torch.2_1_0.aten::detach(%63:<12288xf16>) 
58392:5:0 %79:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%85:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58388:1:0 %194:<12288xf16> = torch.2_1_0.aten::detach(%185:<12288xf16>) 
58392:5:0 %86:<330410496xf16> = torch.2_1_0.aten::slice(%52:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58388:1:0 %195:<12288xf16> = torch.2_1_0.aten::clone(%194:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %87:<330410496xf16> = torch.2_1_0.aten::view(%86:<330410496xf16>, list{330410496:i32}) 
58388:1:0 %194:<12288xf32> = torch.2_1_0.aten::_to_copy(%195:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %195:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58388:1:0 %196:<12288xf16> = torch.2_1_0.aten::detach(%195:<12288xf16>) 
58392:5:0 %88:<6400x12288xf16> = torch.2_1_0.aten::expand(%24:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %195:<12288xf16> = torch.2_1_0.aten::clone(%196:<12288xf16>, memory_format=None:NoneType) 
58388:1:0 %196:<12288xf32> = torch.2_1_0.aten::_to_copy(%195:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %184:<6144xf16> = torch.2_1_0.aten::detach(%107:<6144xf16>) 
58388:1:0 %195:<6144xf16> = torch.2_1_0.aten::detach(%184:<6144xf16>) 
58392:5:0 %89:<2048x12288xf16> = torch.2_1_0.aten::expand(%36:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %186:<6144xf16> = torch.2_1_0.aten::clone(%195:<6144xf16>, memory_format=None:NoneType) 
58392:5:0 %90:<12288xf16> = torch.2_1_0.aten::expand(%7:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %195:<6144xf32> = torch.2_1_0.aten::_to_copy(%186:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %91:<12288xf16> = torch.2_1_0.aten::expand(%11:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58388:1:0 %197:<12288xf16> = torch.2_1_0.aten::detach(%184:<12288xf16>) 
58392:5:0 %92:<12288x1536xf16> = torch.2_1_0.aten::expand(%17:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58388:1:0 %198:<12288xf16> = torch.2_1_0.aten::clone(%197:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %93:<12288xf16> = torch.2_1_0.aten::expand(%13:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %197:<12288xf32> = torch.2_1_0.aten::_to_copy(%198:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %198:<12288xf16> = torch.2_1_0.aten::detach(%122:<12288xf16>) 
58392:5:0 %94:<4608x12288xf16> = torch.2_1_0.aten::expand(%37:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %199:<12288xf16> = torch.2_1_0.aten::detach(%198:<12288xf16>) 
58392:5:0 %95:<4608xf16> = torch.2_1_0.aten::expand(%8:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58388:1:0 %198:<12288xf16> = torch.2_1_0.aten::clone(%199:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %96:<12288xf16> = torch.2_1_0.aten::expand(%21:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %199:<12288xf32> = torch.2_1_0.aten::_to_copy(%198:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %198:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58392:5:0 %94:<12288xf16> = torch.2_1_0.aten::expand(%16:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%198:<12288xf16>) 
58388:1:0 %200:<12288xf16> = torch.2_1_0.aten::clone(%184:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %97:<6144x12288xf16> = torch.2_1_0.aten::expand(%30:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58388:1:0 %201:<12288xf32> = torch.2_1_0.aten::_to_copy(%200:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %98:<6144xf16> = torch.2_1_0.aten::expand(%6:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58392:5:0 %99:<12288x6144xf16> = torch.2_1_0.aten::expand(%31:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58392:5:0 %100:<12288xf16> = torch.2_1_0.aten::expand(%33:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58392:5:0 %101:<12288xf16> = torch.2_1_0.aten::expand(%38:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58388:1:0 %186:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %102:<12288xf16> = torch.2_1_0.aten::expand(%34:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %93:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%118:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %130:<12288xf16> = torch.2_1_0.aten::_to_copy(%2:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %128:<12288xf16> = torch.2_1_0.aten::_to_copy(%45:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%186:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %91:<12288xf16> = torch.2_1_0.aten::_to_copy(%123:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %66:<12288xf16> = torch.2_1_0.aten::_to_copy(%124:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %97:<1xi32> = torch.2_1_0.aten::lift_fresh(%97:<1xi32>) 
58393:6:0 %131:<12288xf16> = torch.2_1_0.aten::detach(%94:<12288xf16>) 
58393:6:0 %107:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58392:5:0 %42:<1xf32> = torch.2_1_0.aten::lift_fresh(%42:<1xf32>) 
58393:6:0 %107:<12288xf16> = torch.2_1_0.aten::detach(%77:<12288xf16>) 
58393:6:0 %129:<12288x6144xf16> = torch.2_1_0.aten::detach(%47:<12288x6144xf16>) 
58392:5:0 %103:<1xf32> = torch.2_1_0.aten::lift_fresh(%103:<1xf32>) 
58393:6:0 %129:<6144xf16> = torch.2_1_0.aten::detach(%98:<6144xf16>) 
58393:6:0 %128:<6144x12288xf16> = torch.2_1_0.aten::detach(%63:<6144x12288xf16>) 
58392:5:0 %104:<1xf32> = torch.2_1_0.aten::lift_fresh(%104:<1xf32>) 
58393:6:0 %130:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58393:6:0 %128:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58392:5:0 %105:<1xf32> = torch.2_1_0.aten::lift_fresh(%105:<1xf32>) 
58393:6:0 %128:<4608xf16> = torch.2_1_0.aten::detach(%73:<4608xf16>) 
58393:6:0 %130:<4608x12288xf16> = torch.2_1_0.aten::detach(%80:<4608x12288xf16>) 
58392:5:0 %106:<1xf32> = torch.2_1_0.aten::lift_fresh(%106:<1xf32>) 
58393:6:0 %107:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58393:6:0 %107:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58392:5:0 %107:<1xi32> = torch.2_1_0.aten::lift_fresh(%107:<1xi32>) 
58393:6:0 %107:<12288xf16> = torch.2_1_0.aten::detach(%45:<12288xf16>) 
58393:6:0 %107:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58393:6:0 %128:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58392:5:0 %108:<6400x12288xf16> = torch.2_1_0.aten::detach(%24:<6400x12288xf16>) 
58393:6:0 %130:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58392:5:0 %109:<6400x12288xf16> = torch.2_1_0.aten::detach(%108:<6400x12288xf16>) 
58392:5:0 %110:<6400x12288xf16> = torch.2_1_0.aten::clone(%109:<6400x12288xf16>, memory_format=None:NoneType) 
58393:6:0 %69:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred) 
58393:6:0 %100:<12288xf16> = torch.2_1_0.aten::detach(%94:<12288xf16>) 
58393:6:0 %130:<12288xf16> = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58393:6:0 %100:<12288xf16> = torch.2_1_0.aten::view(%130:<12288xf16>, list{12288:i32}) 
58393:6:0 %128:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58393:6:0 %130:<12288xf16>+12288 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58392:5:0 %111:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%110:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %107:<12288xf16>+12288 = torch.2_1_0.aten::view(%130:<12288xf16>+12288, list{12288:i32}) 
58393:6:0 %130:<12288xf16> = torch.2_1_0.aten::detach(%77:<12288xf16>) 
58393:6:0 %129:<12288xf16>+24576 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58392:5:0 %112:<2048x12288xf16> = torch.2_1_0.aten::detach(%36:<2048x12288xf16>) 
58393:6:0 %128:<12288xf16>+24576 = torch.2_1_0.aten::view(%129:<12288xf16>+24576, list{12288:i32}) 
58392:5:0 %109:<2048x12288xf16> = torch.2_1_0.aten::detach(%112:<2048x12288xf16>) 
58393:6:0 %66:<12288x6144xf16> = torch.2_1_0.aten::detach(%47:<12288x6144xf16>) 
58393:6:0 %130:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58392:5:0 %113:<2048x12288xf16> = torch.2_1_0.aten::clone(%109:<2048x12288xf16>, memory_format=None:NoneType) 
58393:6:0 %129:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%130:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58393:6:0 %130:<6144xf16> = torch.2_1_0.aten::detach(%98:<6144xf16>) 
58393:6:0 %62:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58392:5:0 %110:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%113:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %91:<6144xf16>+75534336 = torch.2_1_0.aten::view(%62:<6144xf16>+75534336, list{6144:i32}) 
58392:5:0 %108:<12288x1536xf16> = torch.2_1_0.aten::detach(%17:<12288x1536xf16>) 
58393:6:0 %93:<6144x12288xf16> = torch.2_1_0.aten::detach(%63:<6144x12288xf16>) 
58392:5:0 %113:<12288x1536xf16> = torch.2_1_0.aten::detach(%108:<12288x1536xf16>) 
58393:6:0 %130:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58393:6:0 %62:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%130:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58392:5:0 %114:<12288x1536xf16> = torch.2_1_0.aten::clone(%113:<12288x1536xf16>, memory_format=None:NoneType) 
58393:6:0 %132:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58393:6:0 %130:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58392:5:0 %115:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%114:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %67:<12288xf16>+151037952 = torch.2_1_0.aten::view(%130:<12288xf16>+151037952, list{12288:i32}) 
58393:6:0 %133:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58392:5:0 %108:<4608x12288xf16> = torch.2_1_0.aten::detach(%37:<4608x12288xf16>) 
58393:6:0 %130:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58392:5:0 %114:<4608x12288xf16> = torch.2_1_0.aten::detach(%108:<4608x12288xf16>) 
58393:6:0 %132:<12288xf16>+151050240 = torch.2_1_0.aten::view(%130:<12288xf16>+151050240, list{12288:i32}) 
58393:6:0 %130:<4608xf16> = torch.2_1_0.aten::detach(%73:<4608xf16>) 
58393:6:0 %74:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58393:6:0 %133:<4608xf16>+151062528 = torch.2_1_0.aten::view(%74:<4608xf16>+151062528, list{4608:i32}) 
58393:6:0 %134:<4608x12288xf16> = torch.2_1_0.aten::detach(%80:<4608x12288xf16>) 
58393:6:0 %130:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58393:6:0 %74:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%130:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58393:6:0 %135:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58393:6:0 %130:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58393:6:0 %136:<12288xf16>+207690240 = torch.2_1_0.aten::view(%130:<12288xf16>+207690240, list{12288:i32}) 
58393:6:0 %137:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58393:6:0 %135:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58393:6:0 %138:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%135:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58393:6:0 %31:<12288xf16> = torch.2_1_0.aten::detach(%45:<12288xf16>) 
58393:6:0 %135:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58393:6:0 %130:<12288xf16>+226576896 = torch.2_1_0.aten::view(%135:<12288xf16>+226576896, list{12288:i32}) 
58393:6:0 %135:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58393:6:0 %139:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58393:6:0 %135:<12288xf16>+226589184 = torch.2_1_0.aten::view(%139:<12288xf16>+226589184, list{12288:i32}) 
58393:6:0 %140:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58393:6:0 %141:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58393:6:0 %142:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%141:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58393:6:0 %141:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58393:6:0 %140:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58393:6:0 %143:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%140:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58393:6:0 %144:<330410496xf16> = torch.2_1_0.aten::slice(%69:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58393:6:0 %140:<330410496xf16> = torch.2_1_0.aten::view(%144:<330410496xf16>, list{330410496:i32}) 
58393:6:0 %145:<6400x12288xf16> = torch.2_1_0.aten::expand(%116:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58393:6:0 %146:<2048x12288xf16> = torch.2_1_0.aten::expand(%118:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %106:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%117:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %147:<12288xf16> = torch.2_1_0.aten::expand(%2:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %148:<12288xf16> = torch.2_1_0.aten::expand(%45:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %149:<12288x1536xf16> = torch.2_1_0.aten::expand(%120:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58393:6:0 %150:<12288xf16> = torch.2_1_0.aten::expand(%121:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %131:<4608x12288xf16> = torch.2_1_0.aten::expand(%80:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58393:6:0 %151:<4608xf16> = torch.2_1_0.aten::expand(%73:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58393:6:0 %152:<12288xf16> = torch.2_1_0.aten::expand(%123:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %93:<12288xf16> = torch.2_1_0.aten::expand(%124:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %153:<6144x12288xf16> = torch.2_1_0.aten::expand(%63:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58393:6:0 %154:<6144xf16> = torch.2_1_0.aten::expand(%98:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58393:6:0 %152:<12288x6144xf16> = torch.2_1_0.aten::expand(%47:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58393:6:0 %155:<12288xf16> = torch.2_1_0.aten::expand(%77:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %156:<12288xf16> = torch.2_1_0.aten::expand(%126:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58393:6:0 %157:<12288xf16> = torch.2_1_0.aten::expand(%94:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58392:5:0 %109:<4608x12288xf16> = torch.2_1_0.aten::clone(%114:<4608x12288xf16>, memory_format=None:NoneType) 
58393:6:0 %158:<1xi32> = torch.2_1_0.aten::lift_fresh(%158:<1xi32>) 
58393:6:0 %159:<1xf32> = torch.2_1_0.aten::lift_fresh(%159:<1xf32>) 
58393:6:0 %160:<1xf32> = torch.2_1_0.aten::lift_fresh(%160:<1xf32>) 
58393:6:0 %161:<1xf32> = torch.2_1_0.aten::lift_fresh(%161:<1xf32>) 
58393:6:0 %162:<1xf32> = torch.2_1_0.aten::lift_fresh(%162:<1xf32>) 
58393:6:0 %163:<1xf32> = torch.2_1_0.aten::lift_fresh(%163:<1xf32>) 
58393:6:0 %164:<1xi32> = torch.2_1_0.aten::lift_fresh(%164:<1xi32>) 
58393:6:0 %165:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58393:6:0 %166:<6400x12288xf16> = torch.2_1_0.aten::detach(%165:<6400x12288xf16>) 
58394:7:0 %99:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%8:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %167:<6400x12288xf16> = torch.2_1_0.aten::clone(%166:<6400x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %116:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%109:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %108:<6144x12288xf16> = torch.2_1_0.aten::detach(%30:<6144x12288xf16>) 
58392:5:0 %109:<6144x12288xf16> = torch.2_1_0.aten::detach(%108:<6144x12288xf16>) 
58387:0:0 %127:<12288xf32> = torch.2_1_0.aten::_to_copy(%120:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:None:0 %34:<2048x12288xf32> = torch.2_1_0.aten::normal_(%34:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58387:0:0 %128:<12288xf32> = torch.2_1_0.aten::_to_copy(%121:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %129:<12288xf32> = torch.2_1_0.aten::_to_copy(%87:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %130:<12288xf32> = torch.2_1_0.aten::_to_copy(%101:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:None:0 %99:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:None:0 %42:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:None:0 %100:<12288xf32> = torch.2_1_0.aten::fill_(%100:<12288xf32>, 1.0:f32) 
58389:None:0 %9:<12288xf32> = torch.2_1_0.aten::zero_(%9:<12288xf32>) 
58392:5:0 %113:<6144x12288xf16> = torch.2_1_0.aten::clone(%109:<6144x12288xf16>, memory_format=None:NoneType) 
58393:6:0 %168:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%167:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %166:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58393:6:0 %169:<2048x12288xf16> = torch.2_1_0.aten::detach(%166:<2048x12288xf16>) 
58393:6:0 %167:<2048x12288xf16> = torch.2_1_0.aten::clone(%169:<2048x12288xf16>, memory_format=None:NoneType) 
58393:6:0 %170:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%167:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %165:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58393:6:0 %171:<12288x1536xf16> = torch.2_1_0.aten::detach(%165:<12288x1536xf16>) 
58393:6:0 %172:<12288x1536xf16> = torch.2_1_0.aten::clone(%171:<12288x1536xf16>, memory_format=None:NoneType) 
58393:6:0 %173:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%172:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %165:<4608x12288xf16> = torch.2_1_0.aten::detach(%80:<4608x12288xf16>) 
58393:6:0 %174:<4608x12288xf16> = torch.2_1_0.aten::detach(%165:<4608x12288xf16>) 
58394:7:0 %100:<12288xf32> = torch.2_1_0.aten::_to_copy(%2:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %101:<12288xf32> = torch.2_1_0.aten::_to_copy(%87:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %85:<12288xf32> = torch.2_1_0.aten::_to_copy(%31:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %102:<12288xf32> = torch.2_1_0.aten::_to_copy(%89:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:None:0 %127:<2048x12288xf32> = torch.2_1_0.aten::normal_(%127:<2048x12288xf32>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58390:3:0 %127:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%118:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %117:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%113:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %108:<12288x6144xf16> = torch.2_1_0.aten::detach(%31:<12288x6144xf16>) 
58392:5:0 %113:<12288x6144xf16> = torch.2_1_0.aten::detach(%108:<12288x6144xf16>) 
58391:None:0 %38:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %114:<12288x6144xf16> = torch.2_1_0.aten::clone(%113:<12288x6144xf16>, memory_format=None:NoneType) 
58391:None:0 %121:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:None:0 %128:<12288xf32> = torch.2_1_0.aten::fill_(%128:<12288xf32>, 1.0:f32) 
58391:None:0 %37:<12288xf32> = torch.2_1_0.aten::zero_(%37:<12288xf32>) 
58393:6:0 %171:<4608x12288xf16> = torch.2_1_0.aten::clone(%174:<4608x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %118:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%114:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %114:<12288xf16> = torch.2_1_0.aten::detach(%7:<12288xf16>) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%114:<12288xf16>) 
58392:5:0 %108:<12288xf16> = torch.2_1_0.aten::clone(%113:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %112:<12288xf32> = torch.2_1_0.aten::_to_copy(%108:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %108:<12288xf16> = torch.2_1_0.aten::detach(%11:<12288xf16>) 
58392:5:0 %114:<12288xf16> = torch.2_1_0.aten::detach(%108:<12288xf16>) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::clone(%114:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %109:<12288xf32> = torch.2_1_0.aten::_to_copy(%113:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%13:<12288xf16>) 
58392:5:0 %108:<12288xf16> = torch.2_1_0.aten::detach(%113:<12288xf16>) 
58392:5:0 %114:<12288xf16> = torch.2_1_0.aten::clone(%108:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %119:<12288xf32> = torch.2_1_0.aten::_to_copy(%114:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %114:<4608xf16> = torch.2_1_0.aten::detach(%8:<4608xf16>) 
58392:5:0 %108:<4608xf16> = torch.2_1_0.aten::detach(%114:<4608xf16>) 
58392:5:0 %114:<4608xf16> = torch.2_1_0.aten::clone(%108:<4608xf16>, memory_format=None:NoneType) 
58392:5:0 %108:<4608xf32> = torch.2_1_0.aten::_to_copy(%114:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %24:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %114:<12288xf16> = torch.2_1_0.aten::detach(%21:<12288xf16>) 
58392:5:0 %120:<12288xf16> = torch.2_1_0.aten::detach(%114:<12288xf16>) 
58389:2:0 %13:<12288x1536xf16> = torch.2_1_0.aten::normal_(%13:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58389:2:0 %27:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %175:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%171:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %176:<6144x12288xf16> = torch.2_1_0.aten::detach(%63:<6144x12288xf16>) 
58393:6:0 %165:<6144x12288xf16> = torch.2_1_0.aten::detach(%176:<6144x12288xf16>) 
58391:4:0 %111:<12288x1536xf16> = torch.2_1_0.aten::empty(list{12288:i32, 1536:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %129:<12288x1536xf16> = torch.2_1_0.aten::normal_(%129:<12288x1536xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58391:4:0 %130:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %167:<6144x12288xf16> = torch.2_1_0.aten::clone(%165:<6144x12288xf16>, memory_format=None:NoneType) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::clone(%120:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %121:<12288xf32> = torch.2_1_0.aten::_to_copy(%113:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%16:<12288xf16>) 
58392:5:0 %114:<12288xf16> = torch.2_1_0.aten::detach(%113:<12288xf16>) 
58392:5:0 %120:<12288xf16> = torch.2_1_0.aten::clone(%114:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %38:<12288xf16> = torch.2_1_0.aten::zero_(%38:<12288xf16>) 
58392:5:0 %122:<12288xf32> = torch.2_1_0.aten::_to_copy(%120:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %120:<6144xf16> = torch.2_1_0.aten::detach(%6:<6144xf16>) 
58392:5:0 %114:<6144xf16> = torch.2_1_0.aten::detach(%120:<6144xf16>) 
58392:5:0 %120:<6144xf16> = torch.2_1_0.aten::clone(%114:<6144xf16>, memory_format=None:NoneType) 
58392:5:0 %114:<6144xf32> = torch.2_1_0.aten::_to_copy(%120:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %120:<12288xf16> = torch.2_1_0.aten::detach(%33:<12288xf16>) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58392:5:0 %123:<12288xf16> = torch.2_1_0.aten::clone(%113:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %124:<12288xf32> = torch.2_1_0.aten::_to_copy(%123:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %123:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58392:5:0 %120:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::clone(%120:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %125:<12288xf32> = torch.2_1_0.aten::_to_copy(%113:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%34:<12288xf16>) 
58392:5:0 %123:<12288xf16> = torch.2_1_0.aten::detach(%113:<12288xf16>) 
58389:2:0 %35:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %120:<12288xf16> = torch.2_1_0.aten::clone(%123:<12288xf16>, memory_format=None:NoneType) 
58392:5:0 %126:<12288xf32> = torch.2_1_0.aten::_to_copy(%120:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %12:<4608x12288xf16> = torch.2_1_0.aten::normal_(%12:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58392:5:0 %127:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %101:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %18:<4608xf16> = torch.2_1_0.aten::zero_(%18:<4608xf16>) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%127:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %111:<12288xf16> = torch.2_1_0.aten::zero_(%111:<12288xf16>) 
58390:3:0 %9:<12288xf32> = torch.2_1_0.aten::_to_copy(%59:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:None:0 %90:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:None:0 %96:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %128:<12288xf32> = torch.2_1_0.aten::_to_copy(%119:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:None:0 %70:<12288xf32> = torch.2_1_0.aten::fill_(%70:<12288xf32>, 1.0:f32) 
58389:None:0 %60:<12288xf32> = torch.2_1_0.aten::zero_(%60:<12288xf32>) 
58390:3:0 %5:<12288xf32> = torch.2_1_0.aten::_to_copy(%124:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %105:<4608x12288xf16> = torch.2_1_0.aten::empty(list{4608:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %129:<12288xf32> = torch.2_1_0.aten::_to_copy(%48:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %131:<4608x12288xf16> = torch.2_1_0.aten::normal_(%131:<4608x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58391:4:0 %105:<4608xf16> = torch.2_1_0.aten::empty(list{4608:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %99:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %115:<4608xf16> = torch.2_1_0.aten::zero_(%115:<4608xf16>) 
58389:2:0 %84:<6144x12288xf16> = torch.2_1_0.aten::normal_(%84:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58389:2:0 %102:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:None:0 %22:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %103:<6144xf16> = torch.2_1_0.aten::zero_(%103:<6144xf16>) 
58391:None:0 %83:<12288xf32> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:None:0 %132:<12288xf32> = torch.2_1_0.aten::fill_(%132:<12288xf32>, 1.0:f32) 
58391:None:0 %105:<12288xf32> = torch.2_1_0.aten::zero_(%105:<12288xf32>) 
58389:2:0 %104:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %69:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%8:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %102:<12288x6144xf16> = torch.2_1_0.aten::normal_(%102:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58394:7:0 %103:<12288xf16> = torch.2_1_0.aten::_to_copy(%2:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %105:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %99:<12288xf16> = torch.2_1_0.aten::zero_(%99:<12288xf16>) 
58394:7:0 %85:<12288xf16> = torch.2_1_0.aten::_to_copy(%87:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %103:<12288xf16> = torch.2_1_0.aten::_to_copy(%31:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %100:<12288xf16> = torch.2_1_0.aten::_to_copy(%89:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %104:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %90:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %42:<12288xf16> = torch.2_1_0.aten::fill_(%42:<12288xf16>, 1.0:f32) 
58389:2:0 %95:<12288xf16> = torch.2_1_0.aten::zero_(%95:<12288xf16>) 
58394:7:0 %44:<12288xf16> = torch.2_1_0.aten::detach(%56:<12288xf16>) 
58394:7:0 %44:<12288xf16> = torch.2_1_0.aten::detach(%98:<12288xf16>) 
58394:7:0 %83:<12288xf16> = torch.2_1_0.aten::detach(%96:<12288xf16>) 
58394:7:0 %69:<12288x6144xf16> = torch.2_1_0.aten::detach(%94:<12288x6144xf16>) 
58394:7:0 %84:<6144xf16> = torch.2_1_0.aten::detach(%93:<6144xf16>) 
58387:0:0 %106:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%117:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %44:<6144x12288xf16> = torch.2_1_0.aten::detach(%91:<6144x12288xf16>) 
58394:7:0 %102:<12288xf16> = torch.2_1_0.aten::detach(%89:<12288xf16>) 
58394:7:0 %44:<12288xf16> = torch.2_1_0.aten::detach(%31:<12288xf16>) 
58387:0:0 %113:<12288xf16> = torch.2_1_0.aten::_to_copy(%120:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %177:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%167:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %130:<12288xf16> = torch.2_1_0.aten::_to_copy(%121:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %124:<6144x12288xf16> = torch.2_1_0.aten::empty(list{6144:i32, 12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %178:<12288x6144xf16> = torch.2_1_0.aten::detach(%47:<12288x6144xf16>) 
58393:6:0 %179:<12288x6144xf16> = torch.2_1_0.aten::detach(%178:<12288x6144xf16>) 
58387:0:0 %113:<12288xf16> = torch.2_1_0.aten::_to_copy(%87:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %167:<12288x6144xf16> = torch.2_1_0.aten::clone(%179:<12288x6144xf16>, memory_format=None:NoneType) 
58391:4:0 %96:<6144x12288xf16> = torch.2_1_0.aten::normal_(%96:<6144x12288xf16>, 0.0:f32, 0.006:f32, generator=None:NoneType) 
58387:0:0 %127:<12288xf16> = torch.2_1_0.aten::_to_copy(%101:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %102:<6144xf16> = torch.2_1_0.aten::empty(list{6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %104:<6144xf16> = torch.2_1_0.aten::zero_(%104:<6144xf16>) 
58387:0:0 %131:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %28:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58387:0:0 %7:<12288xf16> = torch.2_1_0.aten::detach(%46:<12288xf16>) 
58387:0:0 %132:<12288x6144xf16> = torch.2_1_0.aten::detach(%53:<12288x6144xf16>) 
58387:0:0 %7:<6144xf16> = torch.2_1_0.aten::detach(%82:<6144xf16>) 
58387:0:0 %133:<6144x12288xf16> = torch.2_1_0.aten::detach(%52:<6144x12288xf16>) 
58387:0:0 %81:<12288xf16> = torch.2_1_0.aten::detach(%101:<12288xf16>) 
58387:0:0 %81:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58387:0:0 %81:<4608xf16> = torch.2_1_0.aten::detach(%96:<4608xf16>) 
58387:0:0 %106:<4608x12288xf16> = torch.2_1_0.aten::detach(%77:<4608x12288xf16>) 
58387:0:0 %81:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58387:0:0 %106:<12288x1536xf16> = torch.2_1_0.aten::detach(%1:<12288x1536xf16>) 
58394:7:0 %102:<4608xf16> = torch.2_1_0.aten::detach(%47:<4608xf16>) 
58387:0:0 %81:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58387:0:0 %106:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58387:0:0 %81:<2048x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x12288xf16>) 
58394:7:0 %69:<4608x12288xf16> = torch.2_1_0.aten::detach(%88:<4608x12288xf16>) 
58387:0:0 %106:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58394:7:0 %83:<12288xf16> = torch.2_1_0.aten::detach(%32:<12288xf16>) 
58394:7:0 %44:<12288x1536xf16> = torch.2_1_0.aten::detach(%50:<12288x1536xf16>) 
58394:7:0 %84:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58394:7:0 %44:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58394:7:0 %69:<2048x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x12288xf16>) 
58394:7:0 %84:<6400x12288xf16> = torch.2_1_0.aten::detach(%12:<6400x12288xf16>) 
58387:0:0 %81:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred) 
58387:0:0 %106:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %71:<12288xf16> = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58387:0:0 %106:<12288xf16> = torch.2_1_0.aten::view(%71:<12288xf16>, list{12288:i32}) 
58387:0:0 %71:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58387:0:0 %104:<12288xf16>+12288 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58387:0:0 %71:<12288xf16>+12288 = torch.2_1_0.aten::view(%104:<12288xf16>+12288, list{12288:i32}) 
58387:0:0 %134:<12288xf16> = torch.2_1_0.aten::detach(%46:<12288xf16>) 
58387:0:0 %135:<12288xf16>+24576 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58387:0:0 %136:<12288xf16>+24576 = torch.2_1_0.aten::view(%135:<12288xf16>+24576, list{12288:i32}) 
58391:4:0 %102:<12288x6144xf16> = torch.2_1_0.aten::empty(list{12288:i32, 6144:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %127:<12288x6144xf16> = torch.2_1_0.aten::detach(%53:<12288x6144xf16>) 
58390:3:0 %14:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%118:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %134:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58387:0:0 %135:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%134:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58387:0:0 %134:<6144xf16> = torch.2_1_0.aten::detach(%82:<6144xf16>) 
58391:4:0 %33:<12288x6144xf16> = torch.2_1_0.aten::normal_(%33:<12288x6144xf16>, 0.0:f32, 0.004242640687119285:f32, generator=None:NoneType) 
58387:0:0 %137:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58387:0:0 %113:<6144xf16>+75534336 = torch.2_1_0.aten::view(%137:<6144xf16>+75534336, list{6144:i32}) 
58393:6:0 %180:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%167:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %125:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %138:<6144x12288xf16> = torch.2_1_0.aten::detach(%52:<6144x12288xf16>) 
58393:6:0 %167:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58390:3:0 %127:<12288xf16> = torch.2_1_0.aten::_to_copy(%59:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %181:<12288xf16> = torch.2_1_0.aten::detach(%167:<12288xf16>) 
58387:0:0 %134:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58391:4:0 %88:<12288xf16> = torch.2_1_0.aten::zero_(%88:<12288xf16>) 
58387:0:0 %139:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%134:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58393:6:0 %182:<12288xf16> = torch.2_1_0.aten::clone(%181:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %137:<12288xf16> = torch.2_1_0.aten::detach(%101:<12288xf16>) 
58393:6:0 %179:<12288xf32> = torch.2_1_0.aten::_to_copy(%182:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %28:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58393:6:0 %165:<12288xf16> = torch.2_1_0.aten::detach(%45:<12288xf16>) 
58390:3:0 %11:<12288xf16> = torch.2_1_0.aten::_to_copy(%119:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %140:<12288xf16>+151037952 = torch.2_1_0.aten::view(%28:<12288xf16>+151037952, list{12288:i32}) 
58393:6:0 %183:<12288xf16> = torch.2_1_0.aten::detach(%165:<12288xf16>) 
58387:0:0 %137:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58393:6:0 %184:<12288xf16> = torch.2_1_0.aten::clone(%183:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %28:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58393:6:0 %167:<12288xf32> = torch.2_1_0.aten::_to_copy(%184:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %141:<12288xf16>+151050240 = torch.2_1_0.aten::view(%28:<12288xf16>+151050240, list{12288:i32}) 
58393:6:0 %185:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58387:0:0 %142:<4608xf16> = torch.2_1_0.aten::detach(%96:<4608xf16>) 
58393:6:0 %186:<12288xf16> = torch.2_1_0.aten::detach(%185:<12288xf16>) 
58387:0:0 %28:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58390:3:0 %14:<12288xf16> = torch.2_1_0.aten::_to_copy(%124:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %133:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %187:<12288xf16> = torch.2_1_0.aten::clone(%186:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %143:<4608xf16>+151062528 = torch.2_1_0.aten::view(%28:<4608xf16>+151062528, list{4608:i32}) 
58393:6:0 %165:<12288xf32> = torch.2_1_0.aten::_to_copy(%187:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %92:<12288xf16> = torch.2_1_0.aten::empty(list{12288:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %184:<4608xf16> = torch.2_1_0.aten::detach(%73:<4608xf16>) 
58387:0:0 %142:<4608x12288xf16> = torch.2_1_0.aten::detach(%77:<4608x12288xf16>) 
58390:3:0 %127:<12288xf16> = torch.2_1_0.aten::_to_copy(%48:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %188:<4608xf16> = torch.2_1_0.aten::detach(%184:<4608xf16>) 
58387:0:0 %137:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58393:6:0 %187:<4608xf16> = torch.2_1_0.aten::clone(%188:<4608xf16>, memory_format=None:NoneType) 
58391:4:0 %134:<12288xf16> = torch.2_1_0.aten::fill_(%134:<12288xf16>, 1.0:f32) 
58387:0:0 %144:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%137:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58393:6:0 %188:<4608xf32> = torch.2_1_0.aten::_to_copy(%187:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %142:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58391:4:0 %102:<12288xf16> = torch.2_1_0.aten::zero_(%102:<12288xf16>) 
58393:6:0 %189:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58393:6:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%189:<12288xf16>) 
58387:0:0 %145:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58393:6:0 %184:<12288xf16> = torch.2_1_0.aten::clone(%190:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %137:<12288xf16>+207690240 = torch.2_1_0.aten::view(%145:<12288xf16>+207690240, list{12288:i32}) 
58393:6:0 %187:<12288xf32> = torch.2_1_0.aten::_to_copy(%184:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %146:<12288x1536xf16> = torch.2_1_0.aten::detach(%1:<12288x1536xf16>) 
58393:6:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58387:0:0 %147:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58393:6:0 %189:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58387:0:0 %148:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%147:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58393:6:0 %191:<12288xf16> = torch.2_1_0.aten::clone(%189:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %147:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58393:6:0 %184:<12288xf32> = torch.2_1_0.aten::_to_copy(%191:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %92:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58393:6:0 %192:<6144xf16> = torch.2_1_0.aten::detach(%98:<6144xf16>) 
58387:0:0 %149:<12288xf16>+226576896 = torch.2_1_0.aten::view(%92:<12288xf16>+226576896, list{12288:i32}) 
58393:6:0 %189:<6144xf16> = torch.2_1_0.aten::detach(%192:<6144xf16>) 
58387:0:0 %146:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58393:6:0 %192:<6144xf16> = torch.2_1_0.aten::clone(%189:<6144xf16>, memory_format=None:NoneType) 
58387:0:0 %142:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58393:6:0 %191:<6144xf32> = torch.2_1_0.aten::_to_copy(%192:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %147:<12288xf16>+226589184 = torch.2_1_0.aten::view(%142:<12288xf16>+226589184, list{12288:i32}) 
58393:6:0 %193:<12288xf16> = torch.2_1_0.aten::detach(%77:<12288xf16>) 
58387:0:0 %150:<2048x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x12288xf16>) 
58390:3:0 %9:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58393:6:0 %194:<12288xf16> = torch.2_1_0.aten::detach(%193:<12288xf16>) 
58387:0:0 %151:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58393:6:0 %195:<12288xf16> = torch.2_1_0.aten::clone(%194:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %146:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%151:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58390:3:0 %129:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58387:0:0 %151:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58393:6:0 %196:<12288xf32> = torch.2_1_0.aten::_to_copy(%195:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %130:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58393:6:0 %197:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %150:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58393:6:0 %198:<12288xf16> = torch.2_1_0.aten::detach(%197:<12288xf16>) 
58390:3:0 %127:<12288x6144xf16> = torch.2_1_0.aten::detach(%105:<12288x6144xf16>) 
58387:0:0 %152:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%150:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58393:6:0 %199:<12288xf16> = torch.2_1_0.aten::clone(%198:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %129:<6144xf16> = torch.2_1_0.aten::detach(%64:<6144xf16>) 
58390:3:0 %14:<6144x12288xf16> = torch.2_1_0.aten::detach(%95:<6144x12288xf16>) 
58393:6:0 %195:<12288xf32> = torch.2_1_0.aten::_to_copy(%199:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %145:<330410496xf16> = torch.2_1_0.aten::slice(%81:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58393:6:0 %200:<12288xf16> = torch.2_1_0.aten::detach(%94:<12288xf16>) 
58390:3:0 %9:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58387:0:0 %150:<330410496xf16> = torch.2_1_0.aten::view(%145:<330410496xf16>, list{330410496:i32}) 
58393:6:0 %201:<12288xf16> = torch.2_1_0.aten::detach(%200:<12288xf16>) 
58390:3:0 %5:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58393:6:0 %202:<12288xf16> = torch.2_1_0.aten::clone(%201:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %128:<4608xf16> = torch.2_1_0.aten::detach(%74:<4608xf16>) 
58393:6:0 %203:<12288xf32> = torch.2_1_0.aten::_to_copy(%202:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %3:<4608x12288xf16> = torch.2_1_0.aten::detach(%121:<4608x12288xf16>) 
58390:3:0 %131:<12288xf16> = torch.2_1_0.aten::detach(%61:<12288xf16>) 
58390:3:0 %31:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58390:3:0 %3:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58390:3:0 %131:<12288xf16> = torch.2_1_0.aten::detach(%59:<12288xf16>) 
58390:3:0 %132:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58390:3:0 %41:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58393:6:0 %204:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %7:<6144xf16> = torch.2_1_0.aten::detach(%82:<6144xf16>) 
58387:0:0 %37:<4608x12288xf16> = torch.2_1_0.aten::detach(%77:<4608x12288xf16>) 
58389:2:0 %106:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%34:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %133:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %37:<6144x12288xf16> = torch.2_1_0.aten::detach(%52:<6144x12288xf16>) 
58387:0:0 %132:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58387:0:0 %132:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%204:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %132:<4608xf16> = torch.2_1_0.aten::detach(%96:<4608xf16>) 
58387:0:0 %37:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58387:0:0 %132:<12288x1536xf16> = torch.2_1_0.aten::detach(%1:<12288x1536xf16>) 
58387:0:0 %7:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58387:0:0 %133:<12288x6144xf16> = torch.2_1_0.aten::detach(%53:<12288x6144xf16>) 
58387:0:0 %133:<12288xf16> = torch.2_1_0.aten::detach(%46:<12288xf16>) 
58387:0:0 %37:<12288xf16> = torch.2_1_0.aten::detach(%101:<12288xf16>) 
58387:0:0 %37:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58387:0:0 %133:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58387:0:0 %37:<2048x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x12288xf16>) 
58387:0:0 %153:<6400x12288xf16> = torch.2_1_0.aten::expand(%116:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %154:<2048x12288xf16> = torch.2_1_0.aten::expand(%117:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %155:<12288xf16> = torch.2_1_0.aten::expand(%120:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %156:<12288xf16> = torch.2_1_0.aten::expand(%121:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %154:<12288x1536xf16> = torch.2_1_0.aten::expand(%1:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58387:0:0 %157:<12288xf16> = torch.2_1_0.aten::expand(%119:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %156:<4608x12288xf16> = torch.2_1_0.aten::expand(%77:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %158:<4608xf16> = torch.2_1_0.aten::expand(%96:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58387:0:0 %111:<12288xf16> = torch.2_1_0.aten::expand(%87:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %156:<12288xf16> = torch.2_1_0.aten::expand(%101:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58394:7:0 %83:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred) 
58387:0:0 %159:<6144x12288xf16> = torch.2_1_0.aten::expand(%52:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58387:0:0 %160:<6144xf16> = torch.2_1_0.aten::expand(%82:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58394:7:0 %84:<12288xf16> = torch.2_1_0.aten::detach(%56:<12288xf16>) 
58387:0:0 %161:<12288x6144xf16> = torch.2_1_0.aten::expand(%53:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58387:0:0 %162:<12288xf16> = torch.2_1_0.aten::expand(%46:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %153:<12288xf16> = torch.2_1_0.aten::expand(%48:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58387:0:0 %112:<12288xf16> = torch.2_1_0.aten::expand(%126:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58394:7:0 %69:<12288xf16> = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58394:7:0 %102:<12288xf16> = torch.2_1_0.aten::view(%69:<12288xf16>, list{12288:i32}) 
58394:7:0 %69:<12288xf16> = torch.2_1_0.aten::detach(%98:<12288xf16>) 
58394:7:0 %100:<12288xf16>+12288 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58394:7:0 %69:<12288xf16>+12288 = torch.2_1_0.aten::view(%100:<12288xf16>+12288, list{12288:i32}) 
58394:7:0 %100:<12288xf16> = torch.2_1_0.aten::detach(%96:<12288xf16>) 
58394:7:0 %44:<12288xf16>+24576 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58394:7:0 %100:<12288xf16>+24576 = torch.2_1_0.aten::view(%44:<12288xf16>+24576, list{12288:i32}) 
58394:7:0 %85:<12288x6144xf16> = torch.2_1_0.aten::detach(%94:<12288x6144xf16>) 
58394:7:0 %84:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58394:7:0 %44:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%84:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58387:0:0 %163:<1xi32> = torch.2_1_0.aten::lift_fresh(%163:<1xi32>) 
58394:7:0 %84:<6144xf16> = torch.2_1_0.aten::detach(%93:<6144xf16>) 
58394:7:0 %55:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58394:7:0 %84:<6144xf16>+75534336 = torch.2_1_0.aten::view(%55:<6144xf16>+75534336, list{6144:i32}) 
58394:7:0 %64:<6144x12288xf16> = torch.2_1_0.aten::detach(%91:<6144x12288xf16>) 
58394:7:0 %101:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58387:0:0 %133:<1xf32> = torch.2_1_0.aten::lift_fresh(%133:<1xf32>) 
58394:7:0 %104:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%101:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58387:0:0 %164:<1xf32> = torch.2_1_0.aten::lift_fresh(%164:<1xf32>) 
58394:7:0 %105:<12288xf16> = torch.2_1_0.aten::detach(%89:<12288xf16>) 
58387:0:0 %165:<1xf32> = torch.2_1_0.aten::lift_fresh(%165:<1xf32>) 
58394:7:0 %106:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58387:0:0 %166:<1xf32> = torch.2_1_0.aten::lift_fresh(%166:<1xf32>) 
58394:7:0 %101:<12288xf16>+151037952 = torch.2_1_0.aten::view(%106:<12288xf16>+151037952, list{12288:i32}) 
58387:0:0 %167:<1xf32> = torch.2_1_0.aten::lift_fresh(%167:<1xf32>) 
58394:7:0 %106:<12288xf16> = torch.2_1_0.aten::detach(%31:<12288xf16>) 
58387:0:0 %168:<1xi32> = torch.2_1_0.aten::lift_fresh(%168:<1xi32>) 
58394:7:0 %55:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58387:0:0 %169:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58394:7:0 %106:<12288xf16>+151050240 = torch.2_1_0.aten::view(%55:<12288xf16>+151050240, list{12288:i32}) 
58387:0:0 %170:<6400x12288xf16> = torch.2_1_0.aten::detach(%169:<6400x12288xf16>) 
58394:7:0 %55:<4608xf16> = torch.2_1_0.aten::detach(%47:<4608xf16>) 
58394:7:0 %107:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58394:7:0 %55:<4608xf16>+151062528 = torch.2_1_0.aten::view(%107:<4608xf16>+151062528, list{4608:i32}) 
58394:7:0 %108:<4608x12288xf16> = torch.2_1_0.aten::detach(%88:<4608x12288xf16>) 
58394:7:0 %109:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58394:7:0 %107:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%109:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58394:7:0 %109:<12288xf16> = torch.2_1_0.aten::detach(%32:<12288xf16>) 
58394:7:0 %110:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58394:7:0 %109:<12288xf16>+207690240 = torch.2_1_0.aten::view(%110:<12288xf16>+207690240, list{12288:i32}) 
58394:7:0 %111:<12288x1536xf16> = torch.2_1_0.aten::detach(%50:<12288x1536xf16>) 
58394:7:0 %112:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58394:7:0 %110:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%112:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58394:7:0 %112:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58394:7:0 %113:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58389:2:0 %107:<12288xf32> = torch.2_1_0.aten::_to_copy(%100:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %112:<12288xf16>+226576896 = torch.2_1_0.aten::view(%113:<12288xf16>+226576896, list{12288:i32}) 
58394:7:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58389:2:0 %108:<12288xf32> = torch.2_1_0.aten::_to_copy(%9:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %114:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58394:7:0 %113:<12288xf16>+226589184 = torch.2_1_0.aten::view(%114:<12288xf16>+226589184, list{12288:i32}) 
58394:7:0 %115:<2048x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x12288xf16>) 
58390:3:0 %3:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred) 
58389:2:0 %109:<12288xf32> = torch.2_1_0.aten::_to_copy(%70:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %114:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58394:7:0 %116:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%114:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58389:2:0 %110:<12288xf32> = torch.2_1_0.aten::_to_copy(%60:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %131:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58394:7:0 %117:<6400x12288xf16> = torch.2_1_0.aten::detach(%12:<6400x12288xf16>) 
58394:7:0 %118:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58394:7:0 %119:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%118:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58390:3:0 %133:<12288xf16> = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58394:7:0 %120:<330410496xf16> = torch.2_1_0.aten::slice(%83:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58394:7:0 %121:<330410496xf16> = torch.2_1_0.aten::view(%120:<330410496xf16>, list{330410496:i32}) 
58390:3:0 %13:<12288xf16> = torch.2_1_0.aten::view(%133:<12288xf16>, list{12288:i32}) 
58390:3:0 %134:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58390:3:0 %39:<12288xf16>+12288 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58394:7:0 %122:<6400x12288xf16> = torch.2_1_0.aten::expand(%12:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %132:<12288xf16>+12288 = torch.2_1_0.aten::view(%39:<12288xf16>+12288, list{12288:i32}) 
58390:3:0 %33:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58390:3:0 %2:<12288xf16>+24576 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58390:3:0 %42:<12288xf16>+24576 = torch.2_1_0.aten::view(%2:<12288xf16>+24576, list{12288:i32}) 
58390:3:0 %15:<12288x6144xf16> = torch.2_1_0.aten::detach(%105:<12288x6144xf16>) 
58394:7:0 %123:<2048x12288xf16> = torch.2_1_0.aten::expand(%8:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %109:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58394:7:0 %124:<12288xf16> = torch.2_1_0.aten::expand(%2:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %111:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%109:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58390:3:0 %109:<6144xf16> = torch.2_1_0.aten::detach(%64:<6144xf16>) 
58394:7:0 %125:<12288xf16> = torch.2_1_0.aten::expand(%87:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %114:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58390:3:0 %113:<6144xf16>+75534336 = torch.2_1_0.aten::view(%114:<6144xf16>+75534336, list{6144:i32}) 
58394:7:0 %126:<12288x1536xf16> = torch.2_1_0.aten::expand(%50:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58390:3:0 %109:<6144x12288xf16> = torch.2_1_0.aten::detach(%95:<6144x12288xf16>) 
58394:7:0 %127:<12288xf16> = torch.2_1_0.aten::expand(%32:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %107:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58390:3:0 %135:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%107:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58394:7:0 %128:<4608x12288xf16> = torch.2_1_0.aten::expand(%88:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %114:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58394:7:0 %129:<4608xf16> = torch.2_1_0.aten::expand(%47:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58390:3:0 %109:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58390:3:0 %114:<12288xf16>+151037952 = torch.2_1_0.aten::view(%109:<12288xf16>+151037952, list{12288:i32}) 
58394:7:0 %125:<12288xf16> = torch.2_1_0.aten::expand(%31:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %136:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58394:7:0 %128:<12288xf16> = torch.2_1_0.aten::expand(%89:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %103:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58390:3:0 %137:<12288xf16>+151050240 = torch.2_1_0.aten::view(%103:<12288xf16>+151050240, list{12288:i32}) 
58394:7:0 %127:<6144x12288xf16> = torch.2_1_0.aten::expand(%91:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %18:<4608xf16> = torch.2_1_0.aten::detach(%74:<4608xf16>) 
58390:3:0 %109:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58394:7:0 %126:<6144xf16> = torch.2_1_0.aten::expand(%93:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58390:3:0 %136:<4608xf16>+151062528 = torch.2_1_0.aten::view(%109:<4608xf16>+151062528, list{4608:i32}) 
58394:7:0 %130:<12288x6144xf16> = torch.2_1_0.aten::expand(%94:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58390:3:0 %138:<4608x12288xf16> = torch.2_1_0.aten::detach(%121:<4608x12288xf16>) 
58390:3:0 %139:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58394:7:0 %131:<12288xf16> = torch.2_1_0.aten::expand(%96:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %140:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%139:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58394:7:0 %132:<12288xf16> = torch.2_1_0.aten::expand(%98:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %103:<12288xf16> = torch.2_1_0.aten::detach(%61:<12288xf16>) 
58390:3:0 %107:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58394:7:0 %42:<12288xf16> = torch.2_1_0.aten::expand(%56:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %141:<12288xf16>+207690240 = torch.2_1_0.aten::view(%107:<12288xf16>+207690240, list{12288:i32}) 
58390:3:0 %138:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58390:3:0 %142:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58390:3:0 %139:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%142:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58390:3:0 %142:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58390:3:0 %109:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58390:3:0 %143:<12288xf16>+226576896 = torch.2_1_0.aten::view(%109:<12288xf16>+226576896, list{12288:i32}) 
58390:3:0 %18:<12288xf16> = torch.2_1_0.aten::detach(%59:<12288xf16>) 
58390:3:0 %103:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58390:3:0 %142:<12288xf16>+226589184 = torch.2_1_0.aten::view(%103:<12288xf16>+226589184, list{12288:i32}) 
58390:3:0 %18:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58390:3:0 %109:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58390:3:0 %144:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%109:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58390:3:0 %18:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58394:7:0 %133:<1xi32> = torch.2_1_0.aten::lift_fresh(%133:<1xi32>) 
58390:3:0 %103:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58390:3:0 %145:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%103:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58390:3:0 %18:<330410496xf16> = torch.2_1_0.aten::slice(%3:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58390:3:0 %130:<330410496xf16> = torch.2_1_0.aten::view(%18:<330410496xf16>, list{330410496:i32}) 
58394:7:0 %120:<1xf32> = torch.2_1_0.aten::lift_fresh(%120:<1xf32>) 
58390:3:0 %146:<6400x12288xf16> = torch.2_1_0.aten::expand(%116:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58394:7:0 %41:<1xf32> = torch.2_1_0.aten::lift_fresh(%41:<1xf32>) 
58394:7:0 %134:<1xf32> = torch.2_1_0.aten::lift_fresh(%134:<1xf32>) 
58394:7:0 %135:<1xf32> = torch.2_1_0.aten::lift_fresh(%135:<1xf32>) 
58394:7:0 %136:<1xf32> = torch.2_1_0.aten::lift_fresh(%136:<1xf32>) 
58390:3:0 %147:<2048x12288xf16> = torch.2_1_0.aten::expand(%118:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58394:7:0 %137:<1xi32> = torch.2_1_0.aten::lift_fresh(%137:<1xi32>) 
58390:3:0 %148:<12288xf16> = torch.2_1_0.aten::expand(%59:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58394:7:0 %138:<6400x12288xf16> = torch.2_1_0.aten::detach(%12:<6400x12288xf16>) 
58390:3:0 %147:<12288xf16> = torch.2_1_0.aten::expand(%119:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58394:7:0 %139:<6400x12288xf16> = torch.2_1_0.aten::detach(%138:<6400x12288xf16>) 
58390:3:0 %148:<12288x1536xf16> = torch.2_1_0.aten::expand(%120:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58390:3:0 %147:<12288xf16> = torch.2_1_0.aten::expand(%61:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %148:<4608x12288xf16> = torch.2_1_0.aten::expand(%121:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %37:<4608xf16> = torch.2_1_0.aten::expand(%74:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58390:3:0 %149:<12288xf16> = torch.2_1_0.aten::expand(%124:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %128:<12288xf16> = torch.2_1_0.aten::expand(%48:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %148:<6144x12288xf16> = torch.2_1_0.aten::expand(%95:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58390:3:0 %129:<6144xf16> = torch.2_1_0.aten::expand(%64:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58390:3:0 %148:<12288x6144xf16> = torch.2_1_0.aten::expand(%105:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58390:3:0 %150:<12288xf16> = torch.2_1_0.aten::expand(%104:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %151:<12288xf16> = torch.2_1_0.aten::expand(%123:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %18:<12288xf16> = torch.2_1_0.aten::expand(%126:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58390:3:0 %152:<1xi32> = torch.2_1_0.aten::lift_fresh(%152:<1xi32>) 
58390:3:0 %153:<1xf32> = torch.2_1_0.aten::lift_fresh(%153:<1xf32>) 
58390:3:0 %154:<1xf32> = torch.2_1_0.aten::lift_fresh(%154:<1xf32>) 
58390:3:0 %155:<1xf32> = torch.2_1_0.aten::lift_fresh(%155:<1xf32>) 
58390:3:0 %156:<1xf32> = torch.2_1_0.aten::lift_fresh(%156:<1xf32>) 
58390:3:0 %157:<1xf32> = torch.2_1_0.aten::lift_fresh(%157:<1xf32>) 
58390:3:0 %158:<1xi32> = torch.2_1_0.aten::lift_fresh(%158:<1xi32>) 
58390:3:0 %159:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58390:3:0 %160:<6400x12288xf16> = torch.2_1_0.aten::detach(%159:<6400x12288xf16>) 
58394:7:0 %140:<6400x12288xf16> = torch.2_1_0.aten::clone(%139:<6400x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %141:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%140:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %142:<2048x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x12288xf16>) 
58394:7:0 %140:<2048x12288xf16> = torch.2_1_0.aten::detach(%142:<2048x12288xf16>) 
58394:7:0 %143:<2048x12288xf16> = torch.2_1_0.aten::clone(%140:<2048x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %144:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%143:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %145:<12288x1536xf16> = torch.2_1_0.aten::detach(%50:<12288x1536xf16>) 
58394:7:0 %143:<12288x1536xf16> = torch.2_1_0.aten::detach(%145:<12288x1536xf16>) 
58394:7:0 %142:<12288x1536xf16> = torch.2_1_0.aten::clone(%143:<12288x1536xf16>, memory_format=None:NoneType) 
58394:7:0 %146:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%142:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %147:<4608x12288xf16> = torch.2_1_0.aten::detach(%88:<4608x12288xf16>) 
58394:7:0 %145:<4608x12288xf16> = torch.2_1_0.aten::detach(%147:<4608x12288xf16>) 
58394:7:0 %139:<4608x12288xf16> = torch.2_1_0.aten::clone(%145:<4608x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %148:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%139:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %140:<6144x12288xf16> = torch.2_1_0.aten::detach(%91:<6144x12288xf16>) 
58394:7:0 %139:<6144x12288xf16> = torch.2_1_0.aten::detach(%140:<6144x12288xf16>) 
58390:3:0 %161:<6400x12288xf16> = torch.2_1_0.aten::clone(%160:<6400x12288xf16>, memory_format=None:NoneType) 
58390:3:0 %162:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%161:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %163:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58390:3:0 %161:<2048x12288xf16> = torch.2_1_0.aten::detach(%163:<2048x12288xf16>) 
58390:3:0 %164:<2048x12288xf16> = torch.2_1_0.aten::clone(%161:<2048x12288xf16>, memory_format=None:NoneType) 
58390:3:0 %165:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%164:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %163:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58390:3:0 %164:<12288x1536xf16> = torch.2_1_0.aten::detach(%163:<12288x1536xf16>) 
58390:3:0 %160:<12288x1536xf16> = torch.2_1_0.aten::clone(%164:<12288x1536xf16>, memory_format=None:NoneType) 
58390:3:0 %159:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%160:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %161:<4608x12288xf16> = torch.2_1_0.aten::detach(%121:<4608x12288xf16>) 
58390:3:0 %160:<4608x12288xf16> = torch.2_1_0.aten::detach(%161:<4608x12288xf16>) 
58390:3:0 %163:<4608x12288xf16> = torch.2_1_0.aten::clone(%160:<4608x12288xf16>, memory_format=None:NoneType) 
58390:3:0 %161:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%163:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %47:<6144x12288xf16> = torch.2_1_0.aten::detach(%95:<6144x12288xf16>) 
58390:3:0 %164:<6144x12288xf16> = torch.2_1_0.aten::detach(%47:<6144x12288xf16>) 
58390:3:0 %99:<6144x12288xf16> = torch.2_1_0.aten::clone(%164:<6144x12288xf16>, memory_format=None:NoneType) 
58390:3:0 %164:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%99:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %160:<12288x6144xf16> = torch.2_1_0.aten::detach(%105:<12288x6144xf16>) 
58390:3:0 %83:<12288x6144xf16> = torch.2_1_0.aten::detach(%160:<12288x6144xf16>) 
58390:3:0 %127:<12288x6144xf16> = torch.2_1_0.aten::clone(%83:<12288x6144xf16>, memory_format=None:NoneType) 
58390:3:0 %47:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%127:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %99:<12288xf16> = torch.2_1_0.aten::detach(%59:<12288xf16>) 
58390:3:0 %163:<12288xf16> = torch.2_1_0.aten::detach(%99:<12288xf16>) 
58390:3:0 %166:<12288xf16> = torch.2_1_0.aten::clone(%163:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %167:<12288xf32> = torch.2_1_0.aten::_to_copy(%166:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %166:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58390:3:0 %127:<12288xf16> = torch.2_1_0.aten::detach(%166:<12288xf16>) 
58390:3:0 %168:<12288xf16> = torch.2_1_0.aten::clone(%127:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %169:<12288xf32> = torch.2_1_0.aten::_to_copy(%168:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %170:<12288xf16> = torch.2_1_0.aten::detach(%61:<12288xf16>) 
58390:3:0 %166:<12288xf16> = torch.2_1_0.aten::detach(%170:<12288xf16>) 
58390:3:0 %168:<12288xf16> = torch.2_1_0.aten::clone(%166:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %166:<12288xf32> = torch.2_1_0.aten::_to_copy(%168:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %163:<4608xf16> = torch.2_1_0.aten::detach(%74:<4608xf16>) 
58390:3:0 %168:<4608xf16> = torch.2_1_0.aten::detach(%163:<4608xf16>) 
58390:3:0 %127:<4608xf16> = torch.2_1_0.aten::clone(%168:<4608xf16>, memory_format=None:NoneType) 
58390:3:0 %168:<4608xf32> = torch.2_1_0.aten::_to_copy(%127:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %171:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58390:3:0 %172:<12288xf16> = torch.2_1_0.aten::detach(%171:<12288xf16>) 
58390:3:0 %171:<12288xf16> = torch.2_1_0.aten::clone(%172:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %172:<12288xf32> = torch.2_1_0.aten::_to_copy(%171:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %163:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58390:3:0 %173:<12288xf16> = torch.2_1_0.aten::detach(%163:<12288xf16>) 
58390:3:0 %171:<12288xf16> = torch.2_1_0.aten::clone(%173:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %173:<12288xf32> = torch.2_1_0.aten::_to_copy(%171:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %170:<6144xf16> = torch.2_1_0.aten::detach(%64:<6144xf16>) 
58390:3:0 %171:<6144xf16> = torch.2_1_0.aten::detach(%170:<6144xf16>) 
58390:3:0 %83:<6144xf16> = torch.2_1_0.aten::clone(%171:<6144xf16>, memory_format=None:NoneType) 
58390:3:0 %171:<6144xf32> = torch.2_1_0.aten::_to_copy(%83:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58390:3:0 %175:<12288xf16> = torch.2_1_0.aten::detach(%174:<12288xf16>) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%175:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %175:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58390:3:0 %176:<12288xf16> = torch.2_1_0.aten::detach(%174:<12288xf16>) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%176:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %171:<6400x12288xf16> = torch.2_1_0.aten::clone(%170:<6400x12288xf16>, memory_format=None:NoneType) 
58390:3:0 %176:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58390:3:0 %177:<12288xf16> = torch.2_1_0.aten::detach(%174:<12288xf16>) 
58390:3:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%177:<12288xf16>, memory_format=None:NoneType) 
58390:3:0 %177:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %88:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%88:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 %147:<6144x12288xf16> = torch.2_1_0.aten::clone(%139:<6144x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %143:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%147:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %149:<12288x6144xf16> = torch.2_1_0.aten::detach(%94:<12288x6144xf16>) 
58394:7:0 %147:<12288x6144xf16> = torch.2_1_0.aten::detach(%149:<12288x6144xf16>) 
58394:7:0 %140:<12288x6144xf16> = torch.2_1_0.aten::clone(%147:<12288x6144xf16>, memory_format=None:NoneType) 
58387:0:0 %172:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%171:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %173:<2048x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x12288xf16>) 
58387:0:0 %174:<2048x12288xf16> = torch.2_1_0.aten::detach(%173:<2048x12288xf16>) 
58387:0:0 %175:<2048x12288xf16> = torch.2_1_0.aten::clone(%174:<2048x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %142:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%140:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %149:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58387:0:0 %176:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%175:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %147:<12288xf16> = torch.2_1_0.aten::detach(%149:<12288xf16>) 
58387:0:0 %174:<12288x1536xf16> = torch.2_1_0.aten::detach(%1:<12288x1536xf16>) 
58387:0:0 %177:<12288x1536xf16> = torch.2_1_0.aten::detach(%174:<12288x1536xf16>) 
58394:7:0 %140:<12288xf16> = torch.2_1_0.aten::clone(%147:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %178:<12288x1536xf16> = torch.2_1_0.aten::clone(%177:<12288x1536xf16>, memory_format=None:NoneType) 
58387:0:0 %173:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%178:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %150:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %179:<4608x12288xf16> = torch.2_1_0.aten::detach(%77:<4608x12288xf16>) 
58394:7:0 %145:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58387:0:0 %180:<4608x12288xf16> = torch.2_1_0.aten::detach(%179:<4608x12288xf16>) 
58394:7:0 %149:<12288xf16> = torch.2_1_0.aten::detach(%145:<12288xf16>) 
58394:7:0 %140:<12288xf16> = torch.2_1_0.aten::clone(%149:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %147:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %139:<12288xf16> = torch.2_1_0.aten::detach(%32:<12288xf16>) 
58394:7:0 %145:<12288xf16> = torch.2_1_0.aten::detach(%139:<12288xf16>) 
58387:0:0 %181:<4608x12288xf16> = torch.2_1_0.aten::clone(%180:<4608x12288xf16>, memory_format=None:NoneType) 
58394:7:0 %140:<12288xf16> = torch.2_1_0.aten::clone(%145:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %125:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%127:<2048x12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %149:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %151:<4608xf16> = torch.2_1_0.aten::detach(%47:<4608xf16>) 
58394:7:0 %145:<4608xf16> = torch.2_1_0.aten::detach(%151:<4608xf16>) 
58394:7:0 %140:<4608xf16> = torch.2_1_0.aten::clone(%145:<4608xf16>, memory_format=None:NoneType) 
58394:7:0 %139:<4608xf32> = torch.2_1_0.aten::_to_copy(%140:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %145:<12288xf16> = torch.2_1_0.aten::detach(%31:<12288xf16>) 
58394:7:0 %152:<12288xf16> = torch.2_1_0.aten::detach(%145:<12288xf16>) 
58394:7:0 %140:<12288xf16> = torch.2_1_0.aten::clone(%152:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %151:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %153:<12288xf16> = torch.2_1_0.aten::detach(%89:<12288xf16>) 
58394:7:0 %152:<12288xf16> = torch.2_1_0.aten::detach(%153:<12288xf16>) 
58394:7:0 %154:<12288xf16> = torch.2_1_0.aten::clone(%152:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %140:<12288xf32> = torch.2_1_0.aten::_to_copy(%154:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %145:<6144xf16> = torch.2_1_0.aten::detach(%93:<6144xf16>) 
58394:7:0 %152:<6144xf16> = torch.2_1_0.aten::detach(%145:<6144xf16>) 
58394:7:0 %154:<6144xf16> = torch.2_1_0.aten::clone(%152:<6144xf16>, memory_format=None:NoneType) 
58394:7:0 %153:<6144xf32> = torch.2_1_0.aten::_to_copy(%154:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %152:<12288xf16> = torch.2_1_0.aten::detach(%96:<12288xf16>) 
58394:7:0 %155:<12288xf16> = torch.2_1_0.aten::detach(%152:<12288xf16>) 
58394:7:0 %154:<12288xf16> = torch.2_1_0.aten::clone(%155:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %145:<12288xf32> = torch.2_1_0.aten::_to_copy(%154:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %156:<12288xf16> = torch.2_1_0.aten::detach(%98:<12288xf16>) 
58394:7:0 %152:<12288xf16> = torch.2_1_0.aten::detach(%156:<12288xf16>) 
58394:7:0 %154:<12288xf16> = torch.2_1_0.aten::clone(%152:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %155:<12288xf32> = torch.2_1_0.aten::_to_copy(%154:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %157:<12288xf16> = torch.2_1_0.aten::detach(%56:<12288xf16>) 
58394:7:0 %156:<12288xf16> = torch.2_1_0.aten::detach(%157:<12288xf16>) 
58394:7:0 %154:<12288xf16> = torch.2_1_0.aten::clone(%156:<12288xf16>, memory_format=None:NoneType) 
58394:7:0 %152:<12288xf32> = torch.2_1_0.aten::_to_copy(%154:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %154:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%154:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %81:<12288xf32> = torch.2_1_0.aten::_to_copy(%128:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %89:<12288xf32> = torch.2_1_0.aten::_to_copy(%37:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %47:<12288xf32> = torch.2_1_0.aten::_to_copy(%132:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %52:<12288xf32> = torch.2_1_0.aten::_to_copy(%105:<12288xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %179:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%181:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %174:<6144x12288xf16> = torch.2_1_0.aten::detach(%52:<6144x12288xf16>) 
58387:0:0 %182:<6144x12288xf16> = torch.2_1_0.aten::detach(%174:<6144x12288xf16>) 
58387:0:0 %183:<6144x12288xf16> = torch.2_1_0.aten::clone(%182:<6144x12288xf16>, memory_format=None:NoneType) 
58387:0:0 %174:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%183:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %184:<12288x6144xf16> = torch.2_1_0.aten::detach(%53:<12288x6144xf16>) 
58387:0:0 %181:<12288x6144xf16> = torch.2_1_0.aten::detach(%184:<12288x6144xf16>) 
58387:0:0 %178:<12288x6144xf16> = torch.2_1_0.aten::clone(%181:<12288x6144xf16>, memory_format=None:NoneType) 
58387:0:0 %185:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%178:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %186:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58387:0:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%186:<12288xf16>) 
58387:0:0 %181:<12288xf16> = torch.2_1_0.aten::clone(%187:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %178:<12288xf32> = torch.2_1_0.aten::_to_copy(%181:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %188:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58387:0:0 %189:<12288xf16> = torch.2_1_0.aten::detach(%188:<12288xf16>) 
58387:0:0 %186:<12288xf16> = torch.2_1_0.aten::clone(%189:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %181:<12288xf32> = torch.2_1_0.aten::_to_copy(%186:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58387:0:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%190:<12288xf16>) 
58387:0:0 %188:<12288xf16> = torch.2_1_0.aten::clone(%191:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %192:<12288xf32> = torch.2_1_0.aten::_to_copy(%188:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %193:<4608xf16> = torch.2_1_0.aten::detach(%96:<4608xf16>) 
58387:0:0 %194:<4608xf16> = torch.2_1_0.aten::detach(%193:<4608xf16>) 
58387:0:0 %195:<4608xf16> = torch.2_1_0.aten::clone(%194:<4608xf16>, memory_format=None:NoneType) 
58387:0:0 %194:<4608xf32> = torch.2_1_0.aten::_to_copy(%195:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %196:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58387:0:0 %197:<12288xf16> = torch.2_1_0.aten::detach(%196:<12288xf16>) 
58387:0:0 %198:<12288xf16> = torch.2_1_0.aten::clone(%197:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %193:<12288xf32> = torch.2_1_0.aten::_to_copy(%198:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %199:<12288xf16> = torch.2_1_0.aten::detach(%101:<12288xf16>) 
58387:0:0 %200:<12288xf16> = torch.2_1_0.aten::detach(%199:<12288xf16>) 
58387:0:0 %201:<12288xf16> = torch.2_1_0.aten::clone(%200:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %202:<12288xf32> = torch.2_1_0.aten::_to_copy(%201:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %203:<6144xf16> = torch.2_1_0.aten::detach(%82:<6144xf16>) 
58387:0:0 %204:<6144xf16> = torch.2_1_0.aten::detach(%203:<6144xf16>) 
58387:0:0 %203:<6144xf16> = torch.2_1_0.aten::clone(%204:<6144xf16>, memory_format=None:NoneType) 
58387:0:0 %196:<6144xf32> = torch.2_1_0.aten::_to_copy(%203:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %205:<12288xf16> = torch.2_1_0.aten::detach(%46:<12288xf16>) 
58387:0:0 %206:<12288xf16> = torch.2_1_0.aten::detach(%205:<12288xf16>) 
58387:0:0 %207:<12288xf16> = torch.2_1_0.aten::clone(%206:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %205:<12288xf32> = torch.2_1_0.aten::_to_copy(%207:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %208:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58387:0:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%208:<12288xf16>) 
58387:0:0 %210:<12288xf16> = torch.2_1_0.aten::clone(%209:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %207:<12288xf32> = torch.2_1_0.aten::_to_copy(%210:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %211:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %208:<12288xf16> = torch.2_1_0.aten::detach(%211:<12288xf16>) 
58387:0:0 %211:<12288xf16> = torch.2_1_0.aten::clone(%208:<12288xf16>, memory_format=None:NoneType) 
58387:0:0 %212:<12288xf32> = torch.2_1_0.aten::_to_copy(%211:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %213:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%213:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %81:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%34:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %108:<12288xf16> = torch.2_1_0.aten::_to_copy(%100:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %109:<12288xf16> = torch.2_1_0.aten::_to_copy(%9:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %88:<12288xf16> = torch.2_1_0.aten::_to_copy(%70:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %111:<12288xf16> = torch.2_1_0.aten::_to_copy(%60:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %112:<12288xf16> = torch.2_1_0.aten::detach(%95:<12288xf16>) 
58389:2:0 %113:<12288xf16> = torch.2_1_0.aten::detach(%42:<12288xf16>) 
58389:2:0 %61:<12288xf16> = torch.2_1_0.aten::detach(%99:<12288xf16>) 
58389:2:0 %111:<12288x6144xf16> = torch.2_1_0.aten::detach(%102:<12288x6144xf16>) 
58389:2:0 %113:<6144xf16> = torch.2_1_0.aten::detach(%103:<6144xf16>) 
58389:2:0 %81:<6144x12288xf16> = torch.2_1_0.aten::detach(%84:<6144x12288xf16>) 
58389:2:0 %61:<12288xf16> = torch.2_1_0.aten::detach(%60:<12288xf16>) 
58389:2:0 %88:<12288xf16> = torch.2_1_0.aten::detach(%70:<12288xf16>) 
58389:2:0 %61:<4608xf16> = torch.2_1_0.aten::detach(%18:<4608xf16>) 
58389:2:0 %109:<4608x12288xf16> = torch.2_1_0.aten::detach(%12:<4608x12288xf16>) 
58389:2:0 %85:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58389:2:0 %114:<12288x1536xf16> = torch.2_1_0.aten::detach(%13:<12288x1536xf16>) 
58389:2:0 %88:<12288xf16> = torch.2_1_0.aten::detach(%9:<12288xf16>) 
58389:2:0 %115:<12288xf16> = torch.2_1_0.aten::detach(%100:<12288xf16>) 
58389:2:0 %116:<2048x12288xf16> = torch.2_1_0.aten::detach(%34:<2048x12288xf16>) 
58389:2:0 %107:<6400x12288xf16> = torch.2_1_0.aten::detach(%98:<6400x12288xf16>) 
58389:2:0 %114:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred) 
58389:2:0 %53:<12288xf16> = torch.2_1_0.aten::detach(%95:<12288xf16>) 
58389:2:0 %117:<12288xf16> = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58389:2:0 %118:<12288xf16> = torch.2_1_0.aten::view(%117:<12288xf16>, list{12288:i32}) 
58389:2:0 %119:<12288xf16> = torch.2_1_0.aten::detach(%42:<12288xf16>) 
58389:2:0 %120:<12288xf16>+12288 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58389:2:0 %121:<12288xf16>+12288 = torch.2_1_0.aten::view(%120:<12288xf16>+12288, list{12288:i32}) 
58389:2:0 %122:<12288xf16> = torch.2_1_0.aten::detach(%99:<12288xf16>) 
58389:2:0 %123:<12288xf16>+24576 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58389:2:0 %124:<12288xf16>+24576 = torch.2_1_0.aten::view(%123:<12288xf16>+24576, list{12288:i32}) 
58389:2:0 %125:<12288x6144xf16> = torch.2_1_0.aten::detach(%102:<12288x6144xf16>) 
58389:2:0 %126:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58389:2:0 %127:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%126:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58389:2:0 %128:<6144xf16> = torch.2_1_0.aten::detach(%103:<6144xf16>) 
58389:2:0 %129:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58389:2:0 %130:<6144xf16>+75534336 = torch.2_1_0.aten::view(%129:<6144xf16>+75534336, list{6144:i32}) 
58389:2:0 %131:<6144x12288xf16> = torch.2_1_0.aten::detach(%84:<6144x12288xf16>) 
58389:2:0 %132:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58389:2:0 %133:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%132:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58389:2:0 %134:<12288xf16> = torch.2_1_0.aten::detach(%60:<12288xf16>) 
58389:2:0 %71:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58389:2:0 %135:<12288xf16>+151037952 = torch.2_1_0.aten::view(%71:<12288xf16>+151037952, list{12288:i32}) 
58389:2:0 %134:<12288xf16> = torch.2_1_0.aten::detach(%70:<12288xf16>) 
58389:2:0 %136:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58389:2:0 %132:<12288xf16>+151050240 = torch.2_1_0.aten::view(%136:<12288xf16>+151050240, list{12288:i32}) 
58389:2:0 %134:<4608xf16> = torch.2_1_0.aten::detach(%18:<4608xf16>) 
58389:2:0 %71:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58389:2:0 %131:<4608xf16>+151062528 = torch.2_1_0.aten::view(%71:<4608xf16>+151062528, list{4608:i32}) 
58389:2:0 %137:<4608x12288xf16> = torch.2_1_0.aten::detach(%12:<4608x12288xf16>) 
58389:2:0 %138:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58389:2:0 %139:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%138:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58389:2:0 %136:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58389:2:0 %93:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58389:2:0 %140:<12288xf16>+207690240 = torch.2_1_0.aten::view(%93:<12288xf16>+207690240, list{12288:i32}) 
58389:2:0 %137:<12288x1536xf16> = torch.2_1_0.aten::detach(%13:<12288x1536xf16>) 
58389:2:0 %141:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58389:2:0 %138:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%141:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58389:2:0 %134:<12288xf16> = torch.2_1_0.aten::detach(%9:<12288xf16>) 
58389:2:0 %136:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58391:4:0 %80:<2048x12288xf16> = torch.2_1_0.aten::_to_copy(%127:<2048x12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %142:<12288xf16>+226576896 = torch.2_1_0.aten::view(%136:<12288xf16>+226576896, list{12288:i32}) 
58389:2:0 %137:<12288xf16> = torch.2_1_0.aten::detach(%100:<12288xf16>) 
58389:2:0 %136:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58389:2:0 %141:<12288xf16>+226589184 = torch.2_1_0.aten::view(%136:<12288xf16>+226589184, list{12288:i32}) 
58391:4:0 %95:<12288xf16> = torch.2_1_0.aten::_to_copy(%128:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %137:<2048x12288xf16> = torch.2_1_0.aten::detach(%34:<2048x12288xf16>) 
58389:2:0 %143:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58391:4:0 %77:<12288xf16> = torch.2_1_0.aten::_to_copy(%37:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %144:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%143:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58389:2:0 %71:<6400x12288xf16> = torch.2_1_0.aten::detach(%98:<6400x12288xf16>) 
58389:2:0 %93:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58389:2:0 %143:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%93:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58391:4:0 %52:<12288xf16> = torch.2_1_0.aten::_to_copy(%132:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %71:<330410496xf16> = torch.2_1_0.aten::slice(%114:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58389:2:0 %145:<330410496xf16> = torch.2_1_0.aten::view(%71:<330410496xf16>, list{330410496:i32}) 
58391:4:0 %83:<12288xf16> = torch.2_1_0.aten::_to_copy(%105:<12288xf32>, dtype=torch.float16:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %146:<6400x12288xf16> = torch.2_1_0.aten::expand(%98:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %135:<12288xf16> = torch.2_1_0.aten::detach(%102:<12288xf16>) 
58389:2:0 %147:<2048x12288xf16> = torch.2_1_0.aten::expand(%34:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %136:<12288xf16> = torch.2_1_0.aten::detach(%134:<12288xf16>) 
58391:4:0 %136:<12288xf16> = torch.2_1_0.aten::detach(%88:<12288xf16>) 
58389:2:0 %148:<12288xf16> = torch.2_1_0.aten::expand(%100:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %52:<12288x6144xf16> = torch.2_1_0.aten::detach(%33:<12288x6144xf16>) 
58391:4:0 %136:<6144xf16> = torch.2_1_0.aten::detach(%104:<6144xf16>) 
58389:2:0 %149:<12288xf16> = torch.2_1_0.aten::expand(%9:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %47:<6144x12288xf16> = torch.2_1_0.aten::detach(%96:<6144x12288xf16>) 
58391:4:0 %95:<12288xf16> = torch.2_1_0.aten::detach(%105:<12288xf16>) 
58389:2:0 %148:<12288x1536xf16> = torch.2_1_0.aten::expand(%13:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58391:4:0 %53:<12288xf16> = torch.2_1_0.aten::detach(%132:<12288xf16>) 
58389:2:0 %149:<12288xf16> = torch.2_1_0.aten::expand(%38:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %95:<4608xf16> = torch.2_1_0.aten::detach(%115:<4608xf16>) 
58391:4:0 %83:<4608x12288xf16> = torch.2_1_0.aten::detach(%131:<4608x12288xf16>) 
58389:2:0 %150:<4608x12288xf16> = torch.2_1_0.aten::expand(%12:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %47:<12288xf16> = torch.2_1_0.aten::detach(%111:<12288xf16>) 
58389:2:0 %151:<4608xf16> = torch.2_1_0.aten::expand(%18:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58391:4:0 %125:<12288x1536xf16> = torch.2_1_0.aten::detach(%129:<12288x1536xf16>) 
58389:2:0 %150:<12288xf16> = torch.2_1_0.aten::expand(%70:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %137:<12288xf16> = torch.2_1_0.aten::detach(%37:<12288xf16>) 
58389:2:0 %152:<12288xf16> = torch.2_1_0.aten::expand(%60:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %75:<12288xf16> = torch.2_1_0.aten::detach(%128:<12288xf16>) 
58389:2:0 %148:<6144x12288xf16> = torch.2_1_0.aten::expand(%84:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %138:<2048x12288xf16> = torch.2_1_0.aten::detach(%127:<2048x12288xf16>) 
58389:2:0 %149:<6144xf16> = torch.2_1_0.aten::expand(%103:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58391:4:0 %52:<6400x12288xf16> = torch.2_1_0.aten::detach(%106:<6400x12288xf16>) 
58389:2:0 %148:<12288x6144xf16> = torch.2_1_0.aten::expand(%102:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58389:2:0 %149:<12288xf16> = torch.2_1_0.aten::expand(%99:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58389:2:0 %137:<12288xf16> = torch.2_1_0.aten::expand(%42:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58389:2:0 %107:<12288xf16> = torch.2_1_0.aten::expand(%95:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %62:<330410496xf16> = torch.2_1_0.aten::zeros(list{330410496:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred) 
58391:4:0 %139:<12288xf16> = torch.2_1_0.aten::detach(%102:<12288xf16>) 
58391:4:0 %78:<12288xf16> = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 0:i32, 12288:i32, 1:i32) 
58391:4:0 %140:<12288xf16> = torch.2_1_0.aten::view(%78:<12288xf16>, list{12288:i32}) 
58391:4:0 %141:<12288xf16> = torch.2_1_0.aten::detach(%134:<12288xf16>) 
58391:4:0 %99:<12288xf16>+12288 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 12288:i32, 24576:i32, 1:i32) 
58391:4:0 %78:<12288xf16>+12288 = torch.2_1_0.aten::view(%99:<12288xf16>+12288, list{12288:i32}) 
58391:4:0 %48:<12288xf16> = torch.2_1_0.aten::detach(%88:<12288xf16>) 
58389:2:0 %56:<1xi32> = torch.2_1_0.aten::lift_fresh(%56:<1xi32>) 
58391:4:0 %76:<12288xf16>+24576 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 24576:i32, 36864:i32, 1:i32) 
58391:4:0 %52:<12288xf16>+24576 = torch.2_1_0.aten::view(%76:<12288xf16>+24576, list{12288:i32}) 
58391:4:0 %99:<12288x6144xf16> = torch.2_1_0.aten::detach(%33:<12288x6144xf16>) 
58391:4:0 %94:<75497472xf16>+36864 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 36864:i32, 75534336:i32, 1:i32) 
58391:4:0 %90:<12288x6144xf16>+36864 = torch.2_1_0.aten::view(%94:<75497472xf16>+36864, list{12288:i32, 6144:i32}) 
58391:4:0 %73:<6144xf16> = torch.2_1_0.aten::detach(%104:<6144xf16>) 
58389:2:0 %71:<1xf32> = torch.2_1_0.aten::lift_fresh(%71:<1xf32>) 
58391:4:0 %94:<6144xf16>+75534336 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 75534336:i32, 75540480:i32, 1:i32) 
58389:2:0 %153:<1xf32> = torch.2_1_0.aten::lift_fresh(%153:<1xf32>) 
58391:4:0 %73:<6144xf16>+75534336 = torch.2_1_0.aten::view(%94:<6144xf16>+75534336, list{6144:i32}) 
58391:4:0 %142:<6144x12288xf16> = torch.2_1_0.aten::detach(%96:<6144x12288xf16>) 
58389:2:0 %154:<1xf32> = torch.2_1_0.aten::lift_fresh(%154:<1xf32>) 
58391:4:0 %143:<75497472xf16>+75540480 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 75540480:i32, 151037952:i32, 1:i32) 
58389:2:0 %155:<1xf32> = torch.2_1_0.aten::lift_fresh(%155:<1xf32>) 
58391:4:0 %144:<6144x12288xf16>+75540480 = torch.2_1_0.aten::view(%143:<75497472xf16>+75540480, list{6144:i32, 12288:i32}) 
58391:4:0 %145:<12288xf16> = torch.2_1_0.aten::detach(%105:<12288xf16>) 
58389:2:0 %156:<1xf32> = torch.2_1_0.aten::lift_fresh(%156:<1xf32>) 
58391:4:0 %143:<12288xf16>+151037952 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 151037952:i32, 151050240:i32, 1:i32) 
58389:2:0 %157:<1xi32> = torch.2_1_0.aten::lift_fresh(%157:<1xi32>) 
58391:4:0 %142:<12288xf16>+151037952 = torch.2_1_0.aten::view(%143:<12288xf16>+151037952, list{12288:i32}) 
58391:4:0 %146:<12288xf16> = torch.2_1_0.aten::detach(%132:<12288xf16>) 
58389:2:0 %158:<6400x12288xf16> = torch.2_1_0.aten::detach(%98:<6400x12288xf16>) 
58389:2:0 %159:<6400x12288xf16> = torch.2_1_0.aten::detach(%158:<6400x12288xf16>) 
58391:4:0 %143:<12288xf16>+151050240 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 151050240:i32, 151062528:i32, 1:i32) 
58391:4:0 %145:<12288xf16>+151050240 = torch.2_1_0.aten::view(%143:<12288xf16>+151050240, list{12288:i32}) 
58391:4:0 %143:<4608xf16> = torch.2_1_0.aten::detach(%115:<4608xf16>) 
58391:4:0 %147:<4608xf16>+151062528 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 151062528:i32, 151067136:i32, 1:i32) 
58391:4:0 %146:<4608xf16>+151062528 = torch.2_1_0.aten::view(%147:<4608xf16>+151062528, list{4608:i32}) 
58391:4:0 %148:<4608x12288xf16> = torch.2_1_0.aten::detach(%131:<4608x12288xf16>) 
58391:4:0 %143:<56623104xf16>+151067136 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 151067136:i32, 207690240:i32, 1:i32) 
58389:2:0 %160:<6400x12288xf16> = torch.2_1_0.aten::clone(%159:<6400x12288xf16>, memory_format=None:NoneType) 
58391:4:0 %147:<4608x12288xf16>+151067136 = torch.2_1_0.aten::view(%143:<56623104xf16>+151067136, list{4608:i32, 12288:i32}) 
58391:4:0 %149:<12288xf16> = torch.2_1_0.aten::detach(%111:<12288xf16>) 
58391:4:0 %107:<12288xf16>+207690240 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 207690240:i32, 207702528:i32, 1:i32) 
58391:4:0 %150:<12288xf16>+207690240 = torch.2_1_0.aten::view(%107:<12288xf16>+207690240, list{12288:i32}) 
58391:4:0 %151:<12288x1536xf16> = torch.2_1_0.aten::detach(%129:<12288x1536xf16>) 
58391:4:0 %152:<18874368xf16>+207702528 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 207702528:i32, 226576896:i32, 1:i32) 
58391:4:0 %153:<12288x1536xf16>+207702528 = torch.2_1_0.aten::view(%152:<18874368xf16>+207702528, list{12288:i32, 1536:i32}) 
58391:4:0 %100:<12288xf16> = torch.2_1_0.aten::detach(%37:<12288xf16>) 
58391:4:0 %152:<12288xf16>+226576896 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 226576896:i32, 226589184:i32, 1:i32) 
58391:4:0 %154:<12288xf16>+226576896 = torch.2_1_0.aten::view(%152:<12288xf16>+226576896, list{12288:i32}) 
58391:4:0 %155:<12288xf16> = torch.2_1_0.aten::detach(%128:<12288xf16>) 
58391:4:0 %152:<12288xf16>+226589184 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 226589184:i32, 226601472:i32, 1:i32) 
58391:4:0 %156:<12288xf16>+226589184 = torch.2_1_0.aten::view(%152:<12288xf16>+226589184, list{12288:i32}) 
58391:4:0 %157:<2048x12288xf16> = torch.2_1_0.aten::detach(%127:<2048x12288xf16>) 
58391:4:0 %152:<25165824xf16>+226601472 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 226601472:i32, 251767296:i32, 1:i32) 
58389:2:0 %161:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%160:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %155:<2048x12288xf16>+226601472 = torch.2_1_0.aten::view(%152:<25165824xf16>+226601472, list{2048:i32, 12288:i32}) 
58391:4:0 %158:<6400x12288xf16> = torch.2_1_0.aten::detach(%106:<6400x12288xf16>) 
58389:2:0 %162:<2048x12288xf16> = torch.2_1_0.aten::detach(%34:<2048x12288xf16>) 
58391:4:0 %152:<78643200xf16>+251767296 = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 251767296:i32, 330410496:i32, 1:i32) 
58389:2:0 %163:<2048x12288xf16> = torch.2_1_0.aten::detach(%162:<2048x12288xf16>) 
58391:4:0 %157:<6400x12288xf16>+251767296 = torch.2_1_0.aten::view(%152:<78643200xf16>+251767296, list{6400:i32, 12288:i32}) 
58391:4:0 %159:<330410496xf16> = torch.2_1_0.aten::slice(%62:<330410496xf16>, 0:i32, 0:i32, 330410496:i32, 1:i32) 
58391:4:0 %152:<330410496xf16> = torch.2_1_0.aten::view(%159:<330410496xf16>, list{330410496:i32}) 
58391:4:0 %158:<6400x12288xf16> = torch.2_1_0.aten::expand(%106:<6400x12288xf16>, list{6400:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %160:<2048x12288xf16> = torch.2_1_0.aten::expand(%127:<2048x12288xf16>, list{2048:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %161:<12288xf16> = torch.2_1_0.aten::expand(%128:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %135:<12288xf16> = torch.2_1_0.aten::expand(%37:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %161:<12288x1536xf16> = torch.2_1_0.aten::expand(%129:<12288x1536xf16>, list{12288:i32, 1536:i32}, implicit=False:pred) 
58391:4:0 %162:<12288xf16> = torch.2_1_0.aten::expand(%111:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %161:<4608x12288xf16> = torch.2_1_0.aten::expand(%131:<4608x12288xf16>, list{4608:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %162:<4608xf16> = torch.2_1_0.aten::expand(%115:<4608xf16>, list{4608:i32}, implicit=False:pred) 
58391:4:0 %161:<12288xf16> = torch.2_1_0.aten::expand(%132:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %160:<12288xf16> = torch.2_1_0.aten::expand(%105:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %163:<6144x12288xf16> = torch.2_1_0.aten::expand(%96:<6144x12288xf16>, list{6144:i32, 12288:i32}, implicit=False:pred) 
58391:4:0 %164:<6144xf16> = torch.2_1_0.aten::expand(%104:<6144xf16>, list{6144:i32}, implicit=False:pred) 
58391:4:0 %163:<12288x6144xf16> = torch.2_1_0.aten::expand(%33:<12288x6144xf16>, list{12288:i32, 6144:i32}, implicit=False:pred) 
58389:2:0 %164:<2048x12288xf16> = torch.2_1_0.aten::clone(%163:<2048x12288xf16>, memory_format=None:NoneType) 
58391:4:0 %165:<12288xf16> = torch.2_1_0.aten::expand(%88:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58391:4:0 %141:<12288xf16> = torch.2_1_0.aten::expand(%134:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58389:2:0 %165:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%164:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %79:<12288xf16> = torch.2_1_0.aten::expand(%102:<12288xf16>, list{12288:i32}, implicit=False:pred) 
58389:2:0 %166:<12288x1536xf16> = torch.2_1_0.aten::detach(%13:<12288x1536xf16>) 
58389:2:0 %162:<12288x1536xf16> = torch.2_1_0.aten::detach(%166:<12288x1536xf16>) 
58389:2:0 %164:<12288x1536xf16> = torch.2_1_0.aten::clone(%162:<12288x1536xf16>, memory_format=None:NoneType) 
58389:2:0 %163:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%164:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %167:<4608x12288xf16> = torch.2_1_0.aten::detach(%12:<4608x12288xf16>) 
58389:2:0 %168:<4608x12288xf16> = torch.2_1_0.aten::detach(%167:<4608x12288xf16>) 
58391:4:0 %166:<1xi32> = torch.2_1_0.aten::lift_fresh(%166:<1xi32>) 
58389:2:0 %164:<4608x12288xf16> = torch.2_1_0.aten::clone(%168:<4608x12288xf16>, memory_format=None:NoneType) 
58391:4:0 %35:<1xf32> = torch.2_1_0.aten::lift_fresh(%35:<1xf32>) 
58391:4:0 %167:<1xf32> = torch.2_1_0.aten::lift_fresh(%167:<1xf32>) 
58391:4:0 %168:<1xf32> = torch.2_1_0.aten::lift_fresh(%168:<1xf32>) 
58391:4:0 %169:<1xf32> = torch.2_1_0.aten::lift_fresh(%169:<1xf32>) 
58389:2:0 %169:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%164:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %170:<1xf32> = torch.2_1_0.aten::lift_fresh(%170:<1xf32>) 
58389:2:0 %170:<6144x12288xf16> = torch.2_1_0.aten::detach(%84:<6144x12288xf16>) 
58391:4:0 %171:<1xi32> = torch.2_1_0.aten::lift_fresh(%171:<1xi32>) 
58389:2:0 %171:<6144x12288xf16> = torch.2_1_0.aten::detach(%170:<6144x12288xf16>) 
58391:4:0 %172:<6400x12288xf16> = torch.2_1_0.aten::detach(%106:<6400x12288xf16>) 
58391:4:0 %173:<6400x12288xf16> = torch.2_1_0.aten::detach(%172:<6400x12288xf16>) 
58389:2:0 %164:<6144x12288xf16> = torch.2_1_0.aten::clone(%171:<6144x12288xf16>, memory_format=None:NoneType) 
58391:4:0 %174:<6400x12288xf16> = torch.2_1_0.aten::clone(%173:<6400x12288xf16>, memory_format=None:NoneType) 
58389:2:0 %172:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%164:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %173:<12288x6144xf16> = torch.2_1_0.aten::detach(%102:<12288x6144xf16>) 
58389:2:0 %174:<12288x6144xf16> = torch.2_1_0.aten::detach(%173:<12288x6144xf16>) 
58389:2:0 %175:<12288x6144xf16> = torch.2_1_0.aten::clone(%174:<12288x6144xf16>, memory_format=None:NoneType) 
58391:4:0 %175:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%174:<6400x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %173:<2048x12288xf16> = torch.2_1_0.aten::detach(%127:<2048x12288xf16>) 
58391:4:0 %176:<2048x12288xf16> = torch.2_1_0.aten::detach(%173:<2048x12288xf16>) 
58391:4:0 %177:<2048x12288xf16> = torch.2_1_0.aten::clone(%176:<2048x12288xf16>, memory_format=None:NoneType) 
58391:4:0 %178:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%177:<2048x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %176:<12288x1536xf16> = torch.2_1_0.aten::detach(%129:<12288x1536xf16>) 
58389:2:0 %173:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%175:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %179:<12288x1536xf16> = torch.2_1_0.aten::detach(%176:<12288x1536xf16>) 
58389:2:0 %176:<12288xf16> = torch.2_1_0.aten::detach(%100:<12288xf16>) 
58391:4:0 %177:<12288x1536xf16> = torch.2_1_0.aten::clone(%179:<12288x1536xf16>, memory_format=None:NoneType) 
58389:2:0 %177:<12288xf16> = torch.2_1_0.aten::detach(%176:<12288xf16>) 
58391:4:0 %173:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%177:<12288x1536xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %178:<12288xf16> = torch.2_1_0.aten::clone(%177:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %172:<4608x12288xf16> = torch.2_1_0.aten::detach(%131:<4608x12288xf16>) 
58389:2:0 %164:<12288xf32> = torch.2_1_0.aten::_to_copy(%178:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %179:<4608x12288xf16> = torch.2_1_0.aten::detach(%172:<4608x12288xf16>) 
58389:2:0 %179:<12288xf16> = torch.2_1_0.aten::detach(%9:<12288xf16>) 
58389:2:0 %180:<12288xf16> = torch.2_1_0.aten::detach(%179:<12288xf16>) 
58389:2:0 %181:<12288xf16> = torch.2_1_0.aten::clone(%180:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %182:<12288xf32> = torch.2_1_0.aten::_to_copy(%181:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %183:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58389:2:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%183:<12288xf16>) 
58391:4:0 %177:<4608x12288xf16> = torch.2_1_0.aten::clone(%179:<4608x12288xf16>, memory_format=None:NoneType) 
58389:2:0 %185:<12288xf16> = torch.2_1_0.aten::clone(%184:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %186:<12288xf32> = torch.2_1_0.aten::_to_copy(%185:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %187:<4608xf16> = torch.2_1_0.aten::detach(%18:<4608xf16>) 
58389:2:0 %188:<4608xf16> = torch.2_1_0.aten::detach(%187:<4608xf16>) 
58389:2:0 %189:<4608xf16> = torch.2_1_0.aten::clone(%188:<4608xf16>, memory_format=None:NoneType) 
58389:2:0 %190:<4608xf32> = torch.2_1_0.aten::_to_copy(%189:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%70:<12288xf16>) 
58389:2:0 %192:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58391:4:0 %180:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%177:<4608x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %193:<12288xf16> = torch.2_1_0.aten::clone(%192:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %179:<6144x12288xf16> = torch.2_1_0.aten::detach(%96:<6144x12288xf16>) 
58389:2:0 %194:<12288xf32> = torch.2_1_0.aten::_to_copy(%193:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %181:<6144x12288xf16> = torch.2_1_0.aten::detach(%179:<6144x12288xf16>) 
58389:2:0 %195:<12288xf16> = torch.2_1_0.aten::detach(%60:<12288xf16>) 
58389:2:0 %196:<12288xf16> = torch.2_1_0.aten::detach(%195:<12288xf16>) 
58389:2:0 %197:<12288xf16> = torch.2_1_0.aten::clone(%196:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %198:<12288xf32> = torch.2_1_0.aten::_to_copy(%197:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %199:<6144xf16> = torch.2_1_0.aten::detach(%103:<6144xf16>) 
58389:2:0 %200:<6144xf16> = torch.2_1_0.aten::detach(%199:<6144xf16>) 
58389:2:0 %201:<6144xf16> = torch.2_1_0.aten::clone(%200:<6144xf16>, memory_format=None:NoneType) 
58389:2:0 %202:<6144xf32> = torch.2_1_0.aten::_to_copy(%201:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %203:<12288xf16> = torch.2_1_0.aten::detach(%99:<12288xf16>) 
58391:4:0 %174:<6144x12288xf16> = torch.2_1_0.aten::clone(%181:<6144x12288xf16>, memory_format=None:NoneType) 
58389:2:0 %204:<12288xf16> = torch.2_1_0.aten::detach(%203:<12288xf16>) 
58389:2:0 %205:<12288xf16> = torch.2_1_0.aten::clone(%204:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %206:<12288xf32> = torch.2_1_0.aten::_to_copy(%205:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %207:<12288xf16> = torch.2_1_0.aten::detach(%42:<12288xf16>) 
58389:2:0 %208:<12288xf16> = torch.2_1_0.aten::detach(%207:<12288xf16>) 
58389:2:0 %209:<12288xf16> = torch.2_1_0.aten::clone(%208:<12288xf16>, memory_format=None:NoneType) 
58389:2:0 %204:<12288xf32> = torch.2_1_0.aten::_to_copy(%209:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %182:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%174:<6144x12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%95:<12288xf16>) 
58389:2:0 %208:<12288xf16> = torch.2_1_0.aten::detach(%209:<12288xf16>) 
58391:4:0 %181:<12288x6144xf16> = torch.2_1_0.aten::detach(%33:<12288x6144xf16>) 
58389:2:0 %184:<12288xf16> = torch.2_1_0.aten::clone(%208:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %183:<12288x6144xf16> = torch.2_1_0.aten::detach(%181:<12288x6144xf16>) 
58389:2:0 %207:<12288xf32> = torch.2_1_0.aten::_to_copy(%184:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %174:<12288x6144xf16> = torch.2_1_0.aten::clone(%183:<12288x6144xf16>, memory_format=None:NoneType) 
58389:2:0 %76:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%76:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %179:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%174:<12288x6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %172:<12288xf16> = torch.2_1_0.aten::detach(%128:<12288xf16>) 
58391:4:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%172:<12288xf16>) 
58391:4:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%184:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %177:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%37:<12288xf16>) 
58391:4:0 %183:<12288xf16> = torch.2_1_0.aten::detach(%184:<12288xf16>) 
58391:4:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%183:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %185:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %184:<12288xf16> = torch.2_1_0.aten::detach(%111:<12288xf16>) 
58391:4:0 %172:<12288xf16> = torch.2_1_0.aten::detach(%184:<12288xf16>) 
58391:4:0 %174:<12288xf16> = torch.2_1_0.aten::clone(%172:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %186:<12288xf32> = torch.2_1_0.aten::_to_copy(%174:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %172:<4608xf16> = torch.2_1_0.aten::detach(%115:<4608xf16>) 
58391:4:0 %184:<4608xf16> = torch.2_1_0.aten::detach(%172:<4608xf16>) 
58391:4:0 %172:<4608xf16> = torch.2_1_0.aten::clone(%184:<4608xf16>, memory_format=None:NoneType) 
58391:4:0 %183:<4608xf32> = torch.2_1_0.aten::_to_copy(%172:<4608xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %174:<12288xf16> = torch.2_1_0.aten::detach(%132:<12288xf16>) 
58391:4:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%174:<12288xf16>) 
58391:4:0 %184:<12288xf16> = torch.2_1_0.aten::clone(%187:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %174:<12288xf32> = torch.2_1_0.aten::_to_copy(%184:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %188:<12288xf16> = torch.2_1_0.aten::detach(%105:<12288xf16>) 
58391:4:0 %172:<12288xf16> = torch.2_1_0.aten::detach(%188:<12288xf16>) 
58391:4:0 %187:<12288xf16> = torch.2_1_0.aten::clone(%172:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %188:<12288xf32> = torch.2_1_0.aten::_to_copy(%187:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %187:<6144xf16> = torch.2_1_0.aten::detach(%104:<6144xf16>) 
58391:4:0 %189:<6144xf16> = torch.2_1_0.aten::detach(%187:<6144xf16>) 
58391:4:0 %187:<6144xf16> = torch.2_1_0.aten::clone(%189:<6144xf16>, memory_format=None:NoneType) 
58391:4:0 %184:<6144xf32> = torch.2_1_0.aten::_to_copy(%187:<6144xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%88:<12288xf16>) 
58391:4:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%187:<12288xf16>) 
58391:4:0 %172:<12288xf16> = torch.2_1_0.aten::clone(%190:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %187:<12288xf32> = torch.2_1_0.aten::_to_copy(%172:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%134:<12288xf16>) 
58391:4:0 %189:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58391:4:0 %172:<12288xf16> = torch.2_1_0.aten::clone(%189:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %192:<12288xf32> = torch.2_1_0.aten::_to_copy(%172:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %191:<12288xf16> = torch.2_1_0.aten::detach(%102:<12288xf16>) 
58391:4:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%191:<12288xf16>) 
58391:4:0 %189:<12288xf16> = torch.2_1_0.aten::clone(%190:<12288xf16>, memory_format=None:NoneType) 
58391:4:0 %191:<12288xf32> = torch.2_1_0.aten::_to_copy(%189:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %7:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%7:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %189:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %113:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %128:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %22:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %156:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %108:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %214:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %200:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%189:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%128:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%113:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%108:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%22:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%156:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%214:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
> building train, validation, and test datasets ...58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%200:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 %202:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %158:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%202:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%158:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %193:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%193:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %6:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%6:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %81:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%81:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %128:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%128:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 %178:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%178:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %215:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%215:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %210:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %205:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %92:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %189:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %179:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %159:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %203:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%210:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%205:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%92:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%189:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%179:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%203:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%159:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %216:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%216:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 %158:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %128:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %200:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %108:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %194:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%128:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%158:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%200:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%194:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%108:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %211:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%211:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 %178:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%178:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %217:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%217:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %129:<3xi64> = torch.2_1_0.aten::lift_fresh(%129:<3xi64>) 
58391:4:0 %195:<3xi64> = torch.2_1_0.aten::lift_fresh(%195:<3xi64>) 
58389:2:0 %54:<3xi64> = torch.2_1_0.aten::lift_fresh(%54:<3xi64>) 
58393:6:0 %206:<3xi64> = torch.2_1_0.aten::lift_fresh(%206:<3xi64>) 
58390:3:0 %101:<3xi64> = torch.2_1_0.aten::lift_fresh(%101:<3xi64>) 
58394:7:0 %9:<3xi64> = torch.2_1_0.aten::lift_fresh(%9:<3xi64>) 
58388:1:0 %204:<3xi64> = torch.2_1_0.aten::lift_fresh(%204:<3xi64>) 
58393:None:0 list{%206:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%206:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:None:0 list{%129:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%129:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%54:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%54:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%9:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%9:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:5:0 %130:<i64> = torch.2_1_0.aten::select(%129:<3xi64>, 0:i32, 0:i32) 
58393:6:0 %207:<i64> = torch.2_1_0.aten::select(%206:<3xi64>, 0:i32, 0:i32) 
58388:None:0 list{%204:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%204:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%101:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%101:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:0:0 %218:<3xi64> = torch.2_1_0.aten::lift_fresh(%218:<3xi64>) 
58389:2:0 %212:<i64> = torch.2_1_0.aten::select(%54:<3xi64>, 0:i32, 0:i32) 
58394:7:0 %160:<i64> = torch.2_1_0.aten::select(%9:<3xi64>, 0:i32, 0:i32) 
58388:1:0 %205:<i64> = torch.2_1_0.aten::select(%204:<3xi64>, 0:i32, 0:i32) 
58390:3:0 %180:<i64> = torch.2_1_0.aten::select(%101:<3xi64>, 0:i32, 0:i32) 
58391:None:0 list{%195:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%195:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:4:0 %196:<i64> = torch.2_1_0.aten::select(%195:<3xi64>, 0:i32, 0:i32) 
58393:6:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%207:<i64>) 
58394:7:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%160:<i64>) 
58392:5:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%130:<i64>) 
58389:2:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%212:<i64>) 
58390:3:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%180:<i64>) 
58387:None:0 list{%218:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%218:<3xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58393:6:0 %208:<i64>+1 = torch.2_1_0.aten::select(%206:<3xi64>, 0:i32, 1:i32) 
58392:5:0 %130:<i64>+1 = torch.2_1_0.aten::select(%129:<3xi64>, 0:i32, 1:i32) 
58394:7:0 %161:<i64>+1 = torch.2_1_0.aten::select(%9:<3xi64>, 0:i32, 1:i32) 
58389:2:0 %213:<i64>+1 = torch.2_1_0.aten::select(%54:<3xi64>, 0:i32, 1:i32) 
58388:1:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%205:<i64>) 
58390:3:0 %181:<i64>+1 = torch.2_1_0.aten::select(%101:<3xi64>, 0:i32, 1:i32) 
58391:4:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%196:<i64>) 
58393:6:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%208:<i64>+1) 
58392:5:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%130:<i64>+1) 
58394:7:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%161:<i64>+1) 
58389:2:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%213:<i64>+1) 
58388:1:0 %206:<i64>+1 = torch.2_1_0.aten::select(%204:<3xi64>, 0:i32, 1:i32) 
58387:0:0 %219:<i64> = torch.2_1_0.aten::select(%218:<3xi64>, 0:i32, 0:i32) 
58393:6:0 %209:<i64>+2 = torch.2_1_0.aten::select(%206:<3xi64>, 0:i32, 2:i32) 
58392:5:0 %131:<i64>+2 = torch.2_1_0.aten::select(%129:<3xi64>, 0:i32, 2:i32) 
58391:4:0 %197:<i64>+1 = torch.2_1_0.aten::select(%195:<3xi64>, 0:i32, 1:i32) 
58390:3:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%181:<i64>+1) 
58394:7:0 %162:<i64>+2 = torch.2_1_0.aten::select(%9:<3xi64>, 0:i32, 2:i32) 
58388:1:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%206:<i64>+1) 
58393:6:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%209:<i64>+2) 
58392:5:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%131:<i64>+2) 
58389:2:0 %214:<i64>+2 = torch.2_1_0.aten::select(%54:<3xi64>, 0:i32, 2:i32) 
58387:0:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%219:<i64>) 
58391:4:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%197:<i64>+1) 
58394:7:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%162:<i64>+2) 
58390:3:0 %182:<i64>+2 = torch.2_1_0.aten::select(%101:<3xi64>, 0:i32, 2:i32) 
58388:1:0 %207:<i64>+2 = torch.2_1_0.aten::select(%204:<3xi64>, 0:i32, 2:i32) 
58389:2:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%214:<i64>+2) 
58387:0:0 %220:<i64>+1 = torch.2_1_0.aten::select(%218:<3xi64>, 0:i32, 1:i32) 
58388:1:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%207:<i64>+2) 
58391:4:0 %197:<i64>+2 = torch.2_1_0.aten::select(%195:<3xi64>, 0:i32, 2:i32) 
58390:3:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%182:<i64>+2) 
58387:0:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%220:<i64>+1) 
58392:5:0 %131:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%197:<i64>+2) 
58393:6:0 %209:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %221:<i64>+2 = torch.2_1_0.aten::select(%218:<3xi64>, 0:i32, 2:i32) 
58394:7:0 %163:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %215:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %208:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 1:i32: = torch.2_1_0.aten::_local_scalar_dense(%221:<i64>+2) 
58390:3:0 %183:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%209:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %198:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%131:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%215:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%208:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%163:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:0 %153:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%198:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%183:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:0 %153:<i64> = torch.2_1_0.aten::random_(%153:<i64>, generator=None:NoneType) 
58387:None:0 7256716235238153280:i32: = torch.2_1_0.aten::_local_scalar_dense(%153:<i64>) 
58387:None:0 %222:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:None:0 %222:<i64> = torch.2_1_0.aten::random_(%222:<i64>, generator=None:NoneType) 
58387:None:0 7985687283714577175:i32: = torch.2_1_0.aten::_local_scalar_dense(%222:<i64>) 
58387:None:None %222:<2049xi64> = torch.2_1_0.aten::lift_fresh(%222:<2049xi64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::slice(%222:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %223:<2048xi64>+1 = torch.2_1_0.aten::slice(%222:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %89:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %84:<2048x2048xf32> = torch.2_1_0.aten::tril(%89:<2048x2048xf32>, 0:i32) 
58387:None:None %56:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%84:<2048x2048xf32>, 0:i32) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %224:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:0 %223:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:None:0 %223:<i64> = torch.2_1_0.aten::random_(%223:<i64>, generator=None:NoneType) 
58387:None:None %223:<2049xi64> = torch.2_1_0.aten::lift_fresh(%223:<2049xi64>) 
58387:None:0 6083034692778998115:i32: = torch.2_1_0.aten::_local_scalar_dense(%223:<i64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::slice(%223:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::slice(%223:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %225:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%56:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %59:<2048xunknown> = torch.2_1_0.aten::eq(%223:<2048xi64>+1, -1:i32) 
58387:None:None %84:<f32> = torch.2_1_0.aten::lift_fresh(%84:<f32>) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::index_put_(%72:<2048xf32>, list{%59:<2048xunknown>}, %84:<f32>, False:pred) 
58387:None:None %129:<2048xunknown> = torch.2_1_0.aten::eq(%32:<2048xi64>, -1:i32) 
58387:None:None %84:<i64> = torch.2_1_0.aten::lift_fresh(%84:<i64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::index_put_(%32:<2048xi64>, list{%129:<2048xunknown>}, %84:<i64>, False:pred) 
58387:None:None %226:<2048xunknown> = torch.2_1_0.aten::eq(%223:<2048xi64>+1, -1:i32) 
58387:None:None %84:<i64> = torch.2_1_0.aten::lift_fresh(%84:<i64>) 
58387:None:None %223:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%223:<2048xi64>+1, list{%226:<2048xunknown>}, %84:<i64>, False:pred) 
58387:None:None %57:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %84:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %84:<2048xi64> = torch.2_1_0.aten::set_(%84:<i64>, storage{size=16384}) 
58387:None:None %84:<1x2048xi64> = torch.2_1_0.aten::resize_(%84:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %84:<1x2048xi64> = torch.2_1_0.aten::stack(list{%32:<2048xi64>}, 0:i32, out=%84:<1x2048xi64>) 
58387:None:None %227:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %227:<2048xi64> = torch.2_1_0.aten::set_(%227:<i64>, storage{size=16384}) 
58387:None:None %227:<1x2048xi64> = torch.2_1_0.aten::resize_(%227:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %227:<1x2048xi64> = torch.2_1_0.aten::stack(list{%223:<2048xi64>+1}, 0:i32, out=%227:<1x2048xi64>) 
58387:None:None %228:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<4194304xunknown> = torch.2_1_0.aten::set_(%228:<unknown>, storage{size=4194304}) 
58387:None:None %228:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%228:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%225:<1x2048x2048xunknown>}, 0:i32, out=%228:<1x1x2048x2048xunknown>) 
58387:None:None %229:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %229:<2048xf32> = torch.2_1_0.aten::set_(%229:<f32>, storage{size=8192}) 
58387:None:None %229:<1x2048xf32> = torch.2_1_0.aten::resize_(%229:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %229:<1x2048xf32> = torch.2_1_0.aten::stack(list{%72:<2048xf32>}, 0:i32, out=%229:<1x2048xf32>) 
58387:None:None %230:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %230:<2048xi64> = torch.2_1_0.aten::set_(%230:<i64>, storage{size=16384}) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::resize_(%230:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::stack(list{%224:<2048xi64>}, 0:i32, out=%230:<1x2048xi64>) 
58387:None:None %223:<2049xi64> = torch.2_1_0.aten::lift_fresh(%223:<2049xi64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::slice(%223:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::slice(%223:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %222:<2049xi64> = torch.2_1_0.aten::lift_fresh(%222:<2049xi64>) 
58387:None:None %56:<2049xi64> = torch.2_1_0.aten::lift_fresh(%56:<2049xi64>) 
58387:None:None %231:<2048xi64> = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %223:<2048xi64> = torch.2_1_0.aten::slice(%222:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %232:<2048xi64>+1 = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %233:<2048xunknown> = torch.2_1_0.aten::eq(%232:<2048xi64>+1, -1:i32) 
58387:None:None %56:<2048xi64>+1 = torch.2_1_0.aten::slice(%222:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %234:<f32> = torch.2_1_0.aten::lift_fresh(%234:<f32>) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::index_put_(%72:<2048xf32>, list{%233:<2048xunknown>}, %234:<f32>, False:pred) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%231:<2048xi64>, -1:i32) 
58387:None:None %235:<i64> = torch.2_1_0.aten::lift_fresh(%235:<i64>) 
58387:None:None %231:<2048xi64> = torch.2_1_0.aten::index_put_(%231:<2048xi64>, list{%234:<2048xunknown>}, %235:<i64>, False:pred) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%232:<2048xi64>+1, -1:i32) 
58387:None:None %236:<i64> = torch.2_1_0.aten::lift_fresh(%236:<i64>) 
58387:None:None %232:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%232:<2048xi64>+1, list{%234:<2048xunknown>}, %236:<i64>, False:pred) 
58387:None:None %237:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %237:<2048xi64> = torch.2_1_0.aten::set_(%237:<i64>, storage{size=16384}) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::resize_(%237:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::stack(list{%231:<2048xi64>}, 0:i32, out=%237:<1x2048xi64>) 
58387:None:None %238:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %238:<2048xi64> = torch.2_1_0.aten::set_(%238:<i64>, storage{size=16384}) 
58387:None:None %238:<1x2048xi64> = torch.2_1_0.aten::resize_(%238:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %238:<1x2048xi64> = torch.2_1_0.aten::stack(list{%232:<2048xi64>+1}, 0:i32, out=%238:<1x2048xi64>) 
58387:None:None %239:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %239:<4194304xunknown> = torch.2_1_0.aten::set_(%239:<unknown>, storage{size=4194304}) 
58387:None:None %239:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%239:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %239:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%225:<1x2048x2048xunknown>}, 0:i32, out=%239:<1x1x2048x2048xunknown>) 
58387:None:None %84:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %240:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %240:<2048xf32> = torch.2_1_0.aten::set_(%240:<f32>, storage{size=8192}) 
58387:None:None %240:<1x2048xf32> = torch.2_1_0.aten::resize_(%240:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %240:<1x2048xf32> = torch.2_1_0.aten::stack(list{%72:<2048xf32>}, 0:i32, out=%240:<1x2048xf32>) 
58387:None:None %241:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %241:<2048xi64> = torch.2_1_0.aten::set_(%241:<i64>, storage{size=16384}) 
58387:None:None %241:<1x2048xi64> = torch.2_1_0.aten::resize_(%241:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %241:<1x2048xi64> = torch.2_1_0.aten::stack(list{%224:<2048xi64>}, 0:i32, out=%241:<1x2048xi64>) 
58387:None:None %72:<2048x2048xf32> = torch.2_1_0.aten::tril(%57:<2048x2048xf32>, 0:i32) 
58387:None:None %89:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %84:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%72:<2048x2048xf32>, 0:i32) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %224:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %225:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%84:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %123:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %72:<f32> = torch.2_1_0.aten::lift_fresh(%72:<f32>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%123:<2048xunknown>}, %72:<f32>, False:pred) 
58387:None:None %72:<2048xunknown> = torch.2_1_0.aten::eq(%32:<2048xi64>, -1:i32) 
58387:None:None %226:<i64> = torch.2_1_0.aten::lift_fresh(%226:<i64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::index_put_(%32:<2048xi64>, list{%72:<2048xunknown>}, %226:<i64>, False:pred) 
58387:None:None %227:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %226:<i64> = torch.2_1_0.aten::lift_fresh(%226:<i64>) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%18:<2048xi64>+1, list{%227:<2048xunknown>}, %226:<i64>, False:pred) 
58387:None:None %226:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %226:<2048xi64> = torch.2_1_0.aten::set_(%226:<i64>, storage{size=16384}) 
58387:None:None %226:<1x2048xi64> = torch.2_1_0.aten::resize_(%226:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %226:<1x2048xi64> = torch.2_1_0.aten::stack(list{%32:<2048xi64>}, 0:i32, out=%226:<1x2048xi64>) 
58387:None:None %228:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<2048xi64> = torch.2_1_0.aten::set_(%228:<i64>, storage{size=16384}) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::resize_(%228:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::stack(list{%18:<2048xi64>+1}, 0:i32, out=%228:<1x2048xi64>) 
58387:None:None %229:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %229:<4194304xunknown> = torch.2_1_0.aten::set_(%229:<unknown>, storage{size=4194304}) 
58387:None:None %229:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%229:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %57:<2048x2048xf32> = torch.2_1_0.aten::tril(%84:<2048x2048xf32>, 0:i32) 
58387:None:None %84:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%57:<2048x2048xf32>, 0:i32) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %229:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%225:<1x2048x2048xunknown>}, 0:i32, out=%229:<1x1x2048x2048xunknown>) 
58387:None:None %73:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %230:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %230:<2048xf32> = torch.2_1_0.aten::set_(%230:<f32>, storage{size=8192}) 
58387:None:None %230:<1x2048xf32> = torch.2_1_0.aten::resize_(%230:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %230:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%230:<1x2048xf32>) 
58387:None:None %231:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %231:<2048xi64> = torch.2_1_0.aten::set_(%231:<i64>, storage{size=16384}) 
58387:None:None %231:<1x2048xi64> = torch.2_1_0.aten::resize_(%231:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %231:<1x2048xi64> = torch.2_1_0.aten::stack(list{%224:<2048xi64>}, 0:i32, out=%231:<1x2048xi64>) 
58387:None:None %56:<2049xi64> = torch.2_1_0.aten::lift_fresh(%56:<2049xi64>) 
58387:None:None %232:<2048xi64> = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %233:<2048xi64>+1 = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%233:<2048xi64>+1, -1:i32) 
58387:None:None %235:<f32> = torch.2_1_0.aten::lift_fresh(%235:<f32>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%234:<2048xunknown>}, %235:<f32>, False:pred) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%232:<2048xi64>, -1:i32) 
58387:None:None %236:<i64> = torch.2_1_0.aten::lift_fresh(%236:<i64>) 
58387:None:None %232:<2048xi64> = torch.2_1_0.aten::index_put_(%232:<2048xi64>, list{%234:<2048xunknown>}, %236:<i64>, False:pred) 
58387:None:None %236:<2048xunknown> = torch.2_1_0.aten::eq(%233:<2048xi64>+1, -1:i32) 
58387:None:None %213:<i64> = torch.2_1_0.aten::lift_fresh(%213:<i64>) 
58387:None:None %233:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%233:<2048xi64>+1, list{%236:<2048xunknown>}, %213:<i64>, False:pred) 
58387:None:None %237:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %237:<2048xi64> = torch.2_1_0.aten::set_(%237:<i64>, storage{size=16384}) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::resize_(%237:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::stack(list{%232:<2048xi64>}, 0:i32, out=%237:<1x2048xi64>) 
58387:None:None %238:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %238:<2048xi64> = torch.2_1_0.aten::set_(%238:<i64>, storage{size=16384}) 
58387:None:None %238:<1x2048xi64> = torch.2_1_0.aten::resize_(%238:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %238:<1x2048xi64> = torch.2_1_0.aten::stack(list{%233:<2048xi64>+1}, 0:i32, out=%238:<1x2048xi64>) 
58387:None:None %239:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %239:<4194304xunknown> = torch.2_1_0.aten::set_(%239:<unknown>, storage{size=4194304}) 
58387:None:None %239:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%239:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %61:<2048x2048xf32> = torch.2_1_0.aten::tril(%89:<2048x2048xf32>, 0:i32) 
58387:None:None %56:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%84:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %84:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%61:<2048x2048xf32>, 0:i32) 
58387:None:None %239:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%225:<1x2048x2048xunknown>}, 0:i32, out=%239:<1x1x2048x2048xunknown>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %240:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %240:<2048xf32> = torch.2_1_0.aten::set_(%240:<f32>, storage{size=8192}) 
58387:None:None %240:<1x2048xf32> = torch.2_1_0.aten::resize_(%240:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %240:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%240:<1x2048xf32>) 
58387:None:None %89:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %241:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %241:<2048xi64> = torch.2_1_0.aten::set_(%241:<i64>, storage{size=16384}) 
58387:None:None %241:<1x2048xi64> = torch.2_1_0.aten::resize_(%241:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %241:<1x2048xi64> = torch.2_1_0.aten::stack(list{%224:<2048xi64>}, 0:i32, out=%241:<1x2048xi64>) 
58387:None:None %224:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %57:<f32> = torch.2_1_0.aten::lift_fresh(%57:<f32>) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::index_put_(%72:<2048xf32>, list{%224:<2048xunknown>}, %57:<f32>, False:pred) 
58387:None:None %98:<2048xunknown> = torch.2_1_0.aten::eq(%32:<2048xi64>, -1:i32) 
58387:None:None %57:<i64> = torch.2_1_0.aten::lift_fresh(%57:<i64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::index_put_(%32:<2048xi64>, list{%98:<2048xunknown>}, %57:<i64>, False:pred) 
58387:None:None %74:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %57:<i64> = torch.2_1_0.aten::lift_fresh(%57:<i64>) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%18:<2048xi64>+1, list{%74:<2048xunknown>}, %57:<i64>, False:pred) 
58387:None:None %74:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %74:<2048xi64> = torch.2_1_0.aten::set_(%74:<i64>, storage{size=16384}) 
58387:None:None %74:<1x2048xi64> = torch.2_1_0.aten::resize_(%74:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %74:<1x2048xi64> = torch.2_1_0.aten::stack(list{%32:<2048xi64>}, 0:i32, out=%74:<1x2048xi64>) 
58387:None:None %225:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %225:<2048xi64> = torch.2_1_0.aten::set_(%225:<i64>, storage{size=16384}) 
58387:None:None %225:<1x2048xi64> = torch.2_1_0.aten::resize_(%225:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %225:<1x2048xi64> = torch.2_1_0.aten::stack(list{%18:<2048xi64>+1}, 0:i32, out=%225:<1x2048xi64>) 
58387:None:None %226:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %226:<4194304xunknown> = torch.2_1_0.aten::set_(%226:<unknown>, storage{size=4194304}) 
58387:None:None %226:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%226:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %226:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%56:<1x2048x2048xunknown>}, 0:i32, out=%226:<1x1x2048x2048xunknown>) 
58387:None:None %227:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %227:<2048xf32> = torch.2_1_0.aten::set_(%227:<f32>, storage{size=8192}) 
58387:None:None %227:<1x2048xf32> = torch.2_1_0.aten::resize_(%227:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %227:<1x2048xf32> = torch.2_1_0.aten::stack(list{%72:<2048xf32>}, 0:i32, out=%227:<1x2048xf32>) 
58387:None:None %228:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<2048xi64> = torch.2_1_0.aten::set_(%228:<i64>, storage{size=16384}) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::resize_(%228:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::stack(list{%73:<2048xi64>}, 0:i32, out=%228:<1x2048xi64>) 
58387:None:None %224:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%84:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %224:<2049xi64> = torch.2_1_0.aten::lift_fresh(%224:<2049xi64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::slice(%224:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::slice(%224:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %213:<2049xi64> = torch.2_1_0.aten::lift_fresh(%213:<2049xi64>) 
58387:None:None %229:<2048xi64> = torch.2_1_0.aten::slice(%213:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %230:<2048xi64>+1 = torch.2_1_0.aten::slice(%213:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %231:<2048xunknown> = torch.2_1_0.aten::eq(%230:<2048xi64>+1, -1:i32) 
58387:None:None %232:<f32> = torch.2_1_0.aten::lift_fresh(%232:<f32>) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::index_put_(%72:<2048xf32>, list{%231:<2048xunknown>}, %232:<f32>, False:pred) 
58387:None:None %231:<2048xunknown> = torch.2_1_0.aten::eq(%229:<2048xi64>, -1:i32) 
58387:None:None %232:<i64> = torch.2_1_0.aten::lift_fresh(%232:<i64>) 
58387:None:None %229:<2048xi64> = torch.2_1_0.aten::index_put_(%229:<2048xi64>, list{%231:<2048xunknown>}, %232:<i64>, False:pred) 
58387:None:None %232:<2048xunknown> = torch.2_1_0.aten::eq(%230:<2048xi64>+1, -1:i32) 
58387:None:None %134:<i64> = torch.2_1_0.aten::lift_fresh(%134:<i64>) 
58387:None:None %230:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%230:<2048xi64>+1, list{%232:<2048xunknown>}, %134:<i64>, False:pred) 
58387:None:None %72:<2048xunknown> = torch.2_1_0.aten::eq(%56:<2048xi64>+1, -1:i32) 
58387:None:None %233:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %61:<f32> = torch.2_1_0.aten::lift_fresh(%61:<f32>) 
58387:None:None %233:<2048xi64> = torch.2_1_0.aten::set_(%233:<i64>, storage{size=16384}) 
58387:None:None %233:<1x2048xi64> = torch.2_1_0.aten::resize_(%233:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %233:<1x2048xi64> = torch.2_1_0.aten::stack(list{%229:<2048xi64>}, 0:i32, out=%233:<1x2048xi64>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%72:<2048xunknown>}, %61:<f32>, False:pred) 
58387:None:None %234:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %225:<2048xunknown> = torch.2_1_0.aten::eq(%223:<2048xi64>, -1:i32) 
58387:None:None %234:<2048xi64> = torch.2_1_0.aten::set_(%234:<i64>, storage{size=16384}) 
58387:None:None %61:<i64> = torch.2_1_0.aten::lift_fresh(%61:<i64>) 
58387:None:None %234:<1x2048xi64> = torch.2_1_0.aten::resize_(%234:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %223:<2048xi64> = torch.2_1_0.aten::index_put_(%223:<2048xi64>, list{%225:<2048xunknown>}, %61:<i64>, False:pred) 
58387:None:None %234:<1x2048xi64> = torch.2_1_0.aten::stack(list{%230:<2048xi64>+1}, 0:i32, out=%234:<1x2048xi64>) 
58387:None:None %225:<2048xunknown> = torch.2_1_0.aten::eq(%56:<2048xi64>+1, -1:i32) 
58387:None:None %102:<i64> = torch.2_1_0.aten::lift_fresh(%102:<i64>) 
58387:None:None %235:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %56:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%56:<2048xi64>+1, list{%225:<2048xunknown>}, %102:<i64>, False:pred) 
58387:None:None %235:<4194304xunknown> = torch.2_1_0.aten::set_(%235:<unknown>, storage{size=4194304}) 
58387:None:None %235:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%235:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %102:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %102:<2048xi64> = torch.2_1_0.aten::set_(%102:<i64>, storage{size=16384}) 
58387:None:None %102:<1x2048xi64> = torch.2_1_0.aten::resize_(%102:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %102:<1x2048xi64> = torch.2_1_0.aten::stack(list{%223:<2048xi64>}, 0:i32, out=%102:<1x2048xi64>) 
58387:None:None %226:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %226:<2048xi64> = torch.2_1_0.aten::set_(%226:<i64>, storage{size=16384}) 
58387:None:None %226:<1x2048xi64> = torch.2_1_0.aten::resize_(%226:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %226:<1x2048xi64> = torch.2_1_0.aten::stack(list{%56:<2048xi64>+1}, 0:i32, out=%226:<1x2048xi64>) 
58387:None:None %227:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %227:<4194304xunknown> = torch.2_1_0.aten::set_(%227:<unknown>, storage{size=4194304}) 
58387:None:None %227:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%227:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:0:0 %224:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %235:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%56:<1x2048x2048xunknown>}, 0:i32, out=%235:<1x1x2048x2048xunknown>) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%224:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:None %236:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %236:<2048xf32> = torch.2_1_0.aten::set_(%236:<f32>, storage{size=8192}) 
58392:5:0 %132:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %209:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %108:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %214:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %236:<1x2048xf32> = torch.2_1_0.aten::resize_(%236:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58394:7:0 %164:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %184:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %236:<1x2048xf32> = torch.2_1_0.aten::stack(list{%72:<2048xf32>}, 0:i32, out=%236:<1x2048xf32>) 
58387:0:0 %217:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%214:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:None %237:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%108:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%132:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%209:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%164:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%184:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %199:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%199:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %212:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred) 
58392:5:0 %128:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred) 
58394:7:0 %165:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred) 
58387:None:None %224:<2049xi64> = torch.2_1_0.aten::lift_fresh(%224:<2049xi64>) 
58393:6:0 %207:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred) 
58390:3:0 %182:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred) 
58392:None:0 %129:<f32> = torch.2_1_0.aten::lift_fresh(%129:<f32>) 
58389:None:0 %216:<f32> = torch.2_1_0.aten::lift_fresh(%216:<f32>) 
58388:1:0 %200:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred) 
58394:None:0 %162:<f32> = torch.2_1_0.aten::lift_fresh(%162:<f32>) 
58393:None:0 %210:<f32> = torch.2_1_0.aten::lift_fresh(%210:<f32>) 
58389:2:0 %217:<2xf32>+4 = torch.2_1_0.aten::select(%212:<8x2xf32>, 0:i32, 2:i32) 
58392:5:0 %133:<2xf32>+10 = torch.2_1_0.aten::select(%128:<8x2xf32>, 0:i32, 5:i32) 
58394:7:0 %166:<2xf32>+14 = torch.2_1_0.aten::select(%165:<8x2xf32>, 0:i32, 7:i32) 
58388:None:0 %33:<f32> = torch.2_1_0.aten::lift_fresh(%33:<f32>) 
58390:None:0 %185:<f32> = torch.2_1_0.aten::lift_fresh(%185:<f32>) 
58391:4:0 %194:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred) 
58393:6:0 %211:<2xf32>+12 = torch.2_1_0.aten::select(%207:<8x2xf32>, 0:i32, 6:i32) 
58389:2:0 %218:<f32>+4 = torch.2_1_0.aten::select(%217:<2xf32>+4, 0:i32, 0:i32) 
58392:5:0 %134:<f32>+10 = torch.2_1_0.aten::select(%133:<2xf32>+10, 0:i32, 0:i32) 
58388:1:0 %210:<2xf32>+2 = torch.2_1_0.aten::select(%200:<8x2xf32>, 0:i32, 1:i32) 
58394:7:0 %167:<f32>+14 = torch.2_1_0.aten::select(%166:<2xf32>+14, 0:i32, 0:i32) 
58393:6:0 %212:<f32>+12 = torch.2_1_0.aten::select(%211:<2xf32>+12, 0:i32, 0:i32) 
58390:3:0 %186:<2xf32>+6 = torch.2_1_0.aten::select(%182:<8x2xf32>, 0:i32, 3:i32) 
58391:None:0 %195:<f32> = torch.2_1_0.aten::lift_fresh(%195:<f32>) 
58388:1:0 %211:<f32>+2 = torch.2_1_0.aten::select(%210:<2xf32>+2, 0:i32, 0:i32) 
58392:5:0 %134:<f32>+10 = torch.2_1_0.aten::copy_(%134:<f32>+10, %129:<f32>, False:pred) 
58389:2:0 %218:<f32>+4 = torch.2_1_0.aten::copy_(%218:<f32>+4, %216:<f32>, False:pred) 
58390:3:0 %187:<f32>+6 = torch.2_1_0.aten::select(%186:<2xf32>+6, 0:i32, 0:i32) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::slice(%224:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58393:6:0 %212:<f32>+12 = torch.2_1_0.aten::copy_(%212:<f32>+12, %210:<f32>, False:pred) 
58394:7:0 %167:<f32>+14 = torch.2_1_0.aten::copy_(%167:<f32>+14, %162:<f32>, False:pred) 
58392:None:0 %129:<f32> = torch.2_1_0.aten::lift_fresh(%129:<f32>) 
58391:4:0 %200:<2xf32>+8 = torch.2_1_0.aten::select(%194:<8x2xf32>, 0:i32, 4:i32) 
58389:None:0 %216:<f32> = torch.2_1_0.aten::lift_fresh(%216:<f32>) 
58388:1:0 %211:<f32>+2 = torch.2_1_0.aten::copy_(%211:<f32>+2, %33:<f32>, False:pred) 
58393:None:0 %210:<f32> = torch.2_1_0.aten::lift_fresh(%210:<f32>) 
58392:5:0 %135:<2xf32>+10 = torch.2_1_0.aten::select(%128:<8x2xf32>, 0:i32, 5:i32) 
58389:2:0 %219:<2xf32>+4 = torch.2_1_0.aten::select(%212:<8x2xf32>, 0:i32, 2:i32) 
58394:None:0 %162:<f32> = torch.2_1_0.aten::lift_fresh(%162:<f32>) 
58391:4:0 %201:<f32>+8 = torch.2_1_0.aten::select(%200:<2xf32>+8, 0:i32, 0:i32) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::slice(%224:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58388:None:0 %33:<f32> = torch.2_1_0.aten::lift_fresh(%33:<f32>) 
58389:2:0 %220:<f32>+5 = torch.2_1_0.aten::select(%219:<2xf32>+4, 0:i32, 1:i32) 
58392:5:0 %136:<f32>+11 = torch.2_1_0.aten::select(%135:<2xf32>+10, 0:i32, 1:i32) 
58390:3:0 %187:<f32>+6 = torch.2_1_0.aten::copy_(%187:<f32>+6, %185:<f32>, False:pred) 
58393:6:0 %213:<2xf32>+12 = torch.2_1_0.aten::select(%207:<8x2xf32>, 0:i32, 6:i32) 
58394:7:0 %168:<2xf32>+14 = torch.2_1_0.aten::select(%165:<8x2xf32>, 0:i32, 7:i32) 
58388:1:0 %212:<2xf32>+2 = torch.2_1_0.aten::select(%200:<8x2xf32>, 0:i32, 1:i32) 
58393:6:0 %214:<f32>+13 = torch.2_1_0.aten::select(%213:<2xf32>+12, 0:i32, 1:i32) 
58389:2:0 %220:<f32>+5 = torch.2_1_0.aten::copy_(%220:<f32>+5, %216:<f32>, False:pred) 
58394:7:0 %169:<f32>+15 = torch.2_1_0.aten::select(%168:<2xf32>+14, 0:i32, 1:i32) 
58392:5:0 %136:<f32>+11 = torch.2_1_0.aten::copy_(%136:<f32>+11, %129:<f32>, False:pred) 
58390:None:0 %185:<f32> = torch.2_1_0.aten::lift_fresh(%185:<f32>) 
58388:1:0 %213:<f32>+3 = torch.2_1_0.aten::select(%212:<2xf32>+2, 0:i32, 1:i32) 
58391:4:0 %201:<f32>+8 = torch.2_1_0.aten::copy_(%201:<f32>+8, %195:<f32>, False:pred) 
58393:6:0 %214:<f32>+13 = torch.2_1_0.aten::copy_(%214:<f32>+13, %210:<f32>, False:pred) 
58389:2:0 %221:<16xf32> = torch.2_1_0.aten::view(%212:<8x2xf32>, list{-1:i32}) 
58392:5:0 %73:<16xf32> = torch.2_1_0.aten::view(%128:<8x2xf32>, list{-1:i32}) 
58390:3:0 %188:<2xf32>+6 = torch.2_1_0.aten::select(%182:<8x2xf32>, 0:i32, 3:i32) 
58388:1:0 %213:<f32>+3 = torch.2_1_0.aten::copy_(%213:<f32>+3, %33:<f32>, False:pred) 
58391:None:0 %195:<f32> = torch.2_1_0.aten::lift_fresh(%195:<f32>) 
58389:2:0 %222:<2xf32>+4 = torch.2_1_0.aten::select(%212:<8x2xf32>, 0:i32, 2:i32) 
58393:6:0 %214:<16xf32> = torch.2_1_0.aten::view(%207:<8x2xf32>, list{-1:i32}) 
58392:5:0 %136:<2xf32>+10 = torch.2_1_0.aten::select(%128:<8x2xf32>, 0:i32, 5:i32) 
58390:3:0 %189:<f32>+7 = torch.2_1_0.aten::select(%188:<2xf32>+6, 0:i32, 1:i32) 
58389:2:0 %223:<2xf32>+4 = torch.2_1_0.aten::slice(%222:<2xf32>+4, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58391:4:0 %198:<2xf32>+8 = torch.2_1_0.aten::select(%194:<8x2xf32>, 0:i32, 4:i32) 
58388:1:0 %213:<16xf32> = torch.2_1_0.aten::view(%200:<8x2xf32>, list{-1:i32}) 
58393:6:0 %215:<2xf32>+12 = torch.2_1_0.aten::select(%207:<8x2xf32>, 0:i32, 6:i32) 
58394:7:0 %169:<f32>+15 = torch.2_1_0.aten::copy_(%169:<f32>+15, %162:<f32>, False:pred) 
58392:5:0 %89:<2xf32>+10 = torch.2_1_0.aten::slice(%136:<2xf32>+10, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58389:2:0 %224:<2xf32>+4 = torch.2_1_0.aten::view(%223:<2xf32>+4, list{-1:i32}) 
58391:4:0 %202:<f32>+9 = torch.2_1_0.aten::select(%198:<2xf32>+8, 0:i32, 1:i32) 
58390:3:0 %189:<f32>+7 = torch.2_1_0.aten::copy_(%189:<f32>+7, %185:<f32>, False:pred) 
58388:1:0 %214:<2xf32>+2 = torch.2_1_0.aten::select(%200:<8x2xf32>, 0:i32, 1:i32) 
58392:5:0 %136:<2xf32>+10 = torch.2_1_0.aten::view(%89:<2xf32>+10, list{-1:i32}) 
58393:6:0 %213:<2xf32>+12 = torch.2_1_0.aten::slice(%215:<2xf32>+12, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58394:7:0 %158:<16xf32> = torch.2_1_0.aten::view(%165:<8x2xf32>, list{-1:i32}) 
58391:4:0 %202:<f32>+9 = torch.2_1_0.aten::copy_(%202:<f32>+9, %195:<f32>, False:pred) 
58388:1:0 %212:<2xf32>+2 = torch.2_1_0.aten::slice(%214:<2xf32>+2, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58390:3:0 %185:<16xf32> = torch.2_1_0.aten::view(%182:<8x2xf32>, list{-1:i32}) 
58394:7:0 %170:<2xf32>+14 = torch.2_1_0.aten::select(%165:<8x2xf32>, 0:i32, 7:i32) 
58388:1:0 %214:<2xf32>+2 = torch.2_1_0.aten::view(%212:<2xf32>+2, list{-1:i32}) 
58391:4:0 %39:<16xf32> = torch.2_1_0.aten::view(%194:<8x2xf32>, list{-1:i32}) 
58390:3:0 %189:<2xf32>+6 = torch.2_1_0.aten::select(%182:<8x2xf32>, 0:i32, 3:i32) 
58394:7:0 %171:<2xf32>+14 = torch.2_1_0.aten::slice(%170:<2xf32>+14, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58391:4:0 %202:<2xf32>+8 = torch.2_1_0.aten::select(%194:<8x2xf32>, 0:i32, 4:i32) 
58390:3:0 %190:<2xf32>+6 = torch.2_1_0.aten::slice(%189:<2xf32>+6, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58394:7:0 %172:<2xf32>+14 = torch.2_1_0.aten::view(%171:<2xf32>+14, list{-1:i32}) 
58391:4:0 %203:<2xf32>+8 = torch.2_1_0.aten::slice(%202:<2xf32>+8, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58390:3:0 %189:<2xf32>+6 = torch.2_1_0.aten::view(%190:<2xf32>+6, list{-1:i32}) 
58389:2:0 %221, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%221:<16xf32>, %224:<2xf32>+4, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58391:4:0 %202:<2xf32>+8 = torch.2_1_0.aten::view(%203:<2xf32>+8, list{-1:i32}) 
58389:2:0 %225:<8x2xf32> = torch.2_1_0.aten::slice(%212:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58389:2:0 %226:<8xf32>{2} = torch.2_1_0.aten::select(%225:<8x2xf32>, 1:i32, 0:i32) 
58388:1:0 %213, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%213:<16xf32>, %214:<2xf32>+2, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58388:1:0 %215:<8x2xf32> = torch.2_1_0.aten::slice(%200:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58388:1:0 %216:<8xf32>{2} = torch.2_1_0.aten::select(%215:<8x2xf32>, 1:i32, 0:i32) 
58392:5:0 %73, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%73:<16xf32>, %136:<2xf32>+10, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58394:7:0 %158, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%158:<16xf32>, %172:<2xf32>+14, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58390:3:0 %185, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%185:<16xf32>, %189:<2xf32>+6, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58392:5:0 %136:<8x2xf32> = torch.2_1_0.aten::slice(%128:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58394:7:0 %172:<8x2xf32> = torch.2_1_0.aten::slice(%165:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58390:3:0 %189:<8x2xf32> = torch.2_1_0.aten::slice(%182:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58392:5:0 %137:<8xf32>{2} = torch.2_1_0.aten::select(%136:<8x2xf32>, 1:i32, 0:i32) 
58394:7:0 %173:<8xf32>{2} = torch.2_1_0.aten::select(%172:<8x2xf32>, 1:i32, 0:i32) 
58391:4:0 %39, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%39:<16xf32>, %202:<2xf32>+8, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58390:3:0 %191:<8xf32>{2} = torch.2_1_0.aten::select(%189:<8x2xf32>, 1:i32, 0:i32) 
58391:4:0 %202:<8x2xf32> = torch.2_1_0.aten::slice(%194:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58393:6:0 %215:<2xf32>+12 = torch.2_1_0.aten::view(%213:<2xf32>+12, list{-1:i32}) 
58391:4:0 %204:<8xf32>{2} = torch.2_1_0.aten::select(%202:<8x2xf32>, 1:i32, 0:i32) 
58387:None:None %227:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%224:<1x2048x2048xunknown>}, 0:i32, out=%227:<1x1x2048x2048xunknown>) 
58393:6:0 %214, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%214:<16xf32>, %215:<2xf32>+12, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58387:None:None %228:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<2048xf32> = torch.2_1_0.aten::set_(%228:<f32>, storage{size=8192}) 
58393:6:0 %216:<8x2xf32> = torch.2_1_0.aten::slice(%207:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %237:<2048xi64> = torch.2_1_0.aten::set_(%237:<i64>, storage{size=16384}) 
58387:None:None %228:<1x2048xf32> = torch.2_1_0.aten::resize_(%228:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %57:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58393:6:0 %217:<8xf32>{2} = torch.2_1_0.aten::select(%216:<8x2xf32>, 1:i32, 0:i32) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::resize_(%237:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%228:<1x2048xf32>) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::stack(list{%73:<2048xi64>}, 0:i32, out=%237:<1x2048xi64>) 
58387:None:None %229:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %229:<2048xi64> = torch.2_1_0.aten::set_(%229:<i64>, storage{size=16384}) 
58387:None:None %229:<1x2048xi64> = torch.2_1_0.aten::resize_(%229:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %229:<1x2048xi64> = torch.2_1_0.aten::stack(list{%89:<2048xi64>}, 0:i32, out=%229:<1x2048xi64>) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%217:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %225:<8x2xf32> = torch.2_1_0.aten::zeros(list{8:i32, 2:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred) 
58387:None:0 %226:<f32> = torch.2_1_0.aten::lift_fresh(%226:<f32>) 
58387:0:0 %227:<2xf32> = torch.2_1_0.aten::select(%225:<8x2xf32>, 0:i32, 0:i32) 
58387:0:0 %224:<f32> = torch.2_1_0.aten::select(%227:<2xf32>, 0:i32, 0:i32) 
58387:0:0 %224:<f32> = torch.2_1_0.aten::copy_(%224:<f32>, %226:<f32>, False:pred) 
58387:None:0 %226:<f32> = torch.2_1_0.aten::lift_fresh(%226:<f32>) 
58387:0:0 %227:<2xf32> = torch.2_1_0.aten::select(%225:<8x2xf32>, 0:i32, 0:i32) 
58387:None:None %57:<2048x2048xf32> = torch.2_1_0.aten::ones(list{2048:i32, 2048:i32}, dtype=None:NoneType, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:0:0 %224:<f32>+1 = torch.2_1_0.aten::select(%227:<2xf32>, 0:i32, 1:i32) 
58387:0:0 %224:<f32>+1 = torch.2_1_0.aten::copy_(%224:<f32>+1, %226:<f32>, False:pred) 
58387:0:0 %228:<16xf32> = torch.2_1_0.aten::view(%225:<8x2xf32>, list{-1:i32}) 
58387:0:0 %227:<2xf32> = torch.2_1_0.aten::select(%225:<8x2xf32>, 0:i32, 0:i32) 
58387:0:0 %229:<2xf32> = torch.2_1_0.aten::slice(%227:<2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58387:0:0 %227:<2xf32> = torch.2_1_0.aten::view(%229:<2xf32>, list{-1:i32}) 
58387:0:0 %228, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject:<16xf32>,  = torch.2_1_0.c10d::_allgather_base_(%228:<16xf32>, %227:<2xf32>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject) 
58393:6:0 %216:<8xunknown> = torch.2_1_0.aten::gt(%217:<8xf32>{2}, 0.0:f32) 
58389:2:0 %227:<8xunknown> = torch.2_1_0.aten::gt(%226:<8xf32>{2}, 0.0:f32) 
58392:5:0 %136:<8xunknown> = torch.2_1_0.aten::gt(%137:<8xf32>{2}, 0.0:f32) 
58391:4:0 %202:<8xunknown> = torch.2_1_0.aten::gt(%204:<8xf32>{2}, 0.0:f32) 
58394:7:0 %158:<8xunknown> = torch.2_1_0.aten::gt(%173:<8xf32>{2}, 0.0:f32) 
58390:3:0 %189:<8xunknown> = torch.2_1_0.aten::gt(%191:<8xf32>{2}, 0.0:f32) 
58387:0:0 %227:<8x2xf32> = torch.2_1_0.aten::slice(%225:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58388:1:0 %217:<8xunknown> = torch.2_1_0.aten::gt(%216:<8xf32>{2}, 0.0:f32) 
58387:0:0 %230:<8xf32>{2} = torch.2_1_0.aten::select(%227:<8x2xf32>, 1:i32, 0:i32) 
58387:None:None %72:<2049xi64> = torch.2_1_0.aten::lift_fresh(%72:<2049xi64>) 
58387:None:None %230:<2048xi64> = torch.2_1_0.aten::slice(%72:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %231:<2048xi64>+1 = torch.2_1_0.aten::slice(%72:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %232:<2048xunknown> = torch.2_1_0.aten::eq(%231:<2048xi64>+1, -1:i32) 
58387:None:None %233:<f32> = torch.2_1_0.aten::lift_fresh(%233:<f32>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%232:<2048xunknown>}, %233:<f32>, False:pred) 
58387:None:None %233:<2048xunknown> = torch.2_1_0.aten::eq(%230:<2048xi64>, -1:i32) 
58387:None:None %104:<i64> = torch.2_1_0.aten::lift_fresh(%104:<i64>) 
58387:None:None %230:<2048xi64> = torch.2_1_0.aten::index_put_(%230:<2048xi64>, list{%233:<2048xunknown>}, %104:<i64>, False:pred) 
58387:None:None %233:<2048xunknown> = torch.2_1_0.aten::eq(%231:<2048xi64>+1, -1:i32) 
58387:None:None %234:<i64> = torch.2_1_0.aten::lift_fresh(%234:<i64>) 
58387:None:None %231:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%231:<2048xi64>+1, list{%233:<2048xunknown>}, %234:<i64>, False:pred) 
58387:None:None %235:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %235:<2048xi64> = torch.2_1_0.aten::set_(%235:<i64>, storage{size=16384}) 
58387:None:None %235:<1x2048xi64> = torch.2_1_0.aten::resize_(%235:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %235:<1x2048xi64> = torch.2_1_0.aten::stack(list{%230:<2048xi64>}, 0:i32, out=%235:<1x2048xi64>) 
58387:None:None %236:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %236:<2048xi64> = torch.2_1_0.aten::set_(%236:<i64>, storage{size=16384}) 
58387:None:None %236:<1x2048xi64> = torch.2_1_0.aten::resize_(%236:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %236:<1x2048xi64> = torch.2_1_0.aten::stack(list{%231:<2048xi64>+1}, 0:i32, out=%236:<1x2048xi64>) 
58387:None:None %237:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %237:<4194304xunknown> = torch.2_1_0.aten::set_(%237:<unknown>, storage{size=4194304}) 
58387:None:None %237:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%237:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %72:<2048x2048xf32> = torch.2_1_0.aten::tril(%57:<2048x2048xf32>, 0:i32) 
58387:None:None %84:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%72:<2048x2048xf32>, 0:i32) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %225:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %237:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%224:<1x2048x2048xunknown>}, 0:i32, out=%237:<1x1x2048x2048xunknown>) 
58387:None:None %72:<2048x2048xf32> = torch.2_1_0.aten::tril(%57:<2048x2048xf32>, 0:i32) 
58387:None:None %238:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %238:<2048xf32> = torch.2_1_0.aten::set_(%238:<f32>, storage{size=8192}) 
58387:None:None %57:<1x2048x2048xf32> = torch.2_1_0.aten::unsqueeze(%72:<2048x2048xf32>, 0:i32) 
58387:None:None %60:<2048xf32> = torch.2_1_0.aten::ones(list{2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %225:<2048xi64> = torch.2_1_0.aten::arange(2048:i32, dtype=torch.int64:dtype, layout=None:NoneType, device=cpu:device, pin_memory=False:pred) 
58387:None:None %238:<1x2048xf32> = torch.2_1_0.aten::resize_(%238:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %238:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%238:<1x2048xf32>) 
58387:None:None %228:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<2048xi64> = torch.2_1_0.aten::set_(%228:<i64>, storage{size=16384}) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::resize_(%228:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::stack(list{%89:<2048xi64>}, 0:i32, out=%228:<1x2048xi64>) 
58387:0:0 %231:<8xunknown> = torch.2_1_0.aten::gt(%230:<8xf32>{2}, 0.0:f32) 
58388:1:0 %218:<8xf32> = torch.2_1_0.aten::index(%216:<8xf32>{2}, list{%217:<8xunknown>}) 
58394:7:0 %174:<8xf32> = torch.2_1_0.aten::index(%173:<8xf32>{2}, list{%158:<8xunknown>}) 
58387:None:None %226:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%57:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %226:<1x2048x2048xunknown> = torch.2_1_0.aten::lt(%84:<1x2048x2048xf32>, 0.5:f32) 
58387:None:None %227:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %56:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %72:<f32> = torch.2_1_0.aten::lift_fresh(%72:<f32>) 
58387:None:None %72:<f32> = torch.2_1_0.aten::lift_fresh(%72:<f32>) 
58387:None:None %60:<2048xf32> = torch.2_1_0.aten::index_put_(%60:<2048xf32>, list{%56:<2048xunknown>}, %72:<f32>, False:pred) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%227:<2048xunknown>}, %72:<f32>, False:pred) 
58387:None:None %67:<2048xunknown> = torch.2_1_0.aten::eq(%32:<2048xi64>, -1:i32) 
58387:None:None %228:<2048xunknown> = torch.2_1_0.aten::eq(%32:<2048xi64>, -1:i32) 
58387:None:None %72:<i64> = torch.2_1_0.aten::lift_fresh(%72:<i64>) 
58387:None:None %229:<i64> = torch.2_1_0.aten::lift_fresh(%229:<i64>) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::index_put_(%32:<2048xi64>, list{%67:<2048xunknown>}, %72:<i64>, False:pred) 
58387:None:None %32:<2048xi64> = torch.2_1_0.aten::index_put_(%32:<2048xi64>, list{%228:<2048xunknown>}, %229:<i64>, False:pred) 
58387:None:None %98:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %72:<2048xunknown> = torch.2_1_0.aten::eq(%18:<2048xi64>+1, -1:i32) 
58387:None:None %229:<i64> = torch.2_1_0.aten::lift_fresh(%229:<i64>) 
58387:None:None %227:<i64> = torch.2_1_0.aten::lift_fresh(%227:<i64>) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%18:<2048xi64>+1, list{%98:<2048xunknown>}, %229:<i64>, False:pred) 
58387:None:None %18:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%18:<2048xi64>+1, list{%72:<2048xunknown>}, %227:<i64>, False:pred) 
58387:None:None %227:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %98:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %227:<2048xi64> = torch.2_1_0.aten::set_(%227:<i64>, storage{size=16384}) 
58387:None:None %227:<1x2048xi64> = torch.2_1_0.aten::resize_(%227:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %227:<1x2048xi64> = torch.2_1_0.aten::stack(list{%32:<2048xi64>}, 0:i32, out=%227:<1x2048xi64>) 
58387:None:None %228:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %228:<2048xi64> = torch.2_1_0.aten::set_(%228:<i64>, storage{size=16384}) 
58387:None:None %98:<2048xi64> = torch.2_1_0.aten::set_(%98:<i64>, storage{size=16384}) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::resize_(%228:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %98:<1x2048xi64> = torch.2_1_0.aten::resize_(%98:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %228:<1x2048xi64> = torch.2_1_0.aten::stack(list{%18:<2048xi64>+1}, 0:i32, out=%228:<1x2048xi64>) 
58387:None:None %229:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %98:<1x2048xi64> = torch.2_1_0.aten::stack(list{%32:<2048xi64>}, 0:i32, out=%98:<1x2048xi64>) 
58387:None:None %229:<4194304xunknown> = torch.2_1_0.aten::set_(%229:<unknown>, storage{size=4194304}) 
58387:None:None %229:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%229:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %230:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %230:<2048xi64> = torch.2_1_0.aten::set_(%230:<i64>, storage{size=16384}) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::resize_(%230:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::stack(list{%18:<2048xi64>+1}, 0:i32, out=%230:<1x2048xi64>) 
58387:None:None %231:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %231:<4194304xunknown> = torch.2_1_0.aten::set_(%231:<unknown>, storage{size=4194304}) 
58387:None:None %231:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%231:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58389:2:0 %228:<8xf32> = torch.2_1_0.aten::index(%226:<8xf32>{2}, list{%227:<8xunknown>}) 
58388:1:0 %217:<f32> = torch.2_1_0.aten::min(%218:<8xf32>) 
58388:1:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%217:<f32>) 
58394:7:0 %175:<f32> = torch.2_1_0.aten::min(%174:<8xf32>) 
58394:7:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%175:<f32>) 
58387:None:None %229:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%226:<1x2048x2048xunknown>}, 0:i32, out=%229:<1x1x2048x2048xunknown>) 
58387:None:None %230:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %230:<2048xf32> = torch.2_1_0.aten::set_(%230:<f32>, storage{size=8192}) 
58387:None:None %230:<1x2048xf32> = torch.2_1_0.aten::resize_(%230:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %231:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%226:<1x2048x2048xunknown>}, 0:i32, out=%231:<1x1x2048x2048xunknown>) 
58393:6:0 %218:<8xf32> = torch.2_1_0.aten::index(%217:<8xf32>{2}, list{%216:<8xunknown>}) 
58387:None:None %230:<1x2048xf32> = torch.2_1_0.aten::stack(list{%60:<2048xf32>}, 0:i32, out=%230:<1x2048xf32>) 
58387:None:None %232:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %231:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %232:<2048xf32> = torch.2_1_0.aten::set_(%232:<f32>, storage{size=8192}) 
58387:None:None %231:<2048xi64> = torch.2_1_0.aten::set_(%231:<i64>, storage{size=16384}) 
58387:None:None %232:<1x2048xf32> = torch.2_1_0.aten::resize_(%232:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %231:<1x2048xi64> = torch.2_1_0.aten::resize_(%231:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %232:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%232:<1x2048xf32>) 
58387:None:None %231:<1x2048xi64> = torch.2_1_0.aten::stack(list{%225:<2048xi64>}, 0:i32, out=%231:<1x2048xi64>) 
58387:None:None %233:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %233:<2048xi64> = torch.2_1_0.aten::set_(%233:<i64>, storage{size=16384}) 
58387:None:None %233:<1x2048xi64> = torch.2_1_0.aten::resize_(%233:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %233:<1x2048xi64> = torch.2_1_0.aten::stack(list{%225:<2048xi64>}, 0:i32, out=%233:<1x2048xi64>) 
58387:None:None %56:<2049xi64> = torch.2_1_0.aten::lift_fresh(%56:<2049xi64>) 
58387:None:None %232:<2048xi64> = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %233:<2048xi64>+1 = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%233:<2048xi64>+1, -1:i32) 
58387:None:None %235:<f32> = torch.2_1_0.aten::lift_fresh(%235:<f32>) 
58387:None:None %60:<2048xf32> = torch.2_1_0.aten::index_put_(%60:<2048xf32>, list{%234:<2048xunknown>}, %235:<f32>, False:pred) 
58387:None:None %234:<2048xunknown> = torch.2_1_0.aten::eq(%232:<2048xi64>, -1:i32) 
58387:None:None %235:<i64> = torch.2_1_0.aten::lift_fresh(%235:<i64>) 
58387:None:None %232:<2048xi64> = torch.2_1_0.aten::index_put_(%232:<2048xi64>, list{%234:<2048xunknown>}, %235:<i64>, False:pred) 
58387:None:None %235:<2048xunknown> = torch.2_1_0.aten::eq(%233:<2048xi64>+1, -1:i32) 
58387:None:None %66:<i64> = torch.2_1_0.aten::lift_fresh(%66:<i64>) 
58387:None:None %233:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%233:<2048xi64>+1, list{%235:<2048xunknown>}, %66:<i64>, False:pred) 
58387:None:None %236:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %236:<2048xi64> = torch.2_1_0.aten::set_(%236:<i64>, storage{size=16384}) 
58387:None:None %236:<1x2048xi64> = torch.2_1_0.aten::resize_(%236:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %236:<1x2048xi64> = torch.2_1_0.aten::stack(list{%232:<2048xi64>}, 0:i32, out=%236:<1x2048xi64>) 
58387:None:None %237:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %237:<2048xi64> = torch.2_1_0.aten::set_(%237:<i64>, storage{size=16384}) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::resize_(%237:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %237:<1x2048xi64> = torch.2_1_0.aten::stack(list{%233:<2048xi64>+1}, 0:i32, out=%237:<1x2048xi64>) 
58387:None:None %238:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %238:<4194304xunknown> = torch.2_1_0.aten::set_(%238:<unknown>, storage{size=4194304}) 
58387:None:None %238:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%238:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %56:<2049xi64> = torch.2_1_0.aten::lift_fresh(%56:<2049xi64>) 
58387:None:None %234:<2048xi64> = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %235:<2048xi64>+1 = torch.2_1_0.aten::slice(%56:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %236:<2048xunknown> = torch.2_1_0.aten::eq(%235:<2048xi64>+1, -1:i32) 
58387:None:None %237:<f32> = torch.2_1_0.aten::lift_fresh(%237:<f32>) 
58387:None:None %73:<2048xf32> = torch.2_1_0.aten::index_put_(%73:<2048xf32>, list{%236:<2048xunknown>}, %237:<f32>, False:pred) 
58387:None:None %236:<2048xunknown> = torch.2_1_0.aten::eq(%234:<2048xi64>, -1:i32) 
58387:None:None %238:<i64> = torch.2_1_0.aten::lift_fresh(%238:<i64>) 
58387:None:None %234:<2048xi64> = torch.2_1_0.aten::index_put_(%234:<2048xi64>, list{%236:<2048xunknown>}, %238:<i64>, False:pred) 
58387:None:None %238:<2048xunknown> = torch.2_1_0.aten::eq(%235:<2048xi64>+1, -1:i32) 
58387:None:None %239:<i64> = torch.2_1_0.aten::lift_fresh(%239:<i64>) 
58387:None:None %235:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%235:<2048xi64>+1, list{%238:<2048xunknown>}, %239:<i64>, False:pred) 
58389:2:0 %225:<f32> = torch.2_1_0.aten::min(%228:<8xf32>) 
58389:2:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%225:<f32>) 
58387:None:None %240:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %240:<2048xi64> = torch.2_1_0.aten::set_(%240:<i64>, storage{size=16384}) 
58387:None:None %240:<1x2048xi64> = torch.2_1_0.aten::resize_(%240:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %240:<1x2048xi64> = torch.2_1_0.aten::stack(list{%234:<2048xi64>}, 0:i32, out=%240:<1x2048xi64>) 
58387:None:None %203:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %203:<2048xi64> = torch.2_1_0.aten::set_(%203:<i64>, storage{size=16384}) 
58387:None:None %203:<1x2048xi64> = torch.2_1_0.aten::resize_(%203:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58388:1:0 %219:<f32> = torch.2_1_0.aten::max(%218:<8xf32>) 
58387:None:None %203:<1x2048xi64> = torch.2_1_0.aten::stack(list{%235:<2048xi64>+1}, 0:i32, out=%203:<1x2048xi64>) 
58388:1:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%219:<f32>) 
58387:None:None %241:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %241:<4194304xunknown> = torch.2_1_0.aten::set_(%241:<unknown>, storage{size=4194304}) 
58388:1:0 %220:<8x2xf32> = torch.2_1_0.aten::slice(%200:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58388:1:0 %221:<8xf32>{2}+1 = torch.2_1_0.aten::select(%220:<8x2xf32>, 1:i32, 1:i32) 
58394:7:0 %175:<f32> = torch.2_1_0.aten::max(%174:<8xf32>) 
58388:1:0 %218:<8xunknown> = torch.2_1_0.aten::gt(%221:<8xf32>{2}+1, 0.0:f32) 
58394:7:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%175:<f32>) 
58394:7:0 %175:<8x2xf32> = torch.2_1_0.aten::slice(%165:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58388:1:0 %220:<8xf32> = torch.2_1_0.aten::index(%221:<8xf32>{2}+1, list{%218:<8xunknown>}) 
58394:7:0 %176:<8xf32>{2}+1 = torch.2_1_0.aten::select(%175:<8x2xf32>, 1:i32, 1:i32) 
58388:1:0 %221:<f32> = torch.2_1_0.aten::min(%220:<8xf32>) 
58388:1:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%221:<f32>) 
58394:7:0 %174:<8xunknown> = torch.2_1_0.aten::gt(%176:<8xf32>{2}+1, 0.0:f32) 
58388:1:0 %221:<f32> = torch.2_1_0.aten::max(%220:<8xf32>) 
58388:1:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%221:<f32>) 
58394:7:0 %158:<8xf32> = torch.2_1_0.aten::index(%176:<8xf32>{2}+1, list{%174:<8xunknown>}) 
58394:7:0 %176:<f32> = torch.2_1_0.aten::min(%158:<8xf32>) 
58394:7:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%176:<f32>) 
58394:7:0 %176:<f32> = torch.2_1_0.aten::max(%158:<8xf32>) 
58394:7:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%176:<f32>) 
58387:None:None %238:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%226:<1x2048x2048xunknown>}, 0:i32, out=%238:<1x1x2048x2048xunknown>) 
58388:1:0 %222:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %177:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %239:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %239:<2048xf32> = torch.2_1_0.aten::set_(%239:<f32>, storage{size=8192}) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%222:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:None %239:<1x2048xf32> = torch.2_1_0.aten::resize_(%239:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%177:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:None:None %239:<1x2048xf32> = torch.2_1_0.aten::stack(list{%60:<2048xf32>}, 0:i32, out=%239:<1x2048xf32>) 
58387:None:None %241:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%241:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %240:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %240:<2048xi64> = torch.2_1_0.aten::set_(%240:<i64>, storage{size=16384}) 
58387:None:None %240:<1x2048xi64> = torch.2_1_0.aten::resize_(%240:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58393:6:0 %215:<f32> = torch.2_1_0.aten::min(%218:<8xf32>) 
58387:None:None %240:<1x2048xi64> = torch.2_1_0.aten::stack(list{%225:<2048xi64>}, 0:i32, out=%240:<1x2048xi64>) 
58393:6:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%215:<f32>) 
58392:5:0 %138:<8xf32> = torch.2_1_0.aten::index(%137:<8xf32>{2}, list{%136:<8xunknown>}) 
58390:3:0 %192:<8xf32> = torch.2_1_0.aten::index(%191:<8xf32>{2}, list{%189:<8xunknown>}) 
58387:None:None %241:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%226:<1x2048x2048xunknown>}, 0:i32, out=%241:<1x1x2048x2048xunknown>) 
58387:None:None %242:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %242:<2048xf32> = torch.2_1_0.aten::set_(%242:<f32>, storage{size=8192}) 
58387:None:None %242:<1x2048xf32> = torch.2_1_0.aten::resize_(%242:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %242:<1x2048xf32> = torch.2_1_0.aten::stack(list{%73:<2048xf32>}, 0:i32, out=%242:<1x2048xf32>) 
58387:None:None %243:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %243:<2048xi64> = torch.2_1_0.aten::set_(%243:<i64>, storage{size=16384}) 
58387:None:None %243:<1x2048xi64> = torch.2_1_0.aten::resize_(%243:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %243:<1x2048xi64> = torch.2_1_0.aten::stack(list{%225:<2048xi64>}, 0:i32, out=%243:<1x2048xi64>) 
58389:2:0 %229:<f32> = torch.2_1_0.aten::max(%228:<8xf32>) 
58389:2:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%229:<f32>) 
58389:2:0 %225:<8x2xf32> = torch.2_1_0.aten::slice(%212:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58389:2:0 %230:<8xf32>{2}+1 = torch.2_1_0.aten::select(%225:<8x2xf32>, 1:i32, 1:i32) 
58389:2:0 %231:<8xunknown> = torch.2_1_0.aten::gt(%230:<8xf32>{2}+1, 0.0:f32) 
58389:2:0 %232:<8xf32> = torch.2_1_0.aten::index(%230:<8xf32>{2}+1, list{%231:<8xunknown>}) 
58389:2:0 %225:<f32> = torch.2_1_0.aten::min(%232:<8xf32>) 
58389:2:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%225:<f32>) 
58389:2:0 %233:<f32> = torch.2_1_0.aten::max(%232:<8xf32>) 
58389:2:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%233:<f32>) 
58391:4:0 %205:<8xf32> = torch.2_1_0.aten::index(%204:<8xf32>{2}, list{%202:<8xunknown>}) 
58389:2:0 %88:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%88:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %219:<f32> = torch.2_1_0.aten::max(%218:<8xf32>) 
58393:6:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%219:<f32>) 
58393:6:0 %215:<8x2xf32> = torch.2_1_0.aten::slice(%207:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58393:6:0 %220:<8xf32>{2}+1 = torch.2_1_0.aten::select(%215:<8x2xf32>, 1:i32, 1:i32) 
58393:6:0 %221:<8xunknown> = torch.2_1_0.aten::gt(%220:<8xf32>{2}+1, 0.0:f32) 
58393:6:0 %222:<8xf32> = torch.2_1_0.aten::index(%220:<8xf32>{2}+1, list{%221:<8xunknown>}) 
58393:6:0 %215:<f32> = torch.2_1_0.aten::min(%222:<8xf32>) 
58393:6:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%215:<f32>) 
58393:6:0 %223:<f32> = torch.2_1_0.aten::max(%222:<8xf32>) 
58393:6:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%223:<f32>) 
58393:6:0 %97:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%97:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %137:<f32> = torch.2_1_0.aten::min(%138:<8xf32>) 
58392:5:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%137:<f32>) 
58390:3:0 %178:<f32> = torch.2_1_0.aten::min(%192:<8xf32>) 
58390:3:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%178:<f32>) 
58387:0:0 %232:<8xf32> = torch.2_1_0.aten::index(%230:<8xf32>{2}, list{%231:<8xunknown>}) 
58391:4:0 %202:<f32> = torch.2_1_0.aten::min(%205:<8xf32>) 
58391:4:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%202:<f32>) 
58392:5:0 %137:<f32> = torch.2_1_0.aten::max(%138:<8xf32>) 
58392:5:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%137:<f32>) 
58392:5:0 %137:<8x2xf32> = torch.2_1_0.aten::slice(%128:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58392:5:0 %136:<8xf32>{2}+1 = torch.2_1_0.aten::select(%137:<8x2xf32>, 1:i32, 1:i32) 
58392:5:0 %138:<8xunknown> = torch.2_1_0.aten::gt(%136:<8xf32>{2}+1, 0.0:f32) 
58392:5:0 %137:<8xf32> = torch.2_1_0.aten::index(%136:<8xf32>{2}+1, list{%138:<8xunknown>}) 
58392:5:0 %136:<f32> = torch.2_1_0.aten::min(%137:<8xf32>) 
58392:5:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%136:<f32>) 
58392:5:0 %136:<f32> = torch.2_1_0.aten::max(%137:<8xf32>) 
58392:5:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%136:<f32>) 
58390:3:0 %193:<f32> = torch.2_1_0.aten::max(%192:<8xf32>) 
58390:3:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%193:<f32>) 
58392:5:0 %73:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %193:<8x2xf32> = torch.2_1_0.aten::slice(%182:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58390:3:0 %194:<8xf32>{2}+1 = torch.2_1_0.aten::select(%193:<8x2xf32>, 1:i32, 1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%73:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 %192:<8xunknown> = torch.2_1_0.aten::gt(%194:<8xf32>{2}+1, 0.0:f32) 
58390:3:0 %195:<8xf32> = torch.2_1_0.aten::index(%194:<8xf32>{2}+1, list{%192:<8xunknown>}) 
58390:3:0 %178:<f32> = torch.2_1_0.aten::min(%195:<8xf32>) 
58390:3:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%178:<f32>) 
58390:3:0 %196:<f32> = torch.2_1_0.aten::max(%195:<8xf32>) 
58390:3:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%196:<f32>) 
58390:3:0 %88:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%88:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 %202:<f32> = torch.2_1_0.aten::max(%205:<8xf32>) 
58391:4:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%202:<f32>) 
58391:4:0 %202:<8x2xf32> = torch.2_1_0.aten::slice(%194:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58391:4:0 %206:<8xf32>{2}+1 = torch.2_1_0.aten::select(%202:<8x2xf32>, 1:i32, 1:i32) 
58391:4:0 %198:<8xunknown> = torch.2_1_0.aten::gt(%206:<8xf32>{2}+1, 0.0:f32) 
58391:4:0 %207:<8xf32> = torch.2_1_0.aten::index(%206:<8xf32>{2}+1, list{%198:<8xunknown>}) 
58391:4:0 %202:<f32> = torch.2_1_0.aten::min(%207:<8xf32>) 
58391:4:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%202:<f32>) 
58391:4:0 %202:<f32> = torch.2_1_0.aten::max(%207:<8xf32>) 
58391:4:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%202:<f32>) 
58391:4:0 %118:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%118:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %233:<f32> = torch.2_1_0.aten::min(%232:<8xf32>) 
58387:0:0 0.8647031784057617:f32: = torch.2_1_0.aten::_local_scalar_dense(%233:<f32>) 
58387:0:0 %234:<f32> = torch.2_1_0.aten::max(%232:<8xf32>) 
58387:0:0 1.352414846420288:f32: = torch.2_1_0.aten::_local_scalar_dense(%234:<f32>) 
58387:0:0 %230:<8x2xf32> = torch.2_1_0.aten::slice(%225:<8x2xf32>, 0:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58387:0:0 %235:<8xf32>{2}+1 = torch.2_1_0.aten::select(%230:<8x2xf32>, 1:i32, 1:i32) 
58387:0:0 %232:<8xunknown> = torch.2_1_0.aten::gt(%235:<8xf32>{2}+1, 0.0:f32) 
58387:0:0 %230:<8xf32> = torch.2_1_0.aten::index(%235:<8xf32>{2}+1, list{%232:<8xunknown>}) 
58387:0:0 %235:<f32> = torch.2_1_0.aten::min(%230:<8xf32>) 
58387:0:0 0.09858131408691406:f32: = torch.2_1_0.aten::_local_scalar_dense(%235:<f32>) 
58387:0:0 %235:<f32> = torch.2_1_0.aten::max(%230:<8xf32>) 
58387:0:0 0.46136927604675293:f32: = torch.2_1_0.aten::_local_scalar_dense(%235:<f32>) 
58387:0:0 %225:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %208:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %139:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %108:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %85:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %223:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %85:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %195:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%85:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%139:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%208:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%108:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%223:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%85:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%195:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%225:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %217:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%217:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %52:<330410496xf16> = torch.2_1_0.aten::zero_(%52:<330410496xf16>) 
58389:2:0 %114:<330410496xf16> = torch.2_1_0.aten::zero_(%114:<330410496xf16>) 
58394:7:0 %83:<330410496xf16> = torch.2_1_0.aten::zero_(%83:<330410496xf16>) 
58393:6:0 %69:<330410496xf16> = torch.2_1_0.aten::zero_(%69:<330410496xf16>) 
58388:1:0 %127:<330410496xf16> = torch.2_1_0.aten::zero_(%127:<330410496xf16>) 
58391:4:0 %62:<330410496xf16> = torch.2_1_0.aten::zero_(%62:<330410496xf16>) 
58390:3:0 %3:<330410496xf16> = torch.2_1_0.aten::zero_(%3:<330410496xf16>) 
58387:0:0 %81:<330410496xf16> = torch.2_1_0.aten::zero_(%81:<330410496xf16>) 
58392:None:0 %140:<i32> = torch.2_1_0.aten::lift_fresh(%140:<i32>) 
58389:None:0 %234:<i32> = torch.2_1_0.aten::lift_fresh(%234:<i32>) 
58394:None:0 %1:<i32> = torch.2_1_0.aten::lift_fresh(%1:<i32>) 
58393:None:0 %90:<i32> = torch.2_1_0.aten::lift_fresh(%90:<i32>) 
58388:None:0 %224:<i32> = torch.2_1_0.aten::lift_fresh(%224:<i32>) 
58391:None:0 %194:<i32> = torch.2_1_0.aten::lift_fresh(%194:<i32>) 
58390:None:0 %197:<i32> = torch.2_1_0.aten::lift_fresh(%197:<i32>) 
58387:None:0 %231:<i32> = torch.2_1_0.aten::lift_fresh(%231:<i32>) 
58392:5:0 %141:<i32> = torch.2_1_0.aten::_to_copy(%140:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %178:<i32> = torch.2_1_0.aten::_to_copy(%234:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %92:<i32> = torch.2_1_0.aten::_to_copy(%90:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %59:<i32> = torch.2_1_0.aten::_to_copy(%224:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %209:<i32> = torch.2_1_0.aten::_to_copy(%194:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %178:<i32> = torch.2_1_0.aten::_to_copy(%197:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %82:<i32> = torch.2_1_0.aten::_to_copy(%1:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %236:<i32> = torch.2_1_0.aten::_to_copy(%231:<i32>, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %57:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %142:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %31:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %96:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %181:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %129:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %181:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %90:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %234:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %207:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %184:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %136:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %9:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %214:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %210:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %177:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %222:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %143:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %235:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %182:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %35:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %223:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %158:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %211:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %137:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %225:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %224:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %198:<1x2048xf32> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.float32:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %178:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %212:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %226:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %197:<1x1x2048x2048xunknown> = torch.2_1_0.aten::empty(list{1:i32, 1:i32, 2048:i32, 2048:i32}, dtype=torch.bool:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %164:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %213:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %199:<1x2048xi64> = torch.2_1_0.aten::empty(list{1:i32, 2048:i32}, dtype=torch.int64:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(enumerate(DataLoader)#_MultiProcessingDataLoaderIter.__next__:str, None:NoneType) 
58387:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58387:0:0 %238:<1x2048xi64> = torch.2_1_0.aten::_to_copy(%237:<1x2048xi64>, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=True:pred, memory_format=None:NoneType) 
58387:0:0 %240:<1x2048xi64> = torch.2_1_0.aten::_to_copy(%239:<1x2048xi64>, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=True:pred, memory_format=None:NoneType) 
58387:0:0 %242:<1x2048xf32> = torch.2_1_0.aten::_to_copy(%241:<1x2048xf32>, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=True:pred, memory_format=None:NoneType) 
58387:0:0 %244:<1x1x2048x2048xunknown> = torch.2_1_0.aten::_to_copy(%243:<1x1x2048x2048xunknown>, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=True:pred, memory_format=None:NoneType) 
58387:0:0 %246:<1x2048xi64> = torch.2_1_0.aten::_to_copy(%245:<1x2048xi64>, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=True:pred, memory_format=None:NoneType) 
58387:None:None %242:<2049xi64> = torch.2_1_0.aten::lift_fresh(%242:<2049xi64>) 
58387:None:None %56:<2048xi64> = torch.2_1_0.aten::slice(%242:<2049xi64>, 0:i32, 0:i32, -1:i32, 1:i32) 
58387:None:None %108:<2048xi64>+1 = torch.2_1_0.aten::slice(%242:<2049xi64>, 0:i32, 1:i32, 9223372036854775807:i32, 1:i32) 
58387:None:None %243:<2048xunknown> = torch.2_1_0.aten::eq(%108:<2048xi64>+1, -1:i32) 
58387:None:None %244:<f32> = torch.2_1_0.aten::lift_fresh(%244:<f32>) 
58387:None:None %72:<2048xf32> = torch.2_1_0.aten::index_put_(%72:<2048xf32>, list{%243:<2048xunknown>}, %244:<f32>, False:pred) 
58387:None:None %240:<2048xunknown> = torch.2_1_0.aten::eq(%56:<2048xi64>, -1:i32) 
58387:None:None %244:<i64> = torch.2_1_0.aten::lift_fresh(%244:<i64>) 
58387:None:None %56:<2048xi64> = torch.2_1_0.aten::index_put_(%56:<2048xi64>, list{%240:<2048xunknown>}, %244:<i64>, False:pred) 
58387:None:None %237:<2048xunknown> = torch.2_1_0.aten::eq(%108:<2048xi64>+1, -1:i32) 
58387:None:None %239:<i64> = torch.2_1_0.aten::lift_fresh(%239:<i64>) 
58387:None:None %108:<2048xi64>+1 = torch.2_1_0.aten::index_put_(%108:<2048xi64>+1, list{%237:<2048xunknown>}, %239:<i64>, False:pred) 
58387:None:None %229:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %229:<2048xi64> = torch.2_1_0.aten::set_(%229:<i64>, storage{size=16384}) 
58387:None:None %229:<1x2048xi64> = torch.2_1_0.aten::resize_(%229:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %229:<1x2048xi64> = torch.2_1_0.aten::stack(list{%56:<2048xi64>}, 0:i32, out=%229:<1x2048xi64>) 
58387:None:None %245:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %245:<2048xi64> = torch.2_1_0.aten::set_(%245:<i64>, storage{size=16384}) 
58387:None:None %245:<1x2048xi64> = torch.2_1_0.aten::resize_(%245:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %245:<1x2048xi64> = torch.2_1_0.aten::stack(list{%108:<2048xi64>+1}, 0:i32, out=%245:<1x2048xi64>) 
58387:None:None %246:<unknown> = torch.2_1_0.aten::empty(list{}, dtype=torch.bool:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %246:<4194304xunknown> = torch.2_1_0.aten::set_(%246:<unknown>, storage{size=4194304}) 
58387:None:None %246:<1x1x2048x2048xunknown> = torch.2_1_0.aten::resize_(%246:<4194304xunknown>, list{1:i32, 1:i32, 2048:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %246:<1x1x2048x2048xunknown> = torch.2_1_0.aten::stack(list{%225:<1x2048x2048xunknown>}, 0:i32, out=%246:<1x1x2048x2048xunknown>) 
58387:None:None %247:<f32> = torch.2_1_0.aten::empty(list{}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %247:<2048xf32> = torch.2_1_0.aten::set_(%247:<f32>, storage{size=8192}) 
58387:None:None %247:<1x2048xf32> = torch.2_1_0.aten::resize_(%247:<2048xf32>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %247:<1x2048xf32> = torch.2_1_0.aten::stack(list{%72:<2048xf32>}, 0:i32, out=%247:<1x2048xf32>) 
58387:None:None %230:<i64> = torch.2_1_0.aten::empty(list{}, dtype=torch.int64:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:None:None %230:<2048xi64> = torch.2_1_0.aten::set_(%230:<i64>, storage{size=16384}) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::resize_(%230:<2048xi64>, list{1:i32, 2048:i32}, memory_format=None:NoneType) 
58387:None:None %230:<1x2048xi64> = torch.2_1_0.aten::stack(list{%224:<2048xi64>}, 0:i32, out=%230:<1x2048xi64>) 
58391:None:0 list{%181:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%181:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:None:0 list{%238:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%238:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%57:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%57:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%90:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%90:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%184:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%184:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:None:0 list{%142:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%142:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58393:None:0 list{%96:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%96:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:None:0 list{%240:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%240:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%177:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%177:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:None:0 list{%242:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%242:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%182:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%182:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:None:0 list{%129:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%129:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%181:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%181:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%158:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%158:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:None:0 list{%210:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%210:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:None:0 list{%244:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%244:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58393:None:0 list{%207:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%207:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:None:0 list{%136:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%136:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%198:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%198:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%178:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%178:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%234:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%234:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:None:0 list{%246:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%246:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%197:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%197:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:None:0 list{%164:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%164:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%214:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%214:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58390:None:0 list{%199:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%199:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:None:0 list{%235:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%235:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58394:7:0 %174:<1x2048xi64> = torch.2_1_0.aten::detach(%90:<1x2048xi64>) 
58394:7:0 %165:<1x2048xi64> = torch.2_1_0.aten::detach(%164:<1x2048xi64>) 
58394:7:0 %165:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%178:<1x1x2048x2048xunknown>) 
58390:3:0 %200:<1x2048xi64> = torch.2_1_0.aten::detach(%184:<1x2048xi64>) 
58390:3:0 %201:<1x2048xi64> = torch.2_1_0.aten::detach(%199:<1x2048xi64>) 
58390:3:0 %202:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%197:<1x1x2048x2048xunknown>) 
58389:2:0 %212:<1x2048xi64> = torch.2_1_0.aten::detach(%57:<1x2048xi64>) 
58389:2:0 %212:<1x2048xi64> = torch.2_1_0.aten::detach(%235:<1x2048xi64>) 
58389:2:0 %236:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%214:<1x1x2048x2048xunknown>) 
58394:7:0 %179:<1x2048xunknown> = torch.2_1_0.aten::lt(%90:<1x2048xi64>, 44800:i32) 
58394:7:0 %174:<1x2048xunknown> = torch.2_1_0.aten::ge(%90:<1x2048xi64>, 51200:i32) 
58390:3:0 %183:<1x2048xunknown> = torch.2_1_0.aten::lt(%184:<1x2048xi64>, 19200:i32) 
58390:3:0 %202:<1x2048xunknown> = torch.2_1_0.aten::ge(%184:<1x2048xi64>, 25600:i32) 
58389:2:0 %236:<1x2048xunknown> = torch.2_1_0.aten::lt(%57:<1x2048xi64>, 12800:i32) 
58392:None:0 list{%143:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%143:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58389:2:0 %221:<1x2048xunknown> = torch.2_1_0.aten::ge(%57:<1x2048xi64>, 19200:i32) 
58392:None:0 list{%137:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%137:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:None:0 list{%211:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%211:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:0:0 %243:<1x2048xi64> = torch.2_1_0.aten::detach(%238:<1x2048xi64>) 
58392:5:0 %138:<1x2048xi64> = torch.2_1_0.aten::detach(%142:<1x2048xi64>) 
58392:5:0 %138:<1x2048xi64> = torch.2_1_0.aten::detach(%137:<1x2048xi64>) 
58387:0:0 %241:<1x2048xi64> = torch.2_1_0.aten::detach(%246:<1x2048xi64>) 
58392:5:0 %144:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%143:<1x1x2048x2048xunknown>) 
58387:0:0 %243:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%244:<1x1x2048x2048xunknown>) 
58391:None:0 list{%212:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%212:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:None:0 list{%213:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%213:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:5:0 %144:<1x2048xunknown> = torch.2_1_0.aten::lt(%142:<1x2048xi64>, 32000:i32) 
58387:0:0 %247:<1x2048xunknown> = torch.2_1_0.aten::lt(%238:<1x2048xi64>, 0:i32) 
58392:5:0 %145:<1x2048xunknown> = torch.2_1_0.aten::ge(%142:<1x2048xi64>, 38400:i32) 
58387:0:0 %237:<1x2048xunknown> = torch.2_1_0.aten::ge(%238:<1x2048xi64>, 6400:i32) 
58391:4:0 %194:<1x2048xi64> = torch.2_1_0.aten::detach(%181:<1x2048xi64>) 
58391:4:0 %214:<1x2048xi64> = torch.2_1_0.aten::detach(%213:<1x2048xi64>) 
58391:4:0 %194:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%212:<1x1x2048x2048xunknown>) 
58393:None:0 list{%222:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%222:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:4:0 %215:<1x2048xunknown> = torch.2_1_0.aten::lt(%181:<1x2048xi64>, 25600:i32) 
58393:None:0 list{%223:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%223:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58391:4:0 %202:<1x2048xunknown> = torch.2_1_0.aten::ge(%181:<1x2048xi64>, 32000:i32) 
58393:None:0 list{%224:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%224:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58393:6:0 %225:<1x2048xi64> = torch.2_1_0.aten::detach(%96:<1x2048xi64>) 
58393:6:0 %198:<1x2048xi64> = torch.2_1_0.aten::detach(%224:<1x2048xi64>) 
58393:6:0 %225:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%223:<1x1x2048x2048xunknown>) 
58393:6:0 %225:<1x2048xunknown> = torch.2_1_0.aten::lt(%96:<1x2048xi64>, 38400:i32) 
58393:6:0 %226:<1x2048xunknown> = torch.2_1_0.aten::ge(%96:<1x2048xi64>, 44800:i32) 
58388:None:0 list{%31:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%31:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58388:None:0 list{%9:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%9:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58388:None:0 list{%35:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%35:<1x2048xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58387:0:0 %248:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%247:<1x2048xunknown>, %237:<1x2048xunknown>) 
58387:0:0 %241:<1x2048xi64> = torch.2_1_0.aten::clone(%238:<1x2048xi64>, memory_format=None:NoneType) 
58387:0:0 %245:<1x2048xi64> = torch.2_1_0.aten::sub(%241:<1x2048xi64>, 0:i32, alpha=1:i32) 
58387:None:0 %241:<i64> = torch.2_1_0.aten::lift_fresh(%241:<i64>) 
58388:None:0 list{%225:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%225:<1x1x2048x2048xunknown>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58388:None:0 list{%226:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::broadcast_(list{%226:<1x2048xi64>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, 0:i32, 0:i32, -1:i32) 
58392:5:0 %132:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%144:<1x2048xunknown>, %145:<1x2048xunknown>) 
58389:2:0 %126:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%236:<1x2048xunknown>, %221:<1x2048xunknown>) 
58390:3:0 %203:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%183:<1x2048xunknown>, %202:<1x2048xunknown>) 
58394:7:0 %1:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%179:<1x2048xunknown>, %174:<1x2048xunknown>) 
58391:4:0 %82:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%215:<1x2048xunknown>, %202:<1x2048xunknown>) 
58393:6:0 %227:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%225:<1x2048xunknown>, %226:<1x2048xunknown>) 
58392:5:0 %146:<1x2048xi64> = torch.2_1_0.aten::clone(%142:<1x2048xi64>, memory_format=None:NoneType) 
58389:2:0 %183:<1x2048xi64> = torch.2_1_0.aten::clone(%57:<1x2048xi64>, memory_format=None:NoneType) 
58393:6:0 %225:<1x2048xi64> = torch.2_1_0.aten::clone(%96:<1x2048xi64>, memory_format=None:NoneType) 
58394:7:0 %176:<1x2048xi64> = torch.2_1_0.aten::clone(%90:<1x2048xi64>, memory_format=None:NoneType) 
58390:3:0 %204:<1x2048xi64> = torch.2_1_0.aten::clone(%184:<1x2048xi64>, memory_format=None:NoneType) 
58391:4:0 %214:<1x2048xi64> = torch.2_1_0.aten::clone(%181:<1x2048xi64>, memory_format=None:NoneType) 
58388:1:0 %200:<1x2048xi64> = torch.2_1_0.aten::detach(%31:<1x2048xi64>) 
58388:1:0 %200:<1x2048xi64> = torch.2_1_0.aten::detach(%226:<1x2048xi64>) 
58388:1:0 %207:<1x1x2048x2048xunknown> = torch.2_1_0.aten::detach(%225:<1x1x2048x2048xunknown>) 
58392:5:0 %147:<1x2048xi64> = torch.2_1_0.aten::sub(%146:<1x2048xi64>, 32000:i32, alpha=1:i32) 
58393:6:0 %226:<1x2048xi64> = torch.2_1_0.aten::sub(%225:<1x2048xi64>, 38400:i32, alpha=1:i32) 
58389:2:0 %230:<1x2048xi64> = torch.2_1_0.aten::sub(%183:<1x2048xi64>, 12800:i32, alpha=1:i32) 
58392:None:0 %146:<i64> = torch.2_1_0.aten::lift_fresh(%146:<i64>) 
58391:4:0 %216:<1x2048xi64> = torch.2_1_0.aten::sub(%214:<1x2048xi64>, 25600:i32, alpha=1:i32) 
58390:3:0 %183:<1x2048xi64> = torch.2_1_0.aten::sub(%204:<1x2048xi64>, 19200:i32, alpha=1:i32) 
58393:None:0 %105:<i64> = torch.2_1_0.aten::lift_fresh(%105:<i64>) 
58389:None:0 %183:<i64> = torch.2_1_0.aten::lift_fresh(%183:<i64>) 
58390:None:0 %205:<i64> = torch.2_1_0.aten::lift_fresh(%205:<i64>) 
58394:7:0 %180:<1x2048xi64> = torch.2_1_0.aten::sub(%176:<1x2048xi64>, 44800:i32, alpha=1:i32) 
58391:None:0 %214:<i64> = torch.2_1_0.aten::lift_fresh(%214:<i64>) 
58394:None:0 %65:<i64> = torch.2_1_0.aten::lift_fresh(%65:<i64>) 
58388:1:0 %207:<1x2048xunknown> = torch.2_1_0.aten::lt(%31:<1x2048xi64>, 6400:i32) 
58388:1:0 %227:<1x2048xunknown> = torch.2_1_0.aten::ge(%31:<1x2048xi64>, 12800:i32) 
58388:1:0 %228:<1x2048xunknown> = torch.2_1_0.aten::bitwise_or(%207:<1x2048xunknown>, %227:<1x2048xunknown>) 
58388:1:0 %220:<1x2048xi64> = torch.2_1_0.aten::clone(%31:<1x2048xi64>, memory_format=None:NoneType) 
58388:1:0 %207:<1x2048xi64> = torch.2_1_0.aten::sub(%220:<1x2048xi64>, 6400:i32, alpha=1:i32) 
58388:None:0 %25:<i64> = torch.2_1_0.aten::lift_fresh(%25:<i64>) 
58390:3:0 %183:<1x2048xi64> = torch.2_1_0.aten::index_put_(%183:<1x2048xi64>, list{%203:<1x2048xunknown>}, %205:<i64>, False:pred) 
58390:3:0 %206:<1x2048x12288xf16> = torch.2_1_0.aten::index(%116:<6400x12288xf16>, list{%183:<1x2048xi64>}) 
58390:None:0 %207:<f16> = torch.2_1_0.aten::lift_fresh(%207:<f16>) 
58390:3:0 %208:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%206:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58390:3:0 %208:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%208:<1x2048x12288xf16>, list{%203:<1x2048xunknown>}, %207:<f16>, False:pred) 
58390:None:0 list{%206:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%206:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %209:<1x2048x12288xf16> = torch.2_1_0.aten::view(%206:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58390:3:0 %210:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%118:<2048x12288xf16>, %199:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58390:3:0 %211:<1x2048x12288xf16> = torch.2_1_0.aten::add(%209:<1x2048x12288xf16>, %210:<1x2048x12288xf16>, alpha=1:i32) 
58390:3:0 %212:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%211:<1x2048x12288xf16>, 0:i32, 1:i32) 
58390:3:0 %213, %214:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%212:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %215:<2048x12288xf16> = torch.2_1_0.aten::view(%213:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58390:3:0 %216:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %217:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %218:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %245:<1x2048xi64> = torch.2_1_0.aten::index_put_(%245:<1x2048xi64>, list{%248:<1x2048xunknown>}, %241:<i64>, False:pred) 
58387:0:0 %239:<1x2048x12288xf16> = torch.2_1_0.aten::index(%116:<6400x12288xf16>, list{%245:<1x2048xi64>}) 
58387:None:0 %94:<f16> = torch.2_1_0.aten::lift_fresh(%94:<f16>) 
58387:0:0 %249:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%239:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58387:0:0 %249:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%249:<1x2048x12288xf16>, list{%248:<1x2048xunknown>}, %94:<f16>, False:pred) 
58387:None:0 list{%239:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%239:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %250:<1x2048x12288xf16> = torch.2_1_0.aten::view(%239:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58387:0:0 %94:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%117:<2048x12288xf16>, %246:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58387:0:0 %251:<1x2048x12288xf16> = torch.2_1_0.aten::add(%250:<1x2048x12288xf16>, %94:<1x2048x12288xf16>, alpha=1:i32) 
58387:0:0 %252:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%251:<1x2048x12288xf16>, 0:i32, 1:i32) 
58387:0:0 %253, %254:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%252:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58387:0:0 %250:<2048x12288xf16> = torch.2_1_0.aten::view(%253:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58387:0:0 %255:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %256:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %257:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %230:<1x2048xi64> = torch.2_1_0.aten::index_put_(%230:<1x2048xi64>, list{%126:<1x2048xunknown>}, %183:<i64>, False:pred) 
58389:2:0 %212:<1x2048x12288xf16> = torch.2_1_0.aten::index(%98:<6400x12288xf16>, list{%230:<1x2048xi64>}) 
58389:None:0 %237:<f16> = torch.2_1_0.aten::lift_fresh(%237:<f16>) 
58389:2:0 %236:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%212:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58389:2:0 %236:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%236:<1x2048x12288xf16>, list{%126:<1x2048xunknown>}, %237:<f16>, False:pred) 
58389:None:0 list{%212:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%212:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %238:<1x2048x12288xf16> = torch.2_1_0.aten::view(%212:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58391:4:0 %216:<1x2048xi64> = torch.2_1_0.aten::index_put_(%216:<1x2048xi64>, list{%82:<1x2048xunknown>}, %214:<i64>, False:pred) 
58389:2:0 %237:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%34:<2048x12288xf16>, %235:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58389:2:0 %239:<1x2048x12288xf16> = torch.2_1_0.aten::add(%238:<1x2048x12288xf16>, %237:<1x2048x12288xf16>, alpha=1:i32) 
58391:4:0 %214:<1x2048x12288xf16> = torch.2_1_0.aten::index(%106:<6400x12288xf16>, list{%216:<1x2048xi64>}) 
58391:None:0 %202:<f16> = torch.2_1_0.aten::lift_fresh(%202:<f16>) 
58389:2:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%239:<1x2048x12288xf16>, 0:i32, 1:i32) 
58391:4:0 %217:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%214:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58389:2:0 %241, %80:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%240:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %217:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%217:<1x2048x12288xf16>, list{%82:<1x2048xunknown>}, %202:<f16>, False:pred) 
58389:2:0 %242:<2048x12288xf16> = torch.2_1_0.aten::view(%241:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %239:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %243:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %244:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:None:0 list{%214:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%214:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %218:<1x2048x12288xf16> = torch.2_1_0.aten::view(%214:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58391:4:0 %32:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%127:<2048x12288xf16>, %213:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58391:4:0 %202:<1x2048x12288xf16> = torch.2_1_0.aten::add(%218:<1x2048x12288xf16>, %32:<1x2048x12288xf16>, alpha=1:i32) 
58391:4:0 %219:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%202:<1x2048x12288xf16>, 0:i32, 1:i32) 
58391:4:0 %220, %221:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%219:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %202:<2048x12288xf16> = torch.2_1_0.aten::view(%220:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58391:4:0 %222:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %223:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %224:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %226:<1x2048xi64> = torch.2_1_0.aten::index_put_(%226:<1x2048xi64>, list{%227:<1x2048xunknown>}, %105:<i64>, False:pred) 
58394:7:0 %180:<1x2048xi64> = torch.2_1_0.aten::index_put_(%180:<1x2048xi64>, list{%1:<1x2048xunknown>}, %65:<i64>, False:pred) 
58393:6:0 %216:<1x2048x12288xf16> = torch.2_1_0.aten::index(%116:<6400x12288xf16>, list{%226:<1x2048xi64>}) 
58393:None:0 %228:<f16> = torch.2_1_0.aten::lift_fresh(%228:<f16>) 
58393:6:0 %215:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%216:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58393:6:0 %215:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%215:<1x2048x12288xf16>, list{%227:<1x2048xunknown>}, %228:<f16>, False:pred) 
58394:7:0 %176:<1x2048x12288xf16> = torch.2_1_0.aten::index(%12:<6400x12288xf16>, list{%180:<1x2048xi64>}) 
58394:None:0 %179:<f16> = torch.2_1_0.aten::lift_fresh(%179:<f16>) 
58394:7:0 %181:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%176:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58394:7:0 %181:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%181:<1x2048x12288xf16>, list{%1:<1x2048xunknown>}, %179:<f16>, False:pred) 
58393:None:0 list{%216:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%216:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:None:0 list{%176:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%176:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %229:<1x2048x12288xf16> = torch.2_1_0.aten::view(%216:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58394:7:0 %182:<1x2048x12288xf16> = torch.2_1_0.aten::view(%176:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58393:6:0 %230:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%118:<2048x12288xf16>, %224:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58394:7:0 %162:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%8:<2048x12288xf16>, %164:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58393:6:0 %231:<1x2048x12288xf16> = torch.2_1_0.aten::add(%229:<1x2048x12288xf16>, %230:<1x2048x12288xf16>, alpha=1:i32) 
58394:7:0 %174:<1x2048x12288xf16> = torch.2_1_0.aten::add(%182:<1x2048x12288xf16>, %162:<1x2048x12288xf16>, alpha=1:i32) 
58393:6:0 %232:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%231:<1x2048x12288xf16>, 0:i32, 1:i32) 
58394:7:0 %183:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%174:<1x2048x12288xf16>, 0:i32, 1:i32) 
58394:7:0 %179, %184:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%183:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %228, %233:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%232:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %234:<2048x12288xf16> = torch.2_1_0.aten::view(%228:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58394:7:0 %67:<2048x12288xf16> = torch.2_1_0.aten::view(%179:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58394:7:0 %185:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %235:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %186:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %236:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %187:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %237:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %147:<1x2048xi64> = torch.2_1_0.aten::index_put_(%147:<1x2048xi64>, list{%132:<1x2048xunknown>}, %146:<i64>, False:pred) 
58392:5:0 %148:<1x2048x12288xf16> = torch.2_1_0.aten::index(%24:<6400x12288xf16>, list{%147:<1x2048xi64>}) 
58392:None:0 %149:<f16> = torch.2_1_0.aten::lift_fresh(%149:<f16>) 
58392:5:0 %150:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%148:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58392:5:0 %150:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%150:<1x2048x12288xf16>, list{%132:<1x2048xunknown>}, %149:<f16>, False:pred) 
58392:None:0 list{%148:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%148:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %151:<1x2048x12288xf16> = torch.2_1_0.aten::view(%148:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58392:5:0 %152:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%36:<2048x12288xf16>, %137:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58392:5:0 %153:<1x2048x12288xf16> = torch.2_1_0.aten::add(%151:<1x2048x12288xf16>, %152:<1x2048x12288xf16>, alpha=1:i32) 
58392:5:0 %154:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%153:<1x2048x12288xf16>, 0:i32, 1:i32) 
58392:5:0 %155, %156:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%154:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58392:5:0 %154:<2048x12288xf16> = torch.2_1_0.aten::view(%155:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %157:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %158:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %159:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %207:<1x2048xi64> = torch.2_1_0.aten::index_put_(%207:<1x2048xi64>, list{%228:<1x2048xunknown>}, %25:<i64>, False:pred) 
58388:1:0 %200:<1x2048x12288xf16> = torch.2_1_0.aten::index(%113:<6400x12288xf16>, list{%207:<1x2048xi64>}) 
58388:None:0 %55:<f16> = torch.2_1_0.aten::lift_fresh(%55:<f16>) 
58388:1:0 %202:<1x2048x12288xf16> = torch.2_1_0.aten::slice(%200:<1x2048x12288xf16>, 1:i32, 0:i32, 9223372036854775807:i32, 1:i32) 
58388:1:0 %202:<1x2048x12288xf16> = torch.2_1_0.aten::index_put_(%202:<1x2048x12288xf16>, list{%228:<1x2048xunknown>}, %55:<f16>, False:pred) 
58388:None:0 list{%200:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%200:<1x2048x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %258:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58389:2:0 %245:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58387:0:0 %259:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %220:<1x2048x12288xf16> = torch.2_1_0.aten::view(%200:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}) 
58389:2:0 %246:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %188:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58391:4:0 %225:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58393:6:0 %238:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58387:0:0 %260:<2048x1x12288xf16> = torch.2_1_0.aten::view(%255:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %189:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %247:<2048x1x12288xf16> = torch.2_1_0.aten::view(%239:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %226:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %239:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %190:<2048x1x12288xf16> = torch.2_1_0.aten::view(%185:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %250:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %212:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %225:<2048x1x12288xf16> = torch.2_1_0.aten::view(%222:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %232:<2048x1x12288xf16> = torch.2_1_0.aten::view(%235:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %258:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%260:<2048x1x12288xf16>) 
58390:3:0 %219:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58394:7:0 %183:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %248:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%247:<2048x1x12288xf16>) 
58394:7:0 %191:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%190:<2048x1x12288xf16>) 
58390:3:0 %220:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %227:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %216:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %218:<1x2048x12288xf16> = torch.2_1_0.aten::embedding(%114:<2048x12288xf16>, %226:<1x2048xi64>, -1:i32, False:pred, False:pred) 
58391:4:0 %228:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%225:<2048x1x12288xf16>) 
58390:3:0 %221:<2048x1x12288xf16> = torch.2_1_0.aten::view(%216:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %238:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%232:<2048x1x12288xf16>) 
58387:0:0 %261:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%77:<4608x12288xf16>) 
58394:7:0 %191:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%88:<4608x12288xf16>) 
58389:2:0 %233:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%12:<4608x12288xf16>) 
58390:3:0 %212:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %262:<2048x12288xf16> = torch.2_1_0.aten::view(%250:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %229:<1x2048x12288xf16> = torch.2_1_0.aten::add(%220:<1x2048x12288xf16>, %218:<1x2048x12288xf16>, alpha=1:i32) 
58389:2:0 %199:<2048x12288xf16> = torch.2_1_0.aten::view(%212:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%183:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %228:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%131:<4608x12288xf16>) 
58393:6:0 %240:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%80:<4608x12288xf16>) 
58390:3:0 %219:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%221:<2048x1x12288xf16>) 
58391:4:0 %202:<2048x12288xf16> = torch.2_1_0.aten::view(%227:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %235:<2048x12288xf16> = torch.2_1_0.aten::view(%216:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %221:<2048x1x12288xf16> = torch.2_1_0.aten::transpose(%229:<1x2048x12288xf16>, 0:i32, 1:i32) 
58388:1:0 %108, %230:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%221:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %216:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%121:<4608x12288xf16>) 
58392:5:0 %160:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58392:5:0 %161:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %220:<2048x12288xf16> = torch.2_1_0.aten::view(%212:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %162:<2048x1x12288xf16> = torch.2_1_0.aten::view(%157:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %154:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %163:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%162:<2048x1x12288xf16>) 
58388:1:0 %229:<2048x12288xf16> = torch.2_1_0.aten::view(%108:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %162:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%37:<4608x12288xf16>) 
58388:1:0 %231:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %232:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %163:<2048x12288xf16> = torch.2_1_0.aten::view(%154:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %233:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %234:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58388:1:0 %235:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %236:<2048x1x12288xf16> = torch.2_1_0.aten::view(%231:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %229:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %237:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%236:<2048x1x12288xf16>) 
58388:1:0 %236:<12288x4608xf16>{1,12288} = torch.2_1_0.aten::t(%119:<4608x12288xf16>) 
58388:1:0 %200:<2048x12288xf16> = torch.2_1_0.aten::view(%229:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %237:<2048x4608xf16> = torch.2_1_0.aten::mm(%200:<2048x12288xf16>, %236:<12288x4608xf16>{1,12288}) 
58388:1:0 %221:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%237:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58388:1:0 %237:<2048x1x4608xf16> = torch.2_1_0.aten::add(%221:<2048x1x4608xf16>, %12:<4608xf16>, alpha=1:i32) 
58388:1:0 %238:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%237:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58388:1:0 %239, %200, %240:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%238:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58388:1:0 %241:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%239:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %242:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%241:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58388:1:0 %243:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%200:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58388:1:0 %244:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %245:<50331648xf16> = torch.2_1_0.aten::slice(%244:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58388:1:0 %246:<12x2048x2048xf16> = torch.2_1_0.aten::view(%245:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58388:1:0 %247:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%242:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58388:1:0 %248:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%243:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58388:1:0 %249:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%248:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58388:1:0 %250:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%246:<12x2048x2048xf16>, %247:<12x2048x128xf16>{384,4608,1}, %249:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58388:1:0 %251:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%250:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58388:1:0 %251:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%251:<1x12x2048x2048xf16>, %225:<1x1x2048x2048xunknown>, -10000.0:f32) 
58393:6:0 %241:<2048x4608xf16> = torch.2_1_0.aten::mm(%235:<2048x12288xf16>, %240:<12288x4608xf16>{1,12288}) 
58393:6:0 %242:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%241:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58393:6:0 %241:<2048x1x4608xf16> = torch.2_1_0.aten::add(%242:<2048x1x4608xf16>, %73:<4608xf16>, alpha=1:i32) 
58393:6:0 %243:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%241:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58393:6:0 %244, %235, %245:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%243:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58393:6:0 %246:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%244:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %247:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%246:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58393:6:0 %248:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%235:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58393:6:0 %249:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %250:<50331648xf16> = torch.2_1_0.aten::slice(%249:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58393:6:0 %251:<12x2048x2048xf16> = torch.2_1_0.aten::view(%250:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58393:6:0 %250:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%247:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58393:6:0 %252:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%248:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58393:6:0 %253:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%252:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58393:6:0 %254:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%251:<12x2048x2048xf16>, %250:<12x2048x128xf16>{384,4608,1}, %253:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58393:6:0 %255:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%254:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58393:6:0 %255:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%255:<1x12x2048x2048xf16>, %223:<1x1x2048x2048xunknown>, -10000.0:f32) 
58387:0:0 %263:<2048x4608xf16> = torch.2_1_0.aten::mm(%262:<2048x12288xf16>, %261:<12288x4608xf16>{1,12288}) 
58387:0:0 %177:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%263:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58387:0:0 %263:<2048x1x4608xf16> = torch.2_1_0.aten::add(%177:<2048x1x4608xf16>, %96:<4608xf16>, alpha=1:i32) 
58387:0:0 %255:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%263:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58387:0:0 %262, %260, %258:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%255:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58387:0:0 %264:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%262:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58387:0:0 %265:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%264:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58387:0:0 %266:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%260:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58387:0:0 %251:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %267:<50331648xf16> = torch.2_1_0.aten::slice(%251:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58387:0:0 %268:<12x2048x2048xf16> = torch.2_1_0.aten::view(%267:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58387:0:0 %269:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%265:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58387:0:0 %267:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%266:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58387:0:0 %270:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%267:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58387:0:0 %271:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%268:<12x2048x2048xf16>, %269:<12x2048x128xf16>{384,4608,1}, %270:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58387:0:0 %272:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%271:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58387:0:0 %272:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%272:<1x12x2048x2048xf16>, %244:<1x1x2048x2048xunknown>, -10000.0:f32) 
58389:2:0 %247:<2048x4608xf16> = torch.2_1_0.aten::mm(%199:<2048x12288xf16>, %233:<12288x4608xf16>{1,12288}) 
58389:2:0 %239:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%247:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58389:2:0 %247:<2048x1x4608xf16> = torch.2_1_0.aten::add(%239:<2048x1x4608xf16>, %18:<4608xf16>, alpha=1:i32) 
58389:2:0 %225:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%247:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58389:2:0 %239, %162, %199:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%225:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58389:2:0 %249:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%239:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58389:2:0 %225:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%249:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58389:2:0 %250:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%162:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58389:2:0 %251:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %252:<50331648xf16> = torch.2_1_0.aten::slice(%251:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58389:2:0 %253:<12x2048x2048xf16> = torch.2_1_0.aten::view(%252:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58389:2:0 %254:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%225:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58389:2:0 %255:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%250:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58389:2:0 %256:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%255:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58391:4:0 %225:<2048x4608xf16> = torch.2_1_0.aten::mm(%202:<2048x12288xf16>, %228:<12288x4608xf16>{1,12288}) 
58391:4:0 %226:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%225:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58391:4:0 %202:<2048x1x4608xf16> = torch.2_1_0.aten::add(%226:<2048x1x4608xf16>, %115:<4608xf16>, alpha=1:i32) 
58391:4:0 %229:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%202:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58389:2:0 %257:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%253:<12x2048x2048xf16>, %254:<12x2048x128xf16>{384,4608,1}, %256:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58391:4:0 %55, %228, %219:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%229:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58389:2:0 %258:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%257:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58391:4:0 %230:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%55:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58389:2:0 %258:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%258:<1x12x2048x2048xf16>, %214:<1x1x2048x2048xunknown>, -10000.0:f32) 
58391:4:0 %229:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%230:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58391:4:0 %231:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%228:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58391:4:0 %232:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %233:<50331648xf16> = torch.2_1_0.aten::slice(%232:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58391:4:0 %234:<12x2048x2048xf16> = torch.2_1_0.aten::view(%233:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58391:4:0 %225:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%229:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58394:7:0 %192:<2048x4608xf16> = torch.2_1_0.aten::mm(%190:<2048x12288xf16>, %191:<12288x4608xf16>{1,12288}) 
58391:4:0 %233:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%231:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58391:4:0 %235:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%233:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58394:7:0 %193:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%192:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58394:7:0 %192:<2048x1x4608xf16> = torch.2_1_0.aten::add(%193:<2048x1x4608xf16>, %47:<4608xf16>, alpha=1:i32) 
58394:7:0 %194:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%192:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58394:7:0 %10, %193, %195:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%194:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58394:7:0 %196:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%10:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %190:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%196:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58394:7:0 %197:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%193:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58391:4:0 %222:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%234:<12x2048x2048xf16>, %225:<12x2048x128xf16>{384,4608,1}, %235:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58394:7:0 %189:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %198:<50331648xf16> = torch.2_1_0.aten::slice(%189:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58391:4:0 %233:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%222:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58394:7:0 %199:<12x2048x2048xf16> = torch.2_1_0.aten::view(%198:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58394:7:0 %200:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%190:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58394:7:0 %201:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%197:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58394:7:0 %198:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%201:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58391:4:0 %233:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%233:<1x12x2048x2048xf16>, %212:<1x1x2048x2048xunknown>, -10000.0:f32) 
58388:1:0 %252:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%251:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58388:1:0 %253:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%252:<1x12x2048x2048xf16>) 
58388:1:0 %254, %255:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%252:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58388:1:0 %256:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%240:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58388:1:0 %257:<12x2048x2048xf16> = torch.2_1_0.aten::view(%254:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58388:1:0 %258:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%256:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58388:1:0 %259:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%257:<12x2048x2048xf16>, %258:<12x2048x128xf16>{384,4608,1}+256) 
58388:1:0 %260:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%259:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58390:3:0 %219:<2048x4608xf16> = torch.2_1_0.aten::mm(%220:<2048x12288xf16>, %216:<12288x4608xf16>{1,12288}) 
58393:6:0 %256:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%255:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58388:1:0 %261:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%260:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58393:6:0 %257:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%256:<1x12x2048x2048xf16>) 
58394:7:0 %202:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%199:<12x2048x2048xf16>, %200:<12x2048x128xf16>{384,4608,1}, %198:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58394:7:0 %203:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%202:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58388:1:0 %262:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%261:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58388:1:0 %263:<2048x1x1536xf16> = torch.2_1_0.aten::view(%262:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58388:1:0 %243:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%23:<12288x1536xf16>) 
58393:6:0 %258, %259:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%256:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58394:7:0 %203:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%203:<1x12x2048x2048xf16>, %178:<1x1x2048x2048xunknown>, -10000.0:f32) 
58390:3:0 %215:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%219:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58388:1:0 %246:<2048x1536xf16> = torch.2_1_0.aten::view(%263:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58393:6:0 %260:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%245:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58393:6:0 %261:<12x2048x2048xf16> = torch.2_1_0.aten::view(%258:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58388:1:0 %242:<2048x12288xf16> = torch.2_1_0.aten::mm(%246:<2048x1536xf16>, %243:<1536x12288xf16>{1,1536}) 
58393:6:0 %262:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%260:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58388:1:0 %256:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%242:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %219:<2048x1x4608xf16> = torch.2_1_0.aten::add(%215:<2048x1x4608xf16>, %74:<4608xf16>, alpha=1:i32) 
58388:None:0 list{%256:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%256:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %263:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%261:<12x2048x2048xf16>, %262:<12x2048x128xf16>{384,4608,1}+256) 
58388:1:0 %246:<2048x1x12288xf16> = torch.2_1_0.aten::view(%256:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %264:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%263:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58393:6:0 %265:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%264:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58388:1:0 %241:<2048x1x12288xf16> = torch.2_1_0.aten::add(%246:<2048x1x12288xf16>, %115:<12288xf16>, alpha=1:i32) 
58390:3:0 %222:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%219:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58393:6:0 %266:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%265:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58393:6:0 %267:<2048x1x1536xf16> = torch.2_1_0.aten::view(%266:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58388:1:0 %264, %200:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%241:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %251:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%120:<12288x1536xf16>) 
58393:6:0 %255:<2048x1536xf16> = torch.2_1_0.aten::view(%267:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58393:6:0 %247:<2048x12288xf16> = torch.2_1_0.aten::mm(%255:<2048x1536xf16>, %251:<1536x12288xf16>{1,1536}) 
58393:6:0 %260:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%247:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:None:0 list{%260:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%260:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %215, %223, %220:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%222:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58393:6:0 %255:<2048x1x12288xf16> = torch.2_1_0.aten::view(%260:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %243:<2048x1x12288xf16> = torch.2_1_0.aten::add(%108:<2048x1x12288xf16>, %264:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %235:<2048x1x12288xf16> = torch.2_1_0.aten::add(%255:<2048x1x12288xf16>, %121:<12288xf16>, alpha=1:i32) 
58390:3:0 %224:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%215:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %241:<2048x12288xf16> = torch.2_1_0.aten::view(%243:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %265:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
for debug!! in forward of attention 58388:1:0 %266:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
self.training: 58388:1:0 %267:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %245, %246:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%235:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58388:1:0 %268:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58388:1:0 %269:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %270:<2048x1x12288xf16> = torch.2_1_0.aten::view(%265:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %225:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%224:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58390:3:0 %226:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%223:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58393:6:0 %268:<2048x1x12288xf16> = torch.2_1_0.aten::add(%228:<2048x1x12288xf16>, %245:<2048x1x12288xf16>, alpha=1:i32) 
58388:1:0 %269:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %271:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%270:<2048x1x12288xf16>) 
58388:1:0 %246:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%120:<6144x12288xf16>) 
58393:6:0 %245:<2048x12288xf16> = torch.2_1_0.aten::view(%268:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %265:<2048x12288xf16> = torch.2_1_0.aten::view(%269:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %269:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %271:<2048x6144xf16> = torch.2_1_0.aten::mm(%265:<2048x12288xf16>, %246:<12288x6144xf16>{1,12288}) 
58393:6:0 %270:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %272:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%271:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58393:6:0 %271:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %272:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58388:1:0 %265:<2048x1x6144xf16> = torch.2_1_0.aten::add(%272:<2048x1x6144xf16>, %107:<6144xf16>, alpha=1:i32) 
58393:6:0 %273:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %227:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %274:<2048x1x12288xf16> = torch.2_1_0.aten::view(%269:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %273:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %275:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%274:<2048x1x12288xf16>) 
58393:6:0 %260:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%63:<6144x12288xf16>) 
58390:3:0 %228:<50331648xf16> = torch.2_1_0.aten::slice(%227:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58393:6:0 %274:<2048x12288xf16> = torch.2_1_0.aten::view(%273:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %229:<12x2048x2048xf16> = torch.2_1_0.aten::view(%228:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58393:6:0 %275:<2048x6144xf16> = torch.2_1_0.aten::mm(%274:<2048x12288xf16>, %260:<12288x6144xf16>{1,12288}) 
58393:6:0 %235:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%275:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58393:6:0 %260:<2048x1x6144xf16> = torch.2_1_0.aten::add(%235:<2048x1x6144xf16>, %98:<6144xf16>, alpha=1:i32) 
58390:3:0 %230:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%225:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58390:3:0 %231:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%226:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58390:3:0 %232:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%231:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58392:5:0 %157:<2048x4608xf16> = torch.2_1_0.aten::mm(%163:<2048x12288xf16>, %162:<12288x4608xf16>{1,12288}) 
58392:5:0 %164:<2048x1x4608xf16> = torch.2_1_0.aten::_unsafe_view(%157:<2048x4608xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58392:5:0 %157:<2048x1x4608xf16> = torch.2_1_0.aten::add(%164:<2048x1x4608xf16>, %8:<4608xf16>, alpha=1:i32) 
58392:5:0 %165:<2048x1x12x384xf16> = torch.2_1_0.aten::view(%157:<2048x1x4608xf16>, list{2048:i32, 1:i32, 12:i32, 384:i32}) 
58392:5:0 %161, %163, %166:<2048x1x12x128xf16>{4608,4608,384,1}, <2048x1x12x128xf16>{4608,4608,384,1}+128, <2048x1x12x128xf16>{4608,4608,384,1}+256 = torch.2_1_0.aten::split_with_sizes(%165:<2048x1x12x384xf16>, list{128:i32, 128:i32, 128:i32}, 3:i32) 
58392:5:0 %167:<2048x1x12x128xf16>{4608,4608,384,1} = torch.2_1_0.aten::view(%161:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58392:5:0 %168:<2048x12x128xf16>{4608,384,1} = torch.2_1_0.aten::view(%167:<2048x1x12x128xf16>{4608,4608,384,1}, list{2048:i32, 12:i32, 128:i32}) 
58392:5:0 %169:<2048x12x128xf16>{4608,384,1}+128 = torch.2_1_0.aten::view(%163:<2048x1x12x128xf16>{4608,4608,384,1}+128, list{2048:i32, 12:i32, -1:i32}) 
58392:5:0 %170:<50331648xf16> = torch.2_1_0.aten::empty(list{50331648:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %171:<50331648xf16> = torch.2_1_0.aten::slice(%170:<50331648xf16>, 0:i32, 0:i32, 50331648:i32, 1:i32) 
58392:5:0 %172:<12x2048x2048xf16> = torch.2_1_0.aten::view(%171:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}) 
58392:5:0 %173:<12x2048x128xf16>{384,4608,1} = torch.2_1_0.aten::transpose(%168:<2048x12x128xf16>{4608,384,1}, 0:i32, 1:i32) 
58392:5:0 %174:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%169:<2048x12x128xf16>{4608,384,1}+128, 0:i32, 1:i32) 
58392:5:0 %175:<12x128x2048xf16>{384,1,4608}+128 = torch.2_1_0.aten::transpose(%174:<12x2048x128xf16>{384,4608,1}+128, 1:i32, 2:i32) 
58392:5:0 %176:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%172:<12x2048x2048xf16>, %173:<12x2048x128xf16>{384,4608,1}, %175:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58390:3:0 %233:<12x2048x2048xf16> = torch.2_1_0.aten::baddbmm(%229:<12x2048x2048xf16>, %230:<12x2048x128xf16>{384,4608,1}, %232:<12x128x2048xf16>{384,1,4608}+128, beta=0.0:f32, alpha=0.08838834764831843:f32) 
58392:5:0 %177:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%176:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58390:3:0 %234:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%233:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58392:5:0 %177:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%177:<1x12x2048x2048xf16>, %143:<1x1x2048x2048xunknown>, -10000.0:f32) 
58387:0:0 %273:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%272:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58390:3:0 %234:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill_(%234:<1x12x2048x2048xf16>, %197:<1x1x2048x2048xunknown>, -10000.0:f32) 
58387:0:0 %274:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%273:<1x12x2048x2048xf16>) 
58387:0:0 %275, %276:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%273:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58387:0:0 %277:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%258:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58387:0:0 %278:<12x2048x2048xf16> = torch.2_1_0.aten::view(%275:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58387:0:0 %279:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%277:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58387:0:0 %280:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%278:<12x2048x2048xf16>, %279:<12x2048x128xf16>{384,4608,1}+256) 
58387:0:0 %281:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%280:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58387:0:0 %282:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%281:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58387:0:0 %283:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%282:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58387:0:0 %284:<2048x1x1536xf16> = torch.2_1_0.aten::view(%283:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58387:0:0 %271:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%1:<12288x1536xf16>) 
58387:0:0 %272:<2048x1536xf16> = torch.2_1_0.aten::view(%284:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58387:0:0 %280:<2048x12288xf16> = torch.2_1_0.aten::mm(%272:<2048x1536xf16>, %271:<1536x12288xf16>{1,1536}) 
58387:0:0 %262:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%280:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:None:0 list{%262:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%262:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::view(%262:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %285:<2048x1x12288xf16> = torch.2_1_0.aten::add(%272:<2048x1x12288xf16>, %119:<12288xf16>, alpha=1:i32) 
58387:0:0 %44, %258:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%285:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58389:2:0 %259:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%258:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58389:2:0 %260:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%259:<1x12x2048x2048xf16>) 
58387:0:0 %286:<2048x1x12288xf16> = torch.2_1_0.aten::add(%253:<2048x1x12288xf16>, %44:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %287:<2048x12288xf16> = torch.2_1_0.aten::view(%286:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58387:0:0 %264:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %285:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %281:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %277:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58387:0:0 %268:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %277:<2048x1x12288xf16> = torch.2_1_0.aten::view(%264:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %272:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %280:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%277:<2048x1x12288xf16>) 
58387:0:0 %277:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%52:<6144x12288xf16>) 
58389:2:0 %261, %262:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%259:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58387:0:0 %268:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %263:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%199:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58387:0:0 %264:<2048x6144xf16> = torch.2_1_0.aten::mm(%268:<2048x12288xf16>, %277:<12288x6144xf16>{1,12288}) 
58389:2:0 %264:<12x2048x2048xf16> = torch.2_1_0.aten::view(%261:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58387:0:0 %44:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%264:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58389:2:0 %265:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%263:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58387:0:0 %277:<2048x1x6144xf16> = torch.2_1_0.aten::add(%44:<2048x1x6144xf16>, %82:<6144xf16>, alpha=1:i32) 
58389:2:0 %266:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%264:<12x2048x2048xf16>, %265:<12x2048x128xf16>{384,4608,1}+256) 
58389:2:0 %267:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%266:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58389:2:0 %268:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%267:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58389:2:0 %269:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%268:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58389:2:0 %270:<2048x1x1536xf16> = torch.2_1_0.aten::view(%269:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58389:2:0 %257:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%13:<12288x1536xf16>) 
58389:2:0 %266:<2048x1536xf16> = torch.2_1_0.aten::view(%270:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58389:2:0 %250:<2048x12288xf16> = torch.2_1_0.aten::mm(%266:<2048x1536xf16>, %257:<1536x12288xf16>{1,1536}) 
58389:2:0 %253:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%250:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:None:0 list{%253:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%253:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %257:<2048x1x12288xf16> = torch.2_1_0.aten::view(%253:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %227:<2048x1x12288xf16> = torch.2_1_0.aten::add(%257:<2048x1x12288xf16>, %38:<12288xf16>, alpha=1:i32) 
58389:2:0 %199, %271:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%227:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58389:2:0 %221:<2048x1x12288xf16> = torch.2_1_0.aten::add(%241:<2048x1x12288xf16>, %199:<2048x1x12288xf16>, alpha=1:i32) 
58389:2:0 %257:<2048x12288xf16> = torch.2_1_0.aten::view(%221:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %250:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %162:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %268:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %272:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58389:2:0 %273:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %274:<2048x1x12288xf16> = torch.2_1_0.aten::view(%250:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %257:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %275:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%274:<2048x1x12288xf16>) 
58389:2:0 %274:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%84:<6144x12288xf16>) 
58389:2:0 %276:<2048x12288xf16> = torch.2_1_0.aten::view(%257:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %250:<2048x6144xf16> = torch.2_1_0.aten::mm(%276:<2048x12288xf16>, %274:<12288x6144xf16>{1,12288}) 
58389:2:0 %277:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%250:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58391:4:0 %236:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%233:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58389:2:0 %274:<2048x1x6144xf16> = torch.2_1_0.aten::add(%277:<2048x1x6144xf16>, %103:<6144xf16>, alpha=1:i32) 
58391:4:0 %237:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%236:<1x12x2048x2048xf16>) 
58391:4:0 %238, %239:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%236:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58394:7:0 %204:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%203:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58391:4:0 %240:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%219:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58394:7:0 %205:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%204:<1x12x2048x2048xf16>) 
58391:4:0 %241:<12x2048x2048xf16> = torch.2_1_0.aten::view(%238:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58391:4:0 %242:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%240:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58391:4:0 %243:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%241:<12x2048x2048xf16>, %242:<12x2048x128xf16>{384,4608,1}+256) 
58391:4:0 %244:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%243:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58394:7:0 %206, %207:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%204:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58391:4:0 %245:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%244:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58394:7:0 %208:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%195:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58394:7:0 %209:<12x2048x2048xf16> = torch.2_1_0.aten::view(%206:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58394:7:0 %210:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%208:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58391:4:0 %246:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%245:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58391:4:0 %247:<2048x1x1536xf16> = torch.2_1_0.aten::view(%246:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58394:7:0 %211:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%209:<12x2048x2048xf16>, %210:<12x2048x128xf16>{384,4608,1}+256) 
58391:4:0 %234:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%129:<12288x1536xf16>) 
58394:7:0 %212:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%211:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58391:4:0 %233:<2048x1536xf16> = torch.2_1_0.aten::view(%247:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58394:7:0 %213:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%212:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58391:4:0 %229:<2048x12288xf16> = torch.2_1_0.aten::mm(%233:<2048x1536xf16>, %234:<1536x12288xf16>{1,1536}) 
58391:4:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%229:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %214:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%213:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58394:7:0 %215:<2048x1x1536xf16> = torch.2_1_0.aten::view(%214:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58394:7:0 %199:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%50:<12288x1536xf16>) 
58394:7:0 %203:<2048x1536xf16> = torch.2_1_0.aten::view(%215:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58391:None:0 list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::mm(%203:<2048x1536xf16>, %199:<1536x12288xf16>{1,1536}) 
58391:4:0 %245:<2048x1x12288xf16> = torch.2_1_0.aten::view(%240:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %208:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%190:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %219:<2048x1x12288xf16> = torch.2_1_0.aten::add(%245:<2048x1x12288xf16>, %111:<12288xf16>, alpha=1:i32) 
58394:None:0 list{%208:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%208:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %216:<2048x1x12288xf16> = torch.2_1_0.aten::view(%208:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %230, %228:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%219:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58394:7:0 %195:<2048x1x12288xf16> = torch.2_1_0.aten::add(%216:<2048x1x12288xf16>, %32:<12288xf16>, alpha=1:i32) 
58391:4:0 %233:<2048x1x12288xf16> = torch.2_1_0.aten::add(%220:<2048x1x12288xf16>, %230:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %217, %193:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%195:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %248:<2048x12288xf16> = torch.2_1_0.aten::view(%233:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58391:4:0 %230:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %203:<2048x1x12288xf16> = torch.2_1_0.aten::add(%179:<2048x1x12288xf16>, %217:<2048x1x12288xf16>, alpha=1:i32) 
58391:4:0 %244:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %234:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %216:<2048x12288xf16> = torch.2_1_0.aten::view(%203:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58391:4:0 %249:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58394:7:0 %218:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %250:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %219:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %219:<2048x1x12288xf16> = torch.2_1_0.aten::view(%230:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %220:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %222:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %221:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58391:4:0 %250:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%219:<2048x1x12288xf16>) 
58394:7:0 %222:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %190:<2048x1x12288xf16> = torch.2_1_0.aten::view(%218:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %219:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%96:<6144x12288xf16>) 
58391:4:0 %249:<2048x12288xf16> = torch.2_1_0.aten::view(%222:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %223:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %222:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%190:<2048x1x12288xf16>) 
58391:4:0 %250:<2048x6144xf16> = torch.2_1_0.aten::mm(%249:<2048x12288xf16>, %219:<12288x6144xf16>{1,12288}) 
58391:4:0 %47:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%250:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58394:7:0 %190:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%91:<6144x12288xf16>) 
58394:7:0 %222:<2048x12288xf16> = torch.2_1_0.aten::view(%223:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %219:<2048x1x6144xf16> = torch.2_1_0.aten::add(%47:<2048x1x6144xf16>, %104:<6144xf16>, alpha=1:i32) 
58394:7:0 %218:<2048x6144xf16> = torch.2_1_0.aten::mm(%222:<2048x12288xf16>, %190:<12288x6144xf16>{1,12288}) 
58394:7:0 %46:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%218:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58394:7:0 %222:<2048x1x6144xf16> = torch.2_1_0.aten::add(%46:<2048x1x6144xf16>, %93:<6144xf16>, alpha=1:i32) 
58392:5:0 %178:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%177:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58392:5:0 %179:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%178:<1x12x2048x2048xf16>) 
58392:5:0 %32, %180:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%178:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58392:5:0 %181:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%166:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58392:5:0 %182:<12x2048x2048xf16> = torch.2_1_0.aten::view(%32:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58392:5:0 %183:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%181:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58392:5:0 %184:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%182:<12x2048x2048xf16>, %183:<12x2048x128xf16>{384,4608,1}+256) 
58392:5:0 %185:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%184:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58392:5:0 %186:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%185:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58392:5:0 %187:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%186:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58392:5:0 %186:<2048x1x1536xf16> = torch.2_1_0.aten::view(%187:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58392:5:0 %177:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%17:<12288x1536xf16>) 
58392:5:0 %188:<2048x1536xf16> = torch.2_1_0.aten::view(%186:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58392:5:0 %181:<2048x12288xf16> = torch.2_1_0.aten::mm(%188:<2048x1536xf16>, %177:<1536x12288xf16>{1,1536}) 
58392:5:0 %176:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%181:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:None:0 list{%176:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%176:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %188:<2048x1x12288xf16> = torch.2_1_0.aten::view(%176:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %235:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax(%234:<1x12x2048x2048xf16>, -1:i32, False:pred) 
58392:5:0 %166:<2048x1x12288xf16> = torch.2_1_0.aten::add(%188:<2048x1x12288xf16>, %13:<12288xf16>, alpha=1:i32) 
58390:3:0 %236:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%235:<1x12x2048x2048xf16>) 
58392:5:0 %189, %163:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%166:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58392:5:0 %181:<2048x1x12288xf16> = torch.2_1_0.aten::add(%155:<2048x1x12288xf16>, %189:<2048x1x12288xf16>, alpha=1:i32) 
58392:5:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%181:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %191:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %192:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %177:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %193:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58392:5:0 %194:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %166:<2048x1x12288xf16> = torch.2_1_0.aten::view(%191:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %190:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %176:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%166:<2048x1x12288xf16>) 
58392:5:0 %191:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%30:<6144x12288xf16>) 
58392:5:0 %166:<2048x12288xf16> = torch.2_1_0.aten::view(%190:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %176:<2048x6144xf16> = torch.2_1_0.aten::mm(%166:<2048x12288xf16>, %191:<12288x6144xf16>{1,12288}) 
58392:5:0 %51:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%176:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58392:5:0 %191:<2048x1x6144xf16> = torch.2_1_0.aten::add(%51:<2048x1x6144xf16>, %6:<6144xf16>, alpha=1:i32) 
58390:3:0 %237, %238:<1x12x2048x2048xf16>, <1x12x2048x2048xunknown> = torch.2_1_0.aten::native_dropout(%235:<1x12x2048x2048xf16>, 0.1:f32, True:pred) 
58390:3:0 %239:<2048x12x128xf16>{4608,384,1}+256 = torch.2_1_0.aten::view(%220:<2048x1x12x128xf16>{4608,4608,384,1}+256, list{2048:i32, 12:i32, -1:i32}) 
58390:3:0 %240:<12x2048x2048xf16> = torch.2_1_0.aten::view(%237:<1x12x2048x2048xf16>, list{12:i32, 2048:i32, -1:i32}) 
58390:3:0 %241:<12x2048x128xf16>{384,4608,1}+256 = torch.2_1_0.aten::transpose(%239:<2048x12x128xf16>{4608,384,1}+256, 0:i32, 1:i32) 
58390:3:0 %242:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%240:<12x2048x2048xf16>, %241:<12x2048x128xf16>{384,4608,1}+256) 
58390:3:0 %243:<1x12x2048x128xf16> = torch.2_1_0.aten::view(%242:<12x2048x128xf16>, list{1:i32, 12:i32, 2048:i32, 128:i32}) 
58390:3:0 %244:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::permute(%243:<1x12x2048x128xf16>, list{2:i32, 0:i32, 1:i32, 3:i32}) 
58390:3:0 %245:<2048x1x12x128xf16> = torch.2_1_0.aten::clone(%244:<2048x1x12x128xf16>{128,3145728,262144,1}, memory_format=torch.contiguous_format:memory_format) 
58390:3:0 %246:<2048x1x1536xf16> = torch.2_1_0.aten::view(%245:<2048x1x12x128xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58390:3:0 %229:<1536x12288xf16>{1,1536} = torch.2_1_0.aten::t(%120:<12288x1536xf16>) 
58390:3:0 %234:<2048x1536xf16> = torch.2_1_0.aten::view(%246:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58390:3:0 %225:<2048x12288xf16> = torch.2_1_0.aten::mm(%234:<2048x1536xf16>, %229:<1536x12288xf16>{1,1536}) 
58390:3:0 %239:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%225:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:None:0 list{%239:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%239:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %244:<2048x1x12288xf16> = torch.2_1_0.aten::view(%239:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %247:<2048x1x12288xf16> = torch.2_1_0.aten::add(%244:<2048x1x12288xf16>, %61:<12288xf16>, alpha=1:i32) 
58390:3:0 %223, %220:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%247:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %250:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%219:<2048x1x6144xf16>, approximate=none:str) 
58387:0:0 %252:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%277:<2048x1x6144xf16>, approximate=none:str) 
58388:1:0 %246:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%265:<2048x1x6144xf16>, approximate=none:str) 
58389:2:0 %278:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%274:<2048x1x6144xf16>, approximate=none:str) 
58392:5:0 %189:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%191:<2048x1x6144xf16>, approximate=none:str) 
58394:7:0 %173:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%222:<2048x1x6144xf16>, approximate=none:str) 
58390:3:0 %229:<2048x1x12288xf16> = torch.2_1_0.aten::add(%213:<2048x1x12288xf16>, %223:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %287:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%53:<12288x6144xf16>) 
58392:5:0 %195:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%31:<12288x6144xf16>) 
58391:4:0 %240:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%33:<12288x6144xf16>) 
58389:2:0 %277:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%102:<12288x6144xf16>) 
58388:1:0 %273:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%65:<12288x6144xf16>) 
58394:7:0 %218:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%94:<12288x6144xf16>) 
58387:0:0 %271:<2048x6144xf16> = torch.2_1_0.aten::view(%252:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58392:5:0 %176:<2048x6144xf16> = torch.2_1_0.aten::view(%189:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58391:4:0 %249:<2048x6144xf16> = torch.2_1_0.aten::view(%250:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58389:2:0 %276:<2048x6144xf16> = torch.2_1_0.aten::view(%278:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58388:1:0 %256:<2048x6144xf16> = torch.2_1_0.aten::view(%246:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58394:7:0 %190:<2048x6144xf16> = torch.2_1_0.aten::view(%173:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:0:0 %264:<2048x12288xf16> = torch.2_1_0.aten::mm(%271:<2048x6144xf16>, %287:<6144x12288xf16>{1,6144}) 
58391:4:0 %245:<2048x12288xf16> = torch.2_1_0.aten::mm(%249:<2048x6144xf16>, %240:<6144x12288xf16>{1,6144}) 
58389:2:0 %279:<2048x12288xf16> = torch.2_1_0.aten::mm(%276:<2048x6144xf16>, %277:<6144x12288xf16>{1,6144}) 
58392:5:0 %193:<2048x12288xf16> = torch.2_1_0.aten::mm(%176:<2048x6144xf16>, %195:<6144x12288xf16>{1,6144}) 
58393:6:0 %274:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%260:<2048x1x6144xf16>, approximate=none:str) 
58387:0:0 %288:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%264:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %274:<2048x12288xf16> = torch.2_1_0.aten::mm(%256:<2048x6144xf16>, %273:<6144x12288xf16>{1,6144}) 
58394:7:0 %195:<2048x12288xf16> = torch.2_1_0.aten::mm(%190:<2048x6144xf16>, %218:<6144x12288xf16>{1,6144}) 
58389:2:0 %280:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%279:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %196:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%193:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %230:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%245:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %275:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%274:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %217:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%195:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %245:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%47:<12288x6144xf16>) 
58393:6:0 %276:<2048x6144xf16> = torch.2_1_0.aten::view(%274:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:None:0 list{%288:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%288:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:None:0 list{%275:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%275:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:None:0 list{%196:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%196:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:None:0 list{%280:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%280:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %264:<2048x1x12288xf16> = torch.2_1_0.aten::view(%288:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:None:0 list{%217:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%217:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:None:0 list{%230:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%230:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %277:<2048x12288xf16> = torch.2_1_0.aten::mm(%276:<2048x6144xf16>, %245:<6144x12288xf16>{1,6144}) 
58388:1:0 %256:<2048x1x12288xf16> = torch.2_1_0.aten::view(%275:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %278:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%277:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %197:<2048x1x12288xf16> = torch.2_1_0.aten::view(%196:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %277:<2048x1x12288xf16> = torch.2_1_0.aten::view(%280:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %224:<2048x1x12288xf16> = torch.2_1_0.aten::view(%217:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::view(%230:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:None:0 list{%278:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%278:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %276:<2048x1x12288xf16> = torch.2_1_0.aten::view(%278:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %280:<2048x1x12288xf16> = torch.2_1_0.aten::add(%264:<2048x1x12288xf16>, %46:<12288xf16>, alpha=1:i32) 
58390:3:0 %248:<2048x12288xf16> = torch.2_1_0.aten::view(%229:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %276:<2048x1x12288xf16> = torch.2_1_0.aten::add(%256:<2048x1x12288xf16>, %48:<12288xf16>, alpha=1:i32) 
58389:2:0 %281:<2048x1x12288xf16> = torch.2_1_0.aten::add(%277:<2048x1x12288xf16>, %99:<12288xf16>, alpha=1:i32) 
58392:5:0 %188:<2048x1x12288xf16> = torch.2_1_0.aten::add(%197:<2048x1x12288xf16>, %33:<12288xf16>, alpha=1:i32) 
58394:7:0 %188:<2048x1x12288xf16> = torch.2_1_0.aten::add(%224:<2048x1x12288xf16>, %96:<12288xf16>, alpha=1:i32) 
58391:4:0 %251:<2048x1x12288xf16> = torch.2_1_0.aten::add(%240:<2048x1x12288xf16>, %88:<12288xf16>, alpha=1:i32) 
58393:6:0 %245:<2048x1x12288xf16> = torch.2_1_0.aten::add(%276:<2048x1x12288xf16>, %77:<12288xf16>, alpha=1:i32) 
58390:3:0 %249:<2048x12288xf16> = torch.2_1_0.aten::empty(list{2048:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %250:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %251:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %252:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58390:3:0 %253:<5120xi8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %223:<2048x1x12288xf16> = torch.2_1_0.aten::view(%249:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %253:<1xf16> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float16:dtype, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %233:<2048x1x12288xf16> = torch.2_1_0.aten::detach(%223:<2048x1x12288xf16>) 
58388:1:0 %274, %273:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%276:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %244:<12288x6144xf16>{1,12288} = torch.2_1_0.aten::t(%95:<6144x12288xf16>) 
58390:3:0 %223:<2048x12288xf16> = torch.2_1_0.aten::view(%253:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %282, %227:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%281:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58392:5:0 %35, %193:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%188:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %233:<2048x6144xf16> = torch.2_1_0.aten::mm(%223:<2048x12288xf16>, %244:<12288x6144xf16>{1,12288}) 
58390:3:0 %254:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%233:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58394:7:0 %225, %190:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%188:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58391:4:0 %248, %249:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%251:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58393:6:0 %279, %277:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%245:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %223:<2048x1x6144xf16> = torch.2_1_0.aten::add(%254:<2048x1x6144xf16>, %64:<6144xf16>, alpha=1:i32) 
58388:1:0 %277:<2048x1x12288xf16> = torch.2_1_0.aten::add(%243:<2048x1x12288xf16>, %274:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %280:<2048x1x12288xf16> = torch.2_1_0.aten::add(%268:<2048x1x12288xf16>, %279:<2048x1x12288xf16>, alpha=1:i32) 
58388:1:0 %256:<2048x12288xf16> = torch.2_1_0.aten::view(%277:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %278:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%256:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %278:<2048x12288xf16> = torch.2_1_0.aten::view(%280:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %279:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %280:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %281:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%278:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %282:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %283:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %233:<2048x1x6144xf16> = torch.2_1_0.aten::gelu(%223:<2048x1x6144xf16>, approximate=none:str) 
58390:3:0 %255:<6144x12288xf16>{1,6144} = torch.2_1_0.aten::t(%105:<12288x6144xf16>) 
58390:3:0 %256:<2048x6144xf16> = torch.2_1_0.aten::view(%233:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58390:3:0 %257:<2048x12288xf16> = torch.2_1_0.aten::mm(%256:<2048x6144xf16>, %255:<6144x12288xf16>{1,6144}) 
58390:3:0 %258:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%257:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:None:0 list{%258:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%258:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %256:<2048x1x12288xf16> = torch.2_1_0.aten::view(%258:<2048x1x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %255:<2048x1x12288xf16> = torch.2_1_0.aten::add(%256:<2048x1x12288xf16>, %104:<12288xf16>, alpha=1:i32) 
58390:3:0 %257, %259:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%255:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58390:3:0 %260:<2048x1x12288xf16> = torch.2_1_0.aten::add(%229:<2048x1x12288xf16>, %257:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %209:<2048x12288xf16> = torch.2_1_0.aten::view(%260:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58390:3:0 %257:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%209:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %255:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %261:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %283:<2048x1x12288xf16> = torch.2_1_0.aten::add(%221:<2048x1x12288xf16>, %282:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %226:<2048x1x12288xf16> = torch.2_1_0.aten::add(%203:<2048x1x12288xf16>, %225:<2048x1x12288xf16>, alpha=1:i32) 
58391:4:0 %245:<2048x1x12288xf16> = torch.2_1_0.aten::add(%233:<2048x1x12288xf16>, %248:<2048x1x12288xf16>, alpha=1:i32) 
58389:2:0 %277:<2048x12288xf16> = torch.2_1_0.aten::view(%283:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58394:7:0 %185:<2048x12288xf16> = torch.2_1_0.aten::view(%226:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %279:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%277:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %224:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%185:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %252:<2048x12288xf16> = torch.2_1_0.aten::view(%245:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %284:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %162:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %285:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %217:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %251:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%252:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %230:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %253:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %287, %271:<2048x1x12288xf16>, <2048x1x12288xunknown> = torch.2_1_0.aten::native_dropout(%280:<2048x1x12288xf16>, 0.1:f32, True:pred) 
58387:0:0 %289:<2048x1x12288xf16> = torch.2_1_0.aten::add(%286:<2048x1x12288xf16>, %287:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %288:<2048x12288xf16> = torch.2_1_0.aten::view(%289:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58387:0:0 %264:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%288:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %57:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %280:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %198:<2048x1x12288xf16> = torch.2_1_0.aten::add(%181:<2048x1x12288xf16>, %35:<2048x1x12288xf16>, alpha=1:i32) 
58392:5:0 %151:<2048x12288xf16> = torch.2_1_0.aten::view(%198:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %199:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%151:<2048x12288xf16>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %188:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %200:<2048xf32> = torch.2_1_0.aten::empty(list{2048:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %281:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %282:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58388:1:0 %283:<2048x1x12288xf16> = torch.2_1_0.aten::view(%278:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %240:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%113:<6400x12288xf16>) 
58388:1:0 %221:<2048x12288xf16> = torch.2_1_0.aten::view(%283:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %284:<2048x6400xf16> = torch.2_1_0.aten::mm(%221:<2048x12288xf16>, %240:<12288x6400xf16>{1,12288}) 
58388:1:0 %285:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%284:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58388:1:0 %284:<2048x1xi64> = torch.2_1_0.aten::transpose(%9:<1x2048xi64>, 0:i32, 1:i32) 
58393:6:0 %284:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %245:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58393:6:0 %285:<2048x1x12288xf16> = torch.2_1_0.aten::view(%281:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %286:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%116:<6400x12288xf16>) 
58393:6:0 %276:<2048x12288xf16> = torch.2_1_0.aten::view(%285:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %287:<2048x6400xf16> = torch.2_1_0.aten::mm(%276:<2048x12288xf16>, %286:<12288x6400xf16>{1,12288}) 
58388:1:0 %220:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%285:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %240, %286:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%220:<2048x1x6400xf32>, -1:i32, False:pred) 
58388:None:0 list{%240:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%240:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %287:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%240:<2048x1xf32>, -1:i32) 
58388:1:0 %288:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%220:<2048x1x6400xf32>, %287:<2048x1x1xf32>, alpha=1:i32) 
58388:1:0 %289:<2048x1xunknown> = torch.2_1_0.aten::lt(%284:<2048x1xi64>, 6400:i32) 
58388:1:0 %287:<2048x1xunknown> = torch.2_1_0.aten::ge(%284:<2048x1xi64>, 12800:i32) 
58388:1:0 %290:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%289:<2048x1xunknown>, %287:<2048x1xunknown>) 
58388:1:0 %291:<2048x1xi64> = torch.2_1_0.aten::clone(%284:<2048x1xi64>, memory_format=None:NoneType) 
58388:1:0 %292:<2048x1xi64> = torch.2_1_0.aten::sub(%291:<2048x1xi64>, 6400:i32, alpha=1:i32) 
58388:None:0 %293:<i64> = torch.2_1_0.aten::lift_fresh(%293:<i64>) 
58388:1:0 %292:<2048x1xi64> = torch.2_1_0.aten::index_put_(%292:<2048x1xi64>, list{%290:<2048x1xunknown>}, %293:<i64>, False:pred) 
58388:1:0 %294:<2048x6400xf32> = torch.2_1_0.aten::view(%288:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58388:1:0 %295:<2048xi64> = torch.2_1_0.aten::view(%292:<2048x1xi64>, list{-1:i32}) 
58393:6:0 %245:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%287:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58393:6:0 %286:<2048x1xi64> = torch.2_1_0.aten::transpose(%207:<1x2048xi64>, 0:i32, 1:i32) 
58393:6:0 %288:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%245:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %289, %290:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%288:<2048x1x6400xf32>, -1:i32, False:pred) 
58393:None:0 list{%289:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%289:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %291:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%289:<2048x1xf32>, -1:i32) 
58387:0:0 %287:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %260:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58387:0:0 %290:<2048x1x12288xf16> = torch.2_1_0.aten::view(%264:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %262:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %291:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%116:<6400x12288xf16>) 
58387:0:0 %292:<2048x12288xf16> = torch.2_1_0.aten::view(%290:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %263:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58387:0:0 %260:<2048x6400xf16> = torch.2_1_0.aten::mm(%292:<2048x12288xf16>, %291:<12288x6400xf16>{1,12288}) 
58387:0:0 %59:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%260:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58387:0:0 %260:<2048x1xi64> = torch.2_1_0.aten::transpose(%240:<1x2048xi64>, 0:i32, 1:i32) 
58390:3:0 %256:<2048x1x12288xf16> = torch.2_1_0.aten::view(%257:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %264:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%116:<6400x12288xf16>) 
58389:2:0 %286:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %221:<2048x12288xf16> = torch.2_1_0.aten::view(%256:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %287:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58390:3:0 %263:<2048x6400xf16> = torch.2_1_0.aten::mm(%221:<2048x12288xf16>, %264:<12288x6400xf16>{1,12288}) 
58389:2:0 %288:<2048x1x12288xf16> = torch.2_1_0.aten::view(%279:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %205:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%263:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58390:3:0 %263:<2048x1xi64> = torch.2_1_0.aten::transpose(%182:<1x2048xi64>, 0:i32, 1:i32) 
58389:2:0 %289:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%98:<6400x12288xf16>) 
58389:2:0 %290:<2048x12288xf16> = torch.2_1_0.aten::view(%288:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %282:<2048x6400xf16> = torch.2_1_0.aten::mm(%290:<2048x12288xf16>, %289:<12288x6400xf16>{1,12288}) 
58389:2:0 %291:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%282:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58389:2:0 %282:<2048x1xi64> = torch.2_1_0.aten::transpose(%181:<1x2048xi64>, 0:i32, 1:i32) 
58394:7:0 %188:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %227:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58394:7:0 %182:<2048x1x12288xf16> = torch.2_1_0.aten::view(%224:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %227:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%12:<6400x12288xf16>) 
58394:7:0 %195:<2048x12288xf16> = torch.2_1_0.aten::view(%182:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %161:<2048x6400xf16> = torch.2_1_0.aten::mm(%195:<2048x12288xf16>, %227:<12288x6400xf16>{1,12288}) 
58394:7:0 %228:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%161:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58394:7:0 %161:<2048x1xi64> = torch.2_1_0.aten::transpose(%177:<1x2048xi64>, 0:i32, 1:i32) 
58391:4:0 %254:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %255:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58387:0:0 %291:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%59:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %218:<2048x1x12288xf16> = torch.2_1_0.aten::view(%251:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %292:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%288:<2048x1x6400xf32>, %291:<2048x1x1xf32>, alpha=1:i32) 
58387:0:0 %293, %292:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%291:<2048x1x6400xf32>, -1:i32, False:pred) 
58393:6:0 %293:<2048x1xunknown> = torch.2_1_0.aten::lt(%286:<2048x1xi64>, 38400:i32) 
58391:4:0 %256:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%106:<6400x12288xf16>) 
58393:6:0 %294:<2048x1xunknown> = torch.2_1_0.aten::ge(%286:<2048x1xi64>, 44800:i32) 
58391:4:0 %255:<2048x12288xf16> = torch.2_1_0.aten::view(%218:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %295:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%293:<2048x1xunknown>, %294:<2048x1xunknown>) 
58387:None:0 list{%293:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%293:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %240:<2048x6400xf16> = torch.2_1_0.aten::mm(%255:<2048x12288xf16>, %256:<12288x6400xf16>{1,12288}) 
58393:6:0 %296:<2048x1xi64> = torch.2_1_0.aten::clone(%286:<2048x1xi64>, memory_format=None:NoneType) 
58387:0:0 %294:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%293:<2048x1xf32>, -1:i32) 
58391:4:0 %66:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%240:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58393:6:0 %297:<2048x1xi64> = torch.2_1_0.aten::sub(%296:<2048x1xi64>, 38400:i32, alpha=1:i32) 
58393:None:0 %296:<i64> = torch.2_1_0.aten::lift_fresh(%296:<i64>) 
58391:4:0 %255:<2048x1xi64> = torch.2_1_0.aten::transpose(%210:<1x2048xi64>, 0:i32, 1:i32) 
58393:6:0 %297:<2048x1xi64> = torch.2_1_0.aten::index_put_(%297:<2048x1xi64>, list{%295:<2048x1xunknown>}, %296:<i64>, False:pred) 
58393:6:0 %298:<2048x6400xf32> = torch.2_1_0.aten::view(%292:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58387:0:0 %295:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%291:<2048x1x6400xf32>, %294:<2048x1x1xf32>, alpha=1:i32) 
58393:6:0 %299:<2048xi64> = torch.2_1_0.aten::view(%297:<2048x1xi64>, list{-1:i32}) 
58387:0:0 %294:<2048x1xunknown> = torch.2_1_0.aten::lt(%260:<2048x1xi64>, 0:i32) 
58387:0:0 %296:<2048x1xunknown> = torch.2_1_0.aten::ge(%260:<2048x1xi64>, 6400:i32) 
58387:0:0 %297:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%294:<2048x1xunknown>, %296:<2048x1xunknown>) 
58391:4:0 %257:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%66:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %292:<2048x1xi64> = torch.2_1_0.aten::clone(%260:<2048x1xi64>, memory_format=None:NoneType) 
58387:0:0 %294:<2048x1xi64> = torch.2_1_0.aten::sub(%292:<2048x1xi64>, 0:i32, alpha=1:i32) 
58387:None:0 %61:<i64> = torch.2_1_0.aten::lift_fresh(%61:<i64>) 
58391:4:0 %258, %194:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%257:<2048x1x6400xf32>, -1:i32, False:pred) 
58391:None:0 list{%258:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%258:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %201:<5120xu8> = torch.2_1_0.aten::empty(list{5120:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %240:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%258:<2048x1xf32>, -1:i32) 
58392:5:0 %202:<320xi32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58392:5:0 %197:<2048x1x12288xf16> = torch.2_1_0.aten::view(%199:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %168:<12288x6400xf16>{1,12288} = torch.2_1_0.aten::t(%24:<6400x12288xf16>) 
58392:5:0 %152:<2048x12288xf16> = torch.2_1_0.aten::view(%197:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %153:<2048x6400xf16> = torch.2_1_0.aten::mm(%152:<2048x12288xf16>, %168:<12288x6400xf16>{1,12288}) 
58392:5:0 %35:<2048x1x6400xf16> = torch.2_1_0.aten::_unsafe_view(%153:<2048x6400xf16>, list{2048:i32, 1:i32, 6400:i32}) 
58392:5:0 %153:<2048x1xi64> = torch.2_1_0.aten::transpose(%129:<1x2048xi64>, 0:i32, 1:i32) 
58391:4:0 %194:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%257:<2048x1x6400xf32>, %240:<2048x1x1xf32>, alpha=1:i32) 
58387:0:0 %294:<2048x1xi64> = torch.2_1_0.aten::index_put_(%294:<2048x1xi64>, list{%297:<2048x1xunknown>}, %61:<i64>, False:pred) 
58391:4:0 %240:<2048x1xunknown> = torch.2_1_0.aten::lt(%255:<2048x1xi64>, 25600:i32) 
58387:0:0 %298:<2048x6400xf32> = torch.2_1_0.aten::view(%295:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58387:0:0 %299:<2048xi64> = torch.2_1_0.aten::view(%294:<2048x1xi64>, list{-1:i32}) 
58390:3:0 %265:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%205:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %259:<2048x1xunknown> = torch.2_1_0.aten::ge(%255:<2048x1xi64>, 32000:i32) 
58392:5:0 %203:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%35:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %260:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%240:<2048x1xunknown>, %259:<2048x1xunknown>) 
58391:4:0 %261:<2048x1xi64> = torch.2_1_0.aten::clone(%255:<2048x1xi64>, memory_format=None:NoneType) 
58391:4:0 %240:<2048x1xi64> = torch.2_1_0.aten::sub(%261:<2048x1xi64>, 25600:i32, alpha=1:i32) 
58392:5:0 %204, %152:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%203:<2048x1x6400xf32>, -1:i32, False:pred) 
58391:None:0 %68:<i64> = torch.2_1_0.aten::lift_fresh(%68:<i64>) 
58391:4:0 %240:<2048x1xi64> = torch.2_1_0.aten::index_put_(%240:<2048x1xi64>, list{%260:<2048x1xunknown>}, %68:<i64>, False:pred) 
58392:None:0 list{%204:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%204:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %262:<2048x6400xf32> = torch.2_1_0.aten::view(%194:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58390:3:0 %266, %262:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%265:<2048x1x6400xf32>, -1:i32, False:pred) 
58392:5:0 %168:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%204:<2048x1xf32>, -1:i32) 
58391:4:0 %261:<2048xi64> = torch.2_1_0.aten::view(%240:<2048x1xi64>, list{-1:i32}) 
58392:5:0 %205:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%203:<2048x1x6400xf32>, %168:<2048x1x1xf32>, alpha=1:i32) 
58392:5:0 %168:<2048x1xunknown> = torch.2_1_0.aten::lt(%153:<2048x1xi64>, 32000:i32) 
58390:None:0 list{%266:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%266:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %206:<2048x1xunknown> = torch.2_1_0.aten::ge(%153:<2048x1xi64>, 38400:i32) 
58392:5:0 %207:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%168:<2048x1xunknown>, %206:<2048x1xunknown>) 
58392:5:0 %208:<2048x1xi64> = torch.2_1_0.aten::clone(%153:<2048x1xi64>, memory_format=None:NoneType) 
58390:3:0 %262:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%266:<2048x1xf32>, -1:i32) 
58392:5:0 %209:<2048x1xi64> = torch.2_1_0.aten::sub(%208:<2048x1xi64>, 32000:i32, alpha=1:i32) 
58392:None:0 %208:<i64> = torch.2_1_0.aten::lift_fresh(%208:<i64>) 
58392:5:0 %209:<2048x1xi64> = torch.2_1_0.aten::index_put_(%209:<2048x1xi64>, list{%207:<2048x1xunknown>}, %208:<i64>, False:pred) 
58392:5:0 %210:<2048x6400xf32> = torch.2_1_0.aten::view(%205:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58392:5:0 %168:<2048xi64> = torch.2_1_0.aten::view(%209:<2048x1xi64>, list{-1:i32}) 
58389:2:0 %199:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%291:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %292, %289:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%199:<2048x1x6400xf32>, -1:i32, False:pred) 
58389:None:0 list{%292:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%292:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %293:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%292:<2048x1xf32>, -1:i32) 
58394:7:0 %229:<2048x1x6400xf32> = torch.2_1_0.aten::_to_copy(%228:<2048x1x6400xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %230, %231:<2048x1xf32>, <2048x1xi64> = torch.2_1_0.aten::max(%229:<2048x1x6400xf32>, -1:i32, False:pred) 
58394:None:0 list{%230:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%230:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %232:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%230:<2048x1xf32>, -1:i32) 
58387:0:0 %300:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred) 
58392:5:0 %211:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred) 
58391:4:0 %263:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred) 
58393:6:0 %300:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred) 
58388:1:0 %296:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred) 
58387:0:0 %301:<2048xf32> = torch.2_1_0.aten::index(%298:<2048x6400xf32>, list{%300:<2048xi64>, %299:<2048xi64>}) 
58393:6:0 %301:<2048xf32> = torch.2_1_0.aten::index(%298:<2048x6400xf32>, list{%300:<2048xi64>, %299:<2048xi64>}) 
58391:4:0 %264:<2048xf32> = torch.2_1_0.aten::index(%262:<2048x6400xf32>, list{%263:<2048xi64>, %261:<2048xi64>}) 
58392:5:0 %212:<2048xf32> = torch.2_1_0.aten::index(%210:<2048x6400xf32>, list{%211:<2048xi64>, %168:<2048xi64>}) 
58388:1:0 %297:<2048xf32> = torch.2_1_0.aten::index(%294:<2048x6400xf32>, list{%296:<2048xi64>, %295:<2048xi64>}) 
58387:0:0 %302:<2048xf32> = torch.2_1_0.aten::clone(%301:<2048xf32>, memory_format=None:NoneType) 
58392:5:0 %213:<2048xf32> = torch.2_1_0.aten::clone(%212:<2048xf32>, memory_format=None:NoneType) 
58393:6:0 %302:<2048xf32> = torch.2_1_0.aten::clone(%301:<2048xf32>, memory_format=None:NoneType) 
58388:1:0 %298:<2048xf32> = torch.2_1_0.aten::clone(%297:<2048xf32>, memory_format=None:NoneType) 
58391:4:0 %265:<2048xf32> = torch.2_1_0.aten::clone(%264:<2048xf32>, memory_format=None:NoneType) 
58387:0:0 %303:<2048x1xf32> = torch.2_1_0.aten::view(%302:<2048xf32>, list{2048:i32, 1:i32}) 
58387:None:0 %79:<f32> = torch.2_1_0.aten::lift_fresh(%79:<f32>) 
58392:5:0 %214:<2048x1xf32> = torch.2_1_0.aten::view(%213:<2048xf32>, list{2048:i32, 1:i32}) 
58393:6:0 %301:<2048x1xf32> = torch.2_1_0.aten::view(%302:<2048xf32>, list{2048:i32, 1:i32}) 
58388:1:0 %299:<2048x1xf32> = torch.2_1_0.aten::view(%298:<2048xf32>, list{2048:i32, 1:i32}) 
58391:4:0 %266:<2048x1xf32> = torch.2_1_0.aten::view(%265:<2048xf32>, list{2048:i32, 1:i32}) 
58392:None:0 %61:<f32> = torch.2_1_0.aten::lift_fresh(%61:<f32>) 
58393:None:0 %303:<f32> = torch.2_1_0.aten::lift_fresh(%303:<f32>) 
58391:None:0 %267:<f32> = torch.2_1_0.aten::lift_fresh(%267:<f32>) 
58388:None:0 %300:<f32> = torch.2_1_0.aten::lift_fresh(%300:<f32>) 
58394:7:0 %227:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%229:<2048x1x6400xf32>, %232:<2048x1x1xf32>, alpha=1:i32) 
58394:7:0 %233:<2048x1xunknown> = torch.2_1_0.aten::lt(%161:<2048x1xi64>, 44800:i32) 
58394:7:0 %232:<2048x1xunknown> = torch.2_1_0.aten::ge(%161:<2048x1xi64>, 51200:i32) 
58388:1:0 %299:<2048x1xf32> = torch.2_1_0.aten::index_put_(%299:<2048x1xf32>, list{%290:<2048x1xunknown>}, %300:<f32>, False:pred) 
58392:5:0 %214:<2048x1xf32> = torch.2_1_0.aten::index_put_(%214:<2048x1xf32>, list{%207:<2048x1xunknown>}, %61:<f32>, False:pred) 
58393:6:0 %301:<2048x1xf32> = torch.2_1_0.aten::index_put_(%301:<2048x1xf32>, list{%295:<2048x1xunknown>}, %303:<f32>, False:pred) 
58391:4:0 %266:<2048x1xf32> = torch.2_1_0.aten::index_put_(%266:<2048x1xf32>, list{%260:<2048x1xunknown>}, %267:<f32>, False:pred) 
58394:7:0 %234:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%233:<2048x1xunknown>, %232:<2048x1xunknown>) 
58394:7:0 %235:<2048x1xi64> = torch.2_1_0.aten::clone(%161:<2048x1xi64>, memory_format=None:NoneType) 
58394:7:0 %236:<2048x1xi64> = torch.2_1_0.aten::sub(%235:<2048x1xi64>, 44800:i32, alpha=1:i32) 
58392:None:0 list{%214:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%214:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:None:0 list{%299:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%299:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:None:0 list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:None:0 list{%266:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%266:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:None:0 %235:<i64> = torch.2_1_0.aten::lift_fresh(%235:<i64>) 
58394:7:0 %236:<2048x1xi64> = torch.2_1_0.aten::index_put_(%236:<2048x1xi64>, list{%234:<2048x1xunknown>}, %235:<i64>, False:pred) 
58394:7:0 %237:<2048x6400xf32> = torch.2_1_0.aten::view(%227:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58394:7:0 %238:<2048xi64> = torch.2_1_0.aten::view(%236:<2048x1xi64>, list{-1:i32}) 
58389:2:0 %272:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%199:<2048x1x6400xf32>, %293:<2048x1x1xf32>, alpha=1:i32) 
58390:3:0 %264:<2048x1x6400xf32> = torch.2_1_0.aten::sub(%265:<2048x1x6400xf32>, %262:<2048x1x1xf32>, alpha=1:i32) 
58389:2:0 %294:<2048x1xunknown> = torch.2_1_0.aten::lt(%282:<2048x1xi64>, 12800:i32) 
58389:2:0 %293:<2048x1xunknown> = torch.2_1_0.aten::ge(%282:<2048x1xi64>, 19200:i32) 
58389:2:0 %295:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%294:<2048x1xunknown>, %293:<2048x1xunknown>) 
58387:0:0 %303:<2048x1xf32> = torch.2_1_0.aten::index_put_(%303:<2048x1xf32>, list{%297:<2048x1xunknown>}, %79:<f32>, False:pred) 
58389:2:0 %296:<2048x1xi64> = torch.2_1_0.aten::clone(%282:<2048x1xi64>, memory_format=None:NoneType) 
58389:2:0 %297:<2048x1xi64> = torch.2_1_0.aten::sub(%296:<2048x1xi64>, 12800:i32, alpha=1:i32) 
58389:None:0 %298:<i64> = torch.2_1_0.aten::lift_fresh(%298:<i64>) 
58387:None:0 list{%303:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%303:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %267:<2048x1xunknown> = torch.2_1_0.aten::lt(%263:<2048x1xi64>, 19200:i32) 
58389:2:0 %297:<2048x1xi64> = torch.2_1_0.aten::index_put_(%297:<2048x1xi64>, list{%295:<2048x1xunknown>}, %298:<i64>, False:pred) 
58389:2:0 %299:<2048x6400xf32> = torch.2_1_0.aten::view(%272:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58389:2:0 %300:<2048xi64> = torch.2_1_0.aten::view(%297:<2048x1xi64>, list{-1:i32}) 
58390:3:0 %262:<2048x1xunknown> = torch.2_1_0.aten::ge(%263:<2048x1xi64>, 25600:i32) 
58390:3:0 %268:<2048x1xunknown> = torch.2_1_0.aten::bitwise_or(%267:<2048x1xunknown>, %262:<2048x1xunknown>) 
58390:3:0 %269:<2048x1xi64> = torch.2_1_0.aten::clone(%263:<2048x1xi64>, memory_format=None:NoneType) 
58390:3:0 %270:<2048x1xi64> = torch.2_1_0.aten::sub(%269:<2048x1xi64>, 19200:i32, alpha=1:i32) 
58390:None:0 %269:<i64> = torch.2_1_0.aten::lift_fresh(%269:<i64>) 
58394:7:0 %239:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred) 
58390:3:0 %270:<2048x1xi64> = torch.2_1_0.aten::index_put_(%270:<2048x1xi64>, list{%268:<2048x1xunknown>}, %269:<i64>, False:pred) 
58394:7:0 %240:<2048xf32> = torch.2_1_0.aten::index(%237:<2048x6400xf32>, list{%239:<2048xi64>, %238:<2048xi64>}) 
58390:3:0 %262:<2048x6400xf32> = torch.2_1_0.aten::view(%264:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58394:7:0 %241:<2048xf32> = torch.2_1_0.aten::clone(%240:<2048xf32>, memory_format=None:NoneType) 
58390:3:0 %271:<2048xi64> = torch.2_1_0.aten::view(%270:<2048x1xi64>, list{-1:i32}) 
58394:7:0 %242:<2048x1xf32> = torch.2_1_0.aten::view(%241:<2048xf32>, list{2048:i32, 1:i32}) 
58394:None:0 %243:<f32> = torch.2_1_0.aten::lift_fresh(%243:<f32>) 
58394:7:0 %242:<2048x1xf32> = torch.2_1_0.aten::index_put_(%242:<2048x1xf32>, list{%234:<2048x1xunknown>}, %243:<f32>, False:pred) 
58394:None:0 list{%242:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%242:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %301:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred) 
58389:2:0 %302:<2048xf32> = torch.2_1_0.aten::index(%299:<2048x6400xf32>, list{%301:<2048xi64>, %300:<2048xi64>}) 
58389:2:0 %303:<2048xf32> = torch.2_1_0.aten::clone(%302:<2048xf32>, memory_format=None:NoneType) 
58389:2:0 %293:<2048x1xf32> = torch.2_1_0.aten::view(%303:<2048xf32>, list{2048:i32, 1:i32}) 
58389:None:0 %304:<f32> = torch.2_1_0.aten::lift_fresh(%304:<f32>) 
58389:2:0 %293:<2048x1xf32> = torch.2_1_0.aten::index_put_(%293:<2048x1xf32>, list{%295:<2048x1xunknown>}, %304:<f32>, False:pred) 
58389:None:0 list{%293:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%293:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %272:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred) 
58390:3:0 %273:<2048xf32> = torch.2_1_0.aten::index(%262:<2048x6400xf32>, list{%272:<2048xi64>, %271:<2048xi64>}) 
58390:3:0 %274:<2048xf32> = torch.2_1_0.aten::clone(%273:<2048xf32>, memory_format=None:NoneType) 
58390:3:0 %273:<2048x1xf32> = torch.2_1_0.aten::view(%274:<2048xf32>, list{2048:i32, 1:i32}) 
58390:None:0 %275:<f32> = torch.2_1_0.aten::lift_fresh(%275:<f32>) 
58390:3:0 %273:<2048x1xf32> = torch.2_1_0.aten::index_put_(%273:<2048x1xf32>, list{%268:<2048x1xunknown>}, %275:<f32>, False:pred) 
58390:None:0 list{%273:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%273:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %288:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%288:<2048x1x6400xf32>, out=%288:<2048x1x6400xf32>) 
58387:0:0 %295:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%295:<2048x1x6400xf32>, out=%295:<2048x1x6400xf32>) 
58393:6:0 %292:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%292:<2048x1x6400xf32>, out=%292:<2048x1x6400xf32>) 
58391:4:0 %194:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%194:<2048x1x6400xf32>, out=%194:<2048x1x6400xf32>) 
58392:5:0 %205:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%205:<2048x1x6400xf32>, out=%205:<2048x1x6400xf32>) 
58394:7:0 %227:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%227:<2048x1x6400xf32>, out=%227:<2048x1x6400xf32>) 
58387:0:0 %301:<2048x1xf32> = torch.2_1_0.aten::sum(%295:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58387:None:0 list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %301:<2048x1xf32> = torch.2_1_0.aten::sum(%288:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58388:None:0 list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%301:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %304:<2048x1xf32> = torch.2_1_0.aten::sum(%292:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58391:4:0 %268:<2048x1xf32> = torch.2_1_0.aten::sum(%194:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58393:None:0 list{%304:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%304:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:None:0 list{%268:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%268:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %272:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%272:<2048x1x6400xf32>, out=%272:<2048x1x6400xf32>) 
58394:7:0 %244:<2048x1xf32> = torch.2_1_0.aten::sum(%227:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58392:5:0 %215:<2048x1xf32> = torch.2_1_0.aten::sum(%205:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58394:None:0 list{%244:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%244:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:None:0 list{%215:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%215:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %305:<2048x1xf32> = torch.2_1_0.aten::sum(%272:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58390:3:0 %264:<2048x1x6400xf32> = torch.2_1_0.aten::exp(%264:<2048x1x6400xf32>, out=%264:<2048x1x6400xf32>) 
58389:None:0 list{%305:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%305:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %276:<2048x1xf32> = torch.2_1_0.aten::sum(%264:<2048x1x6400xf32>, list{-1:i32}, False:pred, dtype=None:NoneType) 
58392:5:0 %216:<2048x1xf32> = torch.2_1_0.aten::log(%215:<2048x1xf32>) 
58393:6:0 %303:<2048x1xf32> = torch.2_1_0.aten::log(%304:<2048x1xf32>) 
58388:1:0 %302:<2048x1xf32> = torch.2_1_0.aten::log(%301:<2048x1xf32>) 
58390:None:0 list{%276:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%276:<2048x1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %264:<2048x1xf32> = torch.2_1_0.aten::log(%268:<2048x1xf32>) 
58387:0:0 %296:<2048x1xf32> = torch.2_1_0.aten::log(%301:<2048x1xf32>) 
58394:7:0 %245:<2048x1xf32> = torch.2_1_0.aten::log(%244:<2048x1xf32>) 
58392:5:0 %217:<2048x1xf32> = torch.2_1_0.aten::sub(%216:<2048x1xf32>, %214:<2048x1xf32>, alpha=1:i32) 
58388:1:0 %303:<2048x1xf32> = torch.2_1_0.aten::sub(%302:<2048x1xf32>, %299:<2048x1xf32>, alpha=1:i32) 
58393:6:0 %305:<2048x1xf32> = torch.2_1_0.aten::sub(%303:<2048x1xf32>, %301:<2048x1xf32>, alpha=1:i32) 
58387:0:0 %304:<2048x1xf32> = torch.2_1_0.aten::sub(%296:<2048x1xf32>, %303:<2048x1xf32>, alpha=1:i32) 
58391:4:0 %269:<2048x1xf32> = torch.2_1_0.aten::sub(%264:<2048x1xf32>, %266:<2048x1xf32>, alpha=1:i32) 
58394:7:0 %246:<2048x1xf32> = torch.2_1_0.aten::sub(%245:<2048x1xf32>, %242:<2048x1xf32>, alpha=1:i32) 
58389:2:0 %306:<2048x1xf32> = torch.2_1_0.aten::log(%305:<2048x1xf32>) 
58392:5:0 %218:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%215:<2048x1xf32>, -1:i32) 
58388:1:0 %304:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%301:<2048x1xf32>, -1:i32) 
58393:6:0 %306:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%304:<2048x1xf32>, -1:i32) 
58387:0:0 %305:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%301:<2048x1xf32>, -1:i32) 
58391:4:0 %270:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%268:<2048x1xf32>, -1:i32) 
58394:7:0 %247:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%244:<2048x1xf32>, -1:i32) 
58389:2:0 %307:<2048x1xf32> = torch.2_1_0.aten::sub(%306:<2048x1xf32>, %293:<2048x1xf32>, alpha=1:i32) 
58389:2:0 %308:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%305:<2048x1xf32>, -1:i32) 
58390:3:0 %275:<2048x1xf32> = torch.2_1_0.aten::log(%276:<2048x1xf32>) 
58390:3:0 %277:<2048x1xf32> = torch.2_1_0.aten::sub(%275:<2048x1xf32>, %273:<2048x1xf32>, alpha=1:i32) 
58390:3:0 %278:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%276:<2048x1xf32>, -1:i32) 
58393:6:0 %292:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%292:<2048x1x6400xf32>, %306:<2048x1x1xf32>) 
58393:6:0 %304:<1x2048xf32> = torch.2_1_0.aten::transpose(%305:<2048x1xf32>, 0:i32, 1:i32) 
58393:6:0 %286:<1x2048xf32> = torch.2_1_0.aten::detach(%304:<1x2048xf32>) 
58394:7:0 %227:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%227:<2048x1x6400xf32>, %247:<2048x1x1xf32>) 
58387:0:0 %295:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%295:<2048x1x6400xf32>, %305:<2048x1x1xf32>) 
58393:None:0 %207:<i32> = torch.2_1_0.aten::lift_fresh(%207:<i32>) 
58394:7:0 %242:<1x2048xf32> = torch.2_1_0.aten::transpose(%246:<2048x1xf32>, 0:i32, 1:i32) 
58393:6:0 %286:<2048xf32> = torch.2_1_0.aten::view(%222:<1x2048xf32>, list{-1:i32}) 
58389:2:0 %272:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%272:<2048x1x6400xf32>, %308:<2048x1x1xf32>) 
58387:0:0 %303:<1x2048xf32> = torch.2_1_0.aten::transpose(%304:<2048x1xf32>, 0:i32, 1:i32) 
58394:7:0 %244:<1x2048xf32> = torch.2_1_0.aten::detach(%242:<1x2048xf32>) 
58387:0:0 %301:<1x2048xf32> = torch.2_1_0.aten::detach(%303:<1x2048xf32>) 
58393:6:0 %301:<f32> = torch.2_1_0.aten::sum(%286:<2048xf32>, dtype=None:NoneType) 
58389:2:0 %303:<1x2048xf32> = torch.2_1_0.aten::transpose(%307:<2048x1xf32>, 0:i32, 1:i32) 
58394:None:0 %90:<i32> = torch.2_1_0.aten::lift_fresh(%90:<i32>) 
58393:6:0 %288:<2048xf32> = torch.2_1_0.aten::view(%304:<1x2048xf32>, list{-1:i32}) 
58387:None:0 %240:<i32> = torch.2_1_0.aten::lift_fresh(%240:<i32>) 
58389:2:0 %293:<1x2048xf32> = torch.2_1_0.aten::detach(%303:<1x2048xf32>) 
58394:7:0 %228:<2048xf32> = torch.2_1_0.aten::view(%158:<1x2048xf32>, list{-1:i32}) 
58387:0:0 %260:<2048xf32> = torch.2_1_0.aten::view(%242:<1x2048xf32>, list{-1:i32}) 
58393:6:0 %300:<2048xf32> = torch.2_1_0.aten::mul(%288:<2048xf32>, %286:<2048xf32>) 
58389:None:0 %181:<i32> = torch.2_1_0.aten::lift_fresh(%181:<i32>) 
58387:0:0 %302:<f32> = torch.2_1_0.aten::sum(%260:<2048xf32>, dtype=None:NoneType) 
58393:6:0 %288:<f32> = torch.2_1_0.aten::sum(%300:<2048xf32>, dtype=None:NoneType) 
58394:7:0 %241:<f32> = torch.2_1_0.aten::sum(%228:<2048xf32>, dtype=None:NoneType) 
58387:0:0 %306:<2048xf32> = torch.2_1_0.aten::view(%303:<1x2048xf32>, list{-1:i32}) 
58389:2:0 %238:<2048xf32> = torch.2_1_0.aten::view(%234:<1x2048xf32>, list{-1:i32}) 
58393:6:0 %300:<1xf32> = torch.2_1_0.aten::view(%288:<f32>, list{1:i32}) 
58394:7:0 %237:<2048xf32> = torch.2_1_0.aten::view(%242:<1x2048xf32>, list{-1:i32}) 
58393:6:0 %302:<1xf32> = torch.2_1_0.aten::view(%301:<f32>, list{1:i32}) 
58389:2:0 %301:<f32> = torch.2_1_0.aten::sum(%238:<2048xf32>, dtype=None:NoneType) 
58389:2:0 %5:<2048xf32> = torch.2_1_0.aten::view(%303:<1x2048xf32>, list{-1:i32}) 
58394:7:0 %248:<2048xf32> = torch.2_1_0.aten::mul(%237:<2048xf32>, %228:<2048xf32>) 
58387:0:0 %291:<2048xf32> = torch.2_1_0.aten::mul(%306:<2048xf32>, %260:<2048xf32>) 
58389:2:0 %305:<2048xf32> = torch.2_1_0.aten::mul(%5:<2048xf32>, %238:<2048xf32>) 
58387:0:0 %306:<f32> = torch.2_1_0.aten::sum(%291:<2048xf32>, dtype=None:NoneType) 
58394:7:0 %247:<f32> = torch.2_1_0.aten::sum(%248:<2048xf32>, dtype=None:NoneType) 
58387:0:0 %300:<1xf32> = torch.2_1_0.aten::view(%306:<f32>, list{1:i32}) 
58394:7:0 %249:<1xf32> = torch.2_1_0.aten::view(%247:<f32>, list{1:i32}) 
58389:2:0 %282:<f32> = torch.2_1_0.aten::sum(%305:<2048xf32>, dtype=None:NoneType) 
58387:0:0 %298:<1xf32> = torch.2_1_0.aten::view(%302:<f32>, list{1:i32}) 
58394:7:0 %250:<1xf32> = torch.2_1_0.aten::view(%241:<f32>, list{1:i32}) 
58389:2:0 %306:<1xf32> = torch.2_1_0.aten::view(%282:<f32>, list{1:i32}) 
58389:2:0 %305:<1xf32> = torch.2_1_0.aten::view(%301:<f32>, list{1:i32}) 
58392:5:0 %205:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%205:<2048x1x6400xf32>, %218:<2048x1x1xf32>) 
58392:5:0 %214:<1x2048xf32> = torch.2_1_0.aten::transpose(%217:<2048x1xf32>, 0:i32, 1:i32) 
58392:5:0 %215:<1x2048xf32> = torch.2_1_0.aten::detach(%214:<1x2048xf32>) 
58392:None:0 %142:<i32> = torch.2_1_0.aten::lift_fresh(%142:<i32>) 
58391:4:0 %194:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%194:<2048x1x6400xf32>, %270:<2048x1x1xf32>) 
58392:5:0 %153:<2048xf32> = torch.2_1_0.aten::view(%136:<1x2048xf32>, list{-1:i32}) 
58392:5:0 %213:<f32> = torch.2_1_0.aten::sum(%153:<2048xf32>, dtype=None:NoneType) 
58391:4:0 %268:<1x2048xf32> = torch.2_1_0.aten::transpose(%269:<2048x1xf32>, 0:i32, 1:i32) 
58392:5:0 %215:<2048xf32> = torch.2_1_0.aten::view(%214:<1x2048xf32>, list{-1:i32}) 
58391:4:0 %255:<1x2048xf32> = torch.2_1_0.aten::detach(%268:<1x2048xf32>) 
58392:5:0 %210:<2048xf32> = torch.2_1_0.aten::mul(%215:<2048xf32>, %153:<2048xf32>) 
58392:5:0 %202:<f32> = torch.2_1_0.aten::sum(%210:<2048xf32>, dtype=None:NoneType) 
58391:None:0 %210:<i32> = torch.2_1_0.aten::lift_fresh(%210:<i32>) 
58392:5:0 %210:<1xf32> = torch.2_1_0.aten::view(%202:<f32>, list{1:i32}) 
58388:1:0 %288:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%288:<2048x1x6400xf32>, %304:<2048x1x1xf32>) 
58392:5:0 %215:<1xf32> = torch.2_1_0.aten::view(%213:<f32>, list{1:i32}) 
58391:4:0 %255:<2048xf32> = torch.2_1_0.aten::view(%211:<1x2048xf32>, list{-1:i32}) 
58388:1:0 %298:<1x2048xf32> = torch.2_1_0.aten::transpose(%303:<2048x1xf32>, 0:i32, 1:i32) 
58391:4:0 %265:<f32> = torch.2_1_0.aten::sum(%255:<2048xf32>, dtype=None:NoneType) 
58391:4:0 %266:<2048xf32> = torch.2_1_0.aten::view(%268:<1x2048xf32>, list{-1:i32}) 
58388:1:0 %301:<1x2048xf32> = torch.2_1_0.aten::detach(%298:<1x2048xf32>) 
58391:4:0 %271:<2048xf32> = torch.2_1_0.aten::mul(%266:<2048xf32>, %255:<2048xf32>) 
58388:None:0 %31:<i32> = torch.2_1_0.aten::lift_fresh(%31:<i32>) 
58391:4:0 %266:<f32> = torch.2_1_0.aten::sum(%271:<2048xf32>, dtype=None:NoneType) 
58391:4:0 %263:<1xf32> = torch.2_1_0.aten::view(%266:<f32>, list{1:i32}) 
58388:1:0 %296:<2048xf32> = torch.2_1_0.aten::view(%35:<1x2048xf32>, list{-1:i32}) 
58391:4:0 %262:<1xf32> = torch.2_1_0.aten::view(%265:<f32>, list{1:i32}) 
58388:1:0 %294:<f32> = torch.2_1_0.aten::sum(%296:<2048xf32>, dtype=None:NoneType) 
58388:1:0 %220:<2048xf32> = torch.2_1_0.aten::view(%298:<1x2048xf32>, list{-1:i32}) 
58388:1:0 %240:<2048xf32> = torch.2_1_0.aten::mul(%220:<2048xf32>, %296:<2048xf32>) 
58388:1:0 %305:<f32> = torch.2_1_0.aten::sum(%240:<2048xf32>, dtype=None:NoneType) 
58388:1:0 %220:<1xf32> = torch.2_1_0.aten::view(%305:<f32>, list{1:i32}) 
58388:1:0 %304:<1xf32> = torch.2_1_0.aten::view(%294:<f32>, list{1:i32}) 
58390:3:0 %264:<2048x1x6400xf32> = torch.2_1_0.aten::div_(%264:<2048x1x6400xf32>, %278:<2048x1x1xf32>) 
58390:3:0 %279:<1x2048xf32> = torch.2_1_0.aten::transpose(%277:<2048x1xf32>, 0:i32, 1:i32) 
58390:3:0 %263:<1x2048xf32> = torch.2_1_0.aten::detach(%279:<1x2048xf32>) 
58390:None:0 %182:<i32> = torch.2_1_0.aten::lift_fresh(%182:<i32>) 
58390:3:0 %275:<2048xf32> = torch.2_1_0.aten::view(%198:<1x2048xf32>, list{-1:i32}) 
58390:3:0 %276:<f32> = torch.2_1_0.aten::sum(%275:<2048xf32>, dtype=None:NoneType) 
58393:6:0 %289:<2xf32> = torch.2_1_0.aten::cat(list{%300:<1xf32>, %302:<1xf32>}, 0:i32) 
58393:6:0 %300:<2xf32> = torch.2_1_0.aten::clone(%289:<2xf32>, memory_format=None:NoneType) 
58393:6:0 %288:<2xf32> = torch.2_1_0.aten::detach(%300:<2xf32>) 
58387:0:0 %305:<2xf32> = torch.2_1_0.aten::cat(list{%300:<1xf32>, %298:<1xf32>}, 0:i32) 
58393:6:0 %302:<2xf32> = torch.2_1_0.aten::detach(%288:<2xf32>) 
58387:0:0 %300:<2xf32> = torch.2_1_0.aten::clone(%305:<2xf32>, memory_format=None:NoneType) 
58387:0:0 %293:<2xf32> = torch.2_1_0.aten::detach(%300:<2xf32>) 
58387:0:0 %298:<2xf32> = torch.2_1_0.aten::detach(%293:<2xf32>) 
58394:7:0 %251:<2xf32> = torch.2_1_0.aten::cat(list{%249:<1xf32>, %250:<1xf32>}, 0:i32) 
58394:7:0 %247:<2xf32> = torch.2_1_0.aten::clone(%251:<2xf32>, memory_format=None:NoneType) 
58389:2:0 %309:<2xf32> = torch.2_1_0.aten::cat(list{%306:<1xf32>, %305:<1xf32>}, 0:i32) 
58394:7:0 %250:<2xf32> = torch.2_1_0.aten::detach(%247:<2xf32>) 
58390:3:0 %274:<2048xf32> = torch.2_1_0.aten::view(%279:<1x2048xf32>, list{-1:i32}) 
58394:7:0 %252:<2xf32> = torch.2_1_0.aten::detach(%250:<2xf32>) 
58389:2:0 %282:<2xf32> = torch.2_1_0.aten::clone(%309:<2xf32>, memory_format=None:NoneType) 
58389:2:0 %306:<2xf32> = torch.2_1_0.aten::detach(%282:<2xf32>) 
58389:2:0 %305:<2xf32> = torch.2_1_0.aten::detach(%306:<2xf32>) 
58390:3:0 %272:<2048xf32> = torch.2_1_0.aten::mul(%274:<2048xf32>, %275:<2048xf32>) 
58390:3:0 %274:<f32> = torch.2_1_0.aten::sum(%272:<2048xf32>, dtype=None:NoneType) 
58390:3:0 %272:<1xf32> = torch.2_1_0.aten::view(%274:<f32>, list{1:i32}) 
58390:3:0 %263:<1xf32> = torch.2_1_0.aten::view(%276:<f32>, list{1:i32}) 
58388:1:0 %299:<2xf32> = torch.2_1_0.aten::cat(list{%220:<1xf32>, %304:<1xf32>}, 0:i32) 
58388:1:0 %305:<2xf32> = torch.2_1_0.aten::clone(%299:<2xf32>, memory_format=None:NoneType) 
58388:1:0 %304:<2xf32> = torch.2_1_0.aten::detach(%305:<2xf32>) 
58388:1:0 %306:<2xf32> = torch.2_1_0.aten::detach(%304:<2xf32>) 
58391:4:0 %271:<2xf32> = torch.2_1_0.aten::cat(list{%263:<1xf32>, %262:<1xf32>}, 0:i32) 
58391:4:0 %266:<2xf32> = torch.2_1_0.aten::clone(%271:<2xf32>, memory_format=None:NoneType) 
58391:4:0 %263:<2xf32> = torch.2_1_0.aten::detach(%266:<2xf32>) 
58391:4:0 %258:<2xf32> = torch.2_1_0.aten::detach(%263:<2xf32>) 
58392:5:0 %218:<2xf32> = torch.2_1_0.aten::cat(list{%210:<1xf32>, %215:<1xf32>}, 0:i32) 
58392:5:0 %202:<2xf32> = torch.2_1_0.aten::clone(%218:<2xf32>, memory_format=None:NoneType) 
58392:5:0 %215:<2xf32> = torch.2_1_0.aten::detach(%202:<2xf32>) 
58392:5:0 %211:<2xf32> = torch.2_1_0.aten::detach(%215:<2xf32>) 
58390:3:0 %266:<2xf32> = torch.2_1_0.aten::cat(list{%272:<1xf32>, %263:<1xf32>}, 0:i32) 
58390:3:0 %272:<2xf32> = torch.2_1_0.aten::clone(%266:<2xf32>, memory_format=None:NoneType) 
58390:3:0 %274:<2xf32> = torch.2_1_0.aten::detach(%272:<2xf32>) 
58390:3:0 %263:<2xf32> = torch.2_1_0.aten::detach(%274:<2xf32>) 
58388:None:0 list{%306:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%306:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %11:<f32>+1 = torch.2_1_0.aten::select(%299:<2xf32>, 0:i32, 1:i32) 
58388:1:0 %304:<f32> = torch.2_1_0.aten::clone(%11:<f32>+1, memory_format=None:NoneType) 
58388:1:0 %302:<f32> = torch.2_1_0.aten::detach(%304:<f32>) 
58388:1:0 %305:<f32> = torch.2_1_0.aten::detach(%302:<f32>) 
58388:1:0 %302:<i32> = torch.2_1_0.aten::_to_copy(%305:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %304:<f32> = torch.2_1_0.aten::select(%299:<2xf32>, 0:i32, 0:i32) 
58388:1:0 %307:<f32> = torch.2_1_0.aten::mul(%304:<f32>, 1:i32) 
58388:1:0 %305:<f32> = torch.2_1_0.aten::select(%306:<2xf32>, 0:i32, 0:i32) 
58388:1:0 %308:<f32>+1 = torch.2_1_0.aten::select(%306:<2xf32>, 0:i32, 1:i32) 
58388:1:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, %302:<i32>) 
58388:1:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, 1:i32) 
58388:1:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%302:<i32>) 
58388:1:0 %59:<i32> = torch.2_1_0.aten::add_(%59:<i32>, 2048:i32, alpha=1:i32) 
58388:1:0 %299:<1xf32> = torch.2_1_0.aten::mul(%29:<1xf32>, %307:<f32>) 
58388:1:0 %309:<1xf32> = torch.2_1_0.aten::ones_like(%299:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58391:None:0 list{%258:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%258:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %266:<f32>+1 = torch.2_1_0.aten::select(%271:<2xf32>, 0:i32, 1:i32) 
58391:4:0 %262:<f32> = torch.2_1_0.aten::clone(%266:<f32>+1, memory_format=None:NoneType) 
58391:4:0 %266:<f32> = torch.2_1_0.aten::detach(%262:<f32>) 
58391:4:0 %270:<f32> = torch.2_1_0.aten::detach(%266:<f32>) 
58391:4:0 %262:<i32> = torch.2_1_0.aten::_to_copy(%270:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %266:<f32> = torch.2_1_0.aten::select(%271:<2xf32>, 0:i32, 0:i32) 
58391:4:0 %272:<f32> = torch.2_1_0.aten::mul(%266:<f32>, 1:i32) 
58391:4:0 %266:<f32> = torch.2_1_0.aten::select(%258:<2xf32>, 0:i32, 0:i32) 
58391:4:0 %263:<f32>+1 = torch.2_1_0.aten::select(%258:<2xf32>, 0:i32, 1:i32) 
58391:4:0 %272:<f32> = torch.2_1_0.aten::div_(%272:<f32>, %262:<i32>) 
58391:4:0 %272:<f32> = torch.2_1_0.aten::div_(%272:<f32>, 1:i32) 
58391:4:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%262:<i32>) 
58388:1:0 %310:<1xf32> = torch.2_1_0.aten::mul(%309:<1xf32>, %29:<1xf32>) 
58391:4:0 %209:<i32> = torch.2_1_0.aten::add_(%209:<i32>, 2048:i32, alpha=1:i32) 
58388:1:0 %311:<f32> = torch.2_1_0.aten::sum(%310:<1xf32>, dtype=None:NoneType) 
58388:1:0 %310:<f32> = torch.2_1_0.aten::div(%311:<f32>, 1:i32) 
58391:4:0 %181:<1xf32> = torch.2_1_0.aten::mul(%35:<1xf32>, %272:<f32>) 
58388:1:0 %312:<f32> = torch.2_1_0.aten::div(%310:<f32>, %302:<i32>) 
58388:1:0 %311:<f32> = torch.2_1_0.aten::mul(%312:<f32>, 1:i32) 
58391:4:0 %268:<1xf32> = torch.2_1_0.aten::ones_like(%181:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58388:1:0 %220:<2xf32> = torch.2_1_0.aten::select_backward(%311:<f32>, list{2:i32}, 0:i32, 0:i32) 
58388:1:0 %310:<1xf32> = torch.2_1_0.aten::slice(%220:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58388:1:0 %313:<1xf32>+1 = torch.2_1_0.aten::slice(%220:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58388:1:0 %314:<f32> = torch.2_1_0.aten::view(%310:<1xf32>, list{}) 
58388:1:0 %312:<2048xf32>{0} = torch.2_1_0.aten::expand(%314:<f32>, list{2048:i32}, implicit=False:pred) 
58388:1:0 %315:<2048xf32> = torch.2_1_0.aten::mul(%312:<2048xf32>{0}, %296:<2048xf32>) 
58388:1:0 %220:<1x2048xf32> = torch.2_1_0.aten::view(%315:<2048xf32>, list{1:i32, 2048:i32}) 
58388:1:0 %316:<2048x1xf32> = torch.2_1_0.aten::transpose(%220:<1x2048xf32>, 0:i32, 1:i32) 
58388:1:0 %312:<2048x6400xf32> = torch.2_1_0.aten::view(%288:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58388:1:0 %317:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:1:device, pin_memory=False:pred) 
58388:1:0 %318:<2048xunknown> = torch.2_1_0.aten::view(%290:<2048x1xunknown>, list{-1:i32}) 
58388:1:0 %319:<2048xf32> = torch.2_1_0.aten::_to_copy(%318:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %320:<2048xf32> = torch.2_1_0.aten::rsub(%319:<2048xf32>, 1.0:f32, 1:i32) 
58388:1:0 %318:<2048xf32> = torch.2_1_0.aten::index(%312:<2048x6400xf32>, list{%317:<2048xi64>, %295:<2048xi64>}) 
58388:1:0 %318:<2048xf32> = torch.2_1_0.aten::sub_(%318:<2048xf32>, %320:<2048xf32>, alpha=1:i32) 
58388:1:0 %312:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%312:<2048x6400xf32>, list{%317:<2048xi64>, %295:<2048xi64>}, %318:<2048xf32>, False:pred) 
58388:1:0 %319:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%316:<2048x1xf32>, -1:i32) 
58388:1:0 %288:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%288:<2048x1x6400xf32>, %319:<2048x1x1xf32>) 
58388:1:0 %158:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%288:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %288:<2048x6400xf16> = torch.2_1_0.aten::view(%158:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58387:None:0 list{%298:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%298:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %300:<f32>+1 = torch.2_1_0.aten::select(%305:<2xf32>, 0:i32, 1:i32) 
58387:0:0 %306:<f32> = torch.2_1_0.aten::clone(%300:<f32>+1, memory_format=None:NoneType) 
58387:0:0 %291:<f32> = torch.2_1_0.aten::detach(%306:<f32>) 
58387:0:0 %293:<f32> = torch.2_1_0.aten::detach(%291:<f32>) 
58387:0:0 %291:<i32> = torch.2_1_0.aten::_to_copy(%293:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %293:<f32> = torch.2_1_0.aten::select(%305:<2xf32>, 0:i32, 0:i32) 
58387:0:0 %307:<f32> = torch.2_1_0.aten::mul(%293:<f32>, 1:i32) 
58387:0:0 %293:<f32> = torch.2_1_0.aten::select(%298:<2xf32>, 0:i32, 0:i32) 
58387:0:0 %296:<f32>+1 = torch.2_1_0.aten::select(%298:<2xf32>, 0:i32, 1:i32) 
58387:0:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, %291:<i32>) 
58387:0:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, 1:i32) 
58387:0:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%291:<i32>) 
58387:0:0 %236:<i32> = torch.2_1_0.aten::add_(%236:<i32>, 2048:i32, alpha=1:i32) 
58387:0:0 %308:<1xf32> = torch.2_1_0.aten::mul(%133:<1xf32>, %307:<f32>) 
58387:0:0 %303:<1xf32> = torch.2_1_0.aten::ones_like(%308:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58392:None:0 list{%211:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%211:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %215:<f32>+1 = torch.2_1_0.aten::select(%218:<2xf32>, 0:i32, 1:i32) 
58392:5:0 %210:<f32> = torch.2_1_0.aten::clone(%215:<f32>+1, memory_format=None:NoneType) 
58392:5:0 %204:<f32> = torch.2_1_0.aten::detach(%210:<f32>) 
58392:5:0 %202:<f32> = torch.2_1_0.aten::detach(%204:<f32>) 
58392:5:0 %204:<i32> = torch.2_1_0.aten::_to_copy(%202:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %202:<f32> = torch.2_1_0.aten::select(%218:<2xf32>, 0:i32, 0:i32) 
58392:5:0 %212:<f32> = torch.2_1_0.aten::mul(%202:<f32>, 1:i32) 
58392:5:0 %202:<f32> = torch.2_1_0.aten::select(%211:<2xf32>, 0:i32, 0:i32) 
58392:5:0 %210:<f32>+1 = torch.2_1_0.aten::select(%211:<2xf32>, 0:i32, 1:i32) 
58392:5:0 %212:<f32> = torch.2_1_0.aten::div_(%212:<f32>, %204:<i32>) 
58392:5:0 %212:<f32> = torch.2_1_0.aten::div_(%212:<f32>, 1:i32) 
58392:5:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%204:<i32>) 
58392:5:0 %141:<i32> = torch.2_1_0.aten::add_(%141:<i32>, 2048:i32, alpha=1:i32) 
58392:5:0 %214:<1xf32> = torch.2_1_0.aten::mul(%42:<1xf32>, %212:<f32>) 
58392:5:0 %218:<1xf32> = torch.2_1_0.aten::ones_like(%214:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58388:1:0 %295:<2048x12288xf16> = torch.2_1_0.aten::mm(%288:<2048x6400xf16>, %113:<6400x12288xf16>) 
58388:1:0 %220:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%295:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %320:<2048x6400xf16> = torch.2_1_0.aten::view(%158:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58388:1:0 %316:<2048x12288xf16> = torch.2_1_0.aten::view(%283:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:None:0 list{%220:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%220:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %315:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%320:<2048x6400xf16>) 
58388:1:0 %312:<6400x12288xf16> = torch.2_1_0.aten::mm(%315:<6400x2048xf16>{1,6400}, %316:<2048x12288xf16>) 
58388:1:0 %283:<2048x12288xf16> = torch.2_1_0.aten::view(%220:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %316:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%256:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %315:<12288xf16> = torch.2_1_0.aten::empty_like(%122:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %319:<12288xf16> = torch.2_1_0.aten::empty_like(%122:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %213:<1xf32> = torch.2_1_0.aten::mul(%218:<1xf32>, %42:<1xf32>) 
58391:4:0 %271:<1xf32> = torch.2_1_0.aten::mul(%268:<1xf32>, %35:<1xf32>) 
58391:4:0 %269:<f32> = torch.2_1_0.aten::sum(%271:<1xf32>, dtype=None:NoneType) 
58391:4:0 %257:<f32> = torch.2_1_0.aten::div(%269:<f32>, 1:i32) 
58391:4:0 %269:<f32> = torch.2_1_0.aten::div(%257:<f32>, %262:<i32>) 
58391:4:0 %271:<f32> = torch.2_1_0.aten::mul(%269:<f32>, 1:i32) 
58391:4:0 %257:<2xf32> = torch.2_1_0.aten::select_backward(%271:<f32>, list{2:i32}, 0:i32, 0:i32) 
58391:4:0 %269:<1xf32> = torch.2_1_0.aten::slice(%257:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58391:4:0 %210:<1xf32>+1 = torch.2_1_0.aten::slice(%257:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58391:4:0 %265:<f32> = torch.2_1_0.aten::view(%269:<1xf32>, list{}) 
58391:4:0 %273:<2048xf32>{0} = torch.2_1_0.aten::expand(%265:<f32>, list{2048:i32}, implicit=False:pred) 
58391:4:0 %265:<2048xf32> = torch.2_1_0.aten::mul(%273:<2048xf32>{0}, %255:<2048xf32>) 
58391:4:0 %257:<1x2048xf32> = torch.2_1_0.aten::view(%265:<2048xf32>, list{1:i32, 2048:i32}) 
58391:4:0 %273:<2048x1xf32> = torch.2_1_0.aten::transpose(%257:<1x2048xf32>, 0:i32, 1:i32) 
58391:4:0 %257:<2048x6400xf32> = torch.2_1_0.aten::view(%194:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58391:4:0 %274:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:4:device, pin_memory=False:pred) 
58391:4:0 %269:<2048xunknown> = torch.2_1_0.aten::view(%260:<2048x1xunknown>, list{-1:i32}) 
58391:4:0 %264:<2048xf32> = torch.2_1_0.aten::_to_copy(%269:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %210:<2048xf32> = torch.2_1_0.aten::rsub(%264:<2048xf32>, 1.0:f32, 1:i32) 
58391:4:0 %264:<2048xf32> = torch.2_1_0.aten::index(%257:<2048x6400xf32>, list{%274:<2048xi64>, %261:<2048xi64>}) 
58391:4:0 %264:<2048xf32> = torch.2_1_0.aten::sub_(%264:<2048xf32>, %210:<2048xf32>, alpha=1:i32) 
58391:4:0 %257:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%257:<2048x6400xf32>, list{%274:<2048xi64>, %261:<2048xi64>}, %264:<2048xf32>, False:pred) 
58391:4:0 %255:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%273:<2048x1xf32>, -1:i32) 
58391:4:0 %194:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%194:<2048x1x6400xf32>, %255:<2048x1x1xf32>) 
58391:4:0 %273:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%194:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %194:<2048x6400xf16> = torch.2_1_0.aten::view(%273:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58392:5:0 %217:<f32> = torch.2_1_0.aten::sum(%213:<1xf32>, dtype=None:NoneType) 
58387:0:0 %309:<1xf32> = torch.2_1_0.aten::mul(%303:<1xf32>, %133:<1xf32>) 
58392:5:0 %176:<f32> = torch.2_1_0.aten::div(%217:<f32>, 1:i32) 
58392:5:0 %217:<f32> = torch.2_1_0.aten::div(%176:<f32>, %204:<i32>) 
58387:0:0 %306:<f32> = torch.2_1_0.aten::sum(%309:<1xf32>, dtype=None:NoneType) 
58392:5:0 %219:<f32> = torch.2_1_0.aten::mul(%217:<f32>, 1:i32) 
58387:0:0 %310:<f32> = torch.2_1_0.aten::div(%306:<f32>, 1:i32) 
58387:0:0 %305:<f32> = torch.2_1_0.aten::div(%310:<f32>, %291:<i32>) 
58392:5:0 %220:<2xf32> = torch.2_1_0.aten::select_backward(%219:<f32>, list{2:i32}, 0:i32, 0:i32) 
58387:0:0 %306:<f32> = torch.2_1_0.aten::mul(%305:<f32>, 1:i32) 
58392:5:0 %176:<1xf32> = torch.2_1_0.aten::slice(%220:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58392:5:0 %213:<1xf32>+1 = torch.2_1_0.aten::slice(%220:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58392:5:0 %217:<f32> = torch.2_1_0.aten::view(%176:<1xf32>, list{}) 
58387:0:0 %300:<2xf32> = torch.2_1_0.aten::select_backward(%306:<f32>, list{2:i32}, 0:i32, 0:i32) 
58392:5:0 %176:<2048xf32>{0} = torch.2_1_0.aten::expand(%217:<f32>, list{2048:i32}, implicit=False:pred) 
58387:0:0 %310:<1xf32> = torch.2_1_0.aten::slice(%300:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58387:0:0 %304:<1xf32>+1 = torch.2_1_0.aten::slice(%300:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58392:5:0 %221:<2048xf32> = torch.2_1_0.aten::mul(%176:<2048xf32>{0}, %153:<2048xf32>) 
58387:0:0 %305:<f32> = torch.2_1_0.aten::view(%310:<1xf32>, list{}) 
58392:5:0 %215:<1x2048xf32> = torch.2_1_0.aten::view(%221:<2048xf32>, list{1:i32, 2048:i32}) 
58387:0:0 %310:<2048xf32>{0} = torch.2_1_0.aten::expand(%305:<f32>, list{2048:i32}, implicit=False:pred) 
58392:5:0 %136:<2048x1xf32> = torch.2_1_0.aten::transpose(%215:<1x2048xf32>, 0:i32, 1:i32) 
58392:5:0 %153:<2048x6400xf32> = torch.2_1_0.aten::view(%205:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58387:0:0 %311:<2048xf32> = torch.2_1_0.aten::mul(%310:<2048xf32>{0}, %260:<2048xf32>) 
58392:5:0 %217:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:5:device, pin_memory=False:pred) 
58387:0:0 %242:<1x2048xf32> = torch.2_1_0.aten::view(%311:<2048xf32>, list{1:i32, 2048:i32}) 
58392:5:0 %219:<2048xunknown> = torch.2_1_0.aten::view(%207:<2048x1xunknown>, list{-1:i32}) 
58387:0:0 %260:<2048x1xf32> = torch.2_1_0.aten::transpose(%242:<1x2048xf32>, 0:i32, 1:i32) 
58387:0:0 %242:<2048x6400xf32> = torch.2_1_0.aten::view(%295:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58392:5:0 %176:<2048xf32> = torch.2_1_0.aten::_to_copy(%219:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %310:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:0:device, pin_memory=False:pred) 
58387:0:0 %300:<2048xunknown> = torch.2_1_0.aten::view(%297:<2048x1xunknown>, list{-1:i32}) 
58387:0:0 %304:<2048xf32> = torch.2_1_0.aten::_to_copy(%300:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %306:<2048xf32> = torch.2_1_0.aten::rsub(%304:<2048xf32>, 1.0:f32, 1:i32) 
58387:0:0 %300:<2048xf32> = torch.2_1_0.aten::index(%242:<2048x6400xf32>, list{%310:<2048xi64>, %299:<2048xi64>}) 
58387:0:0 %300:<2048xf32> = torch.2_1_0.aten::sub_(%300:<2048xf32>, %306:<2048xf32>, alpha=1:i32) 
58387:0:0 %242:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%242:<2048x6400xf32>, list{%310:<2048xi64>, %299:<2048xi64>}, %300:<2048xf32>, False:pred) 
58387:0:0 %305:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%260:<2048x1xf32>, -1:i32) 
58387:0:0 %295:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%295:<2048x1x6400xf32>, %305:<2048x1x1xf32>) 
58387:0:0 %311:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%295:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %295:<2048x6400xf16> = torch.2_1_0.aten::view(%311:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58392:5:0 %215:<2048xf32> = torch.2_1_0.aten::rsub(%176:<2048xf32>, 1.0:f32, 1:i32) 
58392:5:0 %213:<2048xf32> = torch.2_1_0.aten::index(%153:<2048x6400xf32>, list{%217:<2048xi64>, %168:<2048xi64>}) 
58392:5:0 %213:<2048xf32> = torch.2_1_0.aten::sub_(%213:<2048xf32>, %215:<2048xf32>, alpha=1:i32) 
58392:5:0 %153:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%153:<2048x6400xf32>, list{%217:<2048xi64>, %168:<2048xi64>}, %213:<2048xf32>, False:pred) 
58392:5:0 %219:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%136:<2048x1xf32>, -1:i32) 
58387:0:0 %299:<2048x12288xf16> = torch.2_1_0.aten::mm(%295:<2048x6400xf16>, %116:<6400x12288xf16>) 
58387:0:0 %312:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%299:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %297:<2048x6400xf16> = torch.2_1_0.aten::view(%311:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58392:5:0 %205:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%205:<2048x1x6400xf32>, %219:<2048x1x1xf32>) 
58387:0:0 %300:<2048x12288xf16> = torch.2_1_0.aten::view(%290:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %168:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%205:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %185:<2048x6400xf16> = torch.2_1_0.aten::view(%168:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58387:None:0 list{%312:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%312:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %294:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%297:<2048x6400xf16>) 
58392:5:0 %205:<2048x12288xf16> = torch.2_1_0.aten::mm(%185:<2048x6400xf16>, %24:<6400x12288xf16>) 
58392:5:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%205:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %221:<2048x6400xf16> = torch.2_1_0.aten::view(%168:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58392:5:0 %209:<2048x12288xf16> = torch.2_1_0.aten::view(%197:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:None:0 list{%74:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%74:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %215:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%221:<2048x6400xf16>) 
58387:0:0 %310:<6400x12288xf16> = torch.2_1_0.aten::mm(%294:<6400x2048xf16>{1,6400}, %300:<2048x12288xf16>) 
58387:0:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %213:<6400x12288xf16> = torch.2_1_0.aten::mm(%215:<6400x2048xf16>{1,6400}, %209:<2048x12288xf16>) 
58387:0:0 %313:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%288:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %290:<12288xf16> = torch.2_1_0.aten::empty_like(%48:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %197:<2048x12288xf16> = torch.2_1_0.aten::view(%74:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %209:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%151:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %62:<12288xf16> = torch.2_1_0.aten::empty_like(%48:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %222:<12288xf16> = torch.2_1_0.aten::empty_like(%38:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %223:<12288xf16> = torch.2_1_0.aten::empty_like(%38:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %260:<2048x12288xf16> = torch.2_1_0.aten::mm(%194:<2048x6400xf16>, %106:<6400x12288xf16>) 
58391:4:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%260:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %260:<2048x6400xf16> = torch.2_1_0.aten::view(%273:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58391:4:0 %210:<2048x12288xf16> = torch.2_1_0.aten::view(%218:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:None:0 list{%74:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%74:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %240:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%260:<2048x6400xf16>) 
58389:None:0 list{%305:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%305:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %282:<f32>+1 = torch.2_1_0.aten::select(%309:<2xf32>, 0:i32, 1:i32) 
58389:2:0 %306:<f32> = torch.2_1_0.aten::clone(%282:<f32>+1, memory_format=None:NoneType) 
58389:2:0 %282:<f32> = torch.2_1_0.aten::detach(%306:<f32>) 
58389:2:0 %308:<f32> = torch.2_1_0.aten::detach(%282:<f32>) 
58389:2:0 %282:<i32> = torch.2_1_0.aten::_to_copy(%308:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %308:<f32> = torch.2_1_0.aten::select(%309:<2xf32>, 0:i32, 0:i32) 
58389:2:0 %310:<f32> = torch.2_1_0.aten::mul(%308:<f32>, 1:i32) 
58389:2:0 %308:<f32> = torch.2_1_0.aten::select(%305:<2xf32>, 0:i32, 0:i32) 
58389:2:0 %306:<f32>+1 = torch.2_1_0.aten::select(%305:<2xf32>, 0:i32, 1:i32) 
58389:2:0 %310:<f32> = torch.2_1_0.aten::div_(%310:<f32>, %282:<i32>) 
58389:2:0 %310:<f32> = torch.2_1_0.aten::div_(%310:<f32>, 1:i32) 
58389:2:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%282:<i32>) 
58389:2:0 %178:<i32> = torch.2_1_0.aten::add_(%178:<i32>, 2048:i32, alpha=1:i32) 
58389:2:0 %298:<1xf32> = torch.2_1_0.aten::mul(%71:<1xf32>, %310:<f32>) 
58389:2:0 %307:<1xf32> = torch.2_1_0.aten::ones_like(%298:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58391:4:0 %257:<6400x12288xf16> = torch.2_1_0.aten::mm(%240:<6400x2048xf16>{1,6400}, %210:<2048x12288xf16>) 
58391:4:0 %251:<2048x12288xf16> = torch.2_1_0.aten::view(%74:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %210:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%252:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %218:<12288xf16> = torch.2_1_0.aten::empty_like(%134:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %240:<12288xf16> = torch.2_1_0.aten::empty_like(%134:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %311:<1xf32> = torch.2_1_0.aten::mul(%307:<1xf32>, %71:<1xf32>) 
58389:2:0 %309:<f32> = torch.2_1_0.aten::sum(%311:<1xf32>, dtype=None:NoneType) 
58389:2:0 %312:<f32> = torch.2_1_0.aten::div(%309:<f32>, 1:i32) 
58389:2:0 %313:<f32> = torch.2_1_0.aten::div(%312:<f32>, %282:<i32>) 
58389:2:0 %309:<f32> = torch.2_1_0.aten::mul(%313:<f32>, 1:i32) 
58389:2:0 %5:<2xf32> = torch.2_1_0.aten::select_backward(%309:<f32>, list{2:i32}, 0:i32, 0:i32) 
58389:2:0 %313:<1xf32> = torch.2_1_0.aten::slice(%5:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58389:2:0 %312:<1xf32>+1 = torch.2_1_0.aten::slice(%5:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58389:2:0 %311:<f32> = torch.2_1_0.aten::view(%313:<1xf32>, list{}) 
58389:2:0 %313:<2048xf32>{0} = torch.2_1_0.aten::expand(%311:<f32>, list{2048:i32}, implicit=False:pred) 
58389:2:0 %314:<2048xf32> = torch.2_1_0.aten::mul(%313:<2048xf32>{0}, %238:<2048xf32>) 
58389:2:0 %5:<1x2048xf32> = torch.2_1_0.aten::view(%314:<2048xf32>, list{1:i32, 2048:i32}) 
58389:2:0 %309:<2048x1xf32> = torch.2_1_0.aten::transpose(%5:<1x2048xf32>, 0:i32, 1:i32) 
58389:2:0 %315:<2048x6400xf32> = torch.2_1_0.aten::view(%272:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58389:2:0 %312:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:2:device, pin_memory=False:pred) 
58389:2:0 %313:<2048xunknown> = torch.2_1_0.aten::view(%295:<2048x1xunknown>, list{-1:i32}) 
58389:2:0 %316:<2048xf32> = torch.2_1_0.aten::_to_copy(%313:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %317:<2048xf32> = torch.2_1_0.aten::rsub(%316:<2048xf32>, 1.0:f32, 1:i32) 
58389:2:0 %316:<2048xf32> = torch.2_1_0.aten::index(%315:<2048x6400xf32>, list{%312:<2048xi64>, %300:<2048xi64>}) 
58389:2:0 %316:<2048xf32> = torch.2_1_0.aten::sub_(%316:<2048xf32>, %317:<2048xf32>, alpha=1:i32) 
58389:2:0 %315:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%315:<2048x6400xf32>, list{%312:<2048xi64>, %300:<2048xi64>}, %316:<2048xf32>, False:pred) 
58389:2:0 %313:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%309:<2048x1xf32>, -1:i32) 
58389:2:0 %272:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%272:<2048x1x6400xf32>, %313:<2048x1x1xf32>) 
58389:2:0 %5:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%272:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %300:<2048x6400xf16> = torch.2_1_0.aten::view(%5:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58389:2:0 %315:<2048x12288xf16> = torch.2_1_0.aten::mm(%300:<2048x6400xf16>, %98:<6400x12288xf16>) 
58389:2:0 %297:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%315:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %318:<2048x6400xf16> = torch.2_1_0.aten::view(%5:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58389:2:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%288:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:None:0 list{%297:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%297:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %314:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%318:<2048x6400xf16>) 
58389:2:0 %309:<6400x12288xf16> = torch.2_1_0.aten::mm(%314:<6400x2048xf16>{1,6400}, %311:<2048x12288xf16>) 
58389:2:0 %319:<2048x12288xf16> = torch.2_1_0.aten::view(%297:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %314:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%277:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %291:<12288xf16> = torch.2_1_0.aten::empty_like(%42:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %311:<12288xf16> = torch.2_1_0.aten::empty_like(%42:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:None:0 list{%302:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%302:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %300:<f32>+1 = torch.2_1_0.aten::select(%289:<2xf32>, 0:i32, 1:i32) 
58393:6:0 %288:<f32> = torch.2_1_0.aten::clone(%300:<f32>+1, memory_format=None:NoneType) 
58393:6:0 %300:<f32> = torch.2_1_0.aten::detach(%288:<f32>) 
58393:6:0 %298:<f32> = torch.2_1_0.aten::detach(%300:<f32>) 
58393:6:0 %288:<i32> = torch.2_1_0.aten::_to_copy(%298:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %298:<f32> = torch.2_1_0.aten::select(%289:<2xf32>, 0:i32, 0:i32) 
58393:6:0 %307:<f32> = torch.2_1_0.aten::mul(%298:<f32>, 1:i32) 
58393:6:0 %306:<f32> = torch.2_1_0.aten::select(%302:<2xf32>, 0:i32, 0:i32) 
58393:6:0 %308:<f32>+1 = torch.2_1_0.aten::select(%302:<2xf32>, 0:i32, 1:i32) 
58393:6:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, %288:<i32>) 
58393:6:0 %307:<f32> = torch.2_1_0.aten::div_(%307:<f32>, 1:i32) 
58393:6:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%288:<i32>) 
58393:6:0 %92:<i32> = torch.2_1_0.aten::add_(%92:<i32>, 2048:i32, alpha=1:i32) 
58393:6:0 %305:<1xf32> = torch.2_1_0.aten::mul(%159:<1xf32>, %307:<f32>) 
58393:6:0 %207:<1xf32> = torch.2_1_0.aten::ones_like(%305:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58393:6:0 %309:<1xf32> = torch.2_1_0.aten::mul(%207:<1xf32>, %159:<1xf32>) 
58393:6:0 %310:<f32> = torch.2_1_0.aten::sum(%309:<1xf32>, dtype=None:NoneType) 
58393:6:0 %301:<f32> = torch.2_1_0.aten::div(%310:<f32>, 1:i32) 
58393:6:0 %311:<f32> = torch.2_1_0.aten::div(%301:<f32>, %288:<i32>) 
58393:6:0 %309:<f32> = torch.2_1_0.aten::mul(%311:<f32>, 1:i32) 
58393:6:0 %245:<2xf32> = torch.2_1_0.aten::select_backward(%309:<f32>, list{2:i32}, 0:i32, 0:i32) 
58393:6:0 %301:<1xf32> = torch.2_1_0.aten::slice(%245:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58393:6:0 %311:<1xf32>+1 = torch.2_1_0.aten::slice(%245:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58393:6:0 %289:<f32> = torch.2_1_0.aten::view(%301:<1xf32>, list{}) 
58393:6:0 %301:<2048xf32>{0} = torch.2_1_0.aten::expand(%289:<f32>, list{2048:i32}, implicit=False:pred) 
58393:6:0 %309:<2048xf32> = torch.2_1_0.aten::mul(%301:<2048xf32>{0}, %286:<2048xf32>) 
58393:6:0 %286:<1x2048xf32> = torch.2_1_0.aten::view(%309:<2048xf32>, list{1:i32, 2048:i32}) 
58393:6:0 %301:<2048x1xf32> = torch.2_1_0.aten::transpose(%286:<1x2048xf32>, 0:i32, 1:i32) 
58393:6:0 %312:<2048x6400xf32> = torch.2_1_0.aten::view(%292:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58393:6:0 %303:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:6:device, pin_memory=False:pred) 
58393:6:0 %286:<2048xunknown> = torch.2_1_0.aten::view(%295:<2048x1xunknown>, list{-1:i32}) 
58393:6:0 %310:<2048xf32> = torch.2_1_0.aten::_to_copy(%286:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %311:<2048xf32> = torch.2_1_0.aten::rsub(%310:<2048xf32>, 1.0:f32, 1:i32) 
58393:6:0 %310:<2048xf32> = torch.2_1_0.aten::index(%312:<2048x6400xf32>, list{%303:<2048xi64>, %299:<2048xi64>}) 
58393:6:0 %310:<2048xf32> = torch.2_1_0.aten::sub_(%310:<2048xf32>, %311:<2048xf32>, alpha=1:i32) 
58393:6:0 %312:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%312:<2048x6400xf32>, list{%303:<2048xi64>, %299:<2048xi64>}, %310:<2048xf32>, False:pred) 
58393:6:0 %286:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%301:<2048x1xf32>, -1:i32) 
58393:6:0 %292:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%292:<2048x1x6400xf32>, %286:<2048x1x1xf32>) 
58393:6:0 %148:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%292:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:None:0 list{%263:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%263:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %292:<2048x6400xf16> = torch.2_1_0.aten::view(%148:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58390:3:0 %244:<f32>+1 = torch.2_1_0.aten::select(%266:<2xf32>, 0:i32, 1:i32) 
58390:3:0 %272:<f32> = torch.2_1_0.aten::clone(%244:<f32>+1, memory_format=None:NoneType) 
58390:3:0 %244:<f32> = torch.2_1_0.aten::detach(%272:<f32>) 
58390:3:0 %273:<f32> = torch.2_1_0.aten::detach(%244:<f32>) 
58390:3:0 %272:<i32> = torch.2_1_0.aten::_to_copy(%273:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %273:<f32> = torch.2_1_0.aten::select(%266:<2xf32>, 0:i32, 0:i32) 
58390:3:0 %278:<f32> = torch.2_1_0.aten::mul(%273:<f32>, 1:i32) 
58390:3:0 %274:<f32> = torch.2_1_0.aten::select(%263:<2xf32>, 0:i32, 0:i32) 
58390:3:0 %280:<f32>+1 = torch.2_1_0.aten::select(%263:<2xf32>, 0:i32, 1:i32) 
58390:3:0 %278:<f32> = torch.2_1_0.aten::div_(%278:<f32>, %272:<i32>) 
58390:3:0 %278:<f32> = torch.2_1_0.aten::div_(%278:<f32>, 1:i32) 
58390:3:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%272:<i32>) 
58393:6:0 %297:<2048x12288xf16> = torch.2_1_0.aten::mm(%292:<2048x6400xf16>, %116:<6400x12288xf16>) 
58390:3:0 %178:<i32> = torch.2_1_0.aten::add_(%178:<i32>, 2048:i32, alpha=1:i32) 
58393:6:0 %312:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%297:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %266:<1xf32> = torch.2_1_0.aten::mul(%153:<1xf32>, %278:<f32>) 
58393:6:0 %309:<2048x6400xf16> = torch.2_1_0.aten::view(%148:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58393:6:0 %295:<2048x12288xf16> = torch.2_1_0.aten::view(%285:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %273:<1xf32> = torch.2_1_0.aten::ones_like(%266:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58393:None:0 list{%312:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%312:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %311:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%309:<2048x6400xf16>) 
58393:6:0 %299:<6400x12288xf16> = torch.2_1_0.aten::mm(%311:<6400x2048xf16>{1,6400}, %295:<2048x12288xf16>) 
58393:6:0 %313:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %309:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%278:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %295:<12288xf16> = torch.2_1_0.aten::empty_like(%126:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %297:<12288xf16> = torch.2_1_0.aten::empty_like(%126:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %276:<1xf32> = torch.2_1_0.aten::mul(%273:<1xf32>, %153:<1xf32>) 
58390:3:0 %279:<f32> = torch.2_1_0.aten::sum(%276:<1xf32>, dtype=None:NoneType) 
58390:3:0 %262:<f32> = torch.2_1_0.aten::div(%279:<f32>, 1:i32) 
58390:3:0 %281:<f32> = torch.2_1_0.aten::div(%262:<f32>, %272:<i32>) 
58390:3:0 %282:<f32> = torch.2_1_0.aten::mul(%281:<f32>, 1:i32) 
58390:3:0 %283:<2xf32> = torch.2_1_0.aten::select_backward(%282:<f32>, list{2:i32}, 0:i32, 0:i32) 
58390:3:0 %208:<1xf32> = torch.2_1_0.aten::slice(%283:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58390:3:0 %262:<1xf32>+1 = torch.2_1_0.aten::slice(%283:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58390:3:0 %284:<f32> = torch.2_1_0.aten::view(%208:<1xf32>, list{}) 
58390:3:0 %285:<2048xf32>{0} = torch.2_1_0.aten::expand(%284:<f32>, list{2048:i32}, implicit=False:pred) 
58390:3:0 %284:<2048xf32> = torch.2_1_0.aten::mul(%285:<2048xf32>{0}, %275:<2048xf32>) 
58390:3:0 %198:<1x2048xf32> = torch.2_1_0.aten::view(%284:<2048xf32>, list{1:i32, 2048:i32}) 
58390:3:0 %275:<2048x1xf32> = torch.2_1_0.aten::transpose(%198:<1x2048xf32>, 0:i32, 1:i32) 
58390:3:0 %286:<2048x6400xf32> = torch.2_1_0.aten::view(%264:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58390:3:0 %285:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:3:device, pin_memory=False:pred) 
58390:3:0 %198:<2048xunknown> = torch.2_1_0.aten::view(%268:<2048x1xunknown>, list{-1:i32}) 
58390:3:0 %287:<2048xf32> = torch.2_1_0.aten::_to_copy(%198:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %208:<2048xf32> = torch.2_1_0.aten::rsub(%287:<2048xf32>, 1.0:f32, 1:i32) 
58390:3:0 %287:<2048xf32> = torch.2_1_0.aten::index(%286:<2048x6400xf32>, list{%285:<2048xi64>, %271:<2048xi64>}) 
58390:3:0 %287:<2048xf32> = torch.2_1_0.aten::sub_(%287:<2048xf32>, %208:<2048xf32>, alpha=1:i32) 
58390:3:0 %286:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%286:<2048x6400xf32>, list{%285:<2048xi64>, %271:<2048xi64>}, %287:<2048xf32>, False:pred) 
58390:3:0 %225:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%275:<2048x1xf32>, -1:i32) 
58390:3:0 %264:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%264:<2048x1x6400xf32>, %225:<2048x1x1xf32>) 
58390:3:0 %268:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%264:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %283:<2048x6400xf16> = torch.2_1_0.aten::view(%268:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58390:3:0 %192:<2048x12288xf16> = torch.2_1_0.aten::mm(%283:<2048x6400xf16>, %116:<6400x12288xf16>) 
58390:3:0 %287:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%192:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %192:<2048x6400xf16> = torch.2_1_0.aten::view(%268:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58390:3:0 %284:<2048x12288xf16> = torch.2_1_0.aten::view(%256:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:None:0 list{%287:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%287:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %264:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%192:<2048x6400xf16>) 
58390:3:0 %208:<6400x12288xf16> = torch.2_1_0.aten::mm(%264:<6400x2048xf16>{1,6400}, %284:<2048x12288xf16>) 
58390:3:0 %257:<2048x12288xf16> = torch.2_1_0.aten::view(%287:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %284:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%209:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %256:<12288xf16> = torch.2_1_0.aten::empty_like(%123:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %244:<12288xf16> = torch.2_1_0.aten::empty_like(%123:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:None:0 list{%252:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%252:<2xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %249:<f32>+1 = torch.2_1_0.aten::select(%251:<2xf32>, 0:i32, 1:i32) 
58394:7:0 %250:<f32> = torch.2_1_0.aten::clone(%249:<f32>+1, memory_format=None:NoneType) 
58394:7:0 %253:<f32> = torch.2_1_0.aten::detach(%250:<f32>) 
58394:7:0 %254:<f32> = torch.2_1_0.aten::detach(%253:<f32>) 
58394:7:0 %249:<i32> = torch.2_1_0.aten::_to_copy(%254:<f32>, dtype=torch.int32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %247:<f32> = torch.2_1_0.aten::select(%251:<2xf32>, 0:i32, 0:i32) 
58394:7:0 %255:<f32> = torch.2_1_0.aten::mul(%247:<f32>, 1:i32) 
58394:7:0 %256:<f32> = torch.2_1_0.aten::select(%252:<2xf32>, 0:i32, 0:i32) 
58394:7:0 %257:<f32>+1 = torch.2_1_0.aten::select(%252:<2xf32>, 0:i32, 1:i32) 
58394:7:0 %255:<f32> = torch.2_1_0.aten::div_(%255:<f32>, %249:<i32>) 
58394:7:0 %255:<f32> = torch.2_1_0.aten::div_(%255:<f32>, 1:i32) 
58394:7:0 2048:i32: = torch.2_1_0.aten::_local_scalar_dense(%249:<i32>) 
58394:7:0 %82:<i32> = torch.2_1_0.aten::add_(%82:<i32>, 2048:i32, alpha=1:i32) 
58394:7:0 %258:<1xf32> = torch.2_1_0.aten::mul(%120:<1xf32>, %255:<f32>) 
58394:7:0 %259:<1xf32> = torch.2_1_0.aten::ones_like(%258:<1xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=torch.preserve_format:memory_format) 
58394:7:0 %260:<1xf32> = torch.2_1_0.aten::mul(%259:<1xf32>, %120:<1xf32>) 
58394:7:0 %261:<f32> = torch.2_1_0.aten::sum(%260:<1xf32>, dtype=None:NoneType) 
58394:7:0 %262:<f32> = torch.2_1_0.aten::div(%261:<f32>, 1:i32) 
58394:7:0 %263:<f32> = torch.2_1_0.aten::div(%262:<f32>, %249:<i32>) 
58394:7:0 %261:<f32> = torch.2_1_0.aten::mul(%263:<f32>, 1:i32) 
58394:7:0 %90:<2xf32> = torch.2_1_0.aten::select_backward(%261:<f32>, list{2:i32}, 0:i32, 0:i32) 
58394:7:0 %262:<1xf32> = torch.2_1_0.aten::slice(%90:<2xf32>, 0:i32, 0:i32, 1:i32, 1:i32) 
58394:7:0 %264:<1xf32>+1 = torch.2_1_0.aten::slice(%90:<2xf32>, 0:i32, 1:i32, 2:i32, 1:i32) 
58394:7:0 %265:<f32> = torch.2_1_0.aten::view(%262:<1xf32>, list{}) 
58394:7:0 %263:<2048xf32>{0} = torch.2_1_0.aten::expand(%265:<f32>, list{2048:i32}, implicit=False:pred) 
58394:7:0 %266:<2048xf32> = torch.2_1_0.aten::mul(%263:<2048xf32>{0}, %228:<2048xf32>) 
58394:7:0 %158:<1x2048xf32> = torch.2_1_0.aten::view(%266:<2048xf32>, list{1:i32, 2048:i32}) 
58394:7:0 %228:<2048x1xf32> = torch.2_1_0.aten::transpose(%158:<1x2048xf32>, 0:i32, 1:i32) 
58394:7:0 %161:<2048x6400xf32> = torch.2_1_0.aten::view(%227:<2048x1x6400xf32>, list{-1:i32, 6400:i32}) 
58394:7:0 %263:<2048xi64> = torch.2_1_0.aten::arange(0:i32, 2048:i32, dtype=None:NoneType, layout=None:NoneType, device=cuda:7:device, pin_memory=False:pred) 
58394:7:0 %158:<2048xunknown> = torch.2_1_0.aten::view(%234:<2048x1xunknown>, list{-1:i32}) 
58394:7:0 %260:<2048xf32> = torch.2_1_0.aten::_to_copy(%158:<2048xunknown>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %267:<2048xf32> = torch.2_1_0.aten::rsub(%260:<2048xf32>, 1.0:f32, 1:i32) 
58394:7:0 %260:<2048xf32> = torch.2_1_0.aten::index(%161:<2048x6400xf32>, list{%263:<2048xi64>, %238:<2048xi64>}) 
58394:7:0 %260:<2048xf32> = torch.2_1_0.aten::sub_(%260:<2048xf32>, %267:<2048xf32>, alpha=1:i32) 
58394:7:0 %161:<2048x6400xf32> = torch.2_1_0.aten::index_put_(%161:<2048x6400xf32>, list{%263:<2048xi64>, %238:<2048xi64>}, %260:<2048xf32>, False:pred) 
58394:7:0 %158:<2048x1x1xf32> = torch.2_1_0.aten::unsqueeze(%228:<2048x1xf32>, -1:i32) 
58394:7:0 %227:<2048x1x6400xf32> = torch.2_1_0.aten::mul_(%227:<2048x1x6400xf32>, %158:<2048x1x1xf32>) 
58394:7:0 %266:<2048x1x6400xf16> = torch.2_1_0.aten::_to_copy(%227:<2048x1x6400xf32>, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %238:<2048x6400xf16> = torch.2_1_0.aten::view(%266:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58394:7:0 %234:<2048x12288xf16> = torch.2_1_0.aten::mm(%238:<2048x6400xf16>, %12:<6400x12288xf16>) 
58394:7:0 %105:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%234:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %263:<2048x6400xf16> = torch.2_1_0.aten::view(%266:<2048x1x6400xf16>, list{2048:i32, 6400:i32}) 
58394:7:0 %228:<2048x12288xf16> = torch.2_1_0.aten::view(%182:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:None:0 list{%105:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%105:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %158:<6400x2048xf16>{1,6400} = torch.2_1_0.aten::t(%263:<2048x6400xf16>) 
58388:1:0 %288:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %168:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %303:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %274:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %317:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %299:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %314:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58391:4:0 %261:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58392:5:0 %219:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58388:1:0 %318:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58387:0:0 %260:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58389:2:0 %320:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58392:5:0 %217:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %321:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %305:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %255:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %315:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %264:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %321:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %176:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %314:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %322:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %316:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %269:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %322:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %275:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58392:5:0 %217:<2048x1x12288xf16> = torch.2_1_0.aten::view(%209:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %314:<2048x1x12288xf16> = torch.2_1_0.aten::view(%309:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %318:<2048x1x12288xf16> = torch.2_1_0.aten::view(%316:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %274:<2048x1x12288xf16> = torch.2_1_0.aten::view(%210:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %260:<2048x1x12288xf16> = torch.2_1_0.aten::view(%313:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %320:<2048x1x12288xf16> = torch.2_1_0.aten::view(%314:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %285:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %188:<12288xf16> = torch.2_1_0.aten::detach(%222:<12288xf16>) 
58393:6:0 %283:<12288xf16> = torch.2_1_0.aten::detach(%295:<12288xf16>) 
58388:1:0 %256:<12288xf16> = torch.2_1_0.aten::detach(%315:<12288xf16>) 
58391:4:0 %253:<12288xf16> = torch.2_1_0.aten::detach(%218:<12288xf16>) 
58392:5:0 %198:<12288xf16> = torch.2_1_0.aten::detach(%188:<12288xf16>) 
58387:0:0 %315:<12288xf16> = torch.2_1_0.aten::detach(%290:<12288xf16>) 
58393:6:0 %282:<12288xf16> = torch.2_1_0.aten::detach(%283:<12288xf16>) 
58389:2:0 %277:<12288xf16> = torch.2_1_0.aten::detach(%291:<12288xf16>) 
58388:1:0 %277:<12288xf16> = torch.2_1_0.aten::detach(%256:<12288xf16>) 
58391:4:0 %230:<12288xf16> = torch.2_1_0.aten::detach(%253:<12288xf16>) 
58392:5:0 %200:<12288xf16> = torch.2_1_0.aten::detach(%198:<12288xf16>) 
58387:0:0 %280:<12288xf16> = torch.2_1_0.aten::detach(%315:<12288xf16>) 
58393:6:0 %280:<12288xf16> = torch.2_1_0.aten::detach(%282:<12288xf16>) 
58388:1:0 %288:<12288xf16> = torch.2_1_0.aten::detach(%277:<12288xf16>) 
58389:2:0 %319:<12288xf16> = torch.2_1_0.aten::detach(%277:<12288xf16>) 
58391:4:0 %253:<12288xf16> = torch.2_1_0.aten::detach(%230:<12288xf16>) 
58387:0:0 %299:<12288xf16> = torch.2_1_0.aten::detach(%280:<12288xf16>) 
58392:5:0 %56:<12288xf16>+12288 = torch.2_1_0.aten::add_(%56:<12288xf16>+12288, %200:<12288xf16>, alpha=1:i32) 
58393:6:0 %107:<12288xf16>+12288 = torch.2_1_0.aten::add_(%107:<12288xf16>+12288, %280:<12288xf16>, alpha=1:i32) 
58392:5:0 %200:<12288xf16> = torch.2_1_0.aten::detach(%223:<12288xf16>) 
58389:2:0 %323:<12288xf16> = torch.2_1_0.aten::detach(%319:<12288xf16>) 
58388:1:0 %75:<12288xf16>+12288 = torch.2_1_0.aten::add_(%75:<12288xf16>+12288, %288:<12288xf16>, alpha=1:i32) 
58387:0:0 %71:<12288xf16>+12288 = torch.2_1_0.aten::add_(%71:<12288xf16>+12288, %299:<12288xf16>, alpha=1:i32) 
58391:4:0 %78:<12288xf16>+12288 = torch.2_1_0.aten::add_(%78:<12288xf16>+12288, %253:<12288xf16>, alpha=1:i32) 
58390:3:0 %286:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %280:<12288xf16> = torch.2_1_0.aten::detach(%297:<12288xf16>) 
58392:5:0 %198:<12288xf16> = torch.2_1_0.aten::detach(%200:<12288xf16>) 
58387:0:0 %290:<12288xf16> = torch.2_1_0.aten::detach(%62:<12288xf16>) 
58392:5:0 %188:<12288xf16> = torch.2_1_0.aten::detach(%198:<12288xf16>) 
58388:1:0 %256:<12288xf16> = torch.2_1_0.aten::detach(%319:<12288xf16>) 
58391:4:0 %230:<12288xf16> = torch.2_1_0.aten::detach(%240:<12288xf16>) 
58393:6:0 %245:<12288xf16> = torch.2_1_0.aten::detach(%280:<12288xf16>) 
58389:2:0 %121:<12288xf16>+12288 = torch.2_1_0.aten::add_(%121:<12288xf16>+12288, %323:<12288xf16>, alpha=1:i32) 
58387:0:0 %299:<12288xf16> = torch.2_1_0.aten::detach(%290:<12288xf16>) 
58391:4:0 %253:<12288xf16> = torch.2_1_0.aten::detach(%230:<12288xf16>) 
58393:6:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%245:<12288xf16>) 
58388:1:0 %276:<12288xf16> = torch.2_1_0.aten::detach(%256:<12288xf16>) 
58392:5:0 %44:<12288xf16> = torch.2_1_0.aten::add_(%44:<12288xf16>, %188:<12288xf16>, alpha=1:i32) 
58387:0:0 %280:<12288xf16> = torch.2_1_0.aten::detach(%299:<12288xf16>) 
58389:2:0 %297:<12288xf16> = torch.2_1_0.aten::detach(%311:<12288xf16>) 
58391:4:0 %218:<12288xf16> = torch.2_1_0.aten::detach(%253:<12288xf16>) 
58388:1:0 %279:<12288xf16> = torch.2_1_0.aten::detach(%276:<12288xf16>) 
58393:6:0 %100:<12288xf16> = torch.2_1_0.aten::add_(%100:<12288xf16>, %190:<12288xf16>, alpha=1:i32) 
58387:0:0 %106:<12288xf16> = torch.2_1_0.aten::add_(%106:<12288xf16>, %280:<12288xf16>, alpha=1:i32) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::detach(%297:<12288xf16>) 
58390:3:0 %264:<2048x1x12288xf16> = torch.2_1_0.aten::view(%284:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %140:<12288xf16> = torch.2_1_0.aten::add_(%140:<12288xf16>, %218:<12288xf16>, alpha=1:i32) 
58388:1:0 %132:<12288xf16> = torch.2_1_0.aten::add_(%132:<12288xf16>, %279:<12288xf16>, alpha=1:i32) 
58389:2:0 %317:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58389:2:0 %118:<12288xf16> = torch.2_1_0.aten::add_(%118:<12288xf16>, %317:<12288xf16>, alpha=1:i32) 
58390:3:0 %255:<12288xf16> = torch.2_1_0.aten::detach(%256:<12288xf16>) 
58393:6:0 %312:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%314:<2048x1x12288xf16>, %277:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58392:5:0 %222:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%217:<2048x1x12288xf16>, %193:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58390:3:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%255:<12288xf16>) 
58391:4:0 %58:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%274:<2048x1x12288xf16>, %249:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58388:1:0 %279:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%318:<2048x1x12288xf16>, %273:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58390:3:0 %261:<12288xf16> = torch.2_1_0.aten::detach(%260:<12288xf16>) 
58387:0:0 %216:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%260:<2048x1x12288xf16>, %271:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58389:2:0 %152:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%320:<2048x1x12288xf16>, %227:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58390:3:0 %132:<12288xf16>+12288 = torch.2_1_0.aten::add_(%132:<12288xf16>+12288, %261:<12288xf16>, alpha=1:i32) 
58390:3:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%244:<12288xf16>) 
58390:3:0 %261:<12288xf16> = torch.2_1_0.aten::detach(%260:<12288xf16>) 
58390:3:0 %256:<12288xf16> = torch.2_1_0.aten::detach(%261:<12288xf16>) 
58390:3:0 %13:<12288xf16> = torch.2_1_0.aten::add_(%13:<12288xf16>, %256:<12288xf16>, alpha=1:i32) 
58390:3:0 %256:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%264:<2048x1x12288xf16>, %259:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58392:5:0 %193:<1x1x12288xf16> = torch.2_1_0.aten::sum(%222:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58391:4:0 %218:<1x1x12288xf16> = torch.2_1_0.aten::sum(%58:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58387:0:0 %315:<1x1x12288xf16> = torch.2_1_0.aten::sum(%216:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58389:2:0 %311:<1x1x12288xf16> = torch.2_1_0.aten::sum(%152:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58392:5:0 %198:<12288xf16> = torch.2_1_0.aten::view(%193:<1x1x12288xf16>, list{12288:i32}) 
58391:4:0 %253:<12288xf16> = torch.2_1_0.aten::view(%218:<1x1x12288xf16>, list{12288:i32}) 
58387:0:0 %280:<12288xf16> = torch.2_1_0.aten::view(%315:<1x1x12288xf16>, list{12288:i32}) 
58392:5:0 %200:<12288xf16> = torch.2_1_0.aten::detach(%198:<12288xf16>) 
58391:4:0 %265:<12288xf16> = torch.2_1_0.aten::detach(%253:<12288xf16>) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::view(%311:<1x1x12288xf16>, list{12288:i32}) 
58392:5:0 %197:<12288xf16> = torch.2_1_0.aten::detach(%200:<12288xf16>) 
58387:0:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%280:<12288xf16>) 
58391:4:0 %275:<12288xf16> = torch.2_1_0.aten::detach(%265:<12288xf16>) 
58392:5:0 %188:<12288xf16> = torch.2_1_0.aten::detach(%197:<12288xf16>) 
58389:2:0 %322:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58387:0:0 %312:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58391:4:0 %249:<12288xf16> = torch.2_1_0.aten::detach(%275:<12288xf16>) 
58392:5:0 %45:<12288xf16>+24576 = torch.2_1_0.aten::add_(%45:<12288xf16>+24576, %188:<12288xf16>, alpha=1:i32) 
58387:0:0 %271:<12288xf16> = torch.2_1_0.aten::detach(%312:<12288xf16>) 
58389:2:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%322:<12288xf16>) 
58391:4:0 %52:<12288xf16>+24576 = torch.2_1_0.aten::add_(%52:<12288xf16>+24576, %249:<12288xf16>, alpha=1:i32) 
58387:0:0 %136:<12288xf16>+24576 = torch.2_1_0.aten::add_(%136:<12288xf16>+24576, %271:<12288xf16>, alpha=1:i32) 
58389:2:0 %317:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58392:5:0 %198:<2048x12288xf16> = torch.2_1_0.aten::view(%222:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %253:<2048x12288xf16> = torch.2_1_0.aten::view(%58:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %124:<12288xf16>+24576 = torch.2_1_0.aten::add_(%124:<12288xf16>+24576, %317:<12288xf16>, alpha=1:i32) 
58387:0:0 %280:<2048x12288xf16> = torch.2_1_0.aten::view(%216:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %197:<2048x6144xf16> = torch.2_1_0.aten::mm(%198:<2048x12288xf16>, %31:<12288x6144xf16>) 
58392:5:0 %48:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%197:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58391:4:0 %218:<2048x6144xf16> = torch.2_1_0.aten::mm(%253:<2048x12288xf16>, %33:<12288x6144xf16>) 
58389:2:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%152:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %271:<2048x6144xf16> = torch.2_1_0.aten::mm(%280:<2048x12288xf16>, %53:<12288x6144xf16>) 
58392:5:0 %197:<2048x12288xf16> = torch.2_1_0.aten::view(%222:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %249:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%218:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58387:0:0 %57:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%271:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58392:5:0 %151:<2048x6144xf16> = torch.2_1_0.aten::view(%189:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58391:4:0 %265:<2048x12288xf16> = torch.2_1_0.aten::view(%58:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %200:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%197:<2048x12288xf16>) 
58387:0:0 %271:<2048x12288xf16> = torch.2_1_0.aten::view(%216:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %316:<2048x6144xf16> = torch.2_1_0.aten::mm(%311:<2048x12288xf16>, %102:<12288x6144xf16>) 
58391:4:0 %218:<2048x6144xf16> = torch.2_1_0.aten::view(%250:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:0:0 %312:<2048x6144xf16> = torch.2_1_0.aten::view(%252:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58391:4:0 %230:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%265:<2048x12288xf16>) 
58387:0:0 %311:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%271:<2048x12288xf16>) 
58389:2:0 %291:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%316:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58389:2:0 %316:<2048x12288xf16> = torch.2_1_0.aten::view(%152:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %321:<2048x6144xf16> = torch.2_1_0.aten::view(%278:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58394:7:0 %268:<6400x12288xf16> = torch.2_1_0.aten::mm(%158:<6400x2048xf16>{1,6400}, %228:<2048x12288xf16>) 
58389:2:0 %317:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%316:<2048x12288xf16>) 
58394:7:0 %182:<2048x12288xf16> = torch.2_1_0.aten::view(%105:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %266:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%185:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %90:<12288xf16> = torch.2_1_0.aten::empty_like(%98:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %158:<12288xf16> = torch.2_1_0.aten::empty_like(%98:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %176:<12288x6144xf16> = torch.2_1_0.aten::mm(%200:<12288x2048xf16>{1,12288}, %151:<2048x6144xf16>) 
58392:5:0 %151:<12288x6144xf16> = torch.2_1_0.aten::detach(%176:<12288x6144xf16>) 
58391:4:0 %245:<12288x6144xf16> = torch.2_1_0.aten::mm(%230:<12288x2048xf16>{1,12288}, %218:<2048x6144xf16>) 
58392:5:0 %200:<12288x6144xf16> = torch.2_1_0.aten::detach(%151:<12288x6144xf16>) 
58387:0:0 %316:<12288x6144xf16> = torch.2_1_0.aten::mm(%311:<12288x2048xf16>{1,12288}, %312:<2048x6144xf16>) 
58392:5:0 %219:<12288x6144xf16> = torch.2_1_0.aten::detach(%200:<12288x6144xf16>) 
58391:4:0 %250:<12288x6144xf16> = torch.2_1_0.aten::detach(%245:<12288x6144xf16>) 
58392:5:0 %60:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%60:<12288x6144xf16>+36864, %219:<12288x6144xf16>, alpha=1:i32) 
58387:0:0 %299:<12288x6144xf16> = torch.2_1_0.aten::detach(%316:<12288x6144xf16>) 
58391:4:0 %218:<12288x6144xf16> = torch.2_1_0.aten::detach(%250:<12288x6144xf16>) 
58387:0:0 %312:<12288x6144xf16> = torch.2_1_0.aten::detach(%299:<12288x6144xf16>) 
58391:4:0 %265:<12288x6144xf16> = torch.2_1_0.aten::detach(%218:<12288x6144xf16>) 
58387:0:0 %314:<12288x6144xf16> = torch.2_1_0.aten::detach(%312:<12288x6144xf16>) 
58391:4:0 %90:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%90:<12288x6144xf16>+36864, %265:<12288x6144xf16>, alpha=1:i32) 
58392:5:0 %207:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%48:<2048x1x6144xf16>, %191:<2048x1x6144xf16>, approximate=none:str) 
58387:0:0 %135:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%135:<12288x6144xf16>+36864, %314:<12288x6144xf16>, alpha=1:i32) 
58392:5:0 %188:<1x1x6144xf16> = torch.2_1_0.aten::sum(%207:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58391:4:0 %276:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%249:<2048x1x6144xf16>, %219:<2048x1x6144xf16>, approximate=none:str) 
58392:5:0 %191:<6144xf16> = torch.2_1_0.aten::view(%188:<1x1x6144xf16>, list{6144:i32}) 
58387:0:0 %311:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%57:<2048x1x6144xf16>, %277:<2048x1x6144xf16>, approximate=none:str) 
58392:5:0 %176:<6144xf16> = torch.2_1_0.aten::detach(%191:<6144xf16>) 
58392:5:0 %193:<6144xf16> = torch.2_1_0.aten::detach(%176:<6144xf16>) 
58391:4:0 %249:<1x1x6144xf16> = torch.2_1_0.aten::sum(%276:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58392:5:0 %176:<6144xf16> = torch.2_1_0.aten::detach(%193:<6144xf16>) 
58387:0:0 %277:<1x1x6144xf16> = torch.2_1_0.aten::sum(%311:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58391:4:0 %240:<6144xf16> = torch.2_1_0.aten::view(%249:<1x1x6144xf16>, list{6144:i32}) 
58392:5:0 %63:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%63:<6144xf16>+75534336, %176:<6144xf16>, alpha=1:i32) 
58387:0:0 %315:<6144xf16> = torch.2_1_0.aten::view(%277:<1x1x6144xf16>, list{6144:i32}) 
58391:4:0 %265:<6144xf16> = torch.2_1_0.aten::detach(%240:<6144xf16>) 
58387:0:0 %312:<6144xf16> = torch.2_1_0.aten::detach(%315:<6144xf16>) 
58391:4:0 %218:<6144xf16> = torch.2_1_0.aten::detach(%265:<6144xf16>) 
58392:5:0 %188:<2048x6144xf16> = torch.2_1_0.aten::view(%207:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:0:0 %316:<6144xf16> = torch.2_1_0.aten::detach(%312:<6144xf16>) 
58391:4:0 %219:<6144xf16> = torch.2_1_0.aten::detach(%218:<6144xf16>) 
58387:0:0 %299:<6144xf16> = torch.2_1_0.aten::detach(%316:<6144xf16>) 
58391:4:0 %73:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%73:<6144xf16>+75534336, %219:<6144xf16>, alpha=1:i32) 
58392:5:0 %193:<2048x12288xf16> = torch.2_1_0.aten::mm(%188:<2048x6144xf16>, %30:<6144x12288xf16>) 
58387:0:0 %113:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%113:<6144xf16>+75534336, %299:<6144xf16>, alpha=1:i32) 
58392:5:0 %176:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%193:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %218:<2048x6144xf16> = torch.2_1_0.aten::view(%276:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58392:5:0 %193:<2048x6144xf16> = torch.2_1_0.aten::view(%207:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:0:0 %315:<2048x6144xf16> = torch.2_1_0.aten::view(%311:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58392:5:0 %153:<2048x12288xf16> = torch.2_1_0.aten::view(%190:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %219:<2048x12288xf16> = torch.2_1_0.aten::mm(%218:<2048x6144xf16>, %96:<6144x12288xf16>) 
58387:0:0 %316:<2048x12288xf16> = torch.2_1_0.aten::mm(%315:<2048x6144xf16>, %52:<6144x12288xf16>) 
58391:4:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%219:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%316:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %219:<2048x6144xf16> = torch.2_1_0.aten::view(%276:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58389:2:0 %312:<12288x6144xf16> = torch.2_1_0.aten::mm(%317:<12288x2048xf16>{1,12288}, %321:<2048x6144xf16>) 
58387:0:0 %316:<2048x6144xf16> = torch.2_1_0.aten::view(%311:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58391:4:0 %230:<2048x12288xf16> = torch.2_1_0.aten::view(%222:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %312:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:None:0 list{%176:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%176:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %197:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%193:<2048x6144xf16>) 
58393:6:0 %190:<1x1x12288xf16> = torch.2_1_0.aten::sum(%312:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58389:2:0 %321:<12288x6144xf16> = torch.2_1_0.aten::detach(%312:<12288x6144xf16>) 
58389:2:0 %317:<12288x6144xf16> = torch.2_1_0.aten::detach(%321:<12288x6144xf16>) 
58393:6:0 %303:<12288xf16> = torch.2_1_0.aten::view(%190:<1x1x12288xf16>, list{12288:i32}) 
58392:5:0 %200:<6144x12288xf16> = torch.2_1_0.aten::mm(%197:<6144x2048xf16>{1,6144}, %153:<2048x12288xf16>) 
58389:2:0 %321:<12288x6144xf16> = torch.2_1_0.aten::detach(%317:<12288x6144xf16>) 
58391:None:0 list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%303:<12288xf16>) 
58387:None:0 list{%44:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%44:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %222:<6144x12288xf16> = torch.2_1_0.aten::detach(%200:<6144x12288xf16>) 
58391:4:0 %265:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%219:<2048x6144xf16>) 
58389:2:0 %127:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%127:<12288x6144xf16>+36864, %321:<12288x6144xf16>, alpha=1:i32) 
58393:6:0 %315:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58392:5:0 %194:<6144x12288xf16> = torch.2_1_0.aten::detach(%222:<6144x12288xf16>) 
58387:0:0 %277:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%316:<2048x6144xf16>) 
58393:6:0 %297:<12288xf16> = torch.2_1_0.aten::detach(%315:<12288xf16>) 
58391:4:0 %218:<6144x12288xf16> = torch.2_1_0.aten::mm(%265:<6144x2048xf16>{1,6144}, %230:<2048x12288xf16>) 
58392:5:0 %224:<6144x12288xf16> = torch.2_1_0.aten::detach(%194:<6144x12288xf16>) 
58393:6:0 %128:<12288xf16>+24576 = torch.2_1_0.aten::add_(%128:<12288xf16>+24576, %297:<12288xf16>, alpha=1:i32) 
58387:0:0 %252:<6144x12288xf16> = torch.2_1_0.aten::mm(%277:<6144x2048xf16>{1,6144}, %312:<2048x12288xf16>) 
58392:5:0 %66:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%66:<6144x12288xf16>+75540480, %224:<6144x12288xf16>, alpha=1:i32) 
58391:4:0 %222:<6144x12288xf16> = torch.2_1_0.aten::detach(%218:<6144x12288xf16>) 
58389:2:0 %324:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%291:<2048x1x6144xf16>, %274:<2048x1x6144xf16>, approximate=none:str) 
58391:4:0 %245:<6144x12288xf16> = torch.2_1_0.aten::detach(%222:<6144x12288xf16>) 
58387:0:0 %312:<6144x12288xf16> = torch.2_1_0.aten::detach(%252:<6144x12288xf16>) 
58392:5:0 %224:<2048x12288xf16> = torch.2_1_0.aten::view(%181:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58391:4:0 %261:<6144x12288xf16> = torch.2_1_0.aten::detach(%245:<6144x12288xf16>) 
58393:6:0 %315:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %200:<2048x12288xf16> = torch.2_1_0.aten::view(%176:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %311:<6144x12288xf16> = torch.2_1_0.aten::detach(%312:<6144x12288xf16>) 
58387:0:0 %316:<6144x12288xf16> = torch.2_1_0.aten::detach(%311:<6144x12288xf16>) 
58389:2:0 %274:<1x1x6144xf16> = torch.2_1_0.aten::sum(%324:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58392:5:0 %193:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%224:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %144:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%144:<6144x12288xf16>+75540480, %261:<6144x12288xf16>, alpha=1:i32) 
58389:2:0 %5:<6144xf16> = torch.2_1_0.aten::view(%274:<1x1x6144xf16>, list{6144:i32}) 
58387:0:0 %139:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%139:<6144x12288xf16>+75540480, %316:<6144x12288xf16>, alpha=1:i32) 
58392:5:0 %74:<12288xf16> = torch.2_1_0.aten::empty_like(%21:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %310:<2048x6144xf16> = torch.2_1_0.aten::mm(%315:<2048x12288xf16>, %47:<12288x6144xf16>) 
58389:2:0 %321:<6144xf16> = torch.2_1_0.aten::detach(%5:<6144xf16>) 
58391:4:0 %74:<2048x12288xf16> = torch.2_1_0.aten::view(%233:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %153:<12288xf16> = torch.2_1_0.aten::empty_like(%21:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %312:<6144xf16> = torch.2_1_0.aten::detach(%321:<6144xf16>) 
58393:6:0 %317:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%310:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58387:0:0 %317:<2048x12288xf16> = torch.2_1_0.aten::view(%286:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58391:4:0 %218:<2048x12288xf16> = torch.2_1_0.aten::view(%240:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %321:<6144xf16> = torch.2_1_0.aten::detach(%312:<6144xf16>) 
58387:0:0 %252:<2048x12288xf16> = torch.2_1_0.aten::view(%44:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %130:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%130:<6144xf16>+75534336, %321:<6144xf16>, alpha=1:i32) 
58391:4:0 %261:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%74:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %316:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%317:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %310:<2048x6144xf16> = torch.2_1_0.aten::view(%274:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58391:4:0 %219:<12288xf16> = torch.2_1_0.aten::empty_like(%132:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %274:<2048x6144xf16> = torch.2_1_0.aten::view(%324:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58387:0:0 %318:<12288xf16> = torch.2_1_0.aten::empty_like(%87:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %318:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%311:<2048x12288xf16>) 
58391:4:0 %249:<12288xf16> = torch.2_1_0.aten::empty_like(%132:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %312:<12288xf16> = torch.2_1_0.aten::empty_like(%87:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %297:<2048x12288xf16> = torch.2_1_0.aten::mm(%274:<2048x6144xf16>, %84:<6144x12288xf16>) 
58389:2:0 %291:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%297:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %322:<2048x6144xf16> = torch.2_1_0.aten::view(%324:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58389:2:0 %321:<2048x12288xf16> = torch.2_1_0.aten::view(%257:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:None:0 list{%291:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%291:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %297:<12288x6144xf16> = torch.2_1_0.aten::mm(%318:<12288x2048xf16>{1,12288}, %310:<2048x6144xf16>) 
58389:2:0 %312:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%322:<2048x6144xf16>) 
58393:6:0 %190:<12288x6144xf16> = torch.2_1_0.aten::detach(%297:<12288x6144xf16>) 
58389:2:0 %311:<6144x12288xf16> = torch.2_1_0.aten::mm(%312:<6144x2048xf16>{1,6144}, %321:<2048x12288xf16>) 
58393:6:0 %319:<12288x6144xf16> = torch.2_1_0.aten::detach(%190:<12288x6144xf16>) 
58393:6:0 %274:<12288x6144xf16> = torch.2_1_0.aten::detach(%319:<12288x6144xf16>) 
58389:2:0 %152:<6144x12288xf16> = torch.2_1_0.aten::detach(%311:<6144x12288xf16>) 
58389:2:0 %322:<6144x12288xf16> = torch.2_1_0.aten::detach(%152:<6144x12288xf16>) 
58393:6:0 %129:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%129:<12288x6144xf16>+36864, %274:<12288x6144xf16>, alpha=1:i32) 
58389:2:0 %319:<6144x12288xf16> = torch.2_1_0.aten::detach(%322:<6144x12288xf16>) 
58389:2:0 %133:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%133:<6144x12288xf16>+75540480, %319:<6144x12288xf16>, alpha=1:i32) 
58393:6:0 %319:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%317:<2048x1x6144xf16>, %260:<2048x1x6144xf16>, approximate=none:str) 
58389:2:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%221:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %322:<2048x12288xf16> = torch.2_1_0.aten::view(%291:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %260:<1x1x6144xf16> = torch.2_1_0.aten::sum(%319:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58389:2:0 %321:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%311:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %274:<6144xf16> = torch.2_1_0.aten::view(%260:<1x1x6144xf16>, list{6144:i32}) 
58389:2:0 %297:<12288xf16> = torch.2_1_0.aten::empty_like(%70:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %190:<6144xf16> = torch.2_1_0.aten::detach(%274:<6144xf16>) 
58389:2:0 %316:<12288xf16> = torch.2_1_0.aten::empty_like(%70:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %303:<6144xf16> = torch.2_1_0.aten::detach(%190:<6144xf16>) 
58393:6:0 %312:<6144xf16> = torch.2_1_0.aten::detach(%303:<6144xf16>) 
58393:6:0 %91:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%91:<6144xf16>+75534336, %312:<6144xf16>, alpha=1:i32) 
58393:6:0 %190:<2048x6144xf16> = torch.2_1_0.aten::view(%319:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58393:6:0 %310:<2048x12288xf16> = torch.2_1_0.aten::mm(%190:<2048x6144xf16>, %63:<6144x12288xf16>) 
58393:6:0 %117:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%310:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %310:<2048x6144xf16> = torch.2_1_0.aten::view(%319:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58393:6:0 %318:<2048x12288xf16> = torch.2_1_0.aten::view(%273:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:None:0 list{%117:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%117:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %303:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%310:<2048x6144xf16>) 
58393:6:0 %295:<6144x12288xf16> = torch.2_1_0.aten::mm(%303:<6144x2048xf16>{1,6144}, %318:<2048x12288xf16>) 
58393:6:0 %313:<6144x12288xf16> = torch.2_1_0.aten::detach(%295:<6144x12288xf16>) 
58393:6:0 %310:<6144x12288xf16> = torch.2_1_0.aten::detach(%313:<6144x12288xf16>) 
58393:6:0 %312:<6144x12288xf16> = torch.2_1_0.aten::detach(%310:<6144x12288xf16>) 
58393:6:0 %62:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%62:<6144x12288xf16>+75540480, %312:<6144x12288xf16>, alpha=1:i32) 
58393:6:0 %313:<2048x12288xf16> = torch.2_1_0.aten::view(%268:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58393:6:0 %295:<2048x12288xf16> = torch.2_1_0.aten::view(%117:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %318:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%313:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %320:<12288xf16> = torch.2_1_0.aten::empty_like(%123:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %321:<12288xf16> = torch.2_1_0.aten::empty_like(%123:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %276:<1x1x12288xf16> = torch.2_1_0.aten::sum(%279:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58388:1:0 %283:<12288xf16> = torch.2_1_0.aten::view(%276:<1x1x12288xf16>, list{12288:i32}) 
58388:1:0 %220:<12288xf16> = torch.2_1_0.aten::detach(%283:<12288xf16>) 
58388:1:0 %319:<12288xf16> = torch.2_1_0.aten::detach(%220:<12288xf16>) 
58388:1:0 %256:<12288xf16> = torch.2_1_0.aten::detach(%319:<12288xf16>) 
58388:1:0 %137:<12288xf16>+24576 = torch.2_1_0.aten::add_(%137:<12288xf16>+24576, %256:<12288xf16>, alpha=1:i32) 
58388:1:0 %296:<2048x12288xf16> = torch.2_1_0.aten::view(%279:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %283:<2048x6144xf16> = torch.2_1_0.aten::mm(%296:<2048x12288xf16>, %65:<12288x6144xf16>) 
58388:1:0 %323:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%283:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58388:1:0 %220:<2048x12288xf16> = torch.2_1_0.aten::view(%279:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %283:<2048x6144xf16> = torch.2_1_0.aten::view(%246:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58388:1:0 %319:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%220:<2048x12288xf16>) 
58388:1:0 %315:<12288x6144xf16> = torch.2_1_0.aten::mm(%319:<12288x2048xf16>{1,12288}, %283:<2048x6144xf16>) 
58388:1:0 %283:<12288x6144xf16> = torch.2_1_0.aten::detach(%315:<12288x6144xf16>) 
58388:1:0 %256:<12288x6144xf16> = torch.2_1_0.aten::detach(%283:<12288x6144xf16>) 
58388:1:0 %246:<12288x6144xf16> = torch.2_1_0.aten::detach(%256:<12288x6144xf16>) 
58388:1:0 %140:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%140:<12288x6144xf16>+36864, %246:<12288x6144xf16>, alpha=1:i32) 
58388:1:0 %87:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%323:<2048x1x6144xf16>, %265:<2048x1x6144xf16>, approximate=none:str) 
58388:1:0 %265:<1x1x6144xf16> = torch.2_1_0.aten::sum(%87:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58388:1:0 %288:<6144xf16> = torch.2_1_0.aten::view(%265:<1x1x6144xf16>, list{6144:i32}) 
58388:1:0 %246:<6144xf16> = torch.2_1_0.aten::detach(%288:<6144xf16>) 
58388:1:0 %324:<6144xf16> = torch.2_1_0.aten::detach(%246:<6144xf16>) 
58388:1:0 %246:<6144xf16> = torch.2_1_0.aten::detach(%324:<6144xf16>) 
58388:1:0 %143:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%143:<6144xf16>+75534336, %246:<6144xf16>, alpha=1:i32) 
58388:1:0 %296:<2048x6144xf16> = torch.2_1_0.aten::view(%87:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58388:1:0 %265:<2048x12288xf16> = torch.2_1_0.aten::mm(%296:<2048x6144xf16>, %120:<6144x12288xf16>) 
58388:1:0 %323:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%265:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %246:<2048x6144xf16> = torch.2_1_0.aten::view(%87:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58388:1:0 %288:<2048x12288xf16> = torch.2_1_0.aten::view(%269:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:None:0 list{%323:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%323:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %203:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%246:<2048x6144xf16>) 
58390:3:0 %261:<1x1x12288xf16> = torch.2_1_0.aten::sum(%256:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58388:1:0 %256:<6144x12288xf16> = torch.2_1_0.aten::mm(%203:<6144x2048xf16>{1,6144}, %288:<2048x12288xf16>) 
58390:3:0 %255:<12288xf16> = torch.2_1_0.aten::view(%261:<1x1x12288xf16>, list{12288:i32}) 
58388:1:0 %269:<6144x12288xf16> = torch.2_1_0.aten::detach(%256:<6144x12288xf16>) 
58390:3:0 %285:<12288xf16> = torch.2_1_0.aten::detach(%255:<12288xf16>) 
58388:1:0 %325:<6144x12288xf16> = torch.2_1_0.aten::detach(%269:<6144x12288xf16>) 
58390:3:0 %286:<12288xf16> = torch.2_1_0.aten::detach(%285:<12288xf16>) 
58388:1:0 %269:<6144x12288xf16> = torch.2_1_0.aten::detach(%325:<6144x12288xf16>) 
58390:3:0 %192:<12288xf16> = torch.2_1_0.aten::detach(%286:<12288xf16>) 
58388:1:0 %146:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%146:<6144x12288xf16>+75540480, %269:<6144x12288xf16>, alpha=1:i32) 
58390:3:0 %42:<12288xf16>+24576 = torch.2_1_0.aten::add_(%42:<12288xf16>+24576, %192:<12288xf16>, alpha=1:i32) 
58388:1:0 %326:<2048x12288xf16> = torch.2_1_0.aten::view(%243:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %256:<2048x12288xf16> = torch.2_1_0.aten::view(%323:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %261:<2048x12288xf16> = torch.2_1_0.aten::view(%256:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %269:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%326:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %246:<3840xu8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.uint8:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %325:<12288xf16> = torch.2_1_0.aten::empty_like(%63:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %288:<12288xf16> = torch.2_1_0.aten::empty_like(%63:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %255:<2048x6144xf16> = torch.2_1_0.aten::mm(%261:<2048x12288xf16>, %105:<12288x6144xf16>) 
58394:7:0 %269:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType) 
58390:3:0 %192:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%255:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58394:7:0 %270:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %271:<2048x12288xf16> = torch.2_1_0.aten::view(%256:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %255:<2048x6144xf16> = torch.2_1_0.aten::view(%233:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58390:3:0 %225:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%271:<2048x12288xf16>) 
58394:7:0 %271:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %270:<2048x1x12288xf16> = torch.2_1_0.aten::view(%266:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::detach(%90:<12288xf16>) 
58394:7:0 %226:<12288xf16> = torch.2_1_0.aten::detach(%217:<12288xf16>) 
58394:7:0 %162:<12288xf16> = torch.2_1_0.aten::detach(%226:<12288xf16>) 
58394:7:0 %69:<12288xf16>+12288 = torch.2_1_0.aten::add_(%69:<12288xf16>+12288, %162:<12288xf16>, alpha=1:i32) 
58394:7:0 %226:<12288xf16> = torch.2_1_0.aten::detach(%158:<12288xf16>) 
58394:7:0 %162:<12288xf16> = torch.2_1_0.aten::detach(%226:<12288xf16>) 
58394:7:0 %242:<12288xf16> = torch.2_1_0.aten::detach(%162:<12288xf16>) 
58394:7:0 %102:<12288xf16> = torch.2_1_0.aten::add_(%102:<12288xf16>, %242:<12288xf16>, alpha=1:i32) 
58394:7:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%270:<2048x1x12288xf16>, %190:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58394:7:0 %190:<1x1x12288xf16> = torch.2_1_0.aten::sum(%272:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58394:7:0 %254:<12288xf16> = torch.2_1_0.aten::view(%190:<1x1x12288xf16>, list{12288:i32}) 
58394:7:0 %185:<12288xf16> = torch.2_1_0.aten::detach(%254:<12288xf16>) 
58394:7:0 %267:<12288xf16> = torch.2_1_0.aten::detach(%185:<12288xf16>) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::detach(%267:<12288xf16>) 
58394:7:0 %100:<12288xf16>+24576 = torch.2_1_0.aten::add_(%100:<12288xf16>+24576, %217:<12288xf16>, alpha=1:i32) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %161:<2048x6144xf16> = torch.2_1_0.aten::mm(%190:<2048x12288xf16>, %94:<12288x6144xf16>) 
58394:7:0 %267:<2048x1x6144xf16> = torch.2_1_0.aten::_unsafe_view(%161:<2048x6144xf16>, list{2048:i32, 1:i32, 6144:i32}) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %273:<2048x6144xf16> = torch.2_1_0.aten::view(%173:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58394:7:0 %182:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%190:<2048x12288xf16>) 
58390:3:0 %275:<12288x6144xf16> = torch.2_1_0.aten::mm(%225:<12288x2048xf16>{1,12288}, %255:<2048x6144xf16>) 
58390:3:0 %256:<12288x6144xf16> = torch.2_1_0.aten::detach(%275:<12288x6144xf16>) 
58390:3:0 %287:<12288x6144xf16> = torch.2_1_0.aten::detach(%256:<12288x6144xf16>) 
58390:3:0 %233:<12288x6144xf16> = torch.2_1_0.aten::detach(%287:<12288x6144xf16>) 
58390:3:0 %111:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%111:<12288x6144xf16>+36864, %233:<12288x6144xf16>, alpha=1:i32) 
58390:3:0 %275:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%192:<2048x1x6144xf16>, %223:<2048x1x6144xf16>, approximate=none:str) 
58390:3:0 %225:<1x1x6144xf16> = torch.2_1_0.aten::sum(%275:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58390:3:0 %287:<6144xf16> = torch.2_1_0.aten::view(%225:<1x1x6144xf16>, list{6144:i32}) 
58390:3:0 %233:<6144xf16> = torch.2_1_0.aten::detach(%287:<6144xf16>) 
58390:3:0 %262:<6144xf16> = torch.2_1_0.aten::detach(%233:<6144xf16>) 
58390:3:0 %268:<6144xf16> = torch.2_1_0.aten::detach(%262:<6144xf16>) 
58390:3:0 %113:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%113:<6144xf16>+75534336, %268:<6144xf16>, alpha=1:i32) 
58390:3:0 %287:<2048x6144xf16> = torch.2_1_0.aten::view(%275:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58390:3:0 %268:<2048x12288xf16> = torch.2_1_0.aten::mm(%287:<2048x6144xf16>, %95:<6144x12288xf16>) 
58390:3:0 %262:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%268:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %268:<2048x6144xf16> = torch.2_1_0.aten::view(%275:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58390:3:0 %288:<2048x12288xf16> = torch.2_1_0.aten::view(%253:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:None:0 list{%262:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%262:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %287:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%268:<2048x6144xf16>) 
58394:7:0 %217:<12288x6144xf16> = torch.2_1_0.aten::mm(%182:<12288x2048xf16>{1,12288}, %273:<2048x6144xf16>) 
58390:3:0 %289:<6144x12288xf16> = torch.2_1_0.aten::mm(%287:<6144x2048xf16>{1,6144}, %288:<2048x12288xf16>) 
58390:3:0 %288:<6144x12288xf16> = torch.2_1_0.aten::detach(%289:<6144x12288xf16>) 
58394:7:0 %173:<12288x6144xf16> = torch.2_1_0.aten::detach(%217:<12288x6144xf16>) 
58390:3:0 %268:<6144x12288xf16> = torch.2_1_0.aten::detach(%288:<6144x12288xf16>) 
58390:3:0 %275:<6144x12288xf16> = torch.2_1_0.aten::detach(%268:<6144x12288xf16>) 
58394:7:0 %190:<12288x6144xf16> = torch.2_1_0.aten::detach(%173:<12288x6144xf16>) 
58394:7:0 %254:<12288x6144xf16> = torch.2_1_0.aten::detach(%190:<12288x6144xf16>) 
58390:3:0 %135:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%135:<6144x12288xf16>+75540480, %275:<6144x12288xf16>, alpha=1:i32) 
58394:7:0 %44:<12288x6144xf16>+36864 = torch.2_1_0.aten::add_(%44:<12288x6144xf16>+36864, %254:<12288x6144xf16>, alpha=1:i32) 
58390:3:0 %268:<2048x12288xf16> = torch.2_1_0.aten::view(%229:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58390:3:0 %275:<2048x12288xf16> = torch.2_1_0.aten::view(%262:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %290:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%268:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %274:<2048x1x6144xf16> = torch.2_1_0.aten::gelu_backward(%267:<2048x1x6144xf16>, %222:<2048x1x6144xf16>, approximate=none:str) 
58390:3:0 %225:<12288xf16> = torch.2_1_0.aten::empty_like(%124:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %233:<12288xf16> = torch.2_1_0.aten::empty_like(%124:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %242:<1x1x6144xf16> = torch.2_1_0.aten::sum(%274:<2048x1x6144xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58394:7:0 %190:<6144xf16> = torch.2_1_0.aten::view(%242:<1x1x6144xf16>, list{6144:i32}) 
58394:7:0 %158:<6144xf16> = torch.2_1_0.aten::detach(%190:<6144xf16>) 
58394:7:0 %181:<6144xf16> = torch.2_1_0.aten::detach(%158:<6144xf16>) 
58394:7:0 %267:<6144xf16> = torch.2_1_0.aten::detach(%181:<6144xf16>) 
58394:7:0 %84:<6144xf16>+75534336 = torch.2_1_0.aten::add_(%84:<6144xf16>+75534336, %267:<6144xf16>, alpha=1:i32) 
58394:7:0 %190:<2048x6144xf16> = torch.2_1_0.aten::view(%274:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58394:7:0 %267:<2048x12288xf16> = torch.2_1_0.aten::mm(%190:<2048x6144xf16>, %91:<6144x12288xf16>) 
58394:7:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%267:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %190:<2048x6144xf16> = torch.2_1_0.aten::view(%274:<2048x1x6144xf16>, list{2048:i32, 6144:i32}) 
58394:7:0 %222:<2048x12288xf16> = torch.2_1_0.aten::view(%223:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:None:0 list{%272:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%272:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %185:<6144x2048xf16>{1,6144} = torch.2_1_0.aten::t(%190:<2048x6144xf16>) 
58394:7:0 %181:<6144x12288xf16> = torch.2_1_0.aten::mm(%185:<6144x2048xf16>{1,6144}, %222:<2048x12288xf16>) 
58394:7:0 %217:<6144x12288xf16> = torch.2_1_0.aten::detach(%181:<6144x12288xf16>) 
58394:7:0 %161:<6144x12288xf16> = torch.2_1_0.aten::detach(%217:<6144x12288xf16>) 
58394:7:0 %267:<6144x12288xf16> = torch.2_1_0.aten::detach(%161:<6144x12288xf16>) 
58394:7:0 %104:<6144x12288xf16>+75540480 = torch.2_1_0.aten::add_(%104:<6144x12288xf16>+75540480, %267:<6144x12288xf16>, alpha=1:i32) 
58394:7:0 %267:<2048x12288xf16> = torch.2_1_0.aten::view(%203:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %269:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%267:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::empty_like(%31:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %275:<12288xf16> = torch.2_1_0.aten::empty_like(%31:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %151:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %322:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %325:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %279:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %255:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %299:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %315:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %191:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %324:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %326:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %323:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %269:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %314:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58392:5:0 %219:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58388:1:0 %283:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58391:4:0 %252:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58393:6:0 %297:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58389:2:0 %327:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58387:0:0 %289:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %188:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %260:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %220:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %324:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %328:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %311:<2048x1x12288xf16> = torch.2_1_0.aten::view(%316:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %260:<2048x1x12288xf16> = torch.2_1_0.aten::view(%261:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %219:<2048x1x12288xf16> = torch.2_1_0.aten::view(%193:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %283:<2048x1x12288xf16> = torch.2_1_0.aten::view(%269:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %324:<2048x1x12288xf16> = torch.2_1_0.aten::view(%318:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %281:<2048x1x12288xf16> = torch.2_1_0.aten::add(%260:<2048x1x12288xf16>, %311:<2048x1x12288xf16>, alpha=1:i32) 
58389:2:0 %312:<2048x1x12288xf16> = torch.2_1_0.aten::view(%321:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %277:<2048x1x12288xf16> = torch.2_1_0.aten::add(%274:<2048x1x12288xf16>, %260:<2048x1x12288xf16>, alpha=1:i32) 
58392:5:0 %222:<2048x1x12288xf16> = torch.2_1_0.aten::add(%217:<2048x1x12288xf16>, %219:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %313:<12288xf16> = torch.2_1_0.aten::detach(%318:<12288xf16>) 
58388:1:0 %326:<2048x1x12288xf16> = torch.2_1_0.aten::add(%318:<2048x1x12288xf16>, %283:<2048x1x12288xf16>, alpha=1:i32) 
58393:6:0 %271:<2048x1x12288xf16> = torch.2_1_0.aten::add(%314:<2048x1x12288xf16>, %324:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%313:<12288xf16>) 
58391:4:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%219:<12288xf16>) 
58392:5:0 %193:<12288xf16> = torch.2_1_0.aten::detach(%74:<12288xf16>) 
58388:1:0 %269:<12288xf16> = torch.2_1_0.aten::detach(%325:<12288xf16>) 
58389:2:0 %221:<2048x1x12288xf16> = torch.2_1_0.aten::add(%320:<2048x1x12288xf16>, %312:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%260:<12288xf16>) 
58393:6:0 %245:<12288xf16> = torch.2_1_0.aten::detach(%320:<12288xf16>) 
58391:4:0 %210:<12288xf16> = torch.2_1_0.aten::detach(%260:<12288xf16>) 
58392:5:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%193:<12288xf16>) 
58388:1:0 %276:<12288xf16> = torch.2_1_0.aten::detach(%269:<12288xf16>) 
58387:0:0 %141:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%141:<12288xf16>+151050240, %316:<12288xf16>, alpha=1:i32) 
58392:5:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%209:<12288xf16>) 
58391:4:0 %261:<12288xf16> = torch.2_1_0.aten::detach(%210:<12288xf16>) 
58389:2:0 %312:<12288xf16> = torch.2_1_0.aten::detach(%297:<12288xf16>) 
58393:6:0 %84:<12288xf16> = torch.2_1_0.aten::detach(%245:<12288xf16>) 
58388:1:0 %278:<12288xf16> = torch.2_1_0.aten::detach(%276:<12288xf16>) 
58387:0:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%312:<12288xf16>) 
58393:6:0 %245:<12288xf16> = torch.2_1_0.aten::detach(%84:<12288xf16>) 
58392:5:0 %65:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%65:<12288xf16>+151050240, %219:<12288xf16>, alpha=1:i32) 
58391:4:0 %145:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%145:<12288xf16>+151050240, %261:<12288xf16>, alpha=1:i32) 
58389:2:0 %314:<12288xf16> = torch.2_1_0.aten::detach(%312:<12288xf16>) 
58387:0:0 %286:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58388:1:0 %151:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%151:<12288xf16>+151050240, %278:<12288xf16>, alpha=1:i32) 
58391:4:0 %210:<12288xf16> = torch.2_1_0.aten::detach(%249:<12288xf16>) 
58392:5:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%153:<12288xf16>) 
58387:0:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%286:<12288xf16>) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::detach(%314:<12288xf16>) 
58393:6:0 %132:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%132:<12288xf16>+151050240, %245:<12288xf16>, alpha=1:i32) 
58388:1:0 %276:<12288xf16> = torch.2_1_0.aten::detach(%288:<12288xf16>) 
58391:4:0 %261:<12288xf16> = torch.2_1_0.aten::detach(%210:<12288xf16>) 
58392:5:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%209:<12288xf16>) 
58387:0:0 %140:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%140:<12288xf16>+151037952, %260:<12288xf16>, alpha=1:i32) 
58388:1:0 %208:<12288xf16> = torch.2_1_0.aten::detach(%276:<12288xf16>) 
58393:6:0 %322:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58391:4:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%261:<12288xf16>) 
58392:5:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%219:<12288xf16>) 
58389:2:0 %132:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%132:<12288xf16>+151050240, %321:<12288xf16>, alpha=1:i32) 
58388:1:0 %21:<12288xf16> = torch.2_1_0.aten::detach(%208:<12288xf16>) 
58393:6:0 %320:<12288xf16> = torch.2_1_0.aten::detach(%322:<12288xf16>) 
58387:0:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%281:<2048x1x12288xf16>, %258:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58391:4:0 %142:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%142:<12288xf16>+151037952, %219:<12288xf16>, alpha=1:i32) 
58392:5:0 %59:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%59:<12288xf16>+151037952, %209:<12288xf16>, alpha=1:i32) 
58389:2:0 %314:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58388:1:0 %145:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%145:<12288xf16>+151037952, %21:<12288xf16>, alpha=1:i32) 
58393:6:0 %322:<12288xf16> = torch.2_1_0.aten::detach(%320:<12288xf16>) 
58389:2:0 %152:<12288xf16> = torch.2_1_0.aten::detach(%314:<12288xf16>) 
58387:0:0 %312:<1x1x12288xf16> = torch.2_1_0.aten::sum(%95:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58391:4:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%277:<2048x1x12288xf16>, %228:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58393:6:0 %67:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%67:<12288xf16>+151037952, %322:<12288xf16>, alpha=1:i32) 
58392:5:0 %74:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%222:<2048x1x12288xf16>, %163:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58388:1:0 %325:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%326:<2048x1x12288xf16>, %200:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58387:0:0 %286:<12288xf16> = torch.2_1_0.aten::view(%312:<1x1x12288xf16>, list{12288:i32}) 
58389:2:0 %314:<12288xf16> = torch.2_1_0.aten::detach(%152:<12288xf16>) 
58387:0:0 %316:<12288xf16> = torch.2_1_0.aten::detach(%286:<12288xf16>) 
58393:6:0 %312:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%271:<2048x1x12288xf16>, %246:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58391:4:0 %261:<1x1x12288xf16> = torch.2_1_0.aten::sum(%44:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58392:5:0 %163:<1x1x12288xf16> = torch.2_1_0.aten::sum(%74:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58387:0:0 %311:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58389:2:0 %135:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%135:<12288xf16>+151037952, %314:<12288xf16>, alpha=1:i32) 
58388:1:0 %200:<1x1x12288xf16> = torch.2_1_0.aten::sum(%325:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58391:4:0 %249:<12288xf16> = torch.2_1_0.aten::view(%261:<1x1x12288xf16>, list{12288:i32}) 
58387:0:0 %260:<12288xf16> = torch.2_1_0.aten::detach(%311:<12288xf16>) 
58392:5:0 %153:<12288xf16> = torch.2_1_0.aten::view(%163:<1x1x12288xf16>, list{12288:i32}) 
58388:1:0 %288:<12288xf16> = torch.2_1_0.aten::view(%200:<1x1x12288xf16>, list{12288:i32}) 
58393:6:0 %321:<1x1x12288xf16> = torch.2_1_0.aten::sum(%312:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58392:5:0 %209:<12288xf16> = torch.2_1_0.aten::detach(%153:<12288xf16>) 
58391:4:0 %274:<12288xf16> = torch.2_1_0.aten::detach(%249:<12288xf16>) 
58387:0:0 %137:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%137:<12288xf16>+207690240, %260:<12288xf16>, alpha=1:i32) 
58389:2:0 %291:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%221:<2048x1x12288xf16>, %271:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58388:1:0 %278:<12288xf16> = torch.2_1_0.aten::detach(%288:<12288xf16>) 
58392:5:0 %217:<12288xf16> = torch.2_1_0.aten::detach(%209:<12288xf16>) 
58391:4:0 %210:<12288xf16> = torch.2_1_0.aten::detach(%274:<12288xf16>) 
58393:6:0 %320:<12288xf16> = torch.2_1_0.aten::view(%321:<1x1x12288xf16>, list{12288:i32}) 
58392:5:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%217:<12288xf16>) 
58388:1:0 %276:<12288xf16> = torch.2_1_0.aten::detach(%278:<12288xf16>) 
58391:4:0 %228:<12288xf16> = torch.2_1_0.aten::detach(%210:<12288xf16>) 
58387:0:0 %312:<2048x12288xf16> = torch.2_1_0.aten::view(%95:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %318:<12288xf16> = torch.2_1_0.aten::detach(%320:<12288xf16>) 
58388:1:0 %278:<12288xf16> = torch.2_1_0.aten::detach(%276:<12288xf16>) 
58392:5:0 %75:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%75:<12288xf16>+207690240, %219:<12288xf16>, alpha=1:i32) 
58391:4:0 %150:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%150:<12288xf16>+207690240, %228:<12288xf16>, alpha=1:i32) 
58389:2:0 %316:<1x1x12288xf16> = torch.2_1_0.aten::sum(%291:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58393:6:0 %311:<12288xf16> = torch.2_1_0.aten::detach(%318:<12288xf16>) 
58388:1:0 %160:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%160:<12288xf16>+207690240, %278:<12288xf16>, alpha=1:i32) 
58393:6:0 %322:<12288xf16> = torch.2_1_0.aten::detach(%311:<12288xf16>) 
58387:0:0 %316:<2048x1536xf16> = torch.2_1_0.aten::mm(%312:<2048x12288xf16>, %1:<12288x1536xf16>) 
58389:2:0 %314:<12288xf16> = torch.2_1_0.aten::view(%316:<1x1x12288xf16>, list{12288:i32}) 
58392:5:0 %217:<2048x12288xf16> = torch.2_1_0.aten::view(%74:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %249:<2048x12288xf16> = torch.2_1_0.aten::view(%44:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %311:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%316:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::detach(%314:<12288xf16>) 
58393:6:0 %136:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%136:<12288xf16>+207690240, %322:<12288xf16>, alpha=1:i32) 
58388:1:0 %296:<2048x12288xf16> = torch.2_1_0.aten::view(%325:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %316:<2048x12288xf16> = torch.2_1_0.aten::view(%95:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %312:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58387:0:0 %286:<2048x1536xf16> = torch.2_1_0.aten::view(%284:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58391:4:0 %261:<2048x1536xf16> = torch.2_1_0.aten::mm(%249:<2048x12288xf16>, %129:<12288x1536xf16>) 
58392:5:0 %153:<2048x1536xf16> = torch.2_1_0.aten::mm(%217:<2048x12288xf16>, %17:<12288x1536xf16>) 
58393:6:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %320:<12288xf16> = torch.2_1_0.aten::detach(%312:<12288xf16>) 
58388:1:0 %288:<2048x1536xf16> = torch.2_1_0.aten::mm(%296:<2048x12288xf16>, %23:<12288x1536xf16>) 
58387:0:0 %260:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%316:<2048x12288xf16>) 
58391:4:0 %228:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%261:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58392:5:0 %193:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%153:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58389:2:0 %140:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%140:<12288xf16>+207690240, %320:<12288xf16>, alpha=1:i32) 
58388:1:0 %323:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%288:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58391:4:0 %261:<2048x12288xf16> = torch.2_1_0.aten::view(%44:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %153:<2048x12288xf16> = torch.2_1_0.aten::view(%74:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %322:<2048x1536xf16> = torch.2_1_0.aten::mm(%311:<2048x12288xf16>, %120:<12288x1536xf16>) 
58387:0:0 %312:<12288x1536xf16> = torch.2_1_0.aten::mm(%260:<12288x2048xf16>{1,12288}, %286:<2048x1536xf16>) 
58388:1:0 %278:<2048x12288xf16> = torch.2_1_0.aten::view(%325:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %210:<2048x1536xf16> = torch.2_1_0.aten::view(%247:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58392:5:0 %219:<2048x1536xf16> = torch.2_1_0.aten::view(%186:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58393:6:0 %319:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%322:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58392:5:0 %209:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%153:<2048x12288xf16>) 
58391:4:0 %260:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%261:<2048x12288xf16>) 
58389:2:0 %316:<2048x12288xf16> = torch.2_1_0.aten::view(%291:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %283:<12288x1536xf16> = torch.2_1_0.aten::detach(%312:<12288x1536xf16>) 
58388:1:0 %200:<2048x1536xf16> = torch.2_1_0.aten::view(%263:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58393:6:0 %322:<2048x12288xf16> = torch.2_1_0.aten::view(%312:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %284:<12288x1536xf16> = torch.2_1_0.aten::detach(%283:<12288x1536xf16>) 
58388:1:0 %283:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%278:<2048x12288xf16>) 
58392:5:0 %181:<12288x1536xf16> = torch.2_1_0.aten::mm(%209:<12288x2048xf16>{1,12288}, %219:<2048x1536xf16>) 
58393:6:0 %318:<2048x1536xf16> = torch.2_1_0.aten::view(%267:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58387:0:0 %260:<12288x1536xf16> = torch.2_1_0.aten::detach(%284:<12288x1536xf16>) 
58391:4:0 %234:<12288x1536xf16> = torch.2_1_0.aten::mm(%260:<12288x2048xf16>{1,12288}, %210:<2048x1536xf16>) 
58389:2:0 %312:<2048x1536xf16> = torch.2_1_0.aten::mm(%316:<2048x12288xf16>, %13:<12288x1536xf16>) 
58393:6:0 %309:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%322:<2048x12288xf16>) 
58387:0:0 %148:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%148:<12288x1536xf16>+207702528, %260:<12288x1536xf16>, alpha=1:i32) 
58392:5:0 %187:<12288x1536xf16> = torch.2_1_0.aten::detach(%181:<12288x1536xf16>) 
58391:4:0 %246:<12288x1536xf16> = torch.2_1_0.aten::detach(%234:<12288x1536xf16>) 
58388:1:0 %208:<12288x1536xf16> = torch.2_1_0.aten::mm(%283:<12288x2048xf16>{1,12288}, %200:<2048x1536xf16>) 
58389:2:0 %152:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%312:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58387:0:0 %312:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%311:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58392:5:0 %219:<12288x1536xf16> = torch.2_1_0.aten::detach(%187:<12288x1536xf16>) 
58391:4:0 %247:<12288x1536xf16> = torch.2_1_0.aten::detach(%246:<12288x1536xf16>) 
58393:6:0 %295:<12288x1536xf16> = torch.2_1_0.aten::mm(%309:<12288x2048xf16>{1,12288}, %318:<2048x1536xf16>) 
58389:2:0 %312:<2048x12288xf16> = torch.2_1_0.aten::view(%291:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %260:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%312:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58392:5:0 %187:<12288x1536xf16> = torch.2_1_0.aten::detach(%219:<12288x1536xf16>) 
58388:1:0 %262:<12288x1536xf16> = torch.2_1_0.aten::detach(%208:<12288x1536xf16>) 
58391:4:0 %260:<12288x1536xf16> = torch.2_1_0.aten::detach(%247:<12288x1536xf16>) 
58389:2:0 %314:<2048x1536xf16> = torch.2_1_0.aten::view(%270:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58387:0:0 %312:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%260:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58393:6:0 %267:<12288x1536xf16> = torch.2_1_0.aten::detach(%295:<12288x1536xf16>) 
58392:5:0 %78:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%78:<12288x1536xf16>+207702528, %187:<12288x1536xf16>, alpha=1:i32) 
58388:1:0 %276:<12288x1536xf16> = torch.2_1_0.aten::detach(%262:<12288x1536xf16>) 
58391:4:0 %153:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%153:<12288x1536xf16>+207702528, %260:<12288x1536xf16>, alpha=1:i32) 
58387:0:0 %284:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%278:<12x2048x2048xf16>, 1:i32, 2:i32) 
58389:2:0 %320:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%312:<2048x12288xf16>) 
58393:6:0 %190:<12288x1536xf16> = torch.2_1_0.aten::detach(%267:<12288x1536xf16>) 
58392:5:0 %74:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%193:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %263:<12288x1536xf16> = torch.2_1_0.aten::detach(%276:<12288x1536xf16>) 
58391:4:0 %234:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%228:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %322:<12288x1536xf16> = torch.2_1_0.aten::detach(%190:<12288x1536xf16>) 
58392:5:0 %219:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%74:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58391:4:0 %260:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%234:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58388:1:0 %159:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%159:<12288x1536xf16>+207702528, %263:<12288x1536xf16>, alpha=1:i32) 
58389:2:0 %322:<12288x1536xf16> = torch.2_1_0.aten::mm(%320:<12288x2048xf16>{1,12288}, %314:<2048x1536xf16>) 
58393:6:0 %138:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%138:<12288x1536xf16>+207702528, %322:<12288x1536xf16>, alpha=1:i32) 
58392:5:0 %224:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%219:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58391:4:0 %234:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%260:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58388:1:0 %208:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%323:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58392:5:0 %181:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%182:<12x2048x2048xf16>, 1:i32, 2:i32) 
58389:2:0 %291:<12288x1536xf16> = torch.2_1_0.aten::detach(%322:<12288x1536xf16>) 
58391:4:0 %247:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%241:<12x2048x2048xf16>, 1:i32, 2:i32) 
58393:6:0 %245:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%319:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %263:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%208:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58389:2:0 %319:<12288x1536xf16> = torch.2_1_0.aten::detach(%291:<12288x1536xf16>) 
58393:6:0 %322:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%245:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58388:1:0 %208:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%263:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58387:0:0 %260:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%284:<12x2048x2048xf16>{4194304,1,2048}, %312:<12x2048x128xf16>{128,1536,1}) 
58389:2:0 %329:<12288x1536xf16> = torch.2_1_0.aten::detach(%319:<12288x1536xf16>) 
58393:6:0 %190:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%322:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58387:0:0 %284:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%279:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58388:1:0 %276:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%257:<12x2048x2048xf16>, 1:i32, 2:i32) 
58393:6:0 %295:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%261:<12x2048x2048xf16>, 1:i32, 2:i32) 
58389:2:0 %138:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%138:<12288x1536xf16>+207702528, %329:<12288x1536xf16>, alpha=1:i32) 
58391:4:0 %260:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%247:<12x2048x2048xf16>{4194304,1,2048}, %234:<12x2048x128xf16>{128,1536,1}) 
58392:5:0 %219:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%181:<12x2048x2048xf16>{4194304,1,2048}, %224:<12x2048x128xf16>{128,1536,1}) 
58389:2:0 %270:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%152:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58387:0:0 %285:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%312:<12x2048x128xf16>{128,1536,1}, %284:<12x128x2048xf16>{384,1,4608}+256) 
58391:4:0 %247:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%242:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58392:5:0 %181:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%183:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58387:0:0 %278:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%260:<12x2048x128xf16>, 0:i32, 1:i32) 
58389:2:0 %5:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%270:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58391:4:0 %261:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%234:<12x2048x128xf16>{128,1536,1}, %247:<12x128x2048xf16>{384,1,4608}+256) 
58387:0:0 %275:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%285:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58392:5:0 %186:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%224:<12x2048x128xf16>{128,1536,1}, %181:<12x128x2048xf16>{384,1,4608}+256) 
58389:2:0 %322:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%5:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58391:4:0 %241:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%260:<12x2048x128xf16>, 0:i32, 1:i32) 
58387:0:0 %311:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%278:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %322:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%295:<12x2048x2048xf16>{4194304,1,2048}, %190:<12x2048x128xf16>{128,1536,1}) 
58392:5:0 %207:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%219:<12x2048x128xf16>, 0:i32, 1:i32) 
58391:4:0 %278:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%261:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58389:2:0 %329:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%264:<12x2048x2048xf16>, 1:i32, 2:i32) 
58387:0:0 %95:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%275:<1x12x2048x2048xf16>, %276:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58393:6:0 %295:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%262:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58392:5:0 %174:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%186:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58391:4:0 %228:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%241:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %263:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%276:<12x2048x2048xf16>{4194304,1,2048}, %208:<12x2048x128xf16>{128,1536,1}) 
58387:0:0 %312:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%274:<1x12x2048x2048xf16>) 
58392:5:0 %183:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%207:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %309:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%190:<12x2048x128xf16>{128,1536,1}, %295:<12x128x2048xf16>{384,1,4608}+256) 
58391:4:0 %279:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%278:<1x12x2048x2048xf16>, %239:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58388:1:0 %276:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%258:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58392:5:0 %224:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%174:<1x12x2048x2048xf16>, %180:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58391:4:0 %278:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%237:<1x12x2048x2048xf16>) 
58393:6:0 %262:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%322:<12x2048x128xf16>, 0:i32, 1:i32) 
58392:5:0 %186:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%179:<1x12x2048x2048xf16>) 
58388:1:0 %200:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%208:<12x2048x128xf16>{128,1536,1}, %276:<12x128x2048xf16>{384,1,4608}+256) 
58393:6:0 %190:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%309:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58387:0:0 %275:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%95:<1x12x2048x2048xf16>, %312:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58389:2:0 %312:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%329:<12x2048x2048xf16>{4194304,1,2048}, %322:<12x2048x128xf16>{128,1536,1}) 
58393:6:0 %320:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%262:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %325:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%263:<12x2048x128xf16>, 0:i32, 1:i32) 
58391:4:0 %72:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%279:<1x12x2048x2048xf16>, %278:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58389:2:0 %5:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%265:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58392:5:0 %48:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%224:<1x12x2048x2048xf16>, %186:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58388:1:0 %254:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%200:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58393:6:0 %312:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%190:<1x12x2048x2048xf16>, %259:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58387:0:0 %274:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%275:<1x12x2048x2048xf16>, 0:i32) 
58391:4:0 %280:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%72:<1x12x2048x2048xf16>, 0:i32) 
58388:1:0 %258:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%325:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58389:2:0 %320:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%322:<12x2048x128xf16>{128,1536,1}, %5:<12x128x2048xf16>{384,1,4608}+256) 
58392:5:0 %179:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%48:<1x12x2048x2048xf16>, 0:i32) 
58393:6:0 %295:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%257:<1x12x2048x2048xf16>) 
58387:0:0 %312:<50331648xf16> = torch.2_1_0.aten::new_zeros(%274:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58389:2:0 %152:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%312:<12x2048x128xf16>, 0:i32, 1:i32) 
58391:4:0 %237:<50331648xf16> = torch.2_1_0.aten::new_zeros(%280:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58388:1:0 %21:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%254:<1x12x2048x2048xf16>, %255:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58387:0:0 %95:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%312:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58392:5:0 %186:<50331648xf16> = torch.2_1_0.aten::new_zeros(%179:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58388:1:0 %200:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%253:<1x12x2048x2048xf16>) 
58389:2:0 %5:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%320:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58391:4:0 %44:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%237:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58393:6:0 %309:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%312:<1x12x2048x2048xf16>, %295:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58387:0:0 %95:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%95:<12x2048x2048xf16>, %274:<12x2048x2048xf16>, False:pred) 
58392:5:0 %84:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%186:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58389:2:0 %322:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%152:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %44:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%44:<12x2048x2048xf16>, %280:<12x2048x2048xf16>, False:pred) 
58387:0:0 %318:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%312:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58393:6:0 %312:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%309:<1x12x2048x2048xf16>, 0:i32) 
58392:5:0 %84:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%84:<12x2048x2048xf16>, %179:<12x2048x2048xf16>, False:pred) 
58391:4:0 %279:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%237:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58389:2:0 %329:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%5:<1x12x2048x2048xf16>, %262:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58392:5:0 %224:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%186:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58388:1:0 %255:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%21:<1x12x2048x2048xf16>, %200:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58387:0:0 %274:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%318:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58389:2:0 %297:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%260:<1x12x2048x2048xf16>) 
58393:6:0 %325:<50331648xf16> = torch.2_1_0.aten::new_zeros(%312:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58387:0:0 %274:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%274:<12x2048x2048xf16>, %318:<12x2048x2048xf16>, False:pred) 
58391:4:0 %280:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%279:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58388:1:0 %253:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%255:<1x12x2048x2048xf16>, 0:i32) 
58392:5:0 %179:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%224:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58393:6:0 %326:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%325:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58387:0:0 %275:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%274:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58391:4:0 %280:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%280:<12x2048x2048xf16>, %279:<12x2048x2048xf16>, False:pred) 
58392:5:0 %179:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%179:<12x2048x2048xf16>, %224:<12x2048x2048xf16>, False:pred) 
58387:0:0 %316:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%275:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58393:6:0 %326:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%326:<12x2048x2048xf16>, %312:<12x2048x2048xf16>, False:pred) 
58391:4:0 %234:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%280:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58389:2:0 %5:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%329:<1x12x2048x2048xf16>, %297:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58388:1:0 %257:<50331648xf16> = torch.2_1_0.aten::new_zeros(%253:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58392:5:0 %32:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%179:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58393:6:0 %219:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%325:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58391:4:0 %239:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%234:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58389:2:0 %260:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%5:<1x12x2048x2048xf16>, 0:i32) 
58392:5:0 %180:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%32:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58388:1:0 %323:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%257:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58387:0:0 %319:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%316:<1x12x2048x2048xf16>, %244:<1x1x2048x2048xunknown>, 0:i32) 
58393:6:0 %131:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%219:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58391:4:0 %242:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%239:<1x12x2048x2048xf16>, %212:<1x1x2048x2048xunknown>, 0:i32) 
58388:1:0 %323:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%323:<12x2048x2048xf16>, %253:<12x2048x2048xf16>, False:pred) 
58387:0:0 %275:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%275:<1x12x2048x2048xf16>, %319:<1x12x2048x2048xf16>, False:pred) 
58392:5:0 %225:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%180:<1x12x2048x2048xf16>, %143:<1x1x2048x2048xunknown>, 0:i32) 
58393:6:0 %131:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%131:<12x2048x2048xf16>, %219:<12x2048x2048xf16>, False:pred) 
58391:4:0 %234:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%234:<1x12x2048x2048xf16>, %242:<1x12x2048x2048xf16>, False:pred) 
58389:2:0 %330:<50331648xf16> = torch.2_1_0.aten::new_zeros(%260:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58388:1:0 %21:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%257:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58387:0:0 %95:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%270:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58392:5:0 %32:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%32:<1x12x2048x2048xf16>, %225:<1x12x2048x2048xf16>, False:pred) 
58393:6:0 %311:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%131:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58391:4:0 %51:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%235:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58392:5:0 %32:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%175:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58393:6:0 %309:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%311:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58389:2:0 %297:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%330:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58387:0:0 %286:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%274:<12x2048x2048xf16>, %95:<12x2048x128xf16>{384,4608,1}+128) 
58391:4:0 %212:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%280:<12x2048x2048xf16>, %51:<12x2048x128xf16>{384,4608,1}+128) 
58388:1:0 %200:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%21:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58387:0:0 %312:<12x2048x128xf16> = torch.2_1_0.aten::mul(%286:<12x2048x128xf16>, 0.08838834764831843:f32) 
58392:5:0 %225:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%179:<12x2048x2048xf16>, %32:<12x2048x128xf16>{384,4608,1}+128) 
58389:2:0 %297:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%297:<12x2048x2048xf16>, %260:<12x2048x2048xf16>, False:pred) 
58393:6:0 %178:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%309:<1x12x2048x2048xf16>, %223:<1x1x2048x2048xunknown>, 0:i32) 
58387:0:0 %286:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%269:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58391:4:0 %234:<12x2048x128xf16> = torch.2_1_0.aten::mul(%212:<12x2048x128xf16>, 0.08838834764831843:f32) 
58388:1:0 %200:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%200:<12x2048x2048xf16>, %21:<12x2048x2048xf16>, False:pred) 
58392:5:0 %186:<12x2048x128xf16> = torch.2_1_0.aten::mul(%225:<12x2048x128xf16>, 0.08838834764831843:f32) 
58389:2:0 %152:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%330:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58393:6:0 %311:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%311:<1x12x2048x2048xf16>, %178:<1x12x2048x2048xf16>, False:pred) 
58391:4:0 %239:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%225:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58388:1:0 %255:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%200:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58392:5:0 %84:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%173:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58387:0:0 %244:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%286:<12x128x2048xf16>{384,1,4608}, %274:<12x2048x2048xf16>) 
58393:6:0 %223:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%253:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58387:0:0 %275:<12x128x2048xf16> = torch.2_1_0.aten::mul(%244:<12x128x2048xf16>, 0.08838834764831843:f32) 
58391:4:0 %212:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%239:<12x128x2048xf16>{384,1,4608}, %280:<12x2048x2048xf16>) 
58388:1:0 %288:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%255:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58392:5:0 %174:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%84:<12x128x2048xf16>{384,1,4608}, %179:<12x2048x2048xf16>) 
58389:2:0 %291:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%152:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58393:6:0 %219:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%131:<12x2048x2048xf16>, %223:<12x2048x128xf16>{384,4608,1}+128) 
58387:0:0 %270:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%275:<12x128x2048xf16>, 1:i32, 2:i32) 
58391:4:0 %242:<12x128x2048xf16> = torch.2_1_0.aten::mul(%212:<12x128x2048xf16>, 0.08838834764831843:f32) 
58392:5:0 %181:<12x128x2048xf16> = torch.2_1_0.aten::mul(%174:<12x128x2048xf16>, 0.08838834764831843:f32) 
58387:0:0 %263:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%270:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58389:2:0 %291:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%291:<12x2048x2048xf16>, %152:<12x2048x2048xf16>, False:pred) 
58391:4:0 %235:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%242:<12x128x2048xf16>, 1:i32, 2:i32) 
58393:6:0 %327:<12x2048x128xf16> = torch.2_1_0.aten::mul(%219:<12x2048x128xf16>, 0.08838834764831843:f32) 
58388:1:0 %278:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%288:<1x12x2048x2048xf16>, %225:<1x1x2048x2048xunknown>, 0:i32) 
58392:5:0 %43:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%181:<12x128x2048xf16>, 1:i32, 2:i32) 
58387:0:0 %269:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%312:<12x2048x128xf16>, 0:i32, 1:i32) 
58393:6:0 %257:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%250:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58391:4:0 %202:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%235:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58389:2:0 %318:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%291:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58392:5:0 %157:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%43:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58387:0:0 %270:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%263:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %255:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%255:<1x12x2048x2048xf16>, %278:<1x12x2048x2048xf16>, False:pred) 
58392:5:0 %175:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%186:<12x2048x128xf16>, 0:i32, 1:i32) 
58391:4:0 %225:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%234:<12x2048x128xf16>, 0:i32, 1:i32) 
58389:2:0 %320:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%318:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58393:6:0 %314:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%257:<12x128x2048xf16>{384,1,4608}, %131:<12x2048x2048xf16>) 
58387:0:0 %263:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%269:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58392:5:0 %187:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%157:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %235:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%202:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %21:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%249:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58387:0:0 %274:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%263:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %297:<12x128x2048xf16> = torch.2_1_0.aten::mul(%314:<12x128x2048xf16>, 0.08838834764831843:f32) 
58392:5:0 %157:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%175:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %202:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%225:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58389:2:0 %6:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%320:<1x12x2048x2048xf16>, %214:<1x1x2048x2048xunknown>, 0:i32) 
58393:6:0 %250:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%297:<12x128x2048xf16>, 1:i32, 2:i32) 
58388:1:0 %257:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%200:<12x2048x2048xf16>, %21:<12x2048x128xf16>{384,4608,1}+128) 
58392:5:0 %173:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%157:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %280:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%202:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %257:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%250:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58389:2:0 %318:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%318:<1x12x2048x2048xf16>, %6:<1x12x2048x2048xf16>, False:pred) 
58387:0:0 %286:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%274:<2048x1x12x128xf16>{128,3145728,262144,1}, %270:<2048x1x12x128xf16>{1,3145728,262144,2048}, %311:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58388:1:0 %278:<12x2048x128xf16> = torch.2_1_0.aten::mul(%257:<12x2048x128xf16>, 0.08838834764831843:f32) 
58393:6:0 %326:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%327:<12x2048x128xf16>, 0:i32, 1:i32) 
58387:0:0 %260:<2048x1x4608xf16> = torch.2_1_0.aten::view(%286:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58388:1:0 %224:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%247:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58393:6:0 %314:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%257:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58392:5:0 %179:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%173:<2048x1x12x128xf16>{128,3145728,262144,1}, %187:<2048x1x12x128xf16>{1,3145728,262144,2048}, %183:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58391:4:0 %239:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%280:<2048x1x12x128xf16>{128,3145728,262144,1}, %235:<2048x1x12x128xf16>{1,3145728,262144,2048}, %228:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58389:2:0 %330:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%256:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58387:0:0 %275:<2048x4608xf16> = torch.2_1_0.aten::view(%260:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58392:5:0 %219:<2048x1x4608xf16> = torch.2_1_0.aten::view(%179:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58393:6:0 %323:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%326:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %257:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%224:<12x128x2048xf16>{384,1,4608}, %200:<12x2048x2048xf16>) 
58391:4:0 %260:<2048x1x4608xf16> = torch.2_1_0.aten::view(%239:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58389:2:0 %6:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%291:<12x2048x2048xf16>, %330:<12x2048x128xf16>{384,4608,1}+128) 
58387:0:0 %311:<2048x12288xf16> = torch.2_1_0.aten::mm(%275:<2048x4608xf16>, %77:<4608x12288xf16>) 
58388:1:0 %283:<12x128x2048xf16> = torch.2_1_0.aten::mul(%257:<12x128x2048xf16>, 0.08838834764831843:f32) 
58392:5:0 %183:<2048x4608xf16> = torch.2_1_0.aten::view(%219:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58393:6:0 %325:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%323:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %228:<2048x4608xf16> = torch.2_1_0.aten::view(%260:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58387:0:0 %95:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%311:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %331:<12x2048x128xf16> = torch.2_1_0.aten::mul(%6:<12x2048x128xf16>, 0.08838834764831843:f32) 
58388:1:0 %323:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%283:<12x128x2048xf16>, 1:i32, 2:i32) 
58387:0:0 %274:<2048x4608xf16> = torch.2_1_0.aten::view(%260:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58392:5:0 %187:<2048x12288xf16> = torch.2_1_0.aten::mm(%183:<2048x4608xf16>, %37:<4608x12288xf16>) 
58389:2:0 %6:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%254:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58391:4:0 %235:<2048x12288xf16> = torch.2_1_0.aten::mm(%228:<2048x4608xf16>, %131:<4608x12288xf16>) 
58387:0:0 %312:<2048x12288xf16> = torch.2_1_0.aten::view(%250:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %237:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%323:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58392:5:0 %181:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%187:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %242:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%235:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %310:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%325:<2048x1x12x128xf16>{128,3145728,262144,1}, %314:<2048x1x12x128xf16>{1,3145728,262144,2048}, %320:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58392:5:0 %187:<2048x4608xf16> = torch.2_1_0.aten::view(%219:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58388:1:0 %224:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%278:<12x2048x128xf16>, 0:i32, 1:i32) 
58389:2:0 %329:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%6:<12x128x2048xf16>{384,1,4608}, %291:<12x2048x2048xf16>) 
58391:4:0 %235:<2048x4608xf16> = torch.2_1_0.aten::view(%260:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58393:6:0 %326:<2048x1x4608xf16> = torch.2_1_0.aten::view(%310:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58392:5:0 %157:<2048x12288xf16> = torch.2_1_0.aten::view(%154:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58388:1:0 %249:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%237:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58391:4:0 %225:<2048x12288xf16> = torch.2_1_0.aten::view(%227:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58389:2:0 %318:<12x128x2048xf16> = torch.2_1_0.aten::mul(%329:<12x128x2048xf16>, 0.08838834764831843:f32) 
58393:6:0 %309:<2048x4608xf16> = torch.2_1_0.aten::view(%326:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58388:1:0 %237:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%224:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58387:None:0 list{%95:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%95:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %291:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%318:<12x128x2048xf16>, 1:i32, 2:i32) 
58388:1:0 %21:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%237:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58393:6:0 %320:<2048x12288xf16> = torch.2_1_0.aten::mm(%309:<2048x4608xf16>, %80:<4608x12288xf16>) 
58387:0:0 %270:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%274:<2048x4608xf16>) 
58389:2:0 %6:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%291:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58393:6:0 %322:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%320:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:None:0 list{%181:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%181:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:None:0 list{%242:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%242:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %152:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%331:<12x2048x128xf16>, 0:i32, 1:i32) 
58387:0:0 %244:<4608x12288xf16> = torch.2_1_0.aten::mm(%270:<4608x2048xf16>{1,4608}, %312:<2048x12288xf16>) 
58393:6:0 %320:<2048x4608xf16> = torch.2_1_0.aten::view(%326:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58392:5:0 %183:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%187:<2048x4608xf16>) 
58393:6:0 %311:<2048x12288xf16> = torch.2_1_0.aten::view(%216:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %280:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%235:<2048x4608xf16>) 
58389:2:0 %254:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%6:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %200:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%21:<2048x1x12x128xf16>{128,3145728,262144,1}, %249:<2048x1x12x128xf16>{1,3145728,262144,2048}, %258:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58387:0:0 %269:<4608xf16> = torch.2_1_0.aten::sum(%274:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58392:5:0 %153:<4608x12288xf16> = torch.2_1_0.aten::mm(%183:<4608x2048xf16>{1,4608}, %157:<2048x12288xf16>) 
58389:2:0 %326:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%152:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58388:1:0 %224:<2048x1x4608xf16> = torch.2_1_0.aten::view(%200:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58391:4:0 %212:<4608x12288xf16> = torch.2_1_0.aten::mm(%280:<4608x2048xf16>{1,4608}, %225:<2048x12288xf16>) 
58387:0:0 %260:<4608x12288xf16> = torch.2_1_0.aten::detach(%244:<4608x12288xf16>) 
58389:2:0 %330:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%326:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58387:0:0 %250:<4608x12288xf16> = torch.2_1_0.aten::detach(%260:<4608x12288xf16>) 
58388:1:0 %296:<2048x4608xf16> = torch.2_1_0.aten::view(%224:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58392:5:0 %174:<4608xf16> = torch.2_1_0.aten::sum(%187:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58393:None:0 list{%322:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%322:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %202:<4608xf16> = torch.2_1_0.aten::sum(%235:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58387:0:0 %270:<4608x12288xf16> = torch.2_1_0.aten::detach(%250:<4608x12288xf16>) 
58393:6:0 %314:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%320:<2048x4608xf16>) 
58387:0:0 %144:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%144:<4608x12288xf16>+151067136, %270:<4608x12288xf16>, alpha=1:i32) 
58388:1:0 %258:<2048x12288xf16> = torch.2_1_0.aten::mm(%296:<2048x4608xf16>, %119:<4608x12288xf16>) 
58392:5:0 %179:<4608x12288xf16> = torch.2_1_0.aten::detach(%153:<4608x12288xf16>) 
58389:2:0 %325:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%330:<2048x1x12x128xf16>{128,3145728,262144,1}, %254:<2048x1x12x128xf16>{1,3145728,262144,2048}, %322:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58387:0:0 %244:<4608xf16> = torch.2_1_0.aten::detach(%269:<4608xf16>) 
58391:4:0 %239:<4608x12288xf16> = torch.2_1_0.aten::detach(%212:<4608x12288xf16>) 
58388:1:0 %21:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%258:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58392:5:0 %154:<4608x12288xf16> = torch.2_1_0.aten::detach(%179:<4608x12288xf16>) 
58387:0:0 %250:<4608xf16> = torch.2_1_0.aten::detach(%244:<4608xf16>) 
58391:4:0 %227:<4608x12288xf16> = torch.2_1_0.aten::detach(%239:<4608x12288xf16>) 
58387:0:0 %312:<4608xf16> = torch.2_1_0.aten::detach(%250:<4608xf16>) 
58393:6:0 %295:<4608x12288xf16> = torch.2_1_0.aten::mm(%314:<4608x2048xf16>{1,4608}, %311:<2048x12288xf16>) 
58392:5:0 %179:<4608x12288xf16> = torch.2_1_0.aten::detach(%154:<4608x12288xf16>) 
58389:2:0 %331:<2048x1x4608xf16> = torch.2_1_0.aten::view(%325:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58388:1:0 %249:<2048x4608xf16> = torch.2_1_0.aten::view(%224:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58391:4:0 %235:<4608x12288xf16> = torch.2_1_0.aten::detach(%227:<4608x12288xf16>) 
58387:0:0 %143:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%143:<4608xf16>+151062528, %312:<4608xf16>, alpha=1:i32) 
58392:5:0 %72:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%72:<4608x12288xf16>+151067136, %179:<4608x12288xf16>, alpha=1:i32) 
58388:1:0 %283:<2048x12288xf16> = torch.2_1_0.aten::view(%229:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58391:4:0 %147:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%147:<4608x12288xf16>+151067136, %235:<4608x12288xf16>, alpha=1:i32) 
58389:2:0 %318:<2048x4608xf16> = torch.2_1_0.aten::view(%331:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58387:0:0 %250:<2048x12288xf16> = torch.2_1_0.aten::view(%253:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58393:6:0 %297:<4608xf16> = torch.2_1_0.aten::sum(%320:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58392:5:0 %153:<4608xf16> = torch.2_1_0.aten::detach(%174:<4608xf16>) 
58391:4:0 %212:<4608xf16> = torch.2_1_0.aten::detach(%202:<4608xf16>) 
58387:0:0 %312:<2048x12288xf16> = torch.2_1_0.aten::view(%95:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %154:<4608xf16> = torch.2_1_0.aten::detach(%153:<4608xf16>) 
58391:4:0 %227:<4608xf16> = torch.2_1_0.aten::detach(%212:<4608xf16>) 
58389:2:0 %330:<2048x12288xf16> = torch.2_1_0.aten::mm(%318:<2048x4608xf16>, %12:<4608x12288xf16>) 
58392:5:0 %153:<4608xf16> = torch.2_1_0.aten::detach(%154:<4608xf16>) 
58387:0:0 %270:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%250:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %131:<4608x12288xf16> = torch.2_1_0.aten::detach(%295:<4608x12288xf16>) 
58391:4:0 %212:<4608xf16> = torch.2_1_0.aten::detach(%227:<4608xf16>) 
58389:2:0 %297:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%330:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %274:<12288xf16> = torch.2_1_0.aten::empty_like(%120:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %311:<4608x12288xf16> = torch.2_1_0.aten::detach(%131:<4608x12288xf16>) 
58392:5:0 %68:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%68:<4608xf16>+151062528, %153:<4608xf16>, alpha=1:i32) 
58389:2:0 %316:<2048x4608xf16> = torch.2_1_0.aten::view(%331:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58391:4:0 %146:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%146:<4608xf16>+151062528, %212:<4608xf16>, alpha=1:i32) 
58387:0:0 %316:<12288xf16> = torch.2_1_0.aten::empty_like(%120:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %257:<4608x12288xf16> = torch.2_1_0.aten::detach(%311:<4608x12288xf16>) 
58388:None:0 list{%21:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%21:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %321:<2048x12288xf16> = torch.2_1_0.aten::view(%212:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %224:<2048x12288xf16> = torch.2_1_0.aten::view(%155:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58387:0:0 %271:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %74:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%74:<4608x12288xf16>+151067136, %257:<4608x12288xf16>, alpha=1:i32) 
58391:4:0 %202:<2048x12288xf16> = torch.2_1_0.aten::view(%220:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58388:1:0 %263:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%249:<2048x4608xf16>) 
58392:5:0 %153:<2048x12288xf16> = torch.2_1_0.aten::view(%181:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %315:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %219:<4608xf16> = torch.2_1_0.aten::detach(%297:<4608xf16>) 
58391:4:0 %212:<2048x12288xf16> = torch.2_1_0.aten::view(%242:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58393:6:0 %131:<4608xf16> = torch.2_1_0.aten::detach(%219:<4608xf16>) 
58392:5:0 %157:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%224:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %239:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%202:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %311:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58393:6:0 %325:<4608xf16> = torch.2_1_0.aten::detach(%131:<4608xf16>) 
58388:1:0 %247:<4608x12288xf16> = torch.2_1_0.aten::mm(%263:<4608x2048xf16>{1,4608}, %283:<2048x12288xf16>) 
58392:5:0 %43:<12288xf16> = torch.2_1_0.aten::empty_like(%7:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %227:<12288xf16> = torch.2_1_0.aten::empty_like(%128:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %285:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %133:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%133:<4608xf16>+151062528, %325:<4608xf16>, alpha=1:i32) 
58389:None:0 list{%297:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%297:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %226:<12288xf16> = torch.2_1_0.aten::empty_like(%7:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %225:<12288xf16> = torch.2_1_0.aten::empty_like(%128:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %285:<2048x1x12288xf16> = torch.2_1_0.aten::view(%270:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58393:6:0 %326:<2048x12288xf16> = torch.2_1_0.aten::view(%228:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58389:2:0 %312:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%316:<2048x4608xf16>) 
58388:1:0 %278:<4608xf16> = torch.2_1_0.aten::sum(%249:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58392:5:0 %180:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %234:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %297:<2048x12288xf16> = torch.2_1_0.aten::view(%322:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58387:0:0 %44:<2048x1x12288xf16> = torch.2_1_0.aten::add(%281:<2048x1x12288xf16>, %285:<2048x1x12288xf16>, alpha=1:i32) 
58392:5:0 %186:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %228:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %311:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%326:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %285:<12288xf16> = torch.2_1_0.aten::detach(%274:<12288xf16>) 
58389:2:0 %326:<4608x12288xf16> = torch.2_1_0.aten::mm(%312:<4608x2048xf16>{1,4608}, %321:<2048x12288xf16>) 
58388:1:0 %327:<4608x12288xf16> = torch.2_1_0.aten::detach(%247:<4608x12288xf16>) 
58387:0:0 %281:<12288xf16> = torch.2_1_0.aten::detach(%285:<12288xf16>) 
58393:6:0 %325:<12288xf16> = torch.2_1_0.aten::empty_like(%2:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %182:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58391:4:0 %280:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58388:1:0 %229:<4608x12288xf16> = torch.2_1_0.aten::detach(%327:<4608x12288xf16>) 
58387:0:0 %270:<12288xf16> = torch.2_1_0.aten::detach(%281:<12288xf16>) 
58393:6:0 %148:<12288xf16> = torch.2_1_0.aten::empty_like(%2:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %322:<4608xf16> = torch.2_1_0.aten::sum(%316:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58388:1:0 %249:<4608x12288xf16> = torch.2_1_0.aten::detach(%229:<4608x12288xf16>) 
58392:5:0 %209:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %237:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %147:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%147:<12288xf16>+226589184, %270:<12288xf16>, alpha=1:i32) 
58393:6:0 %309:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %281:<12288xf16> = torch.2_1_0.aten::detach(%316:<12288xf16>) 
58392:5:0 %180:<2048x1x12288xf16> = torch.2_1_0.aten::view(%157:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %157:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%157:<4608x12288xf16>+151067136, %249:<4608x12288xf16>, alpha=1:i32) 
58391:4:0 %237:<2048x1x12288xf16> = torch.2_1_0.aten::view(%239:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58387:0:0 %270:<12288xf16> = torch.2_1_0.aten::detach(%281:<12288xf16>) 
58389:2:0 %330:<4608x12288xf16> = torch.2_1_0.aten::detach(%326:<4608x12288xf16>) 
58393:6:0 %303:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58387:0:0 %281:<12288xf16> = torch.2_1_0.aten::detach(%270:<12288xf16>) 
58388:1:0 %247:<4608xf16> = torch.2_1_0.aten::detach(%278:<4608xf16>) 
58389:2:0 %325:<4608x12288xf16> = torch.2_1_0.aten::detach(%330:<4608x12288xf16>) 
58392:5:0 %32:<2048x1x12288xf16> = torch.2_1_0.aten::add(%222:<2048x1x12288xf16>, %180:<2048x1x12288xf16>, alpha=1:i32) 
58391:4:0 %50:<2048x1x12288xf16> = torch.2_1_0.aten::add(%277:<2048x1x12288xf16>, %237:<2048x1x12288xf16>, alpha=1:i32) 
58387:0:0 %149:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%149:<12288xf16>+226576896, %281:<12288xf16>, alpha=1:i32) 
58388:1:0 %229:<4608xf16> = torch.2_1_0.aten::detach(%247:<4608xf16>) 
58393:6:0 %314:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58389:2:0 %331:<4608x12288xf16> = torch.2_1_0.aten::detach(%325:<4608x12288xf16>) 
58391:4:0 %239:<12288xf16> = torch.2_1_0.aten::detach(%227:<12288xf16>) 
58392:5:0 %157:<12288xf16> = torch.2_1_0.aten::detach(%43:<12288xf16>) 
58388:1:0 %247:<4608xf16> = torch.2_1_0.aten::detach(%229:<4608xf16>) 
58387:0:0 %318:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%44:<2048x1x12288xf16>, %254:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58391:4:0 %237:<12288xf16> = torch.2_1_0.aten::detach(%239:<12288xf16>) 
58392:5:0 %158:<12288xf16> = torch.2_1_0.aten::detach(%157:<12288xf16>) 
58393:6:0 %328:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %139:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%139:<4608x12288xf16>+151067136, %331:<4608x12288xf16>, alpha=1:i32) 
58388:1:0 %154:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%154:<4608xf16>+151062528, %247:<4608xf16>, alpha=1:i32) 
58391:4:0 %255:<12288xf16> = torch.2_1_0.aten::detach(%237:<12288xf16>) 
58392:5:0 %157:<12288xf16> = torch.2_1_0.aten::detach(%158:<12288xf16>) 
58387:0:0 %270:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%318:<2048x1x12288xf16>, 0:i32, 1:i32) 
58389:2:0 %326:<4608xf16> = torch.2_1_0.aten::detach(%322:<4608xf16>) 
58393:6:0 %323:<2048x1x12288xf16> = torch.2_1_0.aten::view(%311:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58391:4:0 %156:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%156:<12288xf16>+226589184, %255:<12288xf16>, alpha=1:i32) 
58392:5:0 %83:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%83:<12288xf16>+226589184, %157:<12288xf16>, alpha=1:i32) 
58389:2:0 %325:<4608xf16> = torch.2_1_0.aten::detach(%326:<4608xf16>) 
58388:1:0 %278:<2048x12288xf16> = torch.2_1_0.aten::view(%108:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58392:5:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%226:<12288xf16>) 
58391:4:0 %237:<12288xf16> = torch.2_1_0.aten::detach(%225:<12288xf16>) 
58389:2:0 %326:<4608xf16> = torch.2_1_0.aten::detach(%325:<4608xf16>) 
58393:6:0 %228:<2048x1x12288xf16> = torch.2_1_0.aten::add(%271:<2048x1x12288xf16>, %323:<2048x1x12288xf16>, alpha=1:i32) 
58388:1:0 %258:<2048x12288xf16> = torch.2_1_0.aten::view(%21:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %157:<12288xf16> = torch.2_1_0.aten::detach(%187:<12288xf16>) 
58391:4:0 %227:<12288xf16> = torch.2_1_0.aten::detach(%237:<12288xf16>) 
58392:5:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%157:<12288xf16>) 
58389:2:0 %131:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%131:<4608xf16>+151062528, %326:<4608xf16>, alpha=1:i32) 
58391:4:0 %261:<12288xf16> = torch.2_1_0.aten::detach(%227:<12288xf16>) 
58393:6:0 %311:<12288xf16> = torch.2_1_0.aten::detach(%325:<12288xf16>) 
58388:1:0 %249:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%278:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %80:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%80:<12288xf16>+226576896, %187:<12288xf16>, alpha=1:i32) 
58393:6:0 %323:<12288xf16> = torch.2_1_0.aten::detach(%311:<12288xf16>) 
58391:4:0 %154:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%154:<12288xf16>+226576896, %261:<12288xf16>, alpha=1:i32) 
58389:2:0 %322:<2048x12288xf16> = torch.2_1_0.aten::view(%241:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58393:6:0 %311:<12288xf16> = torch.2_1_0.aten::detach(%323:<12288xf16>) 
58388:1:0 %229:<12288xf16> = torch.2_1_0.aten::empty_like(%117:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %325:<2048x12288xf16> = torch.2_1_0.aten::view(%297:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58392:5:0 %222:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%32:<2048x1x12288xf16>, %156:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58391:4:0 %279:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%50:<2048x1x12288xf16>, %221:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58393:6:0 %135:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%135:<12288xf16>+226589184, %311:<12288xf16>, alpha=1:i32) 
58388:1:0 %263:<12288xf16> = torch.2_1_0.aten::empty_like(%117:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %312:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%322:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %156:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%222:<2048x1x12288xf16>, 0:i32, 1:i32) 
58391:4:0 %227:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%279:<2048x1x12288xf16>, 0:i32, 1:i32) 
58393:6:0 %236:<12288xf16> = torch.2_1_0.aten::detach(%148:<12288xf16>) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::empty_like(%100:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %323:<12288xf16> = torch.2_1_0.aten::detach(%236:<12288xf16>) 
58388:1:0 %237:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %320:<12288xf16> = torch.2_1_0.aten::empty_like(%100:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%323:<12288xf16>) 
58388:1:0 %257:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %130:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%130:<12288xf16>+226576896, %219:<12288xf16>, alpha=1:i32) 
58389:2:0 %314:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %313:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %288:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58393:6:0 %325:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%228:<2048x1x12288xf16>, %233:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58393:6:0 %236:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%325:<2048x1x12288xf16>, 0:i32, 1:i32) 
58389:2:0 %311:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58388:1:0 %255:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %288:<2048x1x12288xf16> = torch.2_1_0.aten::view(%249:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58389:2:0 %327:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %311:<2048x1x12288xf16> = torch.2_1_0.aten::view(%312:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %55:<2048x1x12288xf16> = torch.2_1_0.aten::add(%326:<2048x1x12288xf16>, %288:<2048x1x12288xf16>, alpha=1:i32) 
58388:1:0 %224:<12288xf16> = torch.2_1_0.aten::detach(%229:<12288xf16>) 
58389:2:0 %297:<2048x1x12288xf16> = torch.2_1_0.aten::add(%221:<2048x1x12288xf16>, %311:<2048x1x12288xf16>, alpha=1:i32) 
58388:1:0 %249:<12288xf16> = torch.2_1_0.aten::detach(%224:<12288xf16>) 
58389:2:0 %241:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58388:1:0 %323:<12288xf16> = torch.2_1_0.aten::detach(%249:<12288xf16>) 
58389:2:0 %312:<12288xf16> = torch.2_1_0.aten::detach(%241:<12288xf16>) 
58389:2:0 %241:<12288xf16> = torch.2_1_0.aten::detach(%312:<12288xf16>) 
58388:1:0 %167:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%167:<12288xf16>+226589184, %323:<12288xf16>, alpha=1:i32) 
58389:2:0 %141:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%141:<12288xf16>+226589184, %241:<12288xf16>, alpha=1:i32) 
58388:1:0 %225:<12288xf16> = torch.2_1_0.aten::detach(%263:<12288xf16>) 
58389:2:0 %243:<12288xf16> = torch.2_1_0.aten::detach(%320:<12288xf16>) 
58388:1:0 %229:<12288xf16> = torch.2_1_0.aten::detach(%225:<12288xf16>) 
58389:2:0 %321:<12288xf16> = torch.2_1_0.aten::detach(%243:<12288xf16>) 
58388:1:0 %42:<12288xf16> = torch.2_1_0.aten::detach(%229:<12288xf16>) 
58389:2:0 %332:<12288xf16> = torch.2_1_0.aten::detach(%321:<12288xf16>) 
58388:1:0 %164:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%164:<12288xf16>+226576896, %42:<12288xf16>, alpha=1:i32) 
58389:2:0 %142:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%142:<12288xf16>+226576896, %332:<12288xf16>, alpha=1:i32) 
58389:2:0 %329:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%297:<2048x1x12288xf16>, %80:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58388:1:0 %323:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%55:<2048x1x12288xf16>, %230:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58389:2:0 %243:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%329:<2048x1x12288xf16>, 0:i32, 1:i32) 
58388:1:0 %326:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%323:<2048x1x12288xf16>, 0:i32, 1:i32) 
58394:7:0 %276:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %277:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %291:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %278:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58390:3:0 %292:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %279:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %289:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58390:3:0 %293:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %280:<2048x1x12288xf16> = torch.2_1_0.aten::view(%269:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %289:<2048x1x12288xf16> = torch.2_1_0.aten::view(%290:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %220:<2048x1x12288xf16> = torch.2_1_0.aten::add(%270:<2048x1x12288xf16>, %280:<2048x1x12288xf16>, alpha=1:i32) 
58390:3:0 %229:<2048x1x12288xf16> = torch.2_1_0.aten::add(%264:<2048x1x12288xf16>, %289:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %203:<12288xf16> = torch.2_1_0.aten::detach(%217:<12288xf16>) 
58390:3:0 %275:<12288xf16> = torch.2_1_0.aten::detach(%225:<12288xf16>) 
58394:7:0 %219:<12288xf16> = torch.2_1_0.aten::detach(%203:<12288xf16>) 
58390:3:0 %250:<12288xf16> = torch.2_1_0.aten::detach(%275:<12288xf16>) 
58394:7:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%219:<12288xf16>) 
58390:3:0 %264:<12288xf16> = torch.2_1_0.aten::detach(%250:<12288xf16>) 
58394:7:0 %106:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%106:<12288xf16>+151050240, %190:<12288xf16>, alpha=1:i32) 
58390:3:0 %137:<12288xf16>+151050240 = torch.2_1_0.aten::add_(%137:<12288xf16>+151050240, %264:<12288xf16>, alpha=1:i32) 
58390:3:0 %268:<12288xf16> = torch.2_1_0.aten::detach(%233:<12288xf16>) 
58394:7:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%275:<12288xf16>) 
58390:3:0 %275:<12288xf16> = torch.2_1_0.aten::detach(%268:<12288xf16>) 
58394:7:0 %158:<12288xf16> = torch.2_1_0.aten::detach(%190:<12288xf16>) 
58390:3:0 %264:<12288xf16> = torch.2_1_0.aten::detach(%275:<12288xf16>) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::detach(%158:<12288xf16>) 
58390:3:0 %114:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%114:<12288xf16>+151037952, %264:<12288xf16>, alpha=1:i32) 
58394:7:0 %101:<12288xf16>+151037952 = torch.2_1_0.aten::add_(%101:<12288xf16>+151037952, %217:<12288xf16>, alpha=1:i32) 
58390:3:0 %233:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%229:<2048x1x12288xf16>, %220:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58394:7:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%220:<2048x1x12288xf16>, %193:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58390:3:0 %250:<1x1x12288xf16> = torch.2_1_0.aten::sum(%233:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58390:3:0 %275:<12288xf16> = torch.2_1_0.aten::view(%250:<1x1x12288xf16>, list{12288:i32}) 
58394:7:0 %267:<1x1x12288xf16> = torch.2_1_0.aten::sum(%272:<2048x1x12288xf16>, list{0:i32, 1:i32}, True:pred, dtype=None:NoneType) 
58390:3:0 %268:<12288xf16> = torch.2_1_0.aten::detach(%275:<12288xf16>) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::view(%267:<1x1x12288xf16>, list{12288:i32}) 
58390:3:0 %264:<12288xf16> = torch.2_1_0.aten::detach(%268:<12288xf16>) 
58394:7:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%217:<12288xf16>) 
58390:3:0 %262:<12288xf16> = torch.2_1_0.aten::detach(%264:<12288xf16>) 
58394:7:0 %280:<12288xf16> = torch.2_1_0.aten::detach(%190:<12288xf16>) 
58390:3:0 %141:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%141:<12288xf16>+207690240, %262:<12288xf16>, alpha=1:i32) 
58394:7:0 %275:<12288xf16> = torch.2_1_0.aten::detach(%280:<12288xf16>) 
58390:3:0 %220:<2048x12288xf16> = torch.2_1_0.aten::view(%233:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %109:<12288xf16>+207690240 = torch.2_1_0.aten::add_(%109:<12288xf16>+207690240, %275:<12288xf16>, alpha=1:i32) 
58394:7:0 %217:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %275:<2048x1536xf16> = torch.2_1_0.aten::mm(%220:<2048x12288xf16>, %120:<12288x1536xf16>) 
58390:3:0 %262:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%275:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58394:7:0 %190:<2048x1536xf16> = torch.2_1_0.aten::mm(%217:<2048x12288xf16>, %50:<12288x1536xf16>) 
58390:3:0 %244:<2048x12288xf16> = torch.2_1_0.aten::view(%233:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %275:<2048x1536xf16> = torch.2_1_0.aten::view(%246:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58394:7:0 %105:<2048x1x1536xf16> = torch.2_1_0.aten::_unsafe_view(%190:<2048x1536xf16>, list{2048:i32, 1:i32, 1536:i32}) 
58390:3:0 %287:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%244:<2048x12288xf16>) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %268:<12288x1536xf16> = torch.2_1_0.aten::mm(%287:<12288x2048xf16>{1,12288}, %275:<2048x1536xf16>) 
58394:7:0 %193:<2048x1536xf16> = torch.2_1_0.aten::view(%215:<2048x1x1536xf16>, list{2048:i32, 1536:i32}) 
58394:7:0 %280:<12288x2048xf16>{1,12288} = torch.2_1_0.aten::t(%190:<2048x12288xf16>) 
58390:3:0 %233:<12288x1536xf16> = torch.2_1_0.aten::detach(%268:<12288x1536xf16>) 
58390:3:0 %245:<12288x1536xf16> = torch.2_1_0.aten::detach(%233:<12288x1536xf16>) 
58394:7:0 %276:<12288x1536xf16> = torch.2_1_0.aten::mm(%280:<12288x2048xf16>{1,12288}, %193:<2048x1536xf16>) 
58390:3:0 %246:<12288x1536xf16> = torch.2_1_0.aten::detach(%245:<12288x1536xf16>) 
58390:3:0 %139:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%139:<12288x1536xf16>+207702528, %246:<12288x1536xf16>, alpha=1:i32) 
58394:7:0 %215:<12288x1536xf16> = torch.2_1_0.aten::detach(%276:<12288x1536xf16>) 
58390:3:0 %245:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%262:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %158:<12288x1536xf16> = torch.2_1_0.aten::detach(%215:<12288x1536xf16>) 
58394:7:0 %190:<12288x1536xf16> = torch.2_1_0.aten::detach(%158:<12288x1536xf16>) 
58390:3:0 %268:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%245:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58390:3:0 %233:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%268:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58394:7:0 %110:<12288x1536xf16>+207702528 = torch.2_1_0.aten::add_(%110:<12288x1536xf16>+207702528, %190:<12288x1536xf16>, alpha=1:i32) 
58390:3:0 %245:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%240:<12x2048x2048xf16>, 1:i32, 2:i32) 
58394:7:0 %190:<2048x1x12x128xf16> = torch.2_1_0.aten::view(%105:<2048x1x1536xf16>, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %215:<1x12x2048x128xf16>{1536,128,1536,1} = torch.2_1_0.aten::permute(%190:<2048x1x12x128xf16>, list{1:i32, 2:i32, 0:i32, 3:i32}) 
58394:7:0 %214:<12x2048x128xf16>{128,1536,1} = torch.2_1_0.aten::view(%215:<1x12x2048x128xf16>{1536,128,1536,1}, list{12:i32, 2048:i32, 128:i32}) 
58394:7:0 %193:<12x2048x2048xf16>{4194304,1,2048} = torch.2_1_0.aten::transpose(%209:<12x2048x2048xf16>, 1:i32, 2:i32) 
58390:3:0 %246:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%245:<12x2048x2048xf16>{4194304,1,2048}, %233:<12x2048x128xf16>{128,1536,1}) 
58390:3:0 %244:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%241:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58390:3:0 %268:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%233:<12x2048x128xf16>{128,1536,1}, %244:<12x128x2048xf16>{384,1,4608}+256) 
58394:7:0 %217:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%193:<12x2048x2048xf16>{4194304,1,2048}, %214:<12x2048x128xf16>{128,1536,1}) 
58390:3:0 %241:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%246:<12x2048x128xf16>, 0:i32, 1:i32) 
58394:7:0 %158:<12x128x2048xf16>{384,1,4608}+256 = torch.2_1_0.aten::transpose(%210:<12x2048x128xf16>{384,4608,1}+256, 1:i32, 2:i32) 
58390:3:0 %237:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%268:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58390:3:0 %240:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%241:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %190:<12x2048x2048xf16> = torch.2_1_0.aten::bmm(%214:<12x2048x128xf16>{128,1536,1}, %158:<12x128x2048xf16>{384,1,4608}+256) 
58390:3:0 %233:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%237:<1x12x2048x2048xf16>, %238:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58394:7:0 %209:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%217:<12x2048x128xf16>, 0:i32, 1:i32) 
58390:3:0 %237:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%236:<1x12x2048x2048xf16>) 
58394:7:0 %267:<1x12x2048x2048xf16> = torch.2_1_0.aten::view(%190:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}) 
58394:7:0 %214:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%209:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58390:3:0 %220:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%233:<1x12x2048x2048xf16>, %237:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58394:7:0 %158:<1x12x2048x2048xf16> = torch.2_1_0.aten::native_dropout_backward(%267:<1x12x2048x2048xf16>, %207:<1x12x2048x2048xunknown>, 1.1111111111111112:f32) 
58390:3:0 %237:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%220:<1x12x2048x2048xf16>, 0:i32) 
58394:7:0 %210:<1x12x2048x2048xf16> = torch.2_1_0.aten::detach(%205:<1x12x2048x2048xf16>) 
58390:3:0 %238:<50331648xf16> = torch.2_1_0.aten::new_zeros(%237:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58394:7:0 %105:<1x12x2048x2048xf16> = torch.2_1_0.aten::_softmax_backward_data(%158:<1x12x2048x2048xf16>, %210:<1x12x2048x2048xf16>, -1:i32, torch.float16:dtype) 
58390:3:0 %268:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%238:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58390:3:0 %268:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%268:<12x2048x2048xf16>, %237:<12x2048x2048xf16>, False:pred) 
58394:7:0 %210:<12x2048x2048xf16> = torch.2_1_0.aten::squeeze(%105:<1x12x2048x2048xf16>, 0:i32) 
58390:3:0 %275:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%238:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58394:7:0 %206:<50331648xf16> = torch.2_1_0.aten::new_zeros(%210:<12x2048x2048xf16>, list{50331648:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58390:3:0 %262:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%275:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58394:7:0 %272:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%206:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58390:3:0 %262:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%262:<12x2048x2048xf16>, %275:<12x2048x2048xf16>, False:pred) 
58390:3:0 %245:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%262:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58394:7:0 %272:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%272:<12x2048x2048xf16>, %210:<12x2048x2048xf16>, False:pred) 
58390:3:0 %237:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%245:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58394:7:0 %158:<12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%206:<50331648xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58390:3:0 %220:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%237:<1x12x2048x2048xf16>, %197:<1x1x2048x2048xunknown>, 0:i32) 
58394:7:0 %190:<12x2048x2048xf16> = torch.2_1_0.aten::new_empty_strided(%158:<12x2048x2048xf16>, list{12:i32, 2048:i32, 2048:i32}, list{4194304:i32, 2048:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58390:3:0 %245:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%245:<1x12x2048x2048xf16>, %220:<1x12x2048x2048xf16>, False:pred) 
58394:7:0 %190:<12x2048x2048xf16> = torch.2_1_0.aten::copy_(%190:<12x2048x2048xf16>, %158:<12x2048x2048xf16>, False:pred) 
58390:3:0 %275:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%232:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58394:7:0 %280:<1x12x2048x2048xf16> = torch.2_1_0.aten::as_strided(%190:<12x2048x2048xf16>, list{1:i32, 12:i32, 2048:i32, 2048:i32}, list{50331648:i32, 4194304:i32, 2048:i32, 1:i32}, 0:i32) 
58390:3:0 %220:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%262:<12x2048x2048xf16>, %275:<12x2048x128xf16>{384,4608,1}+128) 
58394:7:0 %281:<1x12x2048x2048xf16> = torch.2_1_0.aten::clone(%280:<1x12x2048x2048xf16>, memory_format=torch.contiguous_format:memory_format) 
58390:3:0 %233:<12x2048x128xf16> = torch.2_1_0.aten::mul(%220:<12x2048x128xf16>, 0.08838834764831843:f32) 
58390:3:0 %220:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%230:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58394:7:0 %276:<1x12x2048x2048xf16> = torch.2_1_0.aten::masked_fill(%281:<1x12x2048x2048xf16>, %178:<1x1x2048x2048xunknown>, 0:i32) 
58394:7:0 %280:<1x12x2048x2048xf16> = torch.2_1_0.aten::copy_(%280:<1x12x2048x2048xf16>, %276:<1x12x2048x2048xf16>, False:pred) 
58390:3:0 %287:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%220:<12x128x2048xf16>{384,1,4608}, %262:<12x2048x2048xf16>) 
58390:3:0 %275:<12x128x2048xf16> = torch.2_1_0.aten::mul(%287:<12x128x2048xf16>, 0.08838834764831843:f32) 
58394:7:0 %206:<12x2048x128xf16>{384,4608,1}+128 = torch.2_1_0.aten::transpose(%198:<12x128x2048xf16>{384,1,4608}+128, 1:i32, 2:i32) 
58390:3:0 %232:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%275:<12x128x2048xf16>, 1:i32, 2:i32) 
58394:7:0 %158:<12x2048x128xf16> = torch.2_1_0.aten::bmm(%190:<12x2048x2048xf16>, %206:<12x2048x128xf16>{384,4608,1}+128) 
58390:3:0 %219:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%232:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58390:3:0 %230:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%233:<12x2048x128xf16>, 0:i32, 1:i32) 
58394:7:0 %276:<12x2048x128xf16> = torch.2_1_0.aten::mul(%158:<12x2048x128xf16>, 0.08838834764831843:f32) 
58390:3:0 %262:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%219:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %158:<12x128x2048xf16>{384,1,4608} = torch.2_1_0.aten::transpose(%200:<12x2048x128xf16>{384,4608,1}, 1:i32, 2:i32) 
58390:3:0 %287:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%230:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58390:3:0 %220:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%287:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58394:7:0 %282:<12x128x2048xf16> = torch.2_1_0.aten::bmm(%158:<12x128x2048xf16>{384,1,4608}, %190:<12x2048x2048xf16>) 
58394:7:0 %278:<12x128x2048xf16> = torch.2_1_0.aten::mul(%282:<12x128x2048xf16>, 0.08838834764831843:f32) 
58390:3:0 %277:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%220:<2048x1x12x128xf16>{128,3145728,262144,1}, %262:<2048x1x12x128xf16>{1,3145728,262144,2048}, %240:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58394:7:0 %190:<12x2048x128xf16>{262144,1,2048} = torch.2_1_0.aten::transpose(%278:<12x128x2048xf16>, 1:i32, 2:i32) 
58390:3:0 %246:<2048x1x4608xf16> = torch.2_1_0.aten::view(%277:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58394:7:0 %198:<2048x12x128xf16>{1,262144,2048} = torch.2_1_0.aten::transpose(%190:<12x2048x128xf16>{262144,1,2048}, 0:i32, 1:i32) 
58394:7:0 %158:<2048x12x128xf16>{128,262144,1} = torch.2_1_0.aten::transpose(%276:<12x2048x128xf16>, 0:i32, 1:i32) 
58390:3:0 %287:<2048x4608xf16> = torch.2_1_0.aten::view(%246:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58394:7:0 %190:<2048x1x12x128xf16>{1,3145728,262144,2048} = torch.2_1_0.aten::view(%198:<2048x12x128xf16>{1,262144,2048}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58390:3:0 %262:<2048x12288xf16> = torch.2_1_0.aten::mm(%287:<2048x4608xf16>, %121:<4608x12288xf16>) 
58394:7:0 %198:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%158:<2048x12x128xf16>{128,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58390:3:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%262:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %262:<2048x4608xf16> = torch.2_1_0.aten::view(%246:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58394:7:0 %282:<2048x1x12x128xf16>{128,3145728,262144,1} = torch.2_1_0.aten::view(%198:<2048x1x12x128xf16>{128,3145728,262144,1}, list{2048:i32, 1:i32, 12:i32, 128:i32}) 
58390:3:0 %250:<2048x12288xf16> = torch.2_1_0.aten::view(%212:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %198:<2048x1x12x384xf16> = torch.2_1_0.aten::cat(list{%282:<2048x1x12x128xf16>{128,3145728,262144,1}, %190:<2048x1x12x128xf16>{1,3145728,262144,2048}, %214:<2048x1x12x128xf16>{128,3145728,262144,1}}, 3:i32) 
58394:7:0 %206:<2048x1x4608xf16> = torch.2_1_0.aten::view(%198:<2048x1x12x384xf16>, list{2048:i32, 1:i32, 4608:i32}) 
58390:None:0 list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%240:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %245:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%262:<2048x4608xf16>) 
58394:7:0 %214:<2048x4608xf16> = torch.2_1_0.aten::view(%206:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58390:3:0 %287:<4608x12288xf16> = torch.2_1_0.aten::mm(%245:<4608x2048xf16>{1,4608}, %250:<2048x12288xf16>) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::mm(%214:<2048x4608xf16>, %88:<4608x12288xf16>) 
58394:7:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::_unsafe_view(%190:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58390:3:0 %289:<4608xf16> = torch.2_1_0.aten::sum(%262:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58394:7:0 %190:<2048x4608xf16> = torch.2_1_0.aten::view(%206:<2048x1x4608xf16>, list{2048:i32, 4608:i32}) 
58390:3:0 %246:<4608x12288xf16> = torch.2_1_0.aten::detach(%287:<4608x12288xf16>) 
58394:7:0 %217:<2048x12288xf16> = torch.2_1_0.aten::view(%183:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %212:<4608x12288xf16> = torch.2_1_0.aten::detach(%246:<4608x12288xf16>) 
58390:3:0 %277:<4608x12288xf16> = torch.2_1_0.aten::detach(%212:<4608x12288xf16>) 
58390:3:0 %140:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%140:<4608x12288xf16>+151067136, %277:<4608x12288xf16>, alpha=1:i32) 
58390:3:0 %212:<4608xf16> = torch.2_1_0.aten::detach(%289:<4608xf16>) 
58394:None:0 list{%272:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%272:<2048x1x12288xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %277:<4608xf16> = torch.2_1_0.aten::detach(%212:<4608xf16>) 
58390:3:0 %287:<4608xf16> = torch.2_1_0.aten::detach(%277:<4608xf16>) 
58394:7:0 %270:<4608x2048xf16>{1,4608} = torch.2_1_0.aten::t(%190:<2048x4608xf16>) 
58390:3:0 %136:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%136:<4608xf16>+151062528, %287:<4608xf16>, alpha=1:i32) 
58394:7:0 %276:<4608x12288xf16> = torch.2_1_0.aten::mm(%270:<4608x2048xf16>{1,4608}, %217:<2048x12288xf16>) 
58390:3:0 %287:<2048x12288xf16> = torch.2_1_0.aten::view(%213:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58390:3:0 %275:<2048x12288xf16> = torch.2_1_0.aten::view(%240:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58394:7:0 %280:<4608xf16> = torch.2_1_0.aten::sum(%190:<2048x4608xf16>, list{0:i32}, False:pred, dtype=None:NoneType) 
58390:3:0 %246:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%287:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %262:<12288xf16> = torch.2_1_0.aten::empty_like(%59:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %183:<4608x12288xf16> = torch.2_1_0.aten::detach(%276:<4608x12288xf16>) 
58390:3:0 %277:<12288xf16> = torch.2_1_0.aten::empty_like(%59:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %198:<4608x12288xf16> = torch.2_1_0.aten::detach(%183:<4608x12288xf16>) 
58394:7:0 %190:<4608x12288xf16> = torch.2_1_0.aten::detach(%198:<4608x12288xf16>) 
58390:3:0 %233:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %244:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %107:<4608x12288xf16>+151067136 = torch.2_1_0.aten::add_(%107:<4608x12288xf16>+151067136, %190:<4608x12288xf16>, alpha=1:i32) 
58394:7:0 %198:<4608xf16> = torch.2_1_0.aten::detach(%280:<4608xf16>) 
58390:3:0 %245:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58394:7:0 %190:<4608xf16> = torch.2_1_0.aten::detach(%198:<4608xf16>) 
58390:3:0 %212:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %198:<4608xf16> = torch.2_1_0.aten::detach(%190:<4608xf16>) 
58390:3:0 %245:<2048x1x12288xf16> = torch.2_1_0.aten::view(%246:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58394:7:0 %55:<4608xf16>+151062528 = torch.2_1_0.aten::add_(%55:<4608xf16>+151062528, %198:<4608xf16>, alpha=1:i32) 
58390:3:0 %217:<2048x1x12288xf16> = torch.2_1_0.aten::add(%229:<2048x1x12288xf16>, %245:<2048x1x12288xf16>, alpha=1:i32) 
58394:7:0 %161:<2048x12288xf16> = torch.2_1_0.aten::view(%179:<2048x1x12288xf16>, list{-1:i32, 12288:i32}) 
58390:3:0 %229:<12288xf16> = torch.2_1_0.aten::detach(%262:<12288xf16>) 
58394:7:0 %198:<2048x12288xf16> = torch.2_1_0.aten::view(%272:<2048x1x12288xf16>, list{2048:i32, 12288:i32}) 
58390:3:0 %240:<12288xf16> = torch.2_1_0.aten::detach(%229:<12288xf16>) 
58390:3:0 %246:<12288xf16> = torch.2_1_0.aten::detach(%240:<12288xf16>) 
58394:7:0 %217:<2048x12288xf16> = torch.2_1_0.aten::empty_like(%161:<2048x12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %142:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%142:<12288xf16>+226589184, %246:<12288xf16>, alpha=1:i32) 
58394:7:0 %65:<12288xf16> = torch.2_1_0.aten::empty_like(%2:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %225:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%326:<1x2048x12288xf16>, %226:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58390:3:0 %218:<12288xf16> = torch.2_1_0.aten::detach(%277:<12288xf16>) 
58390:3:0 %262:<12288xf16> = torch.2_1_0.aten::detach(%218:<12288xf16>) 
58394:7:0 %282:<12288xf16> = torch.2_1_0.aten::empty_like(%2:<12288xf16>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %263:<2048x12288xf16> = torch.2_1_0.aten::detach(%225:<2048x12288xf16>) 
58390:3:0 %240:<12288xf16> = torch.2_1_0.aten::detach(%262:<12288xf16>) 
58388:1:0 %229:<2048x12288xf16> = torch.2_1_0.aten::detach(%263:<2048x12288xf16>) 
58394:7:0 %214:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %143:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%143:<12288xf16>+226576896, %240:<12288xf16>, alpha=1:i32) 
58388:1:0 %259:<2048x12288xf16> = torch.2_1_0.aten::detach(%229:<2048x12288xf16>) 
58392:5:0 %226:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%156:<1x2048x12288xf16>, %137:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58391:4:0 %50:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%227:<1x2048x12288xf16>, %213:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58393:6:0 %219:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%236:<1x2048x12288xf16>, %224:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58389:2:0 %80:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%243:<1x2048x12288xf16>, %235:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58387:0:0 %44:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%270:<1x2048x12288xf16>, %246:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58388:1:0 %165:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%165:<2048x12288xf16>+226601472, %259:<2048x12288xf16>, alpha=1:i32) 
58394:7:0 %278:<60x12288xf32> = torch.2_1_0.aten::empty(list{60:i32, 12288:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %240:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%217:<2048x1x12288xf16>, %214:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58390:3:0 %213:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%240:<2048x1x12288xf16>, 0:i32, 1:i32) 
58391:4:0 %255:<2048x12288xf16> = torch.2_1_0.aten::detach(%50:<2048x12288xf16>) 
58387:0:0 %274:<2048x12288xf16> = torch.2_1_0.aten::detach(%44:<2048x12288xf16>) 
58392:5:0 %187:<2048x12288xf16> = torch.2_1_0.aten::detach(%226:<2048x12288xf16>) 
58394:7:0 %283:<120xi32> = torch.2_1_0.aten::zeros(list{120:i32}, dtype=torch.int32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58393:6:0 %310:<2048x12288xf16> = torch.2_1_0.aten::detach(%219:<2048x12288xf16>) 
58388:1:0 %224:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%326:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58389:2:0 %320:<2048x12288xf16> = torch.2_1_0.aten::detach(%80:<2048x12288xf16>) 
58391:4:0 %237:<2048x12288xf16> = torch.2_1_0.aten::detach(%255:<2048x12288xf16>) 
58387:0:0 %302:<2048x12288xf16> = torch.2_1_0.aten::detach(%274:<2048x12288xf16>) 
58392:5:0 %157:<2048x12288xf16> = torch.2_1_0.aten::detach(%187:<2048x12288xf16>) 
58393:6:0 %311:<2048x12288xf16> = torch.2_1_0.aten::detach(%310:<2048x12288xf16>) 
58388:1:0 %224:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%224:<1x2048x12288xf16>, %326:<1x2048x12288xf16>, False:pred) 
58389:2:0 %312:<2048x12288xf16> = torch.2_1_0.aten::detach(%320:<2048x12288xf16>) 
58391:4:0 %261:<2048x12288xf16> = torch.2_1_0.aten::detach(%237:<2048x12288xf16>) 
58387:0:0 %281:<2048x12288xf16> = torch.2_1_0.aten::detach(%302:<2048x12288xf16>) 
58394:7:0 %277:<3840xi8> = torch.2_1_0.aten::empty(list{3840:i32}, dtype=torch.int8:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %174:<2048x12288xf16> = torch.2_1_0.aten::detach(%157:<2048x12288xf16>) 
58393:6:0 %233:<2048x12288xf16> = torch.2_1_0.aten::detach(%311:<2048x12288xf16>) 
58388:1:0 %227:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%224:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58389:2:0 %320:<2048x12288xf16> = torch.2_1_0.aten::detach(%312:<2048x12288xf16>) 
58387:0:0 %146:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%146:<2048x12288xf16>+226601472, %281:<2048x12288xf16>, alpha=1:i32) 
58391:4:0 %155:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%155:<2048x12288xf16>+226601472, %261:<2048x12288xf16>, alpha=1:i32) 
58392:5:0 %50:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%50:<2048x12288xf16>+226601472, %174:<2048x12288xf16>, alpha=1:i32) 
58394:7:0 %190:<2048x1x12288xf16> = torch.2_1_0.aten::view(%217:<2048x12288xf16>, list{2048:i32, 1:i32, 12288:i32}) 
58388:1:0 %288:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%227:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58393:6:0 %142:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%142:<2048x12288xf16>+226601472, %233:<2048x12288xf16>, alpha=1:i32) 
58389:2:0 %144:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%144:<2048x12288xf16>+226601472, %320:<2048x12288xf16>, alpha=1:i32) 
58387:0:0 %302:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%270:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58388:None:0 %257:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58391:4:0 %50:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%227:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58394:7:0 %272:<2048x1x12288xf16> = torch.2_1_0.aten::add(%220:<2048x1x12288xf16>, %190:<2048x1x12288xf16>, alpha=1:i32) 
58392:5:0 %157:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%156:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58387:0:0 %302:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%302:<1x2048x12288xf16>, %270:<1x2048x12288xf16>, False:pred) 
58393:6:0 %233:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%236:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58391:4:0 %50:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%50:<1x2048x12288xf16>, %227:<1x2048x12288xf16>, False:pred) 
58389:2:0 %80:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%243:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58392:5:0 %157:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%157:<1x2048x12288xf16>, %156:<1x2048x12288xf16>, False:pred) 
58394:7:0 %217:<12288xf16> = torch.2_1_0.aten::detach(%65:<12288xf16>) 
58387:0:0 %281:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%302:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58393:6:0 %233:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%233:<1x2048x12288xf16>, %236:<1x2048x12288xf16>, False:pred) 
58388:1:0 %259:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%288:<1x2048x12288xf16>, list{%228:<1x2048xunknown>}, %257:<f16>, False:pred) 
58391:4:0 %237:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%50:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58392:5:0 %174:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%157:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58394:7:0 %220:<12288xf16> = torch.2_1_0.aten::detach(%217:<12288xf16>) 
58389:2:0 %80:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%80:<1x2048x12288xf16>, %243:<1x2048x12288xf16>, False:pred) 
58387:0:0 %271:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%281:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58393:6:0 %224:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%233:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58388:1:0 %227:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%227:<1x2048x12288xf16>, %259:<1x2048x12288xf16>, False:pred) 
58391:4:0 %255:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%237:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58394:7:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%220:<12288xf16>) 
58392:5:0 %179:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%174:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58387:None:0 %285:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58389:2:0 %297:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%80:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58393:6:0 %318:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%224:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58391:None:0 %221:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58392:None:0 %159:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58394:7:0 %113:<12288xf16>+226589184 = torch.2_1_0.aten::add_(%113:<12288xf16>+226589184, %190:<12288xf16>, alpha=1:i32) 
58393:None:0 %310:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58389:2:0 %311:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%297:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58387:0:0 %286:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%271:<1x2048x12288xf16>, list{%248:<1x2048xunknown>}, %285:<f16>, False:pred) 
58394:7:0 %220:<12288xf16> = torch.2_1_0.aten::detach(%282:<12288xf16>) 
58391:4:0 %261:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%255:<1x2048x12288xf16>, list{%82:<1x2048xunknown>}, %221:<f16>, False:pred) 
58389:None:0 %321:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58387:0:0 %281:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%281:<1x2048x12288xf16>, %286:<1x2048x12288xf16>, False:pred) 
58392:5:0 %158:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%179:<1x2048x12288xf16>, list{%132:<1x2048xunknown>}, %159:<f16>, False:pred) 
58394:7:0 %190:<12288xf16> = torch.2_1_0.aten::detach(%220:<12288xf16>) 
58393:6:0 %311:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%318:<1x2048x12288xf16>, list{%227:<1x2048xunknown>}, %310:<f16>, False:pred) 
58391:4:0 %237:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%237:<1x2048x12288xf16>, %261:<1x2048x12288xf16>, False:pred) 
58392:5:0 %174:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%174:<1x2048x12288xf16>, %158:<1x2048x12288xf16>, False:pred) 
58394:7:0 %220:<12288xf16> = torch.2_1_0.aten::detach(%190:<12288xf16>) 
58393:6:0 %224:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%224:<1x2048x12288xf16>, %311:<1x2048x12288xf16>, False:pred) 
58389:2:0 %212:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%311:<1x2048x12288xf16>, list{%126:<1x2048xunknown>}, %321:<f16>, False:pred) 
58394:7:0 %112:<12288xf16>+226576896 = torch.2_1_0.aten::add_(%112:<12288xf16>+226576896, %220:<12288xf16>, alpha=1:i32) 
58389:2:0 %297:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%297:<1x2048x12288xf16>, %212:<1x2048x12288xf16>, False:pred) 
58394:7:0 %65:<2048x1x12288xf16> = torch.2_1_0.aten::native_dropout_backward(%272:<2048x1x12288xf16>, %184:<2048x1x12288xunknown>, 1.1111111111111112:f32) 
58394:7:0 %184:<1x2048x12288xf16> = torch.2_1_0.aten::transpose(%65:<2048x1x12288xf16>, 0:i32, 1:i32) 
58390:3:0 %246:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%213:<1x2048x12288xf16>, %199:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58390:3:0 %218:<2048x12288xf16> = torch.2_1_0.aten::detach(%246:<2048x12288xf16>) 
58390:3:0 %220:<2048x12288xf16> = torch.2_1_0.aten::detach(%218:<2048x12288xf16>) 
58390:3:0 %217:<2048x12288xf16> = torch.2_1_0.aten::detach(%220:<2048x12288xf16>) 
58390:3:0 %144:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%144:<2048x12288xf16>+226601472, %217:<2048x12288xf16>, alpha=1:i32) 
58390:3:0 %246:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%213:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58390:3:0 %246:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%246:<1x2048x12288xf16>, %213:<1x2048x12288xf16>, False:pred) 
58390:3:0 %218:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%246:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58390:3:0 %217:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%218:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58390:None:0 %262:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58390:3:0 %287:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%217:<1x2048x12288xf16>, list{%203:<1x2048xunknown>}, %262:<f16>, False:pred) 
58387:0:0 %44:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%302:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58390:3:0 %218:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%218:<1x2048x12288xf16>, %287:<1x2048x12288xf16>, False:pred) 
58391:4:0 %279:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%50:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58392:5:0 %222:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%157:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58393:6:0 %224:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%233:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58394:7:0 %282:<2048x12288xf16> = torch.2_1_0.aten::embedding_dense_backward(%184:<1x2048x12288xf16>, %164:<1x2048xi64>, 2048:i32, -1:i32, False:pred) 
58394:7:0 %220:<2048x12288xf16> = torch.2_1_0.aten::detach(%282:<2048x12288xf16>) 
58394:7:0 %190:<2048x12288xf16> = torch.2_1_0.aten::detach(%220:<2048x12288xf16>) 
58389:2:0 %329:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%80:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58394:7:0 %158:<2048x12288xf16> = torch.2_1_0.aten::detach(%190:<2048x12288xf16>) 
58394:7:0 %116:<2048x12288xf16>+226601472 = torch.2_1_0.aten::add_(%116:<2048x12288xf16>+226601472, %158:<2048x12288xf16>, alpha=1:i32) 
58394:7:0 %190:<1x2048x12288xf16> = torch.2_1_0.aten::new_empty_strided(%184:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType) 
58394:7:0 %190:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%190:<1x2048x12288xf16>, %184:<1x2048x12288xf16>, False:pred) 
58394:7:0 %158:<1x2048x12288xf16> = torch.2_1_0.aten::as_strided(%190:<1x2048x12288xf16>, list{1:i32, 2048:i32, 12288:i32}, list{25165824:i32, 12288:i32, 1:i32}, 0:i32) 
58394:7:0 %220:<1x2048x12288xf16> = torch.2_1_0.aten::clone(%158:<1x2048x12288xf16>, memory_format=torch.contiguous_format:memory_format) 
58394:None:0 %214:<f16> = torch.2_1_0.aten::zeros(list{}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cpu:device, pin_memory=None:NoneType) 
58394:7:0 %198:<1x2048x12288xf16> = torch.2_1_0.aten::index_put(%220:<1x2048x12288xf16>, list{%1:<1x2048xunknown>}, %214:<f16>, False:pred) 
58394:7:0 %158:<1x2048x12288xf16> = torch.2_1_0.aten::copy_(%158:<1x2048x12288xf16>, %198:<1x2048x12288xf16>, False:pred) 
58390:3:0 %218:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%246:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58388:1:0 %326:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%224:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58394:7:0 %65:<6400x12288xf16> = torch.2_1_0.aten::new_zeros(%190:<1x2048x12288xf16>, list{6400:i32, 12288:i32}, dtype=torch.float16:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58390:3:0 %203:<6400x12288xf16> = torch.2_1_0.aten::index_put(%218:<6400x12288xf16>, list{%183:<1x2048xi64>}, %246:<1x2048x12288xf16>, True:pred) 
58390:3:0 %246:<6400x12288xf16> = torch.2_1_0.aten::add(%208:<6400x12288xf16>, %203:<6400x12288xf16>, alpha=1:i32) 
58390:3:0 %287:<6400x12288xf16> = torch.2_1_0.aten::detach(%246:<6400x12288xf16>) 
58390:3:0 %203:<6400x12288xf16> = torch.2_1_0.aten::detach(%287:<6400x12288xf16>) 
58390:3:0 %208:<6400x12288xf16> = torch.2_1_0.aten::detach(%203:<6400x12288xf16>) 
58390:3:0 %145:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%145:<6400x12288xf16>+251767296, %208:<6400x12288xf16>, alpha=1:i32) 
58392:5:0 %156:<6400x12288xf16> = torch.2_1_0.aten::index_put(%222:<6400x12288xf16>, list{%147:<1x2048xi64>}, %157:<1x2048x12288xf16>, True:pred) 
58392:5:0 %147:<6400x12288xf16> = torch.2_1_0.aten::add(%213:<6400x12288xf16>, %156:<6400x12288xf16>, alpha=1:i32) 
58392:5:0 %156:<6400x12288xf16> = torch.2_1_0.aten::detach(%147:<6400x12288xf16>) 
58392:5:0 %213:<6400x12288xf16> = torch.2_1_0.aten::detach(%156:<6400x12288xf16>) 
58392:5:0 %156:<6400x12288xf16> = torch.2_1_0.aten::detach(%213:<6400x12288xf16>) 
58392:5:0 %79:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%79:<6400x12288xf16>+251767296, %156:<6400x12288xf16>, alpha=1:i32) 
58391:4:0 %227:<6400x12288xf16> = torch.2_1_0.aten::index_put(%279:<6400x12288xf16>, list{%216:<1x2048xi64>}, %50:<1x2048x12288xf16>, True:pred) 
58391:4:0 %50:<6400x12288xf16> = torch.2_1_0.aten::add(%257:<6400x12288xf16>, %227:<6400x12288xf16>, alpha=1:i32) 
58391:4:0 %227:<6400x12288xf16> = torch.2_1_0.aten::detach(%50:<6400x12288xf16>) 
58391:4:0 %216:<6400x12288xf16> = torch.2_1_0.aten::detach(%227:<6400x12288xf16>) 
58391:4:0 %227:<6400x12288xf16> = torch.2_1_0.aten::detach(%216:<6400x12288xf16>) 
58391:4:0 %157:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%157:<6400x12288xf16>+251767296, %227:<6400x12288xf16>, alpha=1:i32) 
58389:2:0 %243:<6400x12288xf16> = torch.2_1_0.aten::index_put(%329:<6400x12288xf16>, list{%230:<1x2048xi64>}, %80:<1x2048x12288xf16>, True:pred) 
58389:2:0 %126:<6400x12288xf16> = torch.2_1_0.aten::add(%309:<6400x12288xf16>, %243:<6400x12288xf16>, alpha=1:i32) 
58389:2:0 %225:<6400x12288xf16> = torch.2_1_0.aten::detach(%126:<6400x12288xf16>) 
58389:2:0 %243:<6400x12288xf16> = torch.2_1_0.aten::detach(%225:<6400x12288xf16>) 
58389:2:0 %230:<6400x12288xf16> = torch.2_1_0.aten::detach(%243:<6400x12288xf16>) 
58389:2:0 %143:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%143:<6400x12288xf16>+251767296, %230:<6400x12288xf16>, alpha=1:i32) 
58393:6:0 %236:<6400x12288xf16> = torch.2_1_0.aten::index_put(%224:<6400x12288xf16>, list{%226:<1x2048xi64>}, %233:<1x2048x12288xf16>, True:pred) 
58393:6:0 %224:<6400x12288xf16> = torch.2_1_0.aten::add(%299:<6400x12288xf16>, %236:<6400x12288xf16>, alpha=1:i32) 
58393:6:0 %326:<6400x12288xf16> = torch.2_1_0.aten::detach(%224:<6400x12288xf16>) 
58393:6:0 %274:<6400x12288xf16> = torch.2_1_0.aten::detach(%326:<6400x12288xf16>) 
58393:6:0 %329:<6400x12288xf16> = torch.2_1_0.aten::detach(%274:<6400x12288xf16>) 
58393:6:0 %143:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%143:<6400x12288xf16>+251767296, %329:<6400x12288xf16>, alpha=1:i32) 
58387:0:0 %248:<6400x12288xf16> = torch.2_1_0.aten::index_put(%44:<6400x12288xf16>, list{%245:<1x2048xi64>}, %302:<1x2048x12288xf16>, True:pred) 
58387:0:0 %318:<6400x12288xf16> = torch.2_1_0.aten::add(%310:<6400x12288xf16>, %248:<6400x12288xf16>, alpha=1:i32) 
58387:0:0 %310:<6400x12288xf16> = torch.2_1_0.aten::detach(%318:<6400x12288xf16>) 
58387:0:0 %287:<6400x12288xf16> = torch.2_1_0.aten::detach(%310:<6400x12288xf16>) 
58387:0:0 %310:<6400x12288xf16> = torch.2_1_0.aten::detach(%287:<6400x12288xf16>) 
58387:0:0 %152:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%152:<6400x12288xf16>+251767296, %310:<6400x12288xf16>, alpha=1:i32) 
58394:7:0 %184:<6400x12288xf16> = torch.2_1_0.aten::index_put(%65:<6400x12288xf16>, list{%180:<1x2048xi64>}, %190:<1x2048x12288xf16>, True:pred) 
58394:7:0 %1:<6400x12288xf16> = torch.2_1_0.aten::add(%268:<6400x12288xf16>, %184:<6400x12288xf16>, alpha=1:i32) 
58394:7:0 %268:<6400x12288xf16> = torch.2_1_0.aten::detach(%1:<6400x12288xf16>) 
58394:7:0 %184:<6400x12288xf16> = torch.2_1_0.aten::detach(%268:<6400x12288xf16>) 
58394:7:0 %198:<6400x12288xf16> = torch.2_1_0.aten::detach(%184:<6400x12288xf16>) 
58394:7:0 %119:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%119:<6400x12288xf16>+251767296, %198:<6400x12288xf16>, alpha=1:i32) 
58388:1:0 %323:<6400x12288xf16> = torch.2_1_0.aten::index_put(%326:<6400x12288xf16>, list{%207:<1x2048xi64>}, %224:<1x2048x12288xf16>, True:pred) 
58388:1:0 %225:<6400x12288xf16> = torch.2_1_0.aten::add(%312:<6400x12288xf16>, %323:<6400x12288xf16>, alpha=1:i32) 
58388:1:0 %228:<6400x12288xf16> = torch.2_1_0.aten::detach(%225:<6400x12288xf16>) 
58388:1:0 %227:<6400x12288xf16> = torch.2_1_0.aten::detach(%228:<6400x12288xf16>) 
58388:1:0 %326:<6400x12288xf16> = torch.2_1_0.aten::detach(%227:<6400x12288xf16>) 
58388:1:0 %168:<6400x12288xf16>+251767296 = torch.2_1_0.aten::add_(%168:<6400x12288xf16>+251767296, %326:<6400x12288xf16>, alpha=1:i32) 
58388:None:0 list{%166:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%166:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %326:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%168:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %130:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%165:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %328:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%159:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %225:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%157:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %228:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%146:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %169:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%140:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %251:<12288xf32> = torch.2_1_0.aten::_to_copy(%167:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %241:<12288xf32> = torch.2_1_0.aten::_to_copy(%164:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %329:<12288xf32> = torch.2_1_0.aten::_to_copy(%160:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %271:<4608xf32> = torch.2_1_0.aten::_to_copy(%154:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %281:<12288xf32> = torch.2_1_0.aten::_to_copy(%151:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %253:<12288xf32> = torch.2_1_0.aten::_to_copy(%145:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %227:<6144xf32> = torch.2_1_0.aten::_to_copy(%143:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %207:<12288xf32> = torch.2_1_0.aten::_to_copy(%137:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %233:<12288xf32> = torch.2_1_0.aten::_to_copy(%75:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %259:<12288xf32> = torch.2_1_0.aten::_to_copy(%132:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %278:<6400x12288xf32> = torch.2_1_0.aten::detach(%326:<6400x12288xf32>) 
58388:1:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%130:<2048x12288xf32>) 
58388:1:0 %229:<12288x1536xf32> = torch.2_1_0.aten::detach(%328:<12288x1536xf32>) 
58388:1:0 %288:<4608x12288xf32> = torch.2_1_0.aten::detach(%225:<4608x12288xf32>) 
58388:1:0 %230:<6144x12288xf32> = torch.2_1_0.aten::detach(%228:<6144x12288xf32>) 
58388:1:0 %283:<12288x6144xf32> = torch.2_1_0.aten::detach(%169:<12288x6144xf32>) 
58388:1:0 %255:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58388:1:0 %258:<12288xf32> = torch.2_1_0.aten::detach(%241:<12288xf32>) 
58388:1:0 %200:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58388:1:0 %330:<4608xf32> = torch.2_1_0.aten::detach(%271:<4608xf32>) 
58388:1:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58388:1:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%253:<12288xf32>) 
58388:1:0 %208:<6144xf32> = torch.2_1_0.aten::detach(%227:<6144xf32>) 
58388:1:0 %266:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58388:1:0 %246:<12288xf32> = torch.2_1_0.aten::detach(%233:<12288xf32>) 
58388:1:0 %269:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58388:1:0 %124:<1xf32> = torch.2_1_0.aten::fill_(%124:<1xf32>, 0.0:f32) 
58388:1:0 %221:<1xf64> = torch.2_1_0.aten::_to_copy(%29:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:None:0 list{%145:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%145:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %62:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%143:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %126:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%144:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %122:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%138:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %333:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%139:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %271:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%133:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:None:0 list{%121:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%121:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 %334:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%127:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %293:<12288xf32> = torch.2_1_0.aten::_to_copy(%141:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %282:<12288xf32> = torch.2_1_0.aten::_to_copy(%142:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %286:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %178:<4608xf32> = torch.2_1_0.aten::_to_copy(%131:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %174:<12288xf32> = torch.2_1_0.aten::_to_copy(%132:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %323:<1xf64> = torch.2_1_0.aten::reciprocal(%221:<1xf64>) 
58389:2:0 %245:<12288xf32> = torch.2_1_0.aten::_to_copy(%135:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %238:<6144xf32> = torch.2_1_0.aten::_to_copy(%130:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 %324:<1xf32> = torch.2_1_0.aten::_to_copy(%323:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %290:<12288xf32> = torch.2_1_0.aten::_to_copy(%124:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %82:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%119:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %243:<12288xf32> = torch.2_1_0.aten::_to_copy(%121:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %230:<12288xf32> = torch.2_1_0.aten::_to_copy(%118:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %212:<6400x12288xf32> = torch.2_1_0.aten::detach(%62:<6400x12288xf32>) 
58389:2:0 %273:<2048x12288xf32> = torch.2_1_0.aten::detach(%126:<2048x12288xf32>) 
58394:7:0 %284:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%116:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %297:<12288x1536xf32> = torch.2_1_0.aten::detach(%122:<12288x1536xf32>) 
58389:2:0 %315:<4608x12288xf32> = torch.2_1_0.aten::detach(%333:<4608x12288xf32>) 
58389:2:0 %330:<6144x12288xf32> = torch.2_1_0.aten::detach(%271:<6144x12288xf32>) 
58389:2:0 %314:<12288x6144xf32> = torch.2_1_0.aten::detach(%334:<12288x6144xf32>) 
58394:7:0 %285:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%110:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %310:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58389:2:0 %307:<12288xf32> = torch.2_1_0.aten::detach(%282:<12288xf32>) 
58389:2:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58389:2:0 %80:<4608xf32> = torch.2_1_0.aten::detach(%178:<4608xf32>) 
58389:2:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58389:2:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%245:<12288xf32>) 
58389:2:0 %335:<6144xf32> = torch.2_1_0.aten::detach(%238:<6144xf32>) 
58389:2:0 %320:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58389:2:0 %325:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58389:2:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58389:2:0 %156:<1xf32> = torch.2_1_0.aten::fill_(%156:<1xf32>, 0.0:f32) 
58389:2:0 %313:<1xf64> = torch.2_1_0.aten::_to_copy(%71:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58388:1:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%278:<6400x12288xf32>, %257:<2048x12288xf32>, %229:<12288x1536xf32>, %288:<4608x12288xf32>, %230:<6144x12288xf32>, %283:<12288x6144xf32>, %255:<12288xf32>, %258:<12288xf32>, %200:<12288xf32>, %330:<4608xf32>, %276:<12288xf32>, %262:<12288xf32>, %208:<6144xf32>, %266:<12288xf32>, %246:<12288xf32>, %269:<12288xf32>}, %124:<1xf32>, %324:<1xf32>) 
58394:7:0 %178:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%107:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %255:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%104:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %286:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%44:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %287:<12288xf32> = torch.2_1_0.aten::_to_copy(%113:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %249:<12288xf32> = torch.2_1_0.aten::_to_copy(%112:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %288:<12288xf32> = torch.2_1_0.aten::_to_copy(%109:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %289:<4608xf32> = torch.2_1_0.aten::_to_copy(%55:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %270:<12288xf32> = torch.2_1_0.aten::_to_copy(%106:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %290:<12288xf32> = torch.2_1_0.aten::_to_copy(%101:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %258:<6144xf32> = torch.2_1_0.aten::_to_copy(%84:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %291:<12288xf32> = torch.2_1_0.aten::_to_copy(%100:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %259:<12288xf32> = torch.2_1_0.aten::_to_copy(%69:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %251:<12288xf32> = torch.2_1_0.aten::_to_copy(%102:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %184:<6400x12288xf32> = torch.2_1_0.aten::detach(%82:<6400x12288xf32>) 
58394:7:0 %268:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58394:7:0 %190:<12288x1536xf32> = torch.2_1_0.aten::detach(%285:<12288x1536xf32>) 
58394:7:0 %180:<4608x12288xf32> = torch.2_1_0.aten::detach(%178:<4608x12288xf32>) 
58394:7:0 %158:<6144x12288xf32> = torch.2_1_0.aten::detach(%255:<6144x12288xf32>) 
58394:7:0 %214:<12288x6144xf32> = torch.2_1_0.aten::detach(%286:<12288x6144xf32>) 
58394:7:0 %206:<12288xf32> = torch.2_1_0.aten::detach(%287:<12288xf32>) 
58394:7:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%249:<12288xf32>) 
58394:7:0 %220:<12288xf32> = torch.2_1_0.aten::detach(%288:<12288xf32>) 
58394:7:0 %292:<4608xf32> = torch.2_1_0.aten::detach(%289:<4608xf32>) 
58394:7:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%270:<12288xf32>) 
58394:7:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58394:7:0 %274:<6144xf32> = torch.2_1_0.aten::detach(%258:<6144xf32>) 
58394:7:0 %283:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58394:7:0 %271:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58394:7:0 %280:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58394:7:0 %136:<1xf32> = torch.2_1_0.aten::fill_(%136:<1xf32>, 0.0:f32) 
58394:7:0 %277:<1xf64> = torch.2_1_0.aten::_to_copy(%120:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:None:0 list{%152:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%152:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %61:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%157:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %281:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%155:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %75:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%153:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 %117:<1xf64> = torch.2_1_0.aten::reciprocal(%313:<1xf64>) 
58389:2:0 %316:<1xf32> = torch.2_1_0.aten::_to_copy(%117:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %234:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%147:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58389:2:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%212:<6400x12288xf32>, %273:<2048x12288xf32>, %297:<12288x1536xf32>, %315:<4608x12288xf32>, %330:<6144x12288xf32>, %314:<12288x6144xf32>, %310:<12288xf32>, %307:<12288xf32>, %309:<12288xf32>, %80:<4608xf32>, %312:<12288xf32>, %321:<12288xf32>, %335:<6144xf32>, %320:<12288xf32>, %325:<12288xf32>, %322:<12288xf32>}, %156:<1xf32>, %316:<1xf32>) 
58390:None:0 list{%130:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%130:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %272:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%144:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:None:0 list{%87:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%87:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:4:0 %282:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%90:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %243:<12288xf32> = torch.2_1_0.aten::_to_copy(%156:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %262:<12288xf32> = torch.2_1_0.aten::_to_copy(%154:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %229:<12288xf32> = torch.2_1_0.aten::_to_copy(%150:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %254:<4608xf32> = torch.2_1_0.aten::_to_copy(%146:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %51:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%79:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %209:<12288xf32> = torch.2_1_0.aten::_to_copy(%145:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %267:<12288xf32> = torch.2_1_0.aten::_to_copy(%142:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %227:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%50:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %181:<6144xf32> = torch.2_1_0.aten::_to_copy(%73:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %268:<12288xf32> = torch.2_1_0.aten::_to_copy(%52:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %228:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%78:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %216:<12288xf32> = torch.2_1_0.aten::_to_copy(%78:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %227:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %257:<6400x12288xf32> = torch.2_1_0.aten::detach(%61:<6400x12288xf32>) 
58391:4:0 %237:<2048x12288xf32> = torch.2_1_0.aten::detach(%281:<2048x12288xf32>) 
58391:4:0 %221:<12288x1536xf32> = torch.2_1_0.aten::detach(%75:<12288x1536xf32>) 
58391:4:0 %239:<4608x12288xf32> = torch.2_1_0.aten::detach(%234:<4608x12288xf32>) 
58391:4:0 %255:<6144x12288xf32> = torch.2_1_0.aten::detach(%272:<6144x12288xf32>) 
58390:3:0 %220:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%145:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %242:<12288x6144xf32> = torch.2_1_0.aten::detach(%282:<12288x6144xf32>) 
58391:4:0 %220:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58391:4:0 %261:<12288xf32> = torch.2_1_0.aten::detach(%262:<12288xf32>) 
58391:4:0 %228:<12288xf32> = torch.2_1_0.aten::detach(%229:<12288xf32>) 
58392:5:0 %229:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%72:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %283:<4608xf32> = torch.2_1_0.aten::detach(%254:<4608xf32>) 
58390:3:0 %272:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%144:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%209:<12288xf32>) 
58391:4:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%267:<12288xf32>) 
58391:4:0 %212:<6144xf32> = torch.2_1_0.aten::detach(%181:<6144xf32>) 
58390:3:0 %278:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%139:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%268:<12288xf32>) 
58391:4:0 %210:<12288xf32> = torch.2_1_0.aten::detach(%216:<12288xf32>) 
58391:4:0 %247:<12288xf32> = torch.2_1_0.aten::detach(%227:<12288xf32>) 
58391:4:0 %170:<1xf32> = torch.2_1_0.aten::fill_(%170:<1xf32>, 0.0:f32) 
58391:4:0 %280:<1xf64> = torch.2_1_0.aten::_to_copy(%35:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %141:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%66:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %225:<1xf64> = torch.2_1_0.aten::reciprocal(%277:<1xf64>) 
58392:5:0 %142:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%60:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %230:<12288xf32> = torch.2_1_0.aten::_to_copy(%83:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 %293:<1xf32> = torch.2_1_0.aten::_to_copy(%225:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %184:<12288xf32> = torch.2_1_0.aten::_to_copy(%80:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %231:<12288xf32> = torch.2_1_0.aten::_to_copy(%75:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %204:<4608xf32> = torch.2_1_0.aten::_to_copy(%68:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %212:<12288xf32> = torch.2_1_0.aten::_to_copy(%65:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %232:<12288xf32> = torch.2_1_0.aten::_to_copy(%59:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %166:<6144xf32> = torch.2_1_0.aten::_to_copy(%63:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %214:<12288xf32> = torch.2_1_0.aten::_to_copy(%45:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %218:<12288xf32> = torch.2_1_0.aten::_to_copy(%56:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %213:<12288xf32> = torch.2_1_0.aten::_to_copy(%44:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %156:<6400x12288xf32> = torch.2_1_0.aten::detach(%51:<6400x12288xf32>) 
58392:5:0 %147:<2048x12288xf32> = torch.2_1_0.aten::detach(%227:<2048x12288xf32>) 
58392:5:0 %158:<12288x1536xf32> = torch.2_1_0.aten::detach(%228:<12288x1536xf32>) 
58392:5:0 %157:<4608x12288xf32> = torch.2_1_0.aten::detach(%229:<4608x12288xf32>) 
58392:5:0 %187:<6144x12288xf32> = torch.2_1_0.aten::detach(%141:<6144x12288xf32>) 
58392:5:0 %174:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58392:5:0 %180:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58392:5:0 %175:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58392:5:0 %179:<12288xf32> = torch.2_1_0.aten::detach(%231:<12288xf32>) 
58392:5:0 %155:<4608xf32> = torch.2_1_0.aten::detach(%204:<4608xf32>) 
58392:5:0 %181:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58392:5:0 %186:<12288xf32> = torch.2_1_0.aten::detach(%232:<12288xf32>) 
58392:5:0 %222:<6144xf32> = torch.2_1_0.aten::detach(%166:<6144xf32>) 
58392:5:0 %219:<12288xf32> = torch.2_1_0.aten::detach(%214:<12288xf32>) 
58392:5:0 %153:<12288xf32> = torch.2_1_0.aten::detach(%218:<12288xf32>) 
58392:5:0 %209:<12288xf32> = torch.2_1_0.aten::detach(%213:<12288xf32>) 
58392:5:0 %106:<1xf32> = torch.2_1_0.aten::fill_(%106:<1xf32>, 0.0:f32) 
58392:5:0 %159:<1xf64> = torch.2_1_0.aten::_to_copy(%42:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58394:7:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%184:<6400x12288xf32>, %268:<2048x12288xf32>, %190:<12288x1536xf32>, %180:<4608x12288xf32>, %158:<6144x12288xf32>, %214:<12288x6144xf32>, %206:<12288xf32>, %198:<12288xf32>, %220:<12288xf32>, %292:<4608xf32>, %278:<12288xf32>, %276:<12288xf32>, %274:<6144xf32>, %283:<12288xf32>, %271:<12288xf32>, %280:<12288xf32>}, %136:<1xf32>, %293:<1xf32>) 
58390:3:0 %181:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%140:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %243:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%135:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %189:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%111:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %178:<12288xf32> = torch.2_1_0.aten::_to_copy(%142:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %260:<12288xf32> = torch.2_1_0.aten::_to_copy(%143:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %294:<12288xf32> = torch.2_1_0.aten::_to_copy(%141:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %244:<4608xf32> = torch.2_1_0.aten::_to_copy(%136:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %286:<12288xf32> = torch.2_1_0.aten::_to_copy(%137:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %266:<12288xf32> = torch.2_1_0.aten::_to_copy(%114:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %44:<1xf64> = torch.2_1_0.aten::reciprocal(%280:<1xf64>) 
58390:3:0 %199:<6144xf32> = torch.2_1_0.aten::_to_copy(%113:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %273:<12288xf32> = torch.2_1_0.aten::_to_copy(%42:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58391:4:0 %278:<1xf32> = torch.2_1_0.aten::_to_copy(%44:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %210:<12288xf32> = torch.2_1_0.aten::_to_copy(%132:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %81:<12288xf32> = torch.2_1_0.aten::_to_copy(%13:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %203:<6400x12288xf32> = torch.2_1_0.aten::detach(%220:<6400x12288xf32>) 
58390:3:0 %246:<2048x12288xf32> = torch.2_1_0.aten::detach(%272:<2048x12288xf32>) 
58390:3:0 %183:<12288x1536xf32> = torch.2_1_0.aten::detach(%278:<12288x1536xf32>) 
58390:3:0 %241:<4608x12288xf32> = torch.2_1_0.aten::detach(%181:<4608x12288xf32>) 
58390:3:0 %208:<6144x12288xf32> = torch.2_1_0.aten::detach(%243:<6144x12288xf32>) 
58390:3:0 %240:<12288x6144xf32> = torch.2_1_0.aten::detach(%189:<12288x6144xf32>) 
58390:3:0 %218:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58390:3:0 %287:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58390:3:0 %212:<12288xf32> = torch.2_1_0.aten::detach(%294:<12288xf32>) 
58390:3:0 %213:<4608xf32> = torch.2_1_0.aten::detach(%244:<4608xf32>) 
58390:3:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58390:3:0 %233:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58390:3:0 %217:<6144xf32> = torch.2_1_0.aten::detach(%199:<6144xf32>) 
58390:3:0 %250:<12288xf32> = torch.2_1_0.aten::detach(%273:<12288xf32>) 
58390:3:0 %50:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58390:3:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%81:<12288xf32>) 
58390:3:0 %157:<1xf32> = torch.2_1_0.aten::fill_(%157:<1xf32>, 0.0:f32) 
58391:4:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%257:<6400x12288xf32>, %237:<2048x12288xf32>, %221:<12288x1536xf32>, %239:<4608x12288xf32>, %255:<6144x12288xf32>, %242:<12288x6144xf32>, %220:<12288xf32>, %261:<12288xf32>, %228:<12288xf32>, %283:<4608xf32>, %202:<12288xf32>, %260:<12288xf32>, %212:<6144xf32>, %235:<12288xf32>, %210:<12288xf32>, %247:<12288xf32>}, %170:<1xf32>, %278:<1xf32>) 
58390:3:0 %289:<1xf64> = torch.2_1_0.aten::_to_copy(%153:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:None:0 list{%140:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%140:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %92:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%143:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %284:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%142:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %330:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%138:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %331:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%74:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %285:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%62:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %209:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%129:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %332:<12288xf32> = torch.2_1_0.aten::_to_copy(%135:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %238:<12288xf32> = torch.2_1_0.aten::_to_copy(%130:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %281:<12288xf32> = torch.2_1_0.aten::_to_copy(%136:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %250:<4608xf32> = torch.2_1_0.aten::_to_copy(%133:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %207:<12288xf32> = torch.2_1_0.aten::_to_copy(%132:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %224:<12288xf32> = torch.2_1_0.aten::_to_copy(%67:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %274:<6144xf32> = torch.2_1_0.aten::_to_copy(%91:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %329:<12288xf32> = torch.2_1_0.aten::_to_copy(%128:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %219:<12288xf32> = torch.2_1_0.aten::_to_copy(%107:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %283:<12288xf32> = torch.2_1_0.aten::_to_copy(%100:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 %333:<6400x12288xf32> = torch.2_1_0.aten::detach(%92:<6400x12288xf32>) 
58393:6:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58393:6:0 %334:<12288x1536xf32> = torch.2_1_0.aten::detach(%330:<12288x1536xf32>) 
58393:6:0 %258:<4608x12288xf32> = torch.2_1_0.aten::detach(%331:<4608x12288xf32>) 
58393:6:0 %307:<6144x12288xf32> = torch.2_1_0.aten::detach(%285:<6144x12288xf32>) 
58393:6:0 %305:<12288x6144xf32> = torch.2_1_0.aten::detach(%209:<12288x6144xf32>) 
58393:6:0 %299:<12288xf32> = torch.2_1_0.aten::detach(%332:<12288xf32>) 
58393:6:0 %288:<12288xf32> = torch.2_1_0.aten::detach(%238:<12288xf32>) 
58393:6:0 %99:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58393:6:0 %311:<4608xf32> = torch.2_1_0.aten::detach(%250:<4608xf32>) 
58393:6:0 %318:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%224:<12288xf32>) 
58393:6:0 %310:<6144xf32> = torch.2_1_0.aten::detach(%274:<6144xf32>) 
58393:6:0 %303:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58393:6:0 %297:<12288xf32> = torch.2_1_0.aten::detach(%219:<12288xf32>) 
58393:6:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%283:<12288xf32>) 
58393:6:0 %163:<1xf32> = torch.2_1_0.aten::fill_(%163:<1xf32>, 0.0:f32) 
58393:6:0 %322:<1xf64> = torch.2_1_0.aten::_to_copy(%159:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 %132:<1xf64> = torch.2_1_0.aten::reciprocal(%159:<1xf64>) 
58392:5:0 %183:<1xf32> = torch.2_1_0.aten::_to_copy(%132:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58390:3:0 %295:<1xf64> = torch.2_1_0.aten::reciprocal(%289:<1xf64>) 
58390:3:0 %293:<1xf32> = torch.2_1_0.aten::_to_copy(%295:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58392:5:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%156:<6400x12288xf32>, %147:<2048x12288xf32>, %158:<12288x1536xf32>, %157:<4608x12288xf32>, %187:<6144x12288xf32>, %174:<12288x6144xf32>, %180:<12288xf32>, %175:<12288xf32>, %179:<12288xf32>, %155:<4608xf32>, %181:<12288xf32>, %186:<12288xf32>, %222:<6144xf32>, %219:<12288xf32>, %153:<12288xf32>, %209:<12288xf32>}, %106:<1xf32>, %183:<1xf32>) 
58390:3:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%203:<6400x12288xf32>, %246:<2048x12288xf32>, %183:<12288x1536xf32>, %241:<4608x12288xf32>, %208:<6144x12288xf32>, %240:<12288x6144xf32>, %218:<12288xf32>, %287:<12288xf32>, %212:<12288xf32>, %213:<4608xf32>, %262:<12288xf32>, %233:<12288xf32>, %217:<6144xf32>, %250:<12288xf32>, %50:<12288xf32>, %292:<12288xf32>}, %157:<1xf32>, %293:<1xf32>) 
58393:6:0 %233:<1xf64> = torch.2_1_0.aten::reciprocal(%322:<1xf64>) 
58393:6:0 %320:<1xf32> = torch.2_1_0.aten::_to_copy(%233:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58393:6:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%333:<6400x12288xf32>, %257:<2048x12288xf32>, %334:<12288x1536xf32>, %258:<4608x12288xf32>, %307:<6144x12288xf32>, %305:<12288x6144xf32>, %299:<12288xf32>, %288:<12288xf32>, %99:<12288xf32>, %311:<4608xf32>, %318:<12288xf32>, %323:<12288xf32>, %310:<6144xf32>, %303:<12288xf32>, %297:<12288xf32>, %309:<12288xf32>}, %163:<1xf32>, %320:<1xf32>) 
58387:None:0 list{%150:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%150:<330410496xf16>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %228:<6400x12288xf32> = torch.2_1_0.aten::_to_copy(%152:<6400x12288xf16>+251767296, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %103:<2048x12288xf32> = torch.2_1_0.aten::_to_copy(%146:<2048x12288xf16>+226601472, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %95:<12288x1536xf32> = torch.2_1_0.aten::_to_copy(%148:<12288x1536xf16>+207702528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %94:<4608x12288xf32> = torch.2_1_0.aten::_to_copy(%144:<4608x12288xf16>+151067136, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %248:<6144x12288xf32> = torch.2_1_0.aten::_to_copy(%139:<6144x12288xf16>+75540480, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %307:<12288x6144xf32> = torch.2_1_0.aten::_to_copy(%135:<12288x6144xf16>+36864, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %301:<12288xf32> = torch.2_1_0.aten::_to_copy(%147:<12288xf16>+226589184, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %291:<12288xf32> = torch.2_1_0.aten::_to_copy(%149:<12288xf16>+226576896, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %320:<12288xf32> = torch.2_1_0.aten::_to_copy(%137:<12288xf16>+207690240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %276:<4608xf32> = torch.2_1_0.aten::_to_copy(%143:<4608xf16>+151062528, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %308:<12288xf32> = torch.2_1_0.aten::_to_copy(%141:<12288xf16>+151050240, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %303:<12288xf32> = torch.2_1_0.aten::_to_copy(%140:<12288xf16>+151037952, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %287:<6144xf32> = torch.2_1_0.aten::_to_copy(%113:<6144xf16>+75534336, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %310:<12288xf32> = torch.2_1_0.aten::_to_copy(%136:<12288xf16>+24576, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %257:<12288xf32> = torch.2_1_0.aten::_to_copy(%71:<12288xf16>+12288, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %271:<12288xf32> = torch.2_1_0.aten::_to_copy(%106:<12288xf16>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %302:<6400x12288xf32> = torch.2_1_0.aten::detach(%228:<6400x12288xf32>) 
58387:0:0 %268:<2048x12288xf32> = torch.2_1_0.aten::detach(%103:<2048x12288xf32>) 
58387:0:0 %270:<12288x1536xf32> = torch.2_1_0.aten::detach(%95:<12288x1536xf32>) 
58387:0:0 %286:<4608x12288xf32> = torch.2_1_0.aten::detach(%94:<4608x12288xf32>) 
58387:0:0 %256:<6144x12288xf32> = torch.2_1_0.aten::detach(%248:<6144x12288xf32>) 
58387:0:0 %274:<12288x6144xf32> = torch.2_1_0.aten::detach(%307:<12288x6144xf32>) 
58387:0:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%301:<12288xf32>) 
58387:0:0 %285:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58387:0:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%320:<12288xf32>) 
58387:0:0 %322:<4608xf32> = torch.2_1_0.aten::detach(%276:<4608xf32>) 
58387:0:0 %253:<12288xf32> = torch.2_1_0.aten::detach(%308:<12288xf32>) 
58387:0:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%303:<12288xf32>) 
58387:0:0 %281:<6144xf32> = torch.2_1_0.aten::detach(%287:<6144xf32>) 
58387:0:0 %250:<12288xf32> = torch.2_1_0.aten::detach(%310:<12288xf32>) 
58387:0:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58387:0:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%271:<12288xf32>) 
58387:0:0 %167:<1xf32> = torch.2_1_0.aten::fill_(%167:<1xf32>, 0.0:f32) 
58387:0:0 %312:<1xf64> = torch.2_1_0.aten::_to_copy(%133:<1xf32>, dtype=torch.float64:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 %236:<1xf64> = torch.2_1_0.aten::reciprocal(%312:<1xf64>) 
58387:0:0 %269:<1xf32> = torch.2_1_0.aten::_to_copy(%236:<1xf64>, dtype=torch.float32:dtype, layout=None:NoneType, device=None:NoneType, pin_memory=None:NoneType, non_blocking=False:pred, memory_format=None:NoneType) 
58387:0:0 None:NoneType: = torch.2_1_0.aten::_amp_foreach_non_finite_check_and_unscale_(list{%302:<6400x12288xf32>, %268:<2048x12288xf32>, %270:<12288x1536xf32>, %286:<4608x12288xf32>, %256:<6144x12288xf32>, %274:<12288x6144xf32>, %321:<12288xf32>, %285:<12288xf32>, %254:<12288xf32>, %322:<4608xf32>, %253:<12288xf32>, %315:<12288xf32>, %281:<6144xf32>, %250:<12288xf32>, %260:<12288xf32>, %311:<12288xf32>}, %167:<1xf32>, %269:<1xf32>) 
58393:None:0 list{%163:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%163:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:None:0 list{%167:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%167:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58391:None:0 list{%170:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%170:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:None:0 list{%156:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%156:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:None:0 list{%136:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%136:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:None:0 list{%106:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%106:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:None:0 list{%124:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%124:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:2:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%156:<1xf32>) 
58392:5:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%106:<1xf32>) 
58388:1:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%124:<1xf32>) 
58387:0:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%167:<1xf32>) 
58391:4:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%170:<1xf32>) 
58394:7:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%136:<1xf32>) 
58391:4:0 %257:<6400x12288xf32> = torch.2_1_0.aten::detach(%61:<6400x12288xf32>) 
58390:None:0 list{%157:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%157:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %147:<6400x12288xf32> = torch.2_1_0.aten::detach(%51:<6400x12288xf32>) 
58387:0:0 %302:<6400x12288xf32> = torch.2_1_0.aten::detach(%228:<6400x12288xf32>) 
58389:2:0 %80:<6400x12288xf32> = torch.2_1_0.aten::detach(%62:<6400x12288xf32>) 
58388:1:0 %278:<6400x12288xf32> = torch.2_1_0.aten::detach(%326:<6400x12288xf32>) 
58392:5:0 %157:<6400x12288xf32> = torch.2_1_0.aten::detach(%147:<6400x12288xf32>) 
58391:4:0 %221:<6400x12288xf32> = torch.2_1_0.aten::detach(%257:<6400x12288xf32>) 
58394:7:0 %190:<6400x12288xf32> = torch.2_1_0.aten::detach(%82:<6400x12288xf32>) 
58387:0:0 %270:<6400x12288xf32> = torch.2_1_0.aten::detach(%302:<6400x12288xf32>) 
58389:2:0 %335:<6400x12288xf32> = torch.2_1_0.aten::detach(%80:<6400x12288xf32>) 
58392:5:0 %187:<2048x12288xf32> = torch.2_1_0.aten::detach(%227:<2048x12288xf32>) 
58391:4:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%281:<2048x12288xf32>) 
58388:1:0 %229:<6400x12288xf32> = torch.2_1_0.aten::detach(%278:<6400x12288xf32>) 
58387:0:0 %286:<2048x12288xf32> = torch.2_1_0.aten::detach(%103:<2048x12288xf32>) 
58394:7:0 %184:<6400x12288xf32> = torch.2_1_0.aten::detach(%190:<6400x12288xf32>) 
58392:5:0 %158:<2048x12288xf32> = torch.2_1_0.aten::detach(%187:<2048x12288xf32>) 
58389:2:0 %74:<2048x12288xf32> = torch.2_1_0.aten::detach(%126:<2048x12288xf32>) 
58391:4:0 %237:<2048x12288xf32> = torch.2_1_0.aten::detach(%257:<2048x12288xf32>) 
58388:1:0 %278:<2048x12288xf32> = torch.2_1_0.aten::detach(%130:<2048x12288xf32>) 
58387:0:0 %302:<2048x12288xf32> = torch.2_1_0.aten::detach(%286:<2048x12288xf32>) 
58394:7:0 %214:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58392:5:0 %174:<12288x1536xf32> = torch.2_1_0.aten::detach(%228:<12288x1536xf32>) 
58391:4:0 %257:<12288x1536xf32> = torch.2_1_0.aten::detach(%75:<12288x1536xf32>) 
58389:2:0 %212:<2048x12288xf32> = torch.2_1_0.aten::detach(%74:<2048x12288xf32>) 
58388:1:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%278:<2048x12288xf32>) 
58387:0:0 %256:<12288x1536xf32> = torch.2_1_0.aten::detach(%95:<12288x1536xf32>) 
58391:4:0 %239:<12288x1536xf32> = torch.2_1_0.aten::detach(%257:<12288x1536xf32>) 
58392:5:0 %156:<12288x1536xf32> = torch.2_1_0.aten::detach(%174:<12288x1536xf32>) 
58394:7:0 %158:<2048x12288xf32> = torch.2_1_0.aten::detach(%214:<2048x12288xf32>) 
58389:2:0 %330:<12288x1536xf32> = torch.2_1_0.aten::detach(%122:<12288x1536xf32>) 
58388:1:0 %278:<12288x1536xf32> = torch.2_1_0.aten::detach(%328:<12288x1536xf32>) 
58391:4:0 %257:<4608x12288xf32> = torch.2_1_0.aten::detach(%234:<4608x12288xf32>) 
58387:0:0 %286:<12288x1536xf32> = torch.2_1_0.aten::detach(%256:<12288x1536xf32>) 
58392:5:0 %180:<4608x12288xf32> = torch.2_1_0.aten::detach(%229:<4608x12288xf32>) 
58394:7:0 %190:<12288x1536xf32> = torch.2_1_0.aten::detach(%285:<12288x1536xf32>) 
58388:1:0 %288:<12288x1536xf32> = torch.2_1_0.aten::detach(%278:<12288x1536xf32>) 
58389:2:0 %273:<12288x1536xf32> = torch.2_1_0.aten::detach(%330:<12288x1536xf32>) 
58391:4:0 %255:<4608x12288xf32> = torch.2_1_0.aten::detach(%257:<4608x12288xf32>) 
58387:0:0 %274:<4608x12288xf32> = torch.2_1_0.aten::detach(%94:<4608x12288xf32>) 
58392:5:0 %147:<4608x12288xf32> = torch.2_1_0.aten::detach(%180:<4608x12288xf32>) 
58394:7:0 %268:<12288x1536xf32> = torch.2_1_0.aten::detach(%190:<12288x1536xf32>) 
58388:1:0 %278:<4608x12288xf32> = torch.2_1_0.aten::detach(%225:<4608x12288xf32>) 
58391:4:0 %257:<6144x12288xf32> = torch.2_1_0.aten::detach(%272:<6144x12288xf32>) 
58389:2:0 %257:<4608x12288xf32> = torch.2_1_0.aten::detach(%333:<4608x12288xf32>) 
58387:0:0 %256:<4608x12288xf32> = torch.2_1_0.aten::detach(%274:<4608x12288xf32>) 
58390:3:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%157:<1xf32>) 
58392:5:0 %175:<6144x12288xf32> = torch.2_1_0.aten::detach(%141:<6144x12288xf32>) 
58394:7:0 %246:<4608x12288xf32> = torch.2_1_0.aten::detach(%178:<4608x12288xf32>) 
58391:4:0 %242:<6144x12288xf32> = torch.2_1_0.aten::detach(%257:<6144x12288xf32>) 
58388:1:0 %230:<4608x12288xf32> = torch.2_1_0.aten::detach(%278:<4608x12288xf32>) 
58389:2:0 %297:<4608x12288xf32> = torch.2_1_0.aten::detach(%257:<4608x12288xf32>) 
58387:0:0 %321:<6144x12288xf32> = torch.2_1_0.aten::detach(%248:<6144x12288xf32>) 
58392:5:0 %187:<6144x12288xf32> = torch.2_1_0.aten::detach(%175:<6144x12288xf32>) 
58391:4:0 %257:<12288x6144xf32> = torch.2_1_0.aten::detach(%282:<12288x6144xf32>) 
58394:7:0 %180:<4608x12288xf32> = torch.2_1_0.aten::detach(%246:<4608x12288xf32>) 
58388:1:0 %278:<6144x12288xf32> = torch.2_1_0.aten::detach(%228:<6144x12288xf32>) 
58389:2:0 %257:<6144x12288xf32> = torch.2_1_0.aten::detach(%271:<6144x12288xf32>) 
58387:0:0 %274:<6144x12288xf32> = torch.2_1_0.aten::detach(%321:<6144x12288xf32>) 
58391:4:0 %220:<12288x6144xf32> = torch.2_1_0.aten::detach(%257:<12288x6144xf32>) 
58392:5:0 %179:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58394:7:0 %198:<6144x12288xf32> = torch.2_1_0.aten::detach(%255:<6144x12288xf32>) 
58388:1:0 %283:<6144x12288xf32> = torch.2_1_0.aten::detach(%278:<6144x12288xf32>) 
58389:2:0 %225:<6144x12288xf32> = torch.2_1_0.aten::detach(%257:<6144x12288xf32>) 
58391:4:0 %257:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58387:0:0 %285:<12288x6144xf32> = torch.2_1_0.aten::detach(%307:<12288x6144xf32>) 
58392:5:0 %174:<12288x6144xf32> = torch.2_1_0.aten::detach(%179:<12288x6144xf32>) 
58394:7:0 %206:<6144x12288xf32> = torch.2_1_0.aten::detach(%198:<6144x12288xf32>) 
58388:1:0 %278:<12288x6144xf32> = torch.2_1_0.aten::detach(%169:<12288x6144xf32>) 
58389:2:0 %336:<12288x6144xf32> = torch.2_1_0.aten::detach(%334:<12288x6144xf32>) 
58391:4:0 %228:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58392:5:0 %155:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58387:0:0 %321:<12288x6144xf32> = torch.2_1_0.aten::detach(%285:<12288x6144xf32>) 
58388:1:0 %255:<12288x6144xf32> = torch.2_1_0.aten::detach(%278:<12288x6144xf32>) 
58394:7:0 %220:<12288x6144xf32> = torch.2_1_0.aten::detach(%286:<12288x6144xf32>) 
58391:4:0 %257:<12288xf32> = torch.2_1_0.aten::detach(%262:<12288xf32>) 
58389:2:0 %215:<12288x6144xf32> = torch.2_1_0.aten::detach(%336:<12288x6144xf32>) 
58392:5:0 %179:<12288xf32> = torch.2_1_0.aten::detach(%155:<12288xf32>) 
58387:0:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%301:<12288xf32>) 
58388:1:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58394:7:0 %214:<12288x6144xf32> = torch.2_1_0.aten::detach(%220:<12288x6144xf32>) 
58391:4:0 %261:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58389:2:0 %330:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58392:5:0 %180:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58387:0:0 %268:<12288xf32> = torch.2_1_0.aten::detach(%254:<12288xf32>) 
58388:1:0 %200:<12288xf32> = torch.2_1_0.aten::detach(%278:<12288xf32>) 
58391:4:0 %257:<12288xf32> = torch.2_1_0.aten::detach(%229:<12288xf32>) 
58394:7:0 %190:<12288xf32> = torch.2_1_0.aten::detach(%287:<12288xf32>) 
58389:2:0 %257:<12288xf32> = torch.2_1_0.aten::detach(%330:<12288xf32>) 
58392:5:0 %155:<12288xf32> = torch.2_1_0.aten::detach(%180:<12288xf32>) 
58387:0:0 %285:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58388:1:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%241:<12288xf32>) 
58394:7:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%190:<12288xf32>) 
58389:2:0 %117:<12288xf32> = torch.2_1_0.aten::detach(%282:<12288xf32>) 
58387:0:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%285:<12288xf32>) 
58391:4:0 %44:<4608xf32> = torch.2_1_0.aten::detach(%254:<4608xf32>) 
58389:2:0 %329:<12288xf32> = torch.2_1_0.aten::detach(%117:<12288xf32>) 
58394:7:0 %220:<12288xf32> = torch.2_1_0.aten::detach(%249:<12288xf32>) 
58388:1:0 %258:<12288xf32> = torch.2_1_0.aten::detach(%278:<12288xf32>) 
58391:4:0 %257:<4608xf32> = torch.2_1_0.aten::detach(%44:<4608xf32>) 
58387:0:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%320:<12288xf32>) 
58389:2:0 %134:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58394:7:0 %246:<12288xf32> = torch.2_1_0.aten::detach(%220:<12288xf32>) 
58391:4:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%209:<12288xf32>) 
58388:1:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58387:0:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%254:<12288xf32>) 
58389:2:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%134:<12288xf32>) 
58391:4:0 %212:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58394:7:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%288:<12288xf32>) 
58388:1:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%278:<12288xf32>) 
58387:0:0 %254:<4608xf32> = torch.2_1_0.aten::detach(%276:<4608xf32>) 
58391:4:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%267:<12288xf32>) 
58389:2:0 %336:<4608xf32> = torch.2_1_0.aten::detach(%178:<4608xf32>) 
58394:7:0 %294:<12288xf32> = torch.2_1_0.aten::detach(%274:<12288xf32>) 
58388:1:0 %21:<4608xf32> = torch.2_1_0.aten::detach(%271:<4608xf32>) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58387:0:0 %285:<4608xf32> = torch.2_1_0.aten::detach(%254:<4608xf32>) 
58389:2:0 %328:<4608xf32> = torch.2_1_0.aten::detach(%336:<4608xf32>) 
58391:4:0 %283:<6144xf32> = torch.2_1_0.aten::detach(%181:<6144xf32>) 
58394:7:0 %190:<4608xf32> = torch.2_1_0.aten::detach(%289:<4608xf32>) 
58387:0:0 %253:<12288xf32> = torch.2_1_0.aten::detach(%308:<12288xf32>) 
58388:1:0 %235:<4608xf32> = torch.2_1_0.aten::detach(%21:<4608xf32>) 
58389:2:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58391:4:0 %260:<6144xf32> = torch.2_1_0.aten::detach(%283:<6144xf32>) 
58394:7:0 %295:<4608xf32> = torch.2_1_0.aten::detach(%190:<4608xf32>) 
58387:0:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%253:<12288xf32>) 
58388:1:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58389:2:0 %310:<12288xf32> = torch.2_1_0.aten::detach(%314:<12288xf32>) 
58391:4:0 %210:<12288xf32> = torch.2_1_0.aten::detach(%268:<12288xf32>) 
58394:7:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%270:<12288xf32>) 
58387:0:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%303:<12288xf32>) 
58391:4:0 %247:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58389:2:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%245:<12288xf32>) 
58388:1:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%278:<12288xf32>) 
58394:7:0 %46:<12288xf32> = torch.2_1_0.aten::detach(%292:<12288xf32>) 
58387:0:0 %281:<12288xf32> = torch.2_1_0.aten::detach(%254:<12288xf32>) 
58391:4:0 %210:<12288xf32> = torch.2_1_0.aten::detach(%216:<12288xf32>) 
58389:2:0 %307:<12288xf32> = torch.2_1_0.aten::detach(%314:<12288xf32>) 
58394:7:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58387:0:0 %254:<6144xf32> = torch.2_1_0.aten::detach(%287:<6144xf32>) 
58388:1:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%253:<12288xf32>) 
58391:4:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58387:0:0 %253:<6144xf32> = torch.2_1_0.aten::detach(%254:<6144xf32>) 
58394:7:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%274:<12288xf32>) 
58389:2:0 %337:<6144xf32> = torch.2_1_0.aten::detach(%238:<6144xf32>) 
58388:1:0 %208:<12288xf32> = torch.2_1_0.aten::detach(%278:<12288xf32>) 
58391:4:0 %210:<12288xf32> = torch.2_1_0.aten::detach(%227:<12288xf32>) 
58394:7:0 %190:<6144xf32> = torch.2_1_0.aten::detach(%258:<6144xf32>) 
58387:0:0 %250:<12288xf32> = torch.2_1_0.aten::detach(%310:<12288xf32>) 
58391:4:0 %82:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58389:2:0 %314:<6144xf32> = torch.2_1_0.aten::detach(%337:<6144xf32>) 
58388:1:0 %331:<6144xf32> = torch.2_1_0.aten::detach(%227:<6144xf32>) 
58387:0:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%250:<12288xf32>) 
58394:7:0 %1:<6144xf32> = torch.2_1_0.aten::detach(%190:<6144xf32>) 
58389:2:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58388:1:0 %278:<6144xf32> = torch.2_1_0.aten::detach(%331:<6144xf32>) 
58387:0:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58394:7:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58389:2:0 %117:<12288xf32> = torch.2_1_0.aten::detach(%309:<12288xf32>) 
58387:0:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%254:<12288xf32>) 
58394:7:0 %283:<12288xf32> = torch.2_1_0.aten::detach(%276:<12288xf32>) 
58388:1:0 %266:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58389:2:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58391:4:0 %284:<1xi32> = torch.2_1_0.aten::lift_fresh(%284:<1xi32>) 
58394:7:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58387:0:0 %250:<12288xf32> = torch.2_1_0.aten::detach(%271:<12288xf32>) 
58388:1:0 %246:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58389:2:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%309:<12288xf32>) 
58387:0:0 %269:<12288xf32> = torch.2_1_0.aten::detach(%250:<12288xf32>) 
58394:7:0 %271:<12288xf32> = torch.2_1_0.aten::detach(%276:<12288xf32>) 
58388:1:0 %266:<12288xf32> = torch.2_1_0.aten::detach(%233:<12288xf32>) 
58389:2:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58394:7:0 %276:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58388:1:0 %332:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58389:2:0 %320:<12288xf32> = torch.2_1_0.aten::detach(%309:<12288xf32>) 
58394:7:0 %280:<12288xf32> = torch.2_1_0.aten::detach(%276:<12288xf32>) 
58388:1:0 %266:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58387:0:0 %250:<1xi32> = torch.2_1_0.aten::lift_fresh(%250:<1xi32>) 
58391:4:0 %249:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType) 
58388:1:0 %269:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58394:7:0 %274:<1xi32> = torch.2_1_0.aten::lift_fresh(%274:<1xi32>) 
58389:2:0 %309:<1xi32> = torch.2_1_0.aten::lift_fresh(%309:<1xi32>) 
58391:4:0 %265:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %323:<1xi32> = torch.2_1_0.aten::lift_fresh(%323:<1xi32>) 
58387:0:0 %317:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType) 
58394:7:0 %296:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType) 
58389:2:0 %313:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType) 
58387:0:0 %277:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 %297:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58389:2:0 %316:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %256:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType) 
58388:1:0 %220:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %244:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:4:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 0.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%163:<1xf32>) 
58387:0:0 %314:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:0:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %301:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:1:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %334:<6400x12288xf32> = torch.2_1_0.aten::detach(%92:<6400x12288xf32>) 
58394:7:0 %298:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:7:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %333:<6400x12288xf32> = torch.2_1_0.aten::detach(%334:<6400x12288xf32>) 
58393:6:0 %334:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58393:6:0 %233:<2048x12288xf32> = torch.2_1_0.aten::detach(%334:<2048x12288xf32>) 
58393:6:0 %258:<12288x1536xf32> = torch.2_1_0.aten::detach(%330:<12288x1536xf32>) 
58393:6:0 %257:<12288x1536xf32> = torch.2_1_0.aten::detach(%258:<12288x1536xf32>) 
58393:6:0 %305:<4608x12288xf32> = torch.2_1_0.aten::detach(%331:<4608x12288xf32>) 
58393:6:0 %299:<4608x12288xf32> = torch.2_1_0.aten::detach(%305:<4608x12288xf32>) 
58393:6:0 %288:<6144x12288xf32> = torch.2_1_0.aten::detach(%285:<6144x12288xf32>) 
58393:6:0 %305:<6144x12288xf32> = torch.2_1_0.aten::detach(%288:<6144x12288xf32>) 
58393:6:0 %318:<12288x6144xf32> = torch.2_1_0.aten::detach(%209:<12288x6144xf32>) 
58393:6:0 %288:<12288x6144xf32> = torch.2_1_0.aten::detach(%318:<12288x6144xf32>) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%332:<12288xf32>) 
58393:6:0 %307:<12288xf32> = torch.2_1_0.aten::detach(%323:<12288xf32>) 
58393:6:0 %318:<12288xf32> = torch.2_1_0.aten::detach(%238:<12288xf32>) 
58393:6:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%318:<12288xf32>) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58393:6:0 %310:<12288xf32> = torch.2_1_0.aten::detach(%323:<12288xf32>) 
58393:6:0 %323:<4608xf32> = torch.2_1_0.aten::detach(%250:<4608xf32>) 
58393:6:0 %335:<4608xf32> = torch.2_1_0.aten::detach(%323:<4608xf32>) 
58393:6:0 %318:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58393:6:0 %303:<12288xf32> = torch.2_1_0.aten::detach(%318:<12288xf32>) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%224:<12288xf32>) 
58393:6:0 %297:<12288xf32> = torch.2_1_0.aten::detach(%323:<12288xf32>) 
58393:6:0 %323:<6144xf32> = torch.2_1_0.aten::detach(%274:<6144xf32>) 
58389:2:0 %327:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:2:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %318:<6144xf32> = torch.2_1_0.aten::detach(%323:<6144xf32>) 
58393:6:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58393:6:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%309:<12288xf32>) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%219:<12288xf32>) 
58393:6:0 %320:<12288xf32> = torch.2_1_0.aten::detach(%323:<12288xf32>) 
58393:6:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%283:<12288xf32>) 
58393:6:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%309:<12288xf32>) 
58393:6:0 %309:<1xi32> = torch.2_1_0.aten::lift_fresh(%309:<1xi32>) 
58393:6:0 %336:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType) 
58393:6:0 %337:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %338:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:6:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %181:<12288xf32> = torch.2_1_0.aten::detach(%231:<12288xf32>) 
58392:5:0 %180:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58392:5:0 %181:<4608xf32> = torch.2_1_0.aten::detach(%204:<4608xf32>) 
58392:5:0 %222:<4608xf32> = torch.2_1_0.aten::detach(%181:<4608xf32>) 
58392:5:0 %175:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58392:5:0 %186:<12288xf32> = torch.2_1_0.aten::detach(%175:<12288xf32>) 
58392:5:0 %181:<12288xf32> = torch.2_1_0.aten::detach(%232:<12288xf32>) 
58392:5:0 %219:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58392:5:0 %181:<6144xf32> = torch.2_1_0.aten::detach(%166:<6144xf32>) 
58392:5:0 %175:<6144xf32> = torch.2_1_0.aten::detach(%181:<6144xf32>) 
58392:5:0 %153:<12288xf32> = torch.2_1_0.aten::detach(%214:<12288xf32>) 
58390:3:0 %241:<6400x12288xf32> = torch.2_1_0.aten::detach(%220:<6400x12288xf32>) 
58392:5:0 %209:<12288xf32> = torch.2_1_0.aten::detach(%153:<12288xf32>) 
58390:3:0 %246:<6400x12288xf32> = torch.2_1_0.aten::detach(%241:<6400x12288xf32>) 
58392:5:0 %181:<12288xf32> = torch.2_1_0.aten::detach(%218:<12288xf32>) 
58390:3:0 %240:<2048x12288xf32> = torch.2_1_0.aten::detach(%272:<2048x12288xf32>) 
58392:5:0 %183:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58390:3:0 %218:<2048x12288xf32> = torch.2_1_0.aten::detach(%240:<2048x12288xf32>) 
58392:5:0 %153:<12288xf32> = torch.2_1_0.aten::detach(%213:<12288xf32>) 
58390:3:0 %287:<12288x1536xf32> = torch.2_1_0.aten::detach(%278:<12288x1536xf32>) 
58392:5:0 %182:<12288xf32> = torch.2_1_0.aten::detach(%153:<12288xf32>) 
58390:3:0 %208:<12288x1536xf32> = torch.2_1_0.aten::detach(%287:<12288x1536xf32>) 
58390:3:0 %212:<4608x12288xf32> = torch.2_1_0.aten::detach(%181:<4608x12288xf32>) 
58390:3:0 %233:<4608x12288xf32> = torch.2_1_0.aten::detach(%212:<4608x12288xf32>) 
58392:5:0 %153:<1xi32> = torch.2_1_0.aten::lift_fresh(%153:<1xi32>) 
58390:3:0 %287:<6144x12288xf32> = torch.2_1_0.aten::detach(%243:<6144x12288xf32>) 
58390:3:0 %217:<6144x12288xf32> = torch.2_1_0.aten::detach(%287:<6144x12288xf32>) 
58390:3:0 %262:<12288x6144xf32> = torch.2_1_0.aten::detach(%189:<12288x6144xf32>) 
58390:3:0 %183:<12288x6144xf32> = torch.2_1_0.aten::detach(%262:<12288x6144xf32>) 
58390:3:0 %296:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58392:5:0 %192:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType) 
58390:3:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%296:<12288xf32>) 
58390:3:0 %296:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58390:3:0 %297:<12288xf32> = torch.2_1_0.aten::detach(%296:<12288xf32>) 
58392:5:0 %176:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %296:<12288xf32> = torch.2_1_0.aten::detach(%294:<12288xf32>) 
58390:3:0 %89:<12288xf32> = torch.2_1_0.aten::detach(%296:<12288xf32>) 
58390:3:0 %287:<4608xf32> = torch.2_1_0.aten::detach(%244:<4608xf32>) 
58390:3:0 %296:<4608xf32> = torch.2_1_0.aten::detach(%287:<4608xf32>) 
58390:3:0 %50:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58390:3:0 %298:<12288xf32> = torch.2_1_0.aten::detach(%50:<12288xf32>) 
58390:3:0 %295:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58390:3:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%295:<12288xf32>) 
58390:3:0 %275:<6144xf32> = torch.2_1_0.aten::detach(%199:<6144xf32>) 
58390:3:0 %250:<6144xf32> = torch.2_1_0.aten::detach(%275:<6144xf32>) 
58390:3:0 %293:<12288xf32> = torch.2_1_0.aten::detach(%273:<12288xf32>) 
58390:3:0 %299:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58390:3:0 %293:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58390:3:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58390:3:0 %50:<12288xf32> = torch.2_1_0.aten::detach(%81:<12288xf32>) 
58392:5:0 %191:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:5:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %293:<12288xf32> = torch.2_1_0.aten::detach(%50:<12288xf32>) 
58390:3:0 %275:<1xi32> = torch.2_1_0.aten::lift_fresh(%275:<1xi32>) 
58390:3:0 %301:<320xf32> = torch.2_1_0.aten::zeros(list{320:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType) 
58390:3:0 %302:<0xf32> = torch.2_1_0.aten::empty(list{0:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58390:3:0 %303:<1xf32> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.float32:dtype, layout=torch.strided:layout, device=cuda:3:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58391:4:0 %249:<1xf32> = torch.2_1_0.aten::pow(%244:<1xf32>, 2.0:f32) 
58391:None:0 list{%249:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%249:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58387:0:0 %252:<1xf32> = torch.2_1_0.aten::pow(%314:<1xf32>, 2.0:f32) 
58387:None:0 list{%252:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%252:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58394:7:0 %293:<1xf32> = torch.2_1_0.aten::pow(%298:<1xf32>, 2.0:f32) 
58394:None:0 list{%293:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%293:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58393:6:0 %339:<1xf32> = torch.2_1_0.aten::pow(%338:<1xf32>, 2.0:f32) 
58393:None:0 list{%339:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%339:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58390:3:0 %304:<1xf32> = torch.2_1_0.aten::pow(%303:<1xf32>, 2.0:f32) 
58389:2:0 %313:<1xf32> = torch.2_1_0.aten::pow(%327:<1xf32>, 2.0:f32) 
58390:None:0 list{%304:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%304:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58389:None:0 list{%313:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%313:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 %217:<1xf32> = torch.2_1_0.aten::pow(%191:<1xf32>, 2.0:f32) 
58392:None:0 list{%217:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%217:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58388:1:0 %256:<1xf32> = torch.2_1_0.aten::pow(%301:<1xf32>, 2.0:f32) 
58388:None:0 list{%256:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::allreduce_(list{%256:<1xf32>}, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, ScriptObject <__torch__.torch.classes.c10d.ReduceOp>:ScriptObject, None:NoneType, -1:i32) 
58392:5:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%217:<1xf32>) 
58389:2:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%313:<1xf32>) 
58394:7:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%293:<1xf32>) 
58387:0:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%252:<1xf32>) 
58393:6:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%339:<1xf32>) 
58390:3:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%304:<1xf32>) 
58391:4:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%249:<1xf32>) 
58388:1:0 792.4017944335938:f32: = torch.2_1_0.aten::_local_scalar_dense(%256:<1xf32>) 
58393:6:0 %339:<1xi32> = torch.2_1_0.aten::lift_fresh(%339:<1xi32>) 
58389:2:0 %291:<1xi32> = torch.2_1_0.aten::lift_fresh(%291:<1xi32>) 
58392:5:0 %217:<1xi32> = torch.2_1_0.aten::lift_fresh(%217:<1xi32>) 
58387:0:0 %252:<1xi32> = torch.2_1_0.aten::lift_fresh(%252:<1xi32>) 
58394:7:0 %293:<1xi32> = torch.2_1_0.aten::lift_fresh(%293:<1xi32>) 
58388:1:0 %256:<1xi32> = torch.2_1_0.aten::lift_fresh(%256:<1xi32>) 
58390:3:0 %304:<1xi32> = torch.2_1_0.aten::lift_fresh(%304:<1xi32>) 
58391:4:0 %249:<1xi32> = torch.2_1_0.aten::lift_fresh(%249:<1xi32>) 
58387:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58387:0:0 %314:<6400x12288xf32> = torch.2_1_0.aten::detach(%228:<6400x12288xf32>) 
58393:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58389:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58387:0:0 %277:<6400x12288xf32> = torch.2_1_0.aten::detach(%172:<6400x12288xf32>) 
58392:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58393:6:0 %181:<6400x12288xf32> = torch.2_1_0.aten::detach(%92:<6400x12288xf32>) 
58389:2:0 %273:<6400x12288xf32> = torch.2_1_0.aten::detach(%62:<6400x12288xf32>) 
58392:5:0 %157:<6400x12288xf32> = torch.2_1_0.aten::detach(%51:<6400x12288xf32>) 
58391:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58393:6:0 %131:<6400x12288xf32> = torch.2_1_0.aten::detach(%168:<6400x12288xf32>) 
58389:2:0 %212:<6400x12288xf32> = torch.2_1_0.aten::detach(%161:<6400x12288xf32>) 
58392:5:0 %191:<6400x12288xf32> = torch.2_1_0.aten::detach(%111:<6400x12288xf32>) 
58388:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58391:4:0 %221:<6400x12288xf32> = torch.2_1_0.aten::detach(%61:<6400x12288xf32>) 
58390:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58388:1:0 %220:<6400x12288xf32> = torch.2_1_0.aten::detach(%326:<6400x12288xf32>) 
58391:4:0 %249:<6400x12288xf32> = torch.2_1_0.aten::detach(%175:<6400x12288xf32>) 
58388:1:0 %256:<6400x12288xf32> = torch.2_1_0.aten::detach(%180:<6400x12288xf32>) 
58390:3:0 %246:<6400x12288xf32> = torch.2_1_0.aten::detach(%220:<6400x12288xf32>) 
58390:3:0 %287:<6400x12288xf32> = torch.2_1_0.aten::detach(%162:<6400x12288xf32>) 
58394:None:0 ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject: = torch.2_1_0.profiler::_record_function_enter_new(Optimizer.step#FusedAdam.step:str, None:NoneType) 
58394:7:0 %198:<6400x12288xf32> = torch.2_1_0.aten::detach(%82:<6400x12288xf32>) 
58394:7:0 %214:<6400x12288xf32> = torch.2_1_0.aten::detach(%141:<6400x12288xf32>) 
58388:1:0 %301:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%256:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %217:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%287:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %220:<6400x12288xf32> = torch.2_1_0.aten::detach(%180:<6400x12288xf32>) 
58390:3:0 %305:<6400x12288xf32> = torch.2_1_0.aten::detach(%162:<6400x12288xf32>) 
58394:7:0 %246:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%214:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %198:<6400x12288xf32> = torch.2_1_0.aten::detach(%141:<6400x12288xf32>) 
58394:7:0 %63:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%198:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %233:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%305:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %206:<6400x12288xf32> = torch.2_1_0.aten::detach(%82:<6400x12288xf32>) 
58390:3:0 %287:<6400x12288xf32> = torch.2_1_0.aten::detach(%220:<6400x12288xf32>) 
58394:7:0 %294:<6400x12288xf32> = torch.2_1_0.aten::detach(%141:<6400x12288xf32>) 
58390:3:0 %262:<6400x12288xf32> = torch.2_1_0.aten::detach(%162:<6400x12288xf32>) 
58394:7:0 %67:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58394:7:0 %67:<2048x12288xf32> = torch.2_1_0.aten::detach(%144:<2048x12288xf32>) 
58390:3:0 %250:<2048x12288xf32> = torch.2_1_0.aten::detach(%272:<2048x12288xf32>) 
58390:3:0 %183:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58394:7:0 %46:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%67:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %299:<2048x12288xf32> = torch.2_1_0.aten::detach(%144:<2048x12288xf32>) 
58390:3:0 %208:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%183:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %67:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%299:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %275:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58394:7:0 %300:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58394:7:0 %198:<2048x12288xf32> = torch.2_1_0.aten::detach(%144:<2048x12288xf32>) 
58390:3:0 %2:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%275:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %1:<12288x1536xf32> = torch.2_1_0.aten::detach(%285:<12288x1536xf32>) 
58390:3:0 %30:<2048x12288xf32> = torch.2_1_0.aten::detach(%272:<2048x12288xf32>) 
58394:7:0 %274:<12288x1536xf32> = torch.2_1_0.aten::detach(%146:<12288x1536xf32>) 
58390:3:0 %275:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58394:7:0 %298:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%274:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %306:<12288x1536xf32> = torch.2_1_0.aten::detach(%278:<12288x1536xf32>) 
58394:7:0 %292:<12288x1536xf32> = torch.2_1_0.aten::detach(%146:<12288x1536xf32>) 
58390:3:0 %302:<12288x1536xf32> = torch.2_1_0.aten::detach(%159:<12288x1536xf32>) 
58394:7:0 %293:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%292:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %217:<12288x1536xf32> = torch.2_1_0.aten::detach(%285:<12288x1536xf32>) 
58390:3:0 %307:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%302:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %297:<12288x1536xf32> = torch.2_1_0.aten::detach(%146:<12288x1536xf32>) 
58390:3:0 %302:<12288x1536xf32> = torch.2_1_0.aten::detach(%159:<12288x1536xf32>) 
58394:7:0 %283:<4608x12288xf32> = torch.2_1_0.aten::detach(%178:<4608x12288xf32>) 
58394:7:0 %283:<4608x12288xf32> = torch.2_1_0.aten::detach(%148:<4608x12288xf32>) 
58390:3:0 %292:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%302:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %286:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%277:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %299:<12288x1536xf32> = torch.2_1_0.aten::detach(%278:<12288x1536xf32>) 
58387:0:0 %314:<6400x12288xf32> = torch.2_1_0.aten::detach(%172:<6400x12288xf32>) 
58390:3:0 %302:<12288x1536xf32> = torch.2_1_0.aten::detach(%159:<12288x1536xf32>) 
58390:3:0 %304:<4608x12288xf32> = torch.2_1_0.aten::detach(%181:<4608x12288xf32>) 
58390:3:0 %293:<4608x12288xf32> = torch.2_1_0.aten::detach(%161:<4608x12288xf32>) 
58394:7:0 %271:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%283:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %283:<4608x12288xf32> = torch.2_1_0.aten::detach(%148:<4608x12288xf32>) 
58387:0:0 %252:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%314:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %277:<6400x12288xf32> = torch.2_1_0.aten::detach(%228:<6400x12288xf32>) 
58387:0:0 %256:<6400x12288xf32> = torch.2_1_0.aten::detach(%172:<6400x12288xf32>) 
58387:0:0 %314:<2048x12288xf32> = torch.2_1_0.aten::detach(%103:<2048x12288xf32>) 
58390:3:0 %300:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%293:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %321:<2048x12288xf32> = torch.2_1_0.aten::detach(%176:<2048x12288xf32>) 
58394:7:0 %280:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%283:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %308:<4608x12288xf32> = torch.2_1_0.aten::detach(%161:<4608x12288xf32>) 
58394:7:0 %283:<4608x12288xf32> = torch.2_1_0.aten::detach(%178:<4608x12288xf32>) 
58387:0:0 %272:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%321:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %278:<4608x12288xf32> = torch.2_1_0.aten::detach(%148:<4608x12288xf32>) 
58387:0:0 %268:<2048x12288xf32> = torch.2_1_0.aten::detach(%176:<2048x12288xf32>) 
58394:7:0 %301:<6144x12288xf32> = torch.2_1_0.aten::detach(%255:<6144x12288xf32>) 
58387:0:0 %314:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%268:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %301:<6144x12288xf32> = torch.2_1_0.aten::detach(%143:<6144x12288xf32>) 
58387:0:0 %312:<2048x12288xf32> = torch.2_1_0.aten::detach(%103:<2048x12288xf32>) 
58387:0:0 %274:<2048x12288xf32> = torch.2_1_0.aten::detach(%176:<2048x12288xf32>) 
58390:3:0 %303:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%308:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %268:<12288x1536xf32> = torch.2_1_0.aten::detach(%95:<12288x1536xf32>) 
58390:3:0 %308:<4608x12288xf32> = torch.2_1_0.aten::detach(%181:<4608x12288xf32>) 
58387:0:0 %322:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58390:3:0 %304:<4608x12288xf32> = torch.2_1_0.aten::detach(%161:<4608x12288xf32>) 
58394:7:0 %302:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%301:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %270:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%322:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %293:<6144x12288xf32> = torch.2_1_0.aten::detach(%243:<6144x12288xf32>) 
58394:7:0 %301:<6144x12288xf32> = torch.2_1_0.aten::detach(%143:<6144x12288xf32>) 
58387:0:0 %321:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58390:3:0 %293:<6144x12288xf32> = torch.2_1_0.aten::detach(%164:<6144x12288xf32>) 
58387:0:0 %285:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%321:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %321:<12288x1536xf32> = torch.2_1_0.aten::detach(%95:<12288x1536xf32>) 
58387:0:0 %322:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58387:0:0 %268:<4608x12288xf32> = torch.2_1_0.aten::detach(%94:<4608x12288xf32>) 
58387:0:0 %315:<4608x12288xf32> = torch.2_1_0.aten::detach(%179:<4608x12288xf32>) 
58394:7:0 %303:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%301:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %309:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%293:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %253:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%315:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %301:<6144x12288xf32> = torch.2_1_0.aten::detach(%255:<6144x12288xf32>) 
58390:3:0 %293:<6144x12288xf32> = torch.2_1_0.aten::detach(%164:<6144x12288xf32>) 
58394:7:0 %304:<6144x12288xf32> = torch.2_1_0.aten::detach(%143:<6144x12288xf32>) 
58387:0:0 %260:<4608x12288xf32> = torch.2_1_0.aten::detach(%179:<4608x12288xf32>) 
58394:7:0 %305:<12288x6144xf32> = torch.2_1_0.aten::detach(%286:<12288x6144xf32>) 
58394:7:0 %305:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58387:0:0 %281:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%260:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %310:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%293:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %311:<4608x12288xf32> = torch.2_1_0.aten::detach(%94:<4608x12288xf32>) 
58394:7:0 %306:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%305:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %260:<4608x12288xf32> = torch.2_1_0.aten::detach(%179:<4608x12288xf32>) 
58390:3:0 %293:<6144x12288xf32> = torch.2_1_0.aten::detach(%243:<6144x12288xf32>) 
58387:0:0 %315:<6144x12288xf32> = torch.2_1_0.aten::detach(%248:<6144x12288xf32>) 
58394:7:0 %305:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58390:3:0 %311:<6144x12288xf32> = torch.2_1_0.aten::detach(%164:<6144x12288xf32>) 
58387:0:0 %250:<6144x12288xf32> = torch.2_1_0.aten::detach(%174:<6144x12288xf32>) 
58390:3:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%189:<12288x6144xf32>) 
58390:3:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%47:<12288x6144xf32>) 
58394:7:0 %307:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%305:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %338:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%212:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %305:<12288x6144xf32> = torch.2_1_0.aten::detach(%286:<12288x6144xf32>) 
58389:2:0 %215:<6400x12288xf32> = torch.2_1_0.aten::detach(%161:<6400x12288xf32>) 
58394:7:0 %308:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58392:5:0 %158:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%191:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %225:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%215:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %147:<6400x12288xf32> = torch.2_1_0.aten::detach(%111:<6400x12288xf32>) 
58389:2:0 %273:<6400x12288xf32> = torch.2_1_0.aten::detach(%62:<6400x12288xf32>) 
58389:2:0 %212:<6400x12288xf32> = torch.2_1_0.aten::detach(%161:<6400x12288xf32>) 
58389:2:0 %15:<2048x12288xf32> = torch.2_1_0.aten::detach(%126:<2048x12288xf32>) 
58389:2:0 %15:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58389:2:0 %31:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%15:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %4:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58392:5:0 %174:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%147:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %26:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%4:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %187:<6400x12288xf32> = torch.2_1_0.aten::detach(%51:<6400x12288xf32>) 
58394:7:0 %206:<12288xf32> = torch.2_1_0.aten::detach(%287:<12288xf32>) 
58389:2:0 %339:<2048x12288xf32> = torch.2_1_0.aten::detach(%126:<2048x12288xf32>) 
58392:5:0 %217:<6400x12288xf32> = torch.2_1_0.aten::detach(%111:<6400x12288xf32>) 
58389:2:0 %4:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58392:5:0 %191:<2048x12288xf32> = torch.2_1_0.aten::detach(%227:<2048x12288xf32>) 
58394:7:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%150:<12288xf32>) 
58389:2:0 %29:<12288x1536xf32> = torch.2_1_0.aten::detach(%122:<12288x1536xf32>) 
58392:5:0 %219:<2048x12288xf32> = torch.2_1_0.aten::detach(%110:<2048x12288xf32>) 
58389:2:0 %152:<12288x1536xf32> = torch.2_1_0.aten::detach(%163:<12288x1536xf32>) 
58394:7:0 %214:<12288xf32> = torch.2_1_0.aten::zeros_like(%198:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %311:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%152:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %179:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%219:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %328:<12288x1536xf32> = torch.2_1_0.aten::detach(%163:<12288x1536xf32>) 
58394:7:0 %206:<12288xf32> = torch.2_1_0.aten::detach(%150:<12288xf32>) 
58392:5:0 %180:<2048x12288xf32> = torch.2_1_0.aten::detach(%110:<2048x12288xf32>) 
58389:2:0 %310:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%328:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %186:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%180:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %217:<12288xf32> = torch.2_1_0.aten::zeros_like(%206:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %307:<12288x1536xf32> = torch.2_1_0.aten::detach(%122:<12288x1536xf32>) 
58392:5:0 %219:<2048x12288xf32> = torch.2_1_0.aten::detach(%227:<2048x12288xf32>) 
58394:7:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%287:<12288xf32>) 
58389:2:0 %328:<12288x1536xf32> = torch.2_1_0.aten::detach(%163:<12288x1536xf32>) 
58392:5:0 %191:<2048x12288xf32> = torch.2_1_0.aten::detach(%110:<2048x12288xf32>) 
58389:2:0 %316:<4608x12288xf32> = torch.2_1_0.aten::detach(%333:<4608x12288xf32>) 
58394:7:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%150:<12288xf32>) 
58392:5:0 %180:<12288x1536xf32> = torch.2_1_0.aten::detach(%228:<12288x1536xf32>) 
58389:2:0 %312:<4608x12288xf32> = torch.2_1_0.aten::detach(%169:<4608x12288xf32>) 
58394:7:0 %1:<12288xf32> = torch.2_1_0.aten::detach(%249:<12288xf32>) 
58392:5:0 %209:<12288x1536xf32> = torch.2_1_0.aten::detach(%115:<12288x1536xf32>) 
58394:7:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%147:<12288xf32>) 
58392:5:0 %182:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%209:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %192:<12288x1536xf32> = torch.2_1_0.aten::detach(%115:<12288x1536xf32>) 
58392:5:0 %200:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%192:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %237:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%249:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %209:<12288x1536xf32> = torch.2_1_0.aten::detach(%228:<12288x1536xf32>) 
58389:2:0 %314:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%312:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %197:<12288x1536xf32> = torch.2_1_0.aten::detach(%115:<12288x1536xf32>) 
58391:4:0 %255:<6400x12288xf32> = torch.2_1_0.aten::detach(%175:<6400x12288xf32>) 
58392:5:0 %183:<4608x12288xf32> = torch.2_1_0.aten::detach(%229:<4608x12288xf32>) 
58389:2:0 %320:<4608x12288xf32> = torch.2_1_0.aten::detach(%169:<4608x12288xf32>) 
58392:5:0 %192:<4608x12288xf32> = torch.2_1_0.aten::detach(%116:<4608x12288xf32>) 
58391:4:0 %244:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%255:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %327:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%320:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %220:<6400x12288xf32> = torch.2_1_0.aten::detach(%61:<6400x12288xf32>) 
58389:2:0 %309:<4608x12288xf32> = torch.2_1_0.aten::detach(%333:<4608x12288xf32>) 
58389:2:0 %320:<4608x12288xf32> = torch.2_1_0.aten::detach(%169:<4608x12288xf32>) 
58391:4:0 %255:<6400x12288xf32> = torch.2_1_0.aten::detach(%175:<6400x12288xf32>) 
58389:2:0 %312:<6144x12288xf32> = torch.2_1_0.aten::detach(%271:<6144x12288xf32>) 
58391:4:0 %239:<2048x12288xf32> = torch.2_1_0.aten::detach(%281:<2048x12288xf32>) 
58389:2:0 %312:<6144x12288xf32> = torch.2_1_0.aten::detach(%172:<6144x12288xf32>) 
58391:4:0 %249:<2048x12288xf32> = torch.2_1_0.aten::detach(%178:<2048x12288xf32>) 
58391:4:0 %228:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%249:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %202:<2048x12288xf32> = torch.2_1_0.aten::detach(%178:<2048x12288xf32>) 
58391:4:0 %261:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%202:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %242:<2048x12288xf32> = torch.2_1_0.aten::detach(%281:<2048x12288xf32>) 
58391:4:0 %202:<2048x12288xf32> = torch.2_1_0.aten::detach(%178:<2048x12288xf32>) 
58389:2:0 %325:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%312:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %257:<12288x1536xf32> = torch.2_1_0.aten::detach(%75:<12288x1536xf32>) 
58389:2:0 %317:<6144x12288xf32> = torch.2_1_0.aten::detach(%172:<6144x12288xf32>) 
58391:4:0 %235:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58391:4:0 %212:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%235:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %239:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58391:4:0 %260:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%239:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %235:<12288x1536xf32> = torch.2_1_0.aten::detach(%75:<12288x1536xf32>) 
58391:4:0 %257:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58394:7:0 %278:<12288xf32> = torch.2_1_0.aten::zeros_like(%309:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %249:<4608x12288xf32> = torch.2_1_0.aten::detach(%234:<4608x12288xf32>) 
58391:4:0 %278:<4608x12288xf32> = torch.2_1_0.aten::detach(%180:<4608x12288xf32>) 
58394:7:0 %206:<12288xf32> = torch.2_1_0.aten::detach(%147:<12288xf32>) 
58394:7:0 %304:<12288xf32> = torch.2_1_0.aten::zeros_like(%206:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%249:<12288xf32>) 
58394:7:0 %297:<12288xf32> = torch.2_1_0.aten::detach(%147:<12288xf32>) 
58394:7:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%288:<12288xf32>) 
58394:7:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%149:<12288xf32>) 
58394:7:0 %301:<12288xf32> = torch.2_1_0.aten::zeros_like(%308:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%149:<12288xf32>) 
58394:7:0 %305:<12288xf32> = torch.2_1_0.aten::zeros_like(%308:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %322:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%317:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%288:<12288xf32>) 
58394:7:0 %310:<12288xf32> = torch.2_1_0.aten::detach(%149:<12288xf32>) 
58389:2:0 %316:<6144x12288xf32> = torch.2_1_0.aten::detach(%271:<6144x12288xf32>) 
58389:2:0 %317:<6144x12288xf32> = torch.2_1_0.aten::detach(%172:<6144x12288xf32>) 
58394:7:0 %282:<4608xf32> = torch.2_1_0.aten::detach(%289:<4608xf32>) 
58389:2:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%334:<12288x6144xf32>) 
58394:7:0 %309:<4608xf32> = torch.2_1_0.aten::detach(%139:<4608xf32>) 
58389:2:0 %340:<12288x6144xf32> = torch.2_1_0.aten::detach(%173:<12288x6144xf32>) 
58394:7:0 %282:<4608xf32> = torch.2_1_0.aten::zeros_like(%309:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %311:<4608xf32> = torch.2_1_0.aten::detach(%139:<4608xf32>) 
58394:7:0 %309:<4608xf32> = torch.2_1_0.aten::zeros_like(%311:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %312:<4608xf32> = torch.2_1_0.aten::detach(%289:<4608xf32>) 
58394:7:0 %313:<4608xf32> = torch.2_1_0.aten::detach(%139:<4608xf32>) 
58394:7:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%270:<12288xf32>) 
58394:7:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%151:<12288xf32>) 
58394:7:0 %316:<12288xf32> = torch.2_1_0.aten::zeros_like(%315:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %317:<12288xf32> = torch.2_1_0.aten::detach(%151:<12288xf32>) 
58394:7:0 %318:<12288xf32> = torch.2_1_0.aten::zeros_like(%317:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%270:<12288xf32>) 
58394:7:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%151:<12288xf32>) 
58394:7:0 %319:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58394:7:0 %317:<12288xf32> = torch.2_1_0.aten::detach(%140:<12288xf32>) 
58394:7:0 %320:<12288xf32> = torch.2_1_0.aten::zeros_like(%317:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%140:<12288xf32>) 
58394:7:0 %322:<12288xf32> = torch.2_1_0.aten::zeros_like(%321:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %319:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58394:7:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%140:<12288xf32>) 
58394:7:0 %321:<6144xf32> = torch.2_1_0.aten::detach(%258:<6144xf32>) 
58394:7:0 %324:<6144xf32> = torch.2_1_0.aten::detach(%153:<6144xf32>) 
58394:7:0 %13:<6144xf32> = torch.2_1_0.aten::zeros_like(%324:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %325:<6144xf32> = torch.2_1_0.aten::detach(%153:<6144xf32>) 
58394:7:0 %24:<6144xf32> = torch.2_1_0.aten::zeros_like(%325:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %326:<6144xf32> = torch.2_1_0.aten::detach(%258:<6144xf32>) 
58394:7:0 %327:<6144xf32> = torch.2_1_0.aten::detach(%153:<6144xf32>) 
58394:7:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58394:7:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%145:<12288xf32>) 
58394:7:0 %329:<12288xf32> = torch.2_1_0.aten::zeros_like(%328:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %330:<12288xf32> = torch.2_1_0.aten::detach(%145:<12288xf32>) 
58394:7:0 %331:<12288xf32> = torch.2_1_0.aten::zeros_like(%330:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %332:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58394:7:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%145:<12288xf32>) 
58394:7:0 %334:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58394:7:0 %335:<12288xf32> = torch.2_1_0.aten::detach(%155:<12288xf32>) 
58394:7:0 %336:<12288xf32> = torch.2_1_0.aten::zeros_like(%335:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%155:<12288xf32>) 
58394:7:0 %338:<12288xf32> = torch.2_1_0.aten::zeros_like(%337:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %335:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58394:7:0 %339:<12288xf32> = torch.2_1_0.aten::detach(%155:<12288xf32>) 
58394:7:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58394:7:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%152:<12288xf32>) 
58394:7:0 %341:<12288xf32> = torch.2_1_0.aten::zeros_like(%337:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%152:<12288xf32>) 
58394:7:0 %342:<12288xf32> = torch.2_1_0.aten::zeros_like(%328:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58394:7:0 %343:<12288xf32> = torch.2_1_0.aten::detach(%152:<12288xf32>) 
58394:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58394:7:0 %297:<6400x12288xf16> = torch.2_1_0.aten::detach(%12:<6400x12288xf16>) 
58394:7:0 %184:<6400x12288xf32> = torch.2_1_0.aten::detach(%141:<6400x12288xf32>) 
58394:7:0 %310:<2048x12288xf16> = torch.2_1_0.aten::detach(%8:<2048x12288xf16>) 
58394:7:0 %315:<2048x12288xf32> = torch.2_1_0.aten::detach(%144:<2048x12288xf32>) 
58394:7:0 %292:<12288x1536xf16> = torch.2_1_0.aten::detach(%50:<12288x1536xf16>) 
58394:7:0 %323:<12288x1536xf32> = torch.2_1_0.aten::detach(%146:<12288x1536xf32>) 
58394:7:0 %313:<4608x12288xf16> = torch.2_1_0.aten::detach(%88:<4608x12288xf16>) 
58394:7:0 %333:<4608x12288xf32> = torch.2_1_0.aten::detach(%148:<4608x12288xf32>) 
58394:7:0 %339:<6144x12288xf16> = torch.2_1_0.aten::detach(%91:<6144x12288xf16>) 
58394:7:0 %343:<6144x12288xf32> = torch.2_1_0.aten::detach(%143:<6144x12288xf32>) 
58394:7:0 %327:<12288x6144xf16> = torch.2_1_0.aten::detach(%94:<12288x6144xf16>) 
58394:7:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58394:7:0 %326:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58394:7:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%150:<12288xf32>) 
58394:7:0 %344:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58394:7:0 %335:<12288xf32> = torch.2_1_0.aten::detach(%147:<12288xf32>) 
58394:7:0 %332:<12288xf16> = torch.2_1_0.aten::detach(%32:<12288xf16>) 
58394:7:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%149:<12288xf32>) 
58394:7:0 %308:<4608xf16> = torch.2_1_0.aten::detach(%47:<4608xf16>) 
58394:7:0 %337:<4608xf32> = torch.2_1_0.aten::detach(%139:<4608xf32>) 
58394:7:0 %345:<12288xf16> = torch.2_1_0.aten::detach(%31:<12288xf16>) 
58394:7:0 %346:<12288xf32> = torch.2_1_0.aten::detach(%151:<12288xf32>) 
58394:7:0 %347:<12288xf16> = torch.2_1_0.aten::detach(%89:<12288xf16>) 
58394:7:0 %348:<12288xf32> = torch.2_1_0.aten::detach(%140:<12288xf32>) 
58394:7:0 %349:<6144xf16> = torch.2_1_0.aten::detach(%93:<6144xf16>) 
58394:7:0 %350:<6144xf32> = torch.2_1_0.aten::detach(%153:<6144xf32>) 
58394:7:0 %351:<12288xf16> = torch.2_1_0.aten::detach(%96:<12288xf16>) 
58394:7:0 %352:<12288xf32> = torch.2_1_0.aten::detach(%145:<12288xf32>) 
58394:7:0 %353:<12288xf16> = torch.2_1_0.aten::detach(%98:<12288xf16>) 
58394:7:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%155:<12288xf32>) 
58394:7:0 %355:<12288xf16> = torch.2_1_0.aten::detach(%56:<12288xf16>) 
58394:7:0 %356:<12288xf32> = torch.2_1_0.aten::detach(%152:<12288xf32>) 
58394:7:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%137:<1xi32>) 
58394:7:0 %297:<6400x12288xf16> = torch.2_1_0.aten::copy_(%297:<6400x12288xf16>, %184:<6400x12288xf32>, False:pred) 
58394:7:0 %310:<2048x12288xf16> = torch.2_1_0.aten::copy_(%310:<2048x12288xf16>, %315:<2048x12288xf32>, False:pred) 
58394:7:0 %292:<12288x1536xf16> = torch.2_1_0.aten::copy_(%292:<12288x1536xf16>, %323:<12288x1536xf32>, False:pred) 
58394:7:0 %313:<4608x12288xf16> = torch.2_1_0.aten::copy_(%313:<4608x12288xf16>, %333:<4608x12288xf32>, False:pred) 
58394:7:0 %339:<6144x12288xf16> = torch.2_1_0.aten::copy_(%339:<6144x12288xf16>, %343:<6144x12288xf32>, False:pred) 
58394:7:0 %327:<12288x6144xf16> = torch.2_1_0.aten::copy_(%327:<12288x6144xf16>, %312:<12288x6144xf32>, False:pred) 
58394:7:0 %326:<12288xf16> = torch.2_1_0.aten::copy_(%326:<12288xf16>, %311:<12288xf32>, False:pred) 
58394:7:0 %344:<12288xf16> = torch.2_1_0.aten::copy_(%344:<12288xf16>, %335:<12288xf32>, False:pred) 
58394:7:0 %332:<12288xf16> = torch.2_1_0.aten::copy_(%332:<12288xf16>, %340:<12288xf32>, False:pred) 
58394:7:0 %308:<4608xf16> = torch.2_1_0.aten::copy_(%308:<4608xf16>, %337:<4608xf32>, False:pred) 
58394:7:0 %345:<12288xf16> = torch.2_1_0.aten::copy_(%345:<12288xf16>, %346:<12288xf32>, False:pred) 
58394:7:0 %347:<12288xf16> = torch.2_1_0.aten::copy_(%347:<12288xf16>, %348:<12288xf32>, False:pred) 
58393:6:0 %340:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%131:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %349:<6144xf16> = torch.2_1_0.aten::copy_(%349:<6144xf16>, %350:<6144xf32>, False:pred) 
58393:6:0 %323:<6400x12288xf32> = torch.2_1_0.aten::detach(%168:<6400x12288xf32>) 
58394:7:0 %351:<12288xf16> = torch.2_1_0.aten::copy_(%351:<12288xf16>, %352:<12288xf32>, False:pred) 
58394:7:0 %353:<12288xf16> = torch.2_1_0.aten::copy_(%353:<12288xf16>, %354:<12288xf32>, False:pred) 
58394:7:0 %355:<12288xf16> = torch.2_1_0.aten::copy_(%355:<12288xf16>, %356:<12288xf32>, False:pred) 
58394:7:0 %296:<f32> = torch.2_1_0.aten::add(%256:<f32>, 0:i32, alpha=1:i32) 
58394:7:0 %357:<f32> = torch.2_1_0.aten::add(%257:<f32>+1, 0:i32, alpha=1:i32) 
58387:0:0 %269:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%250:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58394:7:0 %315:<f32> = torch.2_1_0.aten::div(%296:<f32>, %357:<f32>) 
58387:0:0 %313:<6144x12288xf32> = torch.2_1_0.aten::detach(%174:<6144x12288xf32>) 
58394:7:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%120:<1xf32>) 
58394:7:0 %257:<1xf32> = torch.2_1_0.aten::lift_fresh(%257:<1xf32>) 
58394:7:0 %296:<1xf32> = torch.2_1_0.aten::add(%257:<1xf32>, %315:<f32>, alpha=1:i32) 
58394:7:0 %256:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58394:7:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%256:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58387:0:0 %268:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%313:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %313:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%312:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %280:<6144x12288xf32> = torch.2_1_0.aten::detach(%248:<6144x12288xf32>) 
58387:0:0 %250:<6144x12288xf32> = torch.2_1_0.aten::detach(%174:<6144x12288xf32>) 
58387:0:0 %323:<12288x6144xf32> = torch.2_1_0.aten::detach(%307:<12288x6144xf32>) 
58390:3:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%47:<12288x6144xf32>) 
58387:0:0 %323:<12288x6144xf32> = torch.2_1_0.aten::detach(%185:<12288x6144xf32>) 
58387:0:0 %294:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%323:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %314:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%312:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %323:<12288x6144xf32> = torch.2_1_0.aten::detach(%185:<12288x6144xf32>) 
58390:3:0 %312:<12288x6144xf32> = torch.2_1_0.aten::detach(%189:<12288x6144xf32>) 
58392:5:0 %188:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%192:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %315:<12288x6144xf32> = torch.2_1_0.aten::detach(%47:<12288x6144xf32>) 
58392:5:0 %194:<4608x12288xf32> = torch.2_1_0.aten::detach(%116:<4608x12288xf32>) 
58387:0:0 %306:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%323:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %242:<12288x6144xf32> = torch.2_1_0.aten::detach(%307:<12288x6144xf32>) 
58387:0:0 %323:<12288x6144xf32> = torch.2_1_0.aten::detach(%185:<12288x6144xf32>) 
58392:5:0 %183:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%194:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %192:<4608x12288xf32> = torch.2_1_0.aten::detach(%229:<4608x12288xf32>) 
58392:5:0 %194:<4608x12288xf32> = torch.2_1_0.aten::detach(%116:<4608x12288xf32>) 
58392:5:0 %180:<6144x12288xf32> = torch.2_1_0.aten::detach(%141:<6144x12288xf32>) 
58390:3:0 %287:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58392:5:0 %189:<6144x12288xf32> = torch.2_1_0.aten::detach(%117:<6144x12288xf32>) 
58390:3:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58387:0:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%301:<12288xf32>) 
58390:3:0 %316:<12288xf32> = torch.2_1_0.aten::zeros_like(%262:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %256:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58390:3:0 %287:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58387:0:0 %260:<12288xf32> = torch.2_1_0.aten::zeros_like(%256:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58390:3:0 %317:<12288xf32> = torch.2_1_0.aten::zeros_like(%287:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %250:<12288xf32> = torch.2_1_0.aten::zeros_like(%274:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %262:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58387:0:0 %256:<12288xf32> = torch.2_1_0.aten::detach(%301:<12288xf32>) 
58387:0:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58390:3:0 %304:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58387:0:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58387:0:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58390:3:0 %302:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58387:0:0 %312:<12288xf32> = torch.2_1_0.aten::zeros_like(%323:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %302:<12288xf32> = torch.2_1_0.aten::detach(%169:<12288xf32>) 
58387:0:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58387:0:0 %321:<12288xf32> = torch.2_1_0.aten::zeros_like(%322:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %323:<12288xf32> = torch.2_1_0.aten::detach(%291:<12288xf32>) 
58390:3:0 %315:<12288xf32> = torch.2_1_0.aten::zeros_like(%302:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58387:0:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%320:<12288xf32>) 
58390:3:0 %302:<12288xf32> = torch.2_1_0.aten::detach(%169:<12288xf32>) 
58387:0:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58387:0:0 %280:<12288xf32> = torch.2_1_0.aten::zeros_like(%235:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %299:<12288xf32> = torch.2_1_0.aten::zeros_like(%302:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58390:3:0 %302:<12288xf32> = torch.2_1_0.aten::detach(%260:<12288xf32>) 
58387:0:0 %242:<12288xf32> = torch.2_1_0.aten::zeros_like(%277:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %275:<12288xf32> = torch.2_1_0.aten::detach(%169:<12288xf32>) 
58387:0:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%320:<12288xf32>) 
58390:3:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%294:<12288xf32>) 
58387:0:0 %311:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58387:0:0 %277:<4608xf32> = torch.2_1_0.aten::detach(%276:<4608xf32>) 
58390:3:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%166:<12288xf32>) 
58387:0:0 %277:<4608xf32> = torch.2_1_0.aten::detach(%194:<4608xf32>) 
58387:0:0 %324:<4608xf32> = torch.2_1_0.aten::zeros_like(%277:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %312:<12288xf32> = torch.2_1_0.aten::zeros_like(%308:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %304:<4608xf32> = torch.2_1_0.aten::detach(%194:<4608xf32>) 
58390:3:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%166:<12288xf32>) 
58387:0:0 %315:<4608xf32> = torch.2_1_0.aten::zeros_like(%304:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %297:<4608xf32> = torch.2_1_0.aten::detach(%276:<4608xf32>) 
58387:0:0 %277:<4608xf32> = torch.2_1_0.aten::detach(%194:<4608xf32>) 
58390:3:0 %293:<12288xf32> = torch.2_1_0.aten::zeros_like(%308:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %325:<12288xf32> = torch.2_1_0.aten::detach(%308:<12288xf32>) 
58387:0:0 %304:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58390:3:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%294:<12288xf32>) 
58387:0:0 %326:<12288xf32> = torch.2_1_0.aten::zeros_like(%304:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %318:<12288xf32> = torch.2_1_0.aten::detach(%166:<12288xf32>) 
58387:0:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58390:3:0 %319:<4608xf32> = torch.2_1_0.aten::detach(%244:<4608xf32>) 
58387:0:0 %328:<12288xf32> = torch.2_1_0.aten::zeros_like(%327:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %320:<4608xf32> = torch.2_1_0.aten::detach(%168:<4608xf32>) 
58387:0:0 %304:<12288xf32> = torch.2_1_0.aten::detach(%308:<12288xf32>) 
58387:0:0 %329:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58390:3:0 %319:<4608xf32> = torch.2_1_0.aten::zeros_like(%320:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %330:<12288xf32> = torch.2_1_0.aten::detach(%303:<12288xf32>) 
58390:3:0 %321:<4608xf32> = torch.2_1_0.aten::detach(%168:<4608xf32>) 
58387:0:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%202:<12288xf32>) 
58387:0:0 %331:<12288xf32> = torch.2_1_0.aten::zeros_like(%327:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %298:<4608xf32> = torch.2_1_0.aten::zeros_like(%321:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %325:<12288xf32> = torch.2_1_0.aten::detach(%202:<12288xf32>) 
58390:3:0 %322:<4608xf32> = torch.2_1_0.aten::detach(%244:<4608xf32>) 
58387:0:0 %332:<12288xf32> = torch.2_1_0.aten::zeros_like(%325:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %323:<4608xf32> = torch.2_1_0.aten::detach(%168:<4608xf32>) 
58387:0:0 %330:<12288xf32> = torch.2_1_0.aten::detach(%303:<12288xf32>) 
58387:0:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%202:<12288xf32>) 
58390:3:0 %324:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58387:0:0 %327:<6144xf32> = torch.2_1_0.aten::detach(%287:<6144xf32>) 
58387:0:0 %334:<6144xf32> = torch.2_1_0.aten::detach(%196:<6144xf32>) 
58390:3:0 %324:<12288xf32> = torch.2_1_0.aten::detach(%172:<12288xf32>) 
58387:0:0 %335:<6144xf32> = torch.2_1_0.aten::zeros_like(%334:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %325:<12288xf32> = torch.2_1_0.aten::zeros_like(%324:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %327:<6144xf32> = torch.2_1_0.aten::detach(%196:<6144xf32>) 
58387:0:0 %336:<6144xf32> = torch.2_1_0.aten::zeros_like(%327:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%172:<12288xf32>) 
58387:0:0 %325:<6144xf32> = torch.2_1_0.aten::detach(%287:<6144xf32>) 
58387:0:0 %334:<6144xf32> = torch.2_1_0.aten::detach(%196:<6144xf32>) 
58390:3:0 %326:<12288xf32> = torch.2_1_0.aten::zeros_like(%321:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%310:<12288xf32>) 
58387:0:0 %338:<12288xf32> = torch.2_1_0.aten::detach(%205:<12288xf32>) 
58390:3:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58387:0:0 %339:<12288xf32> = torch.2_1_0.aten::zeros_like(%338:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %324:<12288xf32> = torch.2_1_0.aten::detach(%172:<12288xf32>) 
58387:0:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%205:<12288xf32>) 
58390:3:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58387:0:0 %341:<12288xf32> = torch.2_1_0.aten::zeros_like(%340:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%173:<12288xf32>) 
58387:0:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%310:<12288xf32>) 
58387:0:0 %343:<12288xf32> = torch.2_1_0.aten::detach(%205:<12288xf32>) 
58387:0:0 %344:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58390:3:0 %329:<12288xf32> = torch.2_1_0.aten::zeros_like(%328:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%173:<12288xf32>) 
58387:0:0 %345:<12288xf32> = torch.2_1_0.aten::zeros_like(%340:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 %330:<12288xf32> = torch.2_1_0.aten::zeros_like(%321:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %346:<12288xf32> = torch.2_1_0.aten::zeros_like(%327:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%266:<12288xf32>) 
58387:0:0 %344:<12288xf32> = torch.2_1_0.aten::detach(%257:<12288xf32>) 
58387:0:0 %347:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 %331:<12288xf32> = torch.2_1_0.aten::detach(%173:<12288xf32>) 
58387:0:0 %348:<12288xf32> = torch.2_1_0.aten::detach(%271:<12288xf32>) 
58390:3:0 %321:<6144xf32> = torch.2_1_0.aten::detach(%199:<6144xf32>) 
58387:0:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58390:3:0 %321:<6144xf32> = torch.2_1_0.aten::detach(%171:<6144xf32>) 
58387:0:0 %350:<12288xf32> = torch.2_1_0.aten::zeros_like(%349:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %340:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58387:0:0 %351:<12288xf32> = torch.2_1_0.aten::zeros_like(%340:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %332:<6144xf32> = torch.2_1_0.aten::zeros_like(%321:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%271:<12288xf32>) 
58390:3:0 %333:<6144xf32> = torch.2_1_0.aten::detach(%171:<6144xf32>) 
58387:0:0 %352:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58390:3:0 %334:<6144xf32> = torch.2_1_0.aten::zeros_like(%333:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58390:3:0 %335:<6144xf32> = torch.2_1_0.aten::detach(%199:<6144xf32>) 
58387:0:0 %340:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58387:0:0 %309:<6400x12288xf32> = torch.2_1_0.aten::detach(%172:<6400x12288xf32>) 
58390:3:0 %336:<6144xf32> = torch.2_1_0.aten::detach(%171:<6144xf32>) 
58387:0:0 %274:<2048x12288xf16> = torch.2_1_0.aten::detach(%117:<2048x12288xf16>) 
58390:3:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%273:<12288xf32>) 
58389:2:0 %341:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%340:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %353:<2048x12288xf32> = torch.2_1_0.aten::detach(%176:<2048x12288xf32>) 
58387:0:0 %277:<12288x1536xf16> = torch.2_1_0.aten::detach(%1:<12288x1536xf16>) 
58390:3:0 %338:<12288xf32> = torch.2_1_0.aten::detach(%175:<12288xf32>) 
58387:0:0 %329:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58389:2:0 %342:<12288x6144xf32> = torch.2_1_0.aten::detach(%173:<12288x6144xf32>) 
58387:0:0 %333:<4608x12288xf16> = torch.2_1_0.aten::detach(%77:<4608x12288xf16>) 
58387:0:0 %334:<4608x12288xf32> = torch.2_1_0.aten::detach(%179:<4608x12288xf32>) 
58390:3:0 %339:<12288xf32> = torch.2_1_0.aten::zeros_like(%338:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %343:<6144x12288xf16> = torch.2_1_0.aten::detach(%52:<6144x12288xf16>) 
58390:3:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%175:<12288xf32>) 
58387:0:0 %256:<6144x12288xf32> = torch.2_1_0.aten::detach(%174:<6144x12288xf32>) 
58387:0:0 %322:<12288x6144xf16> = torch.2_1_0.aten::detach(%53:<12288x6144xf16>) 
58387:0:0 %347:<12288x6144xf32> = torch.2_1_0.aten::detach(%185:<12288x6144xf32>) 
58390:3:0 %340:<12288xf32> = torch.2_1_0.aten::zeros_like(%333:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %311:<12288xf16> = torch.2_1_0.aten::detach(%120:<12288xf16>) 
58387:0:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%178:<12288xf32>) 
58390:3:0 %341:<12288xf32> = torch.2_1_0.aten::detach(%273:<12288xf32>) 
58387:0:0 %304:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58387:0:0 %297:<12288xf32> = torch.2_1_0.aten::detach(%181:<12288xf32>) 
58390:3:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%175:<12288xf32>) 
58387:0:0 %325:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58390:3:0 %343:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58387:0:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58387:0:0 %352:<4608xf16> = torch.2_1_0.aten::detach(%96:<4608xf16>) 
58389:2:0 %321:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%342:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%176:<12288xf32>) 
58387:0:0 %344:<4608xf32> = torch.2_1_0.aten::detach(%194:<4608xf32>) 
58387:0:0 %327:<12288xf16> = torch.2_1_0.aten::detach(%87:<12288xf16>) 
58389:2:0 %343:<12288x6144xf32> = torch.2_1_0.aten::detach(%334:<12288x6144xf32>) 
58387:0:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58390:3:0 %344:<12288xf32> = torch.2_1_0.aten::zeros_like(%333:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %342:<12288x6144xf32> = torch.2_1_0.aten::detach(%173:<12288x6144xf32>) 
58387:0:0 %355:<12288xf16> = torch.2_1_0.aten::detach(%101:<12288xf16>) 
58387:0:0 %356:<12288xf32> = torch.2_1_0.aten::detach(%202:<12288xf32>) 
58390:3:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%176:<12288xf32>) 
58387:0:0 %330:<6144xf16> = torch.2_1_0.aten::detach(%82:<6144xf16>) 
58387:0:0 %357:<6144xf32> = torch.2_1_0.aten::detach(%196:<6144xf32>) 
58387:0:0 %358:<12288xf16> = torch.2_1_0.aten::detach(%46:<12288xf16>) 
58390:3:0 %346:<12288xf32> = torch.2_1_0.aten::zeros_like(%345:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %359:<12288xf32> = torch.2_1_0.aten::detach(%205:<12288xf32>) 
58390:3:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%210:<12288xf32>) 
58387:0:0 %360:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58387:0:0 %361:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 %347:<12288xf32> = torch.2_1_0.aten::detach(%176:<12288xf32>) 
58387:0:0 %362:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58387:0:0 %363:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58390:3:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%81:<12288xf32>) 
58387:0:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%168:<1xi32>) 
58387:0:0 %340:<6400x12288xf16> = torch.2_1_0.aten::copy_(%340:<6400x12288xf16>, %309:<6400x12288xf32>, False:pred) 
58387:0:0 %274:<2048x12288xf16> = torch.2_1_0.aten::copy_(%274:<2048x12288xf16>, %353:<2048x12288xf32>, False:pred) 
58387:0:0 %277:<12288x1536xf16> = torch.2_1_0.aten::copy_(%277:<12288x1536xf16>, %329:<12288x1536xf32>, False:pred) 
58388:1:0 %247:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%220:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %273:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58390:3:0 %348:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58387:0:0 %333:<4608x12288xf16> = torch.2_1_0.aten::copy_(%333:<4608x12288xf16>, %334:<4608x12288xf32>, False:pred) 
58389:2:0 %344:<12288xf32> = torch.2_1_0.aten::detach(%164:<12288xf32>) 
58387:0:0 %343:<6144x12288xf16> = torch.2_1_0.aten::copy_(%343:<6144x12288xf16>, %256:<6144x12288xf32>, False:pred) 
58388:1:0 %265:<6400x12288xf32> = torch.2_1_0.aten::detach(%326:<6400x12288xf32>) 
58387:0:0 %322:<12288x6144xf16> = torch.2_1_0.aten::copy_(%322:<12288x6144xf16>, %347:<12288x6144xf32>, False:pred) 
58388:1:0 %220:<6400x12288xf32> = torch.2_1_0.aten::detach(%180:<6400x12288xf32>) 
58390:3:0 %349:<12288xf32> = torch.2_1_0.aten::zeros_like(%348:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %212:<12288xf32> = torch.2_1_0.aten::zeros_like(%344:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %311:<12288xf16> = torch.2_1_0.aten::copy_(%311:<12288xf16>, %235:<12288xf32>, False:pred) 
58388:1:0 %256:<2048x12288xf32> = torch.2_1_0.aten::detach(%130:<2048x12288xf32>) 
58389:2:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%164:<12288xf32>) 
58390:3:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58387:0:0 %304:<12288xf16> = torch.2_1_0.aten::copy_(%304:<12288xf16>, %297:<12288xf32>, False:pred) 
58388:1:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%183:<2048x12288xf32>) 
58387:0:0 %325:<12288xf16> = torch.2_1_0.aten::copy_(%325:<12288xf16>, %342:<12288xf32>, False:pred) 
58389:2:0 %320:<12288xf32> = torch.2_1_0.aten::zeros_like(%328:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %352:<4608xf16> = torch.2_1_0.aten::copy_(%352:<4608xf16>, %344:<4608xf32>, False:pred) 
58390:3:0 %350:<12288xf32> = torch.2_1_0.aten::zeros_like(%345:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %328:<12288xf32> = torch.2_1_0.aten::detach(%293:<12288xf32>) 
58388:1:0 %229:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%257:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %327:<12288xf16> = torch.2_1_0.aten::copy_(%327:<12288xf16>, %354:<12288xf32>, False:pred) 
58389:2:0 %317:<12288xf32> = torch.2_1_0.aten::detach(%164:<12288xf32>) 
58390:3:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%81:<12288xf32>) 
58388:1:0 %288:<2048x12288xf32> = torch.2_1_0.aten::detach(%183:<2048x12288xf32>) 
58387:0:0 %355:<12288xf16> = torch.2_1_0.aten::copy_(%355:<12288xf16>, %356:<12288xf32>, False:pred) 
58389:2:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%282:<12288xf32>) 
58390:3:0 %351:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58389:2:0 %313:<12288xf32> = torch.2_1_0.aten::detach(%182:<12288xf32>) 
58387:0:0 %330:<6144xf16> = torch.2_1_0.aten::copy_(%330:<6144xf16>, %357:<6144xf32>, False:pred) 
58388:1:0 %333:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%288:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %358:<12288xf16> = torch.2_1_0.aten::copy_(%358:<12288xf16>, %359:<12288xf32>, False:pred) 
58389:2:0 %309:<12288xf32> = torch.2_1_0.aten::zeros_like(%313:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 %360:<12288xf16> = torch.2_1_0.aten::copy_(%360:<12288xf16>, %361:<12288xf32>, False:pred) 
58388:1:0 %230:<2048x12288xf32> = torch.2_1_0.aten::detach(%130:<2048x12288xf32>) 
58388:1:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%183:<2048x12288xf32>) 
58387:0:0 %362:<12288xf16> = torch.2_1_0.aten::copy_(%362:<12288xf16>, %363:<12288xf32>, False:pred) 
58389:2:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%182:<12288xf32>) 
58388:1:0 %256:<12288x1536xf32> = torch.2_1_0.aten::detach(%328:<12288x1536xf32>) 
58390:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58389:2:0 %316:<12288xf32> = torch.2_1_0.aten::zeros_like(%342:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %288:<12288x1536xf32> = torch.2_1_0.aten::detach(%105:<12288x1536xf32>) 
58393:6:0 %337:<6400x12288xf32> = torch.2_1_0.aten::zeros_like(%323:<6400x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %313:<12288xf32> = torch.2_1_0.aten::detach(%282:<12288xf32>) 
58389:2:0 %307:<12288xf32> = torch.2_1_0.aten::detach(%182:<12288xf32>) 
58390:3:0 %262:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58388:1:0 %334:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%288:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %343:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58393:6:0 %323:<6400x12288xf32> = torch.2_1_0.aten::detach(%92:<6400x12288xf32>) 
58390:3:0 %318:<6400x12288xf32> = torch.2_1_0.aten::detach(%162:<6400x12288xf32>) 
58388:1:0 %283:<12288x1536xf32> = torch.2_1_0.aten::detach(%105:<12288x1536xf32>) 
58389:2:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58393:6:0 %292:<6400x12288xf32> = torch.2_1_0.aten::detach(%168:<6400x12288xf32>) 
58390:3:0 %323:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58387:0:0 %256:<f32> = torch.2_1_0.aten::add(%293:<f32>, 0:i32, alpha=1:i32) 
58393:6:0 %339:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58388:1:0 %335:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%283:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %340:<12288xf32> = torch.2_1_0.aten::zeros_like(%342:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %324:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58393:6:0 %339:<2048x12288xf32> = torch.2_1_0.aten::detach(%170:<2048x12288xf32>) 
58388:1:0 %255:<12288x1536xf32> = torch.2_1_0.aten::detach(%328:<12288x1536xf32>) 
58387:0:0 %347:<f32> = torch.2_1_0.aten::add(%296:<f32>+1, 0:i32, alpha=1:i32) 
58389:2:0 %343:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58390:3:0 %331:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58388:1:0 %283:<12288x1536xf32> = torch.2_1_0.aten::detach(%105:<12288x1536xf32>) 
58393:6:0 %299:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%339:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %312:<12288xf32> = torch.2_1_0.aten::zeros_like(%343:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %336:<12288x1536xf32> = torch.2_1_0.aten::detach(%159:<12288x1536xf32>) 
58388:1:0 %288:<4608x12288xf32> = torch.2_1_0.aten::detach(%225:<4608x12288xf32>) 
58393:6:0 %339:<2048x12288xf32> = torch.2_1_0.aten::detach(%170:<2048x12288xf32>) 
58389:2:0 %342:<12288xf32> = torch.2_1_0.aten::detach(%286:<12288xf32>) 
58388:1:0 %258:<4608x12288xf32> = torch.2_1_0.aten::detach(%188:<4608x12288xf32>) 
58390:3:0 %342:<4608x12288xf16> = torch.2_1_0.aten::detach(%121:<4608x12288xf16>) 
58389:2:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58390:3:0 %347:<4608x12288xf32> = torch.2_1_0.aten::detach(%161:<4608x12288xf32>) 
58393:6:0 %305:<2048x12288xf32> = torch.2_1_0.aten::zeros_like(%339:<2048x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %343:<4608xf32> = torch.2_1_0.aten::detach(%178:<4608xf32>) 
58387:0:0 %329:<f32> = torch.2_1_0.aten::div(%256:<f32>, %347:<f32>) 
58390:3:0 %351:<6144x12288xf16> = torch.2_1_0.aten::detach(%95:<6144x12288xf16>) 
58393:6:0 %257:<2048x12288xf32> = torch.2_1_0.aten::detach(%284:<2048x12288xf32>) 
58389:2:0 %343:<4608xf32> = torch.2_1_0.aten::detach(%190:<4608xf32>) 
58393:6:0 %339:<2048x12288xf32> = torch.2_1_0.aten::detach(%170:<2048x12288xf32>) 
58389:2:0 %297:<4608xf32> = torch.2_1_0.aten::zeros_like(%343:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%133:<1xf32>) 
58393:6:0 %338:<12288x1536xf32> = torch.2_1_0.aten::detach(%330:<12288x1536xf32>) 
58389:2:0 %346:<4608xf32> = torch.2_1_0.aten::detach(%190:<4608xf32>) 
58393:6:0 %338:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58387:0:0 %293:<1xf32> = torch.2_1_0.aten::lift_fresh(%293:<1xf32>) 
58389:2:0 %347:<4608xf32> = torch.2_1_0.aten::zeros_like(%346:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %200:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%258:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %307:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%338:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %343:<4608xf32> = torch.2_1_0.aten::detach(%178:<4608xf32>) 
58387:0:0 %347:<1xf32> = torch.2_1_0.aten::add(%293:<1xf32>, %329:<f32>, alpha=1:i32) 
58393:6:0 %338:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58388:1:0 %276:<4608x12288xf32> = torch.2_1_0.aten::detach(%188:<4608x12288xf32>) 
58389:2:0 %348:<4608xf32> = torch.2_1_0.aten::detach(%190:<4608xf32>) 
58393:6:0 %310:<12288x1536xf32> = torch.2_1_0.aten::zeros_like(%338:<12288x1536xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58389:2:0 %350:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58393:6:0 %335:<12288x1536xf32> = torch.2_1_0.aten::detach(%330:<12288x1536xf32>) 
58387:0:0 %21:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %338:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58389:2:0 %351:<12288xf32> = torch.2_1_0.aten::zeros_like(%350:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %311:<4608x12288xf32> = torch.2_1_0.aten::detach(%331:<4608x12288xf32>) 
58389:2:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58393:6:0 %297:<4608x12288xf32> = torch.2_1_0.aten::detach(%175:<4608x12288xf32>) 
58388:1:0 %256:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%276:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %352:<12288xf32> = torch.2_1_0.aten::zeros_like(%349:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58387:0:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%21:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58388:1:0 %276:<4608x12288xf32> = torch.2_1_0.aten::detach(%225:<4608x12288xf32>) 
58389:2:0 %350:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58388:1:0 %286:<4608x12288xf32> = torch.2_1_0.aten::detach(%188:<4608x12288xf32>) 
58389:2:0 %353:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58388:1:0 %288:<6144x12288xf32> = torch.2_1_0.aten::detach(%228:<6144x12288xf32>) 
58389:2:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%245:<12288xf32>) 
58388:1:0 %262:<6144x12288xf32> = torch.2_1_0.aten::detach(%190:<6144x12288xf32>) 
58389:2:0 %355:<12288xf32> = torch.2_1_0.aten::detach(%198:<12288xf32>) 
58393:6:0 %303:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%297:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %318:<4608x12288xf32> = torch.2_1_0.aten::detach(%175:<4608x12288xf32>) 
58389:2:0 %356:<12288xf32> = torch.2_1_0.aten::zeros_like(%355:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%198:<12288xf32>) 
58389:2:0 %357:<12288xf32> = torch.2_1_0.aten::zeros_like(%349:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %258:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%262:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %358:<12288xf32> = torch.2_1_0.aten::detach(%245:<12288xf32>) 
58388:1:0 %235:<6144x12288xf32> = torch.2_1_0.aten::detach(%190:<6144x12288xf32>) 
58393:6:0 %288:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%318:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %359:<12288xf32> = torch.2_1_0.aten::detach(%198:<12288xf32>) 
58389:2:0 %360:<6144xf32> = torch.2_1_0.aten::detach(%238:<6144xf32>) 
58393:6:0 %318:<4608x12288xf32> = torch.2_1_0.aten::detach(%331:<4608x12288xf32>) 
58389:2:0 %361:<6144xf32> = torch.2_1_0.aten::detach(%202:<6144xf32>) 
58393:6:0 %311:<4608x12288xf32> = torch.2_1_0.aten::detach(%175:<4608x12288xf32>) 
58393:6:0 %322:<6144x12288xf32> = torch.2_1_0.aten::detach(%285:<6144x12288xf32>) 
58389:2:0 %104:<6144xf32> = torch.2_1_0.aten::zeros_like(%361:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %314:<6144x12288xf32> = torch.2_1_0.aten::detach(%177:<6144x12288xf32>) 
58388:1:0 %278:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%235:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %361:<6144xf32> = torch.2_1_0.aten::detach(%202:<6144xf32>) 
58388:1:0 %235:<6144x12288xf32> = torch.2_1_0.aten::detach(%228:<6144x12288xf32>) 
58389:2:0 %362:<6144xf32> = torch.2_1_0.aten::zeros_like(%361:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %288:<6144x12288xf32> = torch.2_1_0.aten::detach(%190:<6144x12288xf32>) 
58389:2:0 %363:<6144xf32> = torch.2_1_0.aten::detach(%238:<6144xf32>) 
58388:1:0 %290:<12288x6144xf32> = torch.2_1_0.aten::detach(%169:<12288x6144xf32>) 
58389:2:0 %364:<6144xf32> = torch.2_1_0.aten::detach(%202:<6144xf32>) 
58390:3:0 %304:<6144x12288xf32> = torch.2_1_0.aten::detach(%164:<6144x12288xf32>) 
58388:1:0 %290:<12288x6144xf32> = torch.2_1_0.aten::detach(%192:<12288x6144xf32>) 
58393:6:0 %297:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%314:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %365:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58389:2:0 %366:<12288xf32> = torch.2_1_0.aten::detach(%206:<12288xf32>) 
58393:6:0 %320:<6144x12288xf32> = torch.2_1_0.aten::detach(%177:<6144x12288xf32>) 
58390:3:0 %302:<12288x6144xf16> = torch.2_1_0.aten::detach(%105:<12288x6144xf16>) 
58389:2:0 %367:<12288xf32> = torch.2_1_0.aten::zeros_like(%366:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %322:<12288x6144xf32> = torch.2_1_0.aten::detach(%47:<12288x6144xf32>) 
58389:2:0 %368:<12288xf32> = torch.2_1_0.aten::detach(%206:<12288xf32>) 
58390:3:0 %328:<12288xf16> = torch.2_1_0.aten::detach(%59:<12288xf16>) 
58388:1:0 %246:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%290:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58389:2:0 %369:<12288xf32> = torch.2_1_0.aten::zeros_like(%368:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %335:<12288xf16> = torch.2_1_0.aten::detach(%119:<12288xf16>) 
58388:1:0 %290:<12288x6144xf32> = torch.2_1_0.aten::detach(%192:<12288x6144xf32>) 
58389:2:0 %365:<12288xf32> = torch.2_1_0.aten::detach(%290:<12288xf32>) 
58393:6:0 %336:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%320:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %327:<12288xf32> = torch.2_1_0.aten::detach(%169:<12288xf32>) 
58389:2:0 %370:<12288xf32> = torch.2_1_0.aten::detach(%206:<12288xf32>) 
58393:6:0 %295:<6144x12288xf32> = torch.2_1_0.aten::detach(%285:<6144x12288xf32>) 
58389:2:0 %371:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58390:3:0 %229:<12288xf16> = torch.2_1_0.aten::detach(%61:<12288xf16>) 
58393:6:0 %314:<6144x12288xf32> = torch.2_1_0.aten::detach(%177:<6144x12288xf32>) 
58389:2:0 %368:<12288xf32> = torch.2_1_0.aten::detach(%204:<12288xf32>) 
58390:3:0 %333:<12288xf32> = torch.2_1_0.aten::detach(%166:<12288xf32>) 
58393:6:0 %322:<12288x6144xf32> = torch.2_1_0.aten::detach(%209:<12288x6144xf32>) 
58390:3:0 %352:<4608xf16> = torch.2_1_0.aten::detach(%74:<4608xf16>) 
58389:2:0 %372:<12288xf32> = torch.2_1_0.aten::zeros_like(%368:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %320:<12288x6144xf32> = torch.2_1_0.aten::detach(%180:<12288x6144xf32>) 
58388:1:0 %332:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%290:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %341:<4608xf32> = torch.2_1_0.aten::detach(%168:<4608xf32>) 
58389:2:0 %366:<12288xf32> = torch.2_1_0.aten::detach(%204:<12288xf32>) 
58388:1:0 %290:<12288x6144xf32> = torch.2_1_0.aten::detach(%169:<12288x6144xf32>) 
58390:3:0 %353:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58388:1:0 %277:<12288x6144xf32> = torch.2_1_0.aten::detach(%192:<12288x6144xf32>) 
58390:3:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%172:<12288xf32>) 
58389:2:0 %373:<12288xf32> = torch.2_1_0.aten::zeros_like(%366:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %371:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58390:3:0 %225:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58393:6:0 %341:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%320:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %374:<12288xf32> = torch.2_1_0.aten::detach(%204:<12288xf32>) 
58390:3:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%173:<12288xf32>) 
58389:2:0 %375:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58393:6:0 %342:<12288x6144xf32> = torch.2_1_0.aten::detach(%180:<12288x6144xf32>) 
58390:3:0 %355:<6144xf16> = torch.2_1_0.aten::detach(%64:<6144xf16>) 
58390:3:0 %356:<6144xf32> = torch.2_1_0.aten::detach(%171:<6144xf32>) 
58390:3:0 %357:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58389:2:0 %366:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 %358:<12288xf32> = torch.2_1_0.aten::detach(%175:<12288xf32>) 
58393:6:0 %343:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%342:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %359:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58389:2:0 %376:<12288xf32> = torch.2_1_0.aten::zeros_like(%366:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %344:<12288x6144xf32> = torch.2_1_0.aten::detach(%209:<12288x6144xf32>) 
58390:3:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%176:<12288xf32>) 
58389:2:0 %368:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58393:6:0 %342:<12288x6144xf32> = torch.2_1_0.aten::detach(%180:<12288x6144xf32>) 
58390:3:0 %361:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58389:2:0 %377:<12288xf32> = torch.2_1_0.aten::zeros_like(%368:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %362:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58388:1:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58389:2:0 %366:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58388:1:0 %267:<12288xf32> = torch.2_1_0.aten::detach(%189:<12288xf32>) 
58389:2:0 %378:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%158:<1xi32>) 
58388:1:0 %220:<12288xf32> = torch.2_1_0.aten::zeros_like(%267:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %267:<12288xf32> = torch.2_1_0.aten::detach(%189:<12288xf32>) 
58389:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58388:1:0 %257:<12288xf32> = torch.2_1_0.aten::zeros_like(%267:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %262:<6400x12288xf16> = torch.2_1_0.aten::copy_(%262:<6400x12288xf16>, %318:<6400x12288xf32>, False:pred) 
58389:2:0 %317:<6400x12288xf16> = torch.2_1_0.aten::detach(%98:<6400x12288xf16>) 
58389:2:0 %345:<6400x12288xf32> = torch.2_1_0.aten::detach(%161:<6400x12288xf32>) 
58388:1:0 %280:<12288xf32> = torch.2_1_0.aten::detach(%251:<12288xf32>) 
58390:3:0 %323:<2048x12288xf16> = torch.2_1_0.aten::copy_(%323:<2048x12288xf16>, %324:<2048x12288xf32>, False:pred) 
58389:2:0 %353:<2048x12288xf16> = torch.2_1_0.aten::detach(%34:<2048x12288xf16>) 
58388:1:0 %267:<12288xf32> = torch.2_1_0.aten::detach(%189:<12288xf32>) 
58389:2:0 %364:<2048x12288xf32> = torch.2_1_0.aten::detach(%165:<2048x12288xf32>) 
58393:6:0 %233:<12288xf32> = torch.2_1_0.aten::detach(%332:<12288xf32>) 
58390:3:0 %331:<12288x1536xf16> = torch.2_1_0.aten::copy_(%331:<12288x1536xf16>, %336:<12288x1536xf32>, False:pred) 
58388:1:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%241:<12288xf32>) 
58389:2:0 %348:<12288x1536xf16> = torch.2_1_0.aten::detach(%13:<12288x1536xf16>) 
58393:6:0 %325:<12288xf32> = torch.2_1_0.aten::detach(%179:<12288xf32>) 
58388:1:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58389:2:0 %378:<12288x1536xf32> = torch.2_1_0.aten::detach(%163:<12288x1536xf32>) 
58390:3:0 %342:<4608x12288xf16> = torch.2_1_0.aten::copy_(%342:<4608x12288xf16>, %347:<4608x12288xf32>, False:pred) 
58389:2:0 %328:<4608x12288xf16> = torch.2_1_0.aten::detach(%12:<4608x12288xf16>) 
58388:1:0 %286:<12288xf32> = torch.2_1_0.aten::zeros_like(%263:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %338:<12288xf32> = torch.2_1_0.aten::zeros_like(%325:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %374:<4608x12288xf32> = torch.2_1_0.aten::detach(%169:<4608x12288xf32>) 
58390:3:0 %351:<6144x12288xf16> = torch.2_1_0.aten::copy_(%351:<6144x12288xf16>, %304:<6144x12288xf32>, False:pred) 
58389:2:0 %359:<6144x12288xf16> = torch.2_1_0.aten::detach(%84:<6144x12288xf16>) 
58393:6:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%179:<12288xf32>) 
58388:1:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58390:3:0 %302:<12288x6144xf16> = torch.2_1_0.aten::copy_(%302:<12288x6144xf16>, %322:<12288x6144xf32>, False:pred) 
58389:2:0 %358:<6144x12288xf32> = torch.2_1_0.aten::detach(%172:<6144x12288xf32>) 
58389:2:0 %313:<12288x6144xf16> = torch.2_1_0.aten::detach(%102:<12288x6144xf16>) 
58393:6:0 %311:<12288xf32> = torch.2_1_0.aten::zeros_like(%292:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %288:<12288xf32> = torch.2_1_0.aten::zeros_like(%263:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %328:<12288xf16> = torch.2_1_0.aten::copy_(%328:<12288xf16>, %308:<12288xf32>, False:pred) 
58389:2:0 %370:<12288x6144xf32> = torch.2_1_0.aten::detach(%173:<12288x6144xf32>) 
58393:6:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%332:<12288xf32>) 
58388:1:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%241:<12288xf32>) 
58389:2:0 %350:<12288xf16> = torch.2_1_0.aten::detach(%100:<12288xf16>) 
58390:3:0 %335:<12288xf16> = torch.2_1_0.aten::copy_(%335:<12288xf16>, %327:<12288xf32>, False:pred) 
58393:6:0 %339:<12288xf32> = torch.2_1_0.aten::detach(%179:<12288xf32>) 
58388:1:0 %283:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58389:2:0 %371:<12288xf32> = torch.2_1_0.aten::detach(%164:<12288xf32>) 
58393:6:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%238:<12288xf32>) 
58390:3:0 %229:<12288xf16> = torch.2_1_0.aten::copy_(%229:<12288xf16>, %333:<12288xf32>, False:pred) 
58388:1:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58389:2:0 %365:<12288xf16> = torch.2_1_0.aten::detach(%9:<12288xf16>) 
58393:6:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58389:2:0 %368:<12288xf32> = torch.2_1_0.aten::detach(%182:<12288xf32>) 
58388:1:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58390:3:0 %352:<4608xf16> = torch.2_1_0.aten::copy_(%352:<4608xf16>, %341:<4608xf32>, False:pred) 
58389:2:0 %343:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58393:6:0 %342:<12288xf32> = torch.2_1_0.aten::zeros_like(%315:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %353:<12288xf16> = torch.2_1_0.aten::copy_(%353:<12288xf16>, %345:<12288xf32>, False:pred) 
58389:2:0 %379:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58388:1:0 %230:<12288xf32> = torch.2_1_0.aten::zeros_like(%277:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58390:3:0 %225:<12288xf16> = torch.2_1_0.aten::copy_(%225:<12288xf16>, %354:<12288xf32>, False:pred) 
58389:2:0 %375:<4608xf16> = torch.2_1_0.aten::detach(%18:<4608xf16>) 
58388:1:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58389:2:0 %380:<4608xf32> = torch.2_1_0.aten::detach(%190:<4608xf32>) 
58390:3:0 %355:<6144xf16> = torch.2_1_0.aten::copy_(%355:<6144xf16>, %356:<6144xf32>, False:pred) 
58393:6:0 %323:<12288xf32> = torch.2_1_0.aten::zeros_like(%315:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %366:<12288xf16> = torch.2_1_0.aten::detach(%70:<12288xf16>) 
58388:1:0 %255:<12288xf32> = torch.2_1_0.aten::zeros_like(%277:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %357:<12288xf16> = torch.2_1_0.aten::copy_(%357:<12288xf16>, %358:<12288xf32>, False:pred) 
58393:6:0 %315:<12288xf32> = torch.2_1_0.aten::detach(%238:<12288xf32>) 
58389:2:0 %381:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58388:1:0 %277:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58393:6:0 %290:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58390:3:0 %359:<12288xf16> = torch.2_1_0.aten::copy_(%359:<12288xf16>, %360:<12288xf32>, False:pred) 
58389:2:0 %382:<12288xf16> = torch.2_1_0.aten::detach(%60:<12288xf16>) 
58388:1:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58393:6:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58389:2:0 %383:<12288xf32> = torch.2_1_0.aten::detach(%198:<12288xf32>) 
58390:3:0 %361:<12288xf16> = torch.2_1_0.aten::copy_(%361:<12288xf16>, %362:<12288xf32>, False:pred) 
58388:1:0 %330:<4608xf32> = torch.2_1_0.aten::detach(%271:<4608xf32>) 
58393:6:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%165:<12288xf32>) 
58389:2:0 %384:<6144xf16> = torch.2_1_0.aten::detach(%103:<6144xf16>) 
58389:2:0 %385:<6144xf32> = torch.2_1_0.aten::detach(%202:<6144xf32>) 
58388:1:0 %55:<4608xf32> = torch.2_1_0.aten::detach(%191:<4608xf32>) 
58393:6:0 %318:<12288xf32> = torch.2_1_0.aten::zeros_like(%314:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %386:<12288xf16> = torch.2_1_0.aten::detach(%99:<12288xf16>) 
58389:2:0 %387:<12288xf32> = torch.2_1_0.aten::detach(%206:<12288xf32>) 
58393:6:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%165:<12288xf32>) 
58388:1:0 %330:<4608xf32> = torch.2_1_0.aten::zeros_like(%55:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %388:<12288xf16> = torch.2_1_0.aten::detach(%42:<12288xf16>) 
58388:1:0 %290:<4608xf32> = torch.2_1_0.aten::detach(%191:<4608xf32>) 
58393:6:0 %295:<12288xf32> = torch.2_1_0.aten::zeros_like(%314:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %389:<12288xf32> = torch.2_1_0.aten::detach(%204:<12288xf32>) 
58389:2:0 %390:<12288xf16> = torch.2_1_0.aten::detach(%95:<12288xf16>) 
58393:6:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58388:1:0 %108:<4608xf32> = torch.2_1_0.aten::zeros_like(%290:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %391:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58388:1:0 %269:<4608xf32> = torch.2_1_0.aten::detach(%271:<4608xf32>) 
58388:1:0 %262:<4608xf32> = torch.2_1_0.aten::detach(%191:<4608xf32>) 
58388:1:0 %203:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58389:2:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%157:<1xi32>) 
58390:3:0 %318:<f32> = torch.2_1_0.aten::add(%274:<f32>, 0:i32, alpha=1:i32) 
58388:1:0 %336:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58389:2:0 %317:<6400x12288xf16> = torch.2_1_0.aten::copy_(%317:<6400x12288xf16>, %345:<6400x12288xf32>, False:pred) 
58388:1:0 %307:<12288xf32> = torch.2_1_0.aten::zeros_like(%336:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 %336:<f32> = torch.2_1_0.aten::add(%280:<f32>+1, 0:i32, alpha=1:i32) 
58389:2:0 %353:<2048x12288xf16> = torch.2_1_0.aten::copy_(%353:<2048x12288xf16>, %364:<2048x12288xf32>, False:pred) 
58388:1:0 %336:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58389:2:0 %348:<12288x1536xf16> = torch.2_1_0.aten::copy_(%348:<12288x1536xf16>, %378:<12288x1536xf32>, False:pred) 
58388:1:0 %299:<12288xf32> = torch.2_1_0.aten::zeros_like(%336:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %328:<4608x12288xf16> = torch.2_1_0.aten::copy_(%328:<4608x12288xf16>, %374:<4608x12288xf32>, False:pred) 
58388:1:0 %302:<12288xf32> = torch.2_1_0.aten::detach(%281:<12288xf32>) 
58389:2:0 %359:<6144x12288xf16> = torch.2_1_0.aten::copy_(%359:<6144x12288xf16>, %358:<6144x12288xf32>, False:pred) 
58388:1:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58389:2:0 %313:<12288x6144xf16> = torch.2_1_0.aten::copy_(%313:<12288x6144xf16>, %370:<12288x6144xf32>, False:pred) 
58388:1:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%253:<12288xf32>) 
58389:2:0 %350:<12288xf16> = torch.2_1_0.aten::copy_(%350:<12288xf16>, %371:<12288xf32>, False:pred) 
58388:1:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58390:3:0 %324:<f32> = torch.2_1_0.aten::div(%318:<f32>, %336:<f32>) 
58389:2:0 %365:<12288xf16> = torch.2_1_0.aten::copy_(%365:<12288xf16>, %368:<12288xf32>, False:pred) 
58388:1:0 %318:<12288xf32> = torch.2_1_0.aten::zeros_like(%312:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58389:2:0 %343:<12288xf16> = torch.2_1_0.aten::copy_(%343:<12288xf16>, %379:<12288xf32>, False:pred) 
58388:1:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58389:2:0 %375:<4608xf16> = torch.2_1_0.aten::copy_(%375:<4608xf16>, %380:<4608xf32>, False:pred) 
58388:1:0 %315:<12288xf32> = torch.2_1_0.aten::zeros_like(%312:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58390:3:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%153:<1xf32>) 
58389:2:0 %366:<12288xf16> = torch.2_1_0.aten::copy_(%366:<12288xf16>, %381:<12288xf32>, False:pred) 
58388:1:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%253:<12288xf32>) 
58389:2:0 %382:<12288xf16> = torch.2_1_0.aten::copy_(%382:<12288xf16>, %383:<12288xf32>, False:pred) 
58389:2:0 %384:<6144xf16> = torch.2_1_0.aten::copy_(%384:<6144xf16>, %385:<6144xf32>, False:pred) 
58388:1:0 %337:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58390:3:0 %228:<1xf32> = torch.2_1_0.aten::lift_fresh(%228:<1xf32>) 
58389:2:0 %386:<12288xf16> = torch.2_1_0.aten::copy_(%386:<12288xf16>, %387:<12288xf32>, False:pred) 
58388:1:0 %338:<6144xf32> = torch.2_1_0.aten::detach(%227:<6144xf32>) 
58390:3:0 %262:<1xf32> = torch.2_1_0.aten::add(%228:<1xf32>, %324:<f32>, alpha=1:i32) 
58389:2:0 %388:<12288xf16> = torch.2_1_0.aten::copy_(%388:<12288xf16>, %389:<12288xf32>, False:pred) 
58388:1:0 %338:<6144xf32> = torch.2_1_0.aten::detach(%195:<6144xf32>) 
58389:2:0 %390:<12288xf16> = torch.2_1_0.aten::copy_(%390:<12288xf16>, %391:<12288xf32>, False:pred) 
58388:1:0 %339:<6144xf32> = torch.2_1_0.aten::zeros_like(%338:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %203:<6144xf32> = torch.2_1_0.aten::detach(%195:<6144xf32>) 
58390:3:0 %280:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58388:1:0 %338:<6144xf32> = torch.2_1_0.aten::zeros_like(%203:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %316:<6144xf32> = torch.2_1_0.aten::detach(%227:<6144xf32>) 
58389:2:0 %378:<f32> = torch.2_1_0.aten::add(%308:<f32>, 0:i32, alpha=1:i32) 
58388:1:0 %319:<6144xf32> = torch.2_1_0.aten::detach(%195:<6144xf32>) 
58388:1:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58390:3:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%280:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58389:2:0 %358:<f32> = torch.2_1_0.aten::add(%306:<f32>+1, 0:i32, alpha=1:i32) 
58393:6:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%165:<12288xf32>) 
58393:6:0 %325:<4608xf32> = torch.2_1_0.aten::detach(%250:<4608xf32>) 
58393:6:0 %146:<4608xf32> = torch.2_1_0.aten::detach(%188:<4608xf32>) 
58389:2:0 %370:<f32> = torch.2_1_0.aten::div(%378:<f32>, %358:<f32>) 
58393:6:0 %325:<4608xf32> = torch.2_1_0.aten::zeros_like(%146:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %335:<4608xf32> = torch.2_1_0.aten::detach(%188:<4608xf32>) 
58389:2:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%71:<1xf32>) 
58393:6:0 %233:<4608xf32> = torch.2_1_0.aten::zeros_like(%335:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %320:<4608xf32> = torch.2_1_0.aten::detach(%250:<4608xf32>) 
58389:2:0 %305:<1xf32> = torch.2_1_0.aten::lift_fresh(%305:<1xf32>) 
58393:6:0 %346:<4608xf32> = torch.2_1_0.aten::detach(%188:<4608xf32>) 
58393:6:0 %347:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58389:2:0 %308:<1xf32> = torch.2_1_0.aten::add(%305:<1xf32>, %370:<f32>, alpha=1:i32) 
58393:6:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58393:6:0 %348:<12288xf32> = torch.2_1_0.aten::zeros_like(%322:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58389:2:0 %308:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %350:<12288xf32> = torch.2_1_0.aten::zeros_like(%349:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58393:6:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58393:6:0 %351:<12288xf32> = torch.2_1_0.aten::detach(%224:<12288xf32>) 
58389:2:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%308:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %352:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58393:6:0 %353:<12288xf32> = torch.2_1_0.aten::zeros_like(%352:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58393:6:0 %355:<12288xf32> = torch.2_1_0.aten::zeros_like(%354:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %356:<12288xf32> = torch.2_1_0.aten::detach(%224:<12288xf32>) 
58393:6:0 %357:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58393:6:0 %354:<6144xf32> = torch.2_1_0.aten::detach(%274:<6144xf32>) 
58393:6:0 %347:<6144xf32> = torch.2_1_0.aten::detach(%191:<6144xf32>) 
58393:6:0 %358:<6144xf32> = torch.2_1_0.aten::zeros_like(%347:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %359:<6144xf32> = torch.2_1_0.aten::detach(%191:<6144xf32>) 
58393:6:0 %182:<6144xf32> = torch.2_1_0.aten::zeros_like(%359:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%197:<12288xf32>) 
58393:6:0 %359:<6144xf32> = torch.2_1_0.aten::detach(%274:<6144xf32>) 
58393:6:0 %10:<6144xf32> = torch.2_1_0.aten::detach(%191:<6144xf32>) 
58393:6:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58388:1:0 %317:<12288xf32> = torch.2_1_0.aten::zeros_like(%321:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58393:6:0 %361:<12288xf32> = torch.2_1_0.aten::zeros_like(%354:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%197:<12288xf32>) 
58393:6:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58388:1:0 %340:<12288xf32> = torch.2_1_0.aten::zeros_like(%322:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %362:<12288xf32> = torch.2_1_0.aten::zeros_like(%360:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %363:<12288xf32> = torch.2_1_0.aten::detach(%329:<12288xf32>) 
58388:1:0 %321:<12288xf32> = torch.2_1_0.aten::detach(%207:<12288xf32>) 
58393:6:0 %364:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58388:1:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%197:<12288xf32>) 
58393:6:0 %365:<12288xf32> = torch.2_1_0.aten::detach(%219:<12288xf32>) 
58393:6:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%195:<12288xf32>) 
58388:1:0 %341:<12288xf32> = torch.2_1_0.aten::detach(%233:<12288xf32>) 
58393:6:0 %366:<12288xf32> = torch.2_1_0.aten::zeros_like(%360:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%199:<12288xf32>) 
58393:6:0 %367:<12288xf32> = torch.2_1_0.aten::detach(%195:<12288xf32>) 
58393:6:0 %368:<12288xf32> = torch.2_1_0.aten::zeros_like(%367:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %342:<12288xf32> = torch.2_1_0.aten::zeros_like(%322:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %369:<12288xf32> = torch.2_1_0.aten::detach(%219:<12288xf32>) 
58393:6:0 %370:<12288xf32> = torch.2_1_0.aten::detach(%195:<12288xf32>) 
58388:1:0 %341:<12288xf32> = torch.2_1_0.aten::detach(%199:<12288xf32>) 
58393:6:0 %371:<12288xf32> = torch.2_1_0.aten::detach(%283:<12288xf32>) 
58388:1:0 %343:<12288xf32> = torch.2_1_0.aten::zeros_like(%341:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%233:<12288xf32>) 
58388:1:0 %344:<12288xf32> = torch.2_1_0.aten::detach(%199:<12288xf32>) 
58393:6:0 %367:<12288xf32> = torch.2_1_0.aten::detach(%203:<12288xf32>) 
58388:1:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58393:6:0 %372:<12288xf32> = torch.2_1_0.aten::zeros_like(%367:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %373:<12288xf32> = torch.2_1_0.aten::detach(%203:<12288xf32>) 
58393:6:0 %374:<12288xf32> = torch.2_1_0.aten::zeros_like(%373:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%283:<12288xf32>) 
58388:1:0 %341:<12288xf32> = torch.2_1_0.aten::detach(%201:<12288xf32>) 
58393:6:0 %375:<12288xf32> = torch.2_1_0.aten::detach(%203:<12288xf32>) 
58388:1:0 %346:<12288xf32> = torch.2_1_0.aten::zeros_like(%341:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58388:1:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%201:<12288xf32>) 
58393:6:0 %166:<6400x12288xf16> = torch.2_1_0.aten::detach(%116:<6400x12288xf16>) 
58393:6:0 %290:<6400x12288xf32> = torch.2_1_0.aten::detach(%168:<6400x12288xf32>) 
58388:1:0 %347:<12288xf32> = torch.2_1_0.aten::zeros_like(%345:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %322:<2048x12288xf16> = torch.2_1_0.aten::detach(%118:<2048x12288xf16>) 
58393:6:0 %376:<2048x12288xf32> = torch.2_1_0.aten::detach(%170:<2048x12288xf32>) 
58388:1:0 %341:<12288xf32> = torch.2_1_0.aten::detach(%259:<12288xf32>) 
58393:6:0 %346:<12288x1536xf16> = torch.2_1_0.aten::detach(%120:<12288x1536xf16>) 
58388:1:0 %348:<12288xf32> = torch.2_1_0.aten::detach(%201:<12288xf32>) 
58393:6:0 %357:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58393:6:0 %364:<4608x12288xf16> = torch.2_1_0.aten::detach(%80:<4608x12288xf16>) 
58393:6:0 %375:<4608x12288xf32> = torch.2_1_0.aten::detach(%175:<4608x12288xf32>) 
58393:6:0 %345:<6144x12288xf16> = torch.2_1_0.aten::detach(%63:<6144x12288xf16>) 
58393:6:0 %351:<6144x12288xf32> = torch.2_1_0.aten::detach(%177:<6144x12288xf32>) 
58388:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58393:6:0 %370:<12288x6144xf16> = torch.2_1_0.aten::detach(%47:<12288x6144xf16>) 
58393:6:0 %292:<12288x6144xf32> = torch.2_1_0.aten::detach(%180:<12288x6144xf32>) 
58393:6:0 %314:<12288xf16> = torch.2_1_0.aten::detach(%2:<12288xf16>) 
58388:1:0 %277:<6400x12288xf16> = torch.2_1_0.aten::detach(%113:<6400x12288xf16>) 
58393:6:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%179:<12288xf32>) 
58388:1:0 %283:<6400x12288xf32> = torch.2_1_0.aten::detach(%180:<6400x12288xf32>) 
58393:6:0 %315:<12288xf16> = torch.2_1_0.aten::detach(%45:<12288xf16>) 
58388:1:0 %269:<2048x12288xf16> = torch.2_1_0.aten::detach(%114:<2048x12288xf16>) 
58393:6:0 %349:<12288xf32> = torch.2_1_0.aten::detach(%167:<12288xf32>) 
58393:6:0 %356:<12288xf16> = torch.2_1_0.aten::detach(%121:<12288xf16>) 
58388:1:0 %309:<2048x12288xf32> = torch.2_1_0.aten::detach(%183:<2048x12288xf32>) 
58393:6:0 %359:<12288xf32> = torch.2_1_0.aten::detach(%165:<12288xf32>) 
58388:1:0 %337:<12288x1536xf16> = torch.2_1_0.aten::detach(%23:<12288x1536xf16>) 
58393:6:0 %320:<4608xf16> = torch.2_1_0.aten::detach(%73:<4608xf16>) 
58393:6:0 %363:<4608xf32> = torch.2_1_0.aten::detach(%188:<4608xf32>) 
58388:1:0 %314:<12288x1536xf32> = torch.2_1_0.aten::detach(%105:<12288x1536xf32>) 
58393:6:0 %354:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58388:1:0 %344:<4608x12288xf16> = torch.2_1_0.aten::detach(%119:<4608x12288xf16>) 
58393:6:0 %373:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58388:1:0 %348:<4608x12288xf32> = torch.2_1_0.aten::detach(%188:<4608x12288xf32>) 
58393:6:0 %377:<12288xf16> = torch.2_1_0.aten::detach(%124:<12288xf16>) 
58393:6:0 %378:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58388:1:0 %302:<6144x12288xf16> = torch.2_1_0.aten::detach(%120:<6144x12288xf16>) 
58393:6:0 %369:<6144xf16> = torch.2_1_0.aten::detach(%98:<6144xf16>) 
58388:1:0 %312:<6144x12288xf32> = torch.2_1_0.aten::detach(%190:<6144x12288xf32>) 
58393:6:0 %379:<6144xf32> = torch.2_1_0.aten::detach(%191:<6144xf32>) 
58393:6:0 %380:<12288xf16> = torch.2_1_0.aten::detach(%77:<12288xf16>) 
58388:1:0 %316:<12288x6144xf16> = torch.2_1_0.aten::detach(%65:<12288x6144xf16>) 
58393:6:0 %381:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58388:1:0 %321:<12288x6144xf32> = torch.2_1_0.aten::detach(%192:<12288x6144xf32>) 
58393:6:0 %382:<12288xf16> = torch.2_1_0.aten::detach(%126:<12288xf16>) 
58388:1:0 %322:<12288xf16> = torch.2_1_0.aten::detach(%117:<12288xf16>) 
58393:6:0 %383:<12288xf32> = torch.2_1_0.aten::detach(%195:<12288xf32>) 
58393:6:0 %384:<12288xf16> = torch.2_1_0.aten::detach(%94:<12288xf16>) 
58388:1:0 %319:<12288xf32> = torch.2_1_0.aten::detach(%189:<12288xf32>) 
58393:6:0 %385:<12288xf32> = torch.2_1_0.aten::detach(%203:<12288xf32>) 
58388:1:0 %341:<12288xf16> = torch.2_1_0.aten::detach(%118:<12288xf16>) 
58393:6:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%164:<1xi32>) 
58388:1:0 %345:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58388:1:0 %349:<12288xf16> = torch.2_1_0.aten::detach(%115:<12288xf16>) 
58393:6:0 %166:<6400x12288xf16> = torch.2_1_0.aten::copy_(%166:<6400x12288xf16>, %290:<6400x12288xf32>, False:pred) 
58388:1:0 %350:<12288xf32> = torch.2_1_0.aten::detach(%193:<12288xf32>) 
58393:6:0 %322:<2048x12288xf16> = torch.2_1_0.aten::copy_(%322:<2048x12288xf16>, %376:<2048x12288xf32>, False:pred) 
58388:1:0 %351:<4608xf16> = torch.2_1_0.aten::detach(%12:<4608xf16>) 
58393:6:0 %346:<12288x1536xf16> = torch.2_1_0.aten::copy_(%346:<12288x1536xf16>, %357:<12288x1536xf32>, False:pred) 
58393:6:0 %364:<4608x12288xf16> = torch.2_1_0.aten::copy_(%364:<4608x12288xf16>, %375:<4608x12288xf32>, False:pred) 
58388:1:0 %352:<4608xf32> = torch.2_1_0.aten::detach(%191:<4608xf32>) 
58393:6:0 %345:<6144x12288xf16> = torch.2_1_0.aten::copy_(%345:<6144x12288xf16>, %351:<6144x12288xf32>, False:pred) 
58388:1:0 %353:<12288xf16> = torch.2_1_0.aten::detach(%63:<12288xf16>) 
58393:6:0 %370:<12288x6144xf16> = torch.2_1_0.aten::copy_(%370:<12288x6144xf16>, %292:<12288x6144xf32>, False:pred) 
58388:1:0 %354:<12288xf32> = torch.2_1_0.aten::detach(%194:<12288xf32>) 
58393:6:0 %314:<12288xf16> = torch.2_1_0.aten::copy_(%314:<12288xf16>, %360:<12288xf32>, False:pred) 
58388:1:0 %355:<12288xf16> = torch.2_1_0.aten::detach(%104:<12288xf16>) 
58393:6:0 %315:<12288xf16> = torch.2_1_0.aten::copy_(%315:<12288xf16>, %349:<12288xf32>, False:pred) 
58388:1:0 %356:<12288xf32> = torch.2_1_0.aten::detach(%196:<12288xf32>) 
58393:6:0 %356:<12288xf16> = torch.2_1_0.aten::copy_(%356:<12288xf16>, %359:<12288xf32>, False:pred) 
58388:1:0 %357:<6144xf16> = torch.2_1_0.aten::detach(%107:<6144xf16>) 
58388:1:0 %358:<6144xf32> = torch.2_1_0.aten::detach(%195:<6144xf32>) 
58393:6:0 %320:<4608xf16> = torch.2_1_0.aten::copy_(%320:<4608xf16>, %363:<4608xf32>, False:pred) 
58388:1:0 %359:<12288xf16> = torch.2_1_0.aten::detach(%48:<12288xf16>) 
58393:6:0 %354:<12288xf16> = torch.2_1_0.aten::copy_(%354:<12288xf16>, %373:<12288xf32>, False:pred) 
58388:1:0 %360:<12288xf32> = torch.2_1_0.aten::detach(%197:<12288xf32>) 
58388:1:0 %361:<12288xf16> = torch.2_1_0.aten::detach(%122:<12288xf16>) 
58393:6:0 %377:<12288xf16> = torch.2_1_0.aten::copy_(%377:<12288xf16>, %378:<12288xf32>, False:pred) 
58388:1:0 %362:<12288xf32> = torch.2_1_0.aten::detach(%199:<12288xf32>) 
58393:6:0 %369:<6144xf16> = torch.2_1_0.aten::copy_(%369:<6144xf16>, %379:<6144xf32>, False:pred) 
58388:1:0 %363:<12288xf16> = torch.2_1_0.aten::detach(%123:<12288xf16>) 
58388:1:0 %364:<12288xf32> = torch.2_1_0.aten::detach(%201:<12288xf32>) 
58393:6:0 %380:<12288xf16> = torch.2_1_0.aten::copy_(%380:<12288xf16>, %381:<12288xf32>, False:pred) 
58392:5:0 %185:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%189:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58393:6:0 %382:<12288xf16> = torch.2_1_0.aten::copy_(%382:<12288xf16>, %383:<12288xf32>, False:pred) 
58388:1:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%179:<1xi32>) 
58393:6:0 %384:<12288xf16> = torch.2_1_0.aten::copy_(%384:<12288xf16>, %385:<12288xf32>, False:pred) 
58392:5:0 %198:<6144x12288xf32> = torch.2_1_0.aten::detach(%117:<6144x12288xf32>) 
58388:1:0 %277:<6400x12288xf16> = torch.2_1_0.aten::copy_(%277:<6400x12288xf16>, %283:<6400x12288xf32>, False:pred) 
58388:1:0 %269:<2048x12288xf16> = torch.2_1_0.aten::copy_(%269:<2048x12288xf16>, %309:<2048x12288xf32>, False:pred) 
58388:1:0 %337:<12288x1536xf16> = torch.2_1_0.aten::copy_(%337:<12288x1536xf16>, %314:<12288x1536xf32>, False:pred) 
58388:1:0 %344:<4608x12288xf16> = torch.2_1_0.aten::copy_(%344:<4608x12288xf16>, %348:<4608x12288xf32>, False:pred) 
58393:6:0 %376:<f32> = torch.2_1_0.aten::add(%306:<f32>, 0:i32, alpha=1:i32) 
58388:1:0 %302:<6144x12288xf16> = torch.2_1_0.aten::copy_(%302:<6144x12288xf16>, %312:<6144x12288xf32>, False:pred) 
58388:1:0 %316:<12288x6144xf16> = torch.2_1_0.aten::copy_(%316:<12288x6144xf16>, %321:<12288x6144xf32>, False:pred) 
58393:6:0 %375:<f32> = torch.2_1_0.aten::add(%308:<f32>+1, 0:i32, alpha=1:i32) 
58388:1:0 %322:<12288xf16> = torch.2_1_0.aten::copy_(%322:<12288xf16>, %319:<12288xf32>, False:pred) 
58388:1:0 %341:<12288xf16> = torch.2_1_0.aten::copy_(%341:<12288xf16>, %345:<12288xf32>, False:pred) 
58388:1:0 %349:<12288xf16> = torch.2_1_0.aten::copy_(%349:<12288xf16>, %350:<12288xf32>, False:pred) 
58388:1:0 %351:<4608xf16> = torch.2_1_0.aten::copy_(%351:<4608xf16>, %352:<4608xf32>, False:pred) 
58393:6:0 %352:<f32> = torch.2_1_0.aten::div(%376:<f32>, %375:<f32>) 
58388:1:0 %353:<12288xf16> = torch.2_1_0.aten::copy_(%353:<12288xf16>, %354:<12288xf32>, False:pred) 
58388:1:0 %355:<12288xf16> = torch.2_1_0.aten::copy_(%355:<12288xf16>, %356:<12288xf32>, False:pred) 
58388:1:0 %357:<6144xf16> = torch.2_1_0.aten::copy_(%357:<6144xf16>, %358:<6144xf32>, False:pred) 
58393:6:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%159:<1xf32>) 
58392:5:0 %180:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%198:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58388:1:0 %359:<12288xf16> = torch.2_1_0.aten::copy_(%359:<12288xf16>, %360:<12288xf32>, False:pred) 
58388:1:0 %361:<12288xf16> = torch.2_1_0.aten::copy_(%361:<12288xf16>, %362:<12288xf32>, False:pred) 
58392:5:0 %233:<6144x12288xf32> = torch.2_1_0.aten::detach(%141:<6144x12288xf32>) 
58388:1:0 %363:<12288xf16> = torch.2_1_0.aten::copy_(%363:<12288xf16>, %364:<12288xf32>, False:pred) 
58388:1:0 %314:<f32> = torch.2_1_0.aten::add(%305:<f32>, 0:i32, alpha=1:i32) 
58388:1:0 %348:<f32> = torch.2_1_0.aten::add(%308:<f32>+1, 0:i32, alpha=1:i32) 
58391:4:0 %239:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%278:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %219:<4608x12288xf32> = torch.2_1_0.aten::detach(%180:<4608x12288xf32>) 
58388:1:0 %312:<f32> = torch.2_1_0.aten::div(%314:<f32>, %348:<f32>) 
58388:1:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%29:<1xf32>) 
58388:1:0 %365:<1xf32> = torch.2_1_0.aten::lift_fresh(%365:<1xf32>) 
58388:1:0 %308:<1xf32> = torch.2_1_0.aten::add(%365:<1xf32>, %312:<f32>, alpha=1:i32) 
58388:1:0 %365:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58393:6:0 %306:<1xf32> = torch.2_1_0.aten::lift_fresh(%306:<1xf32>) 
58388:1:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%365:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58393:6:0 %357:<1xf32> = torch.2_1_0.aten::add(%306:<1xf32>, %352:<f32>, alpha=1:i32) 
58393:6:0 %166:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %221:<6144x12288xf32> = torch.2_1_0.aten::detach(%117:<6144x12288xf32>) 
58393:6:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%166:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %205:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58392:5:0 %198:<12288x6144xf32> = torch.2_1_0.aten::detach(%118:<12288x6144xf32>) 
58391:4:0 %247:<4608x12288xf32> = torch.2_1_0.aten::zeros_like(%219:<4608x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %234:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%198:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %233:<4608x12288xf32> = torch.2_1_0.aten::detach(%234:<4608x12288xf32>) 
58391:4:0 %219:<4608x12288xf32> = torch.2_1_0.aten::detach(%180:<4608x12288xf32>) 
58392:5:0 %235:<12288x6144xf32> = torch.2_1_0.aten::detach(%118:<12288x6144xf32>) 
58391:4:0 %278:<6144x12288xf32> = torch.2_1_0.aten::detach(%272:<6144x12288xf32>) 
58391:4:0 %269:<6144x12288xf32> = torch.2_1_0.aten::detach(%182:<6144x12288xf32>) 
58392:5:0 %236:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%235:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %235:<12288x6144xf32> = torch.2_1_0.aten::detach(%142:<12288x6144xf32>) 
58391:4:0 %240:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%269:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %205:<12288x6144xf32> = torch.2_1_0.aten::detach(%118:<12288x6144xf32>) 
58391:4:0 %249:<6144x12288xf32> = torch.2_1_0.aten::detach(%182:<6144x12288xf32>) 
58391:4:0 %245:<6144x12288xf32> = torch.2_1_0.aten::zeros_like(%249:<6144x12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %252:<6144x12288xf32> = torch.2_1_0.aten::detach(%272:<6144x12288xf32>) 
58391:4:0 %249:<6144x12288xf32> = torch.2_1_0.aten::detach(%182:<6144x12288xf32>) 
58391:4:0 %269:<12288x6144xf32> = torch.2_1_0.aten::detach(%282:<12288x6144xf32>) 
58391:4:0 %269:<12288x6144xf32> = torch.2_1_0.aten::detach(%179:<12288x6144xf32>) 
58392:5:0 %191:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58392:5:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%112:<12288xf32>) 
58392:5:0 %194:<12288xf32> = torch.2_1_0.aten::zeros_like(%198:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %191:<12288xf32> = torch.2_1_0.aten::detach(%112:<12288xf32>) 
58391:4:0 %273:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%269:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %221:<12288xf32> = torch.2_1_0.aten::zeros_like(%191:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %269:<12288x6144xf32> = torch.2_1_0.aten::detach(%179:<12288x6144xf32>) 
58392:5:0 %191:<12288xf32> = torch.2_1_0.aten::detach(%230:<12288xf32>) 
58392:5:0 %197:<12288xf32> = torch.2_1_0.aten::detach(%112:<12288xf32>) 
58392:5:0 %205:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58392:5:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%109:<12288xf32>) 
58392:5:0 %219:<12288xf32> = torch.2_1_0.aten::zeros_like(%198:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %194:<12288x6144xf32> = torch.2_1_0.aten::zeros_like(%269:<12288x6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %217:<12288xf32> = torch.2_1_0.aten::detach(%109:<12288xf32>) 
58391:4:0 %285:<12288x6144xf32> = torch.2_1_0.aten::detach(%282:<12288x6144xf32>) 
58392:5:0 %209:<12288xf32> = torch.2_1_0.aten::zeros_like(%217:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %269:<12288x6144xf32> = torch.2_1_0.aten::detach(%179:<12288x6144xf32>) 
58392:5:0 %205:<12288xf32> = torch.2_1_0.aten::detach(%184:<12288xf32>) 
58392:5:0 %187:<12288xf32> = torch.2_1_0.aten::detach(%109:<12288xf32>) 
58392:5:0 %192:<12288xf32> = torch.2_1_0.aten::detach(%231:<12288xf32>) 
58392:5:0 %217:<12288xf32> = torch.2_1_0.aten::detach(%119:<12288xf32>) 
58392:5:0 %235:<12288xf32> = torch.2_1_0.aten::zeros_like(%217:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %198:<12288xf32> = torch.2_1_0.aten::detach(%119:<12288xf32>) 
58392:5:0 %189:<12288xf32> = torch.2_1_0.aten::zeros_like(%198:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %192:<12288xf32> = torch.2_1_0.aten::detach(%231:<12288xf32>) 
58392:5:0 %237:<12288xf32> = torch.2_1_0.aten::detach(%119:<12288xf32>) 
58392:5:0 %198:<4608xf32> = torch.2_1_0.aten::detach(%204:<4608xf32>) 
58392:5:0 %198:<4608xf32> = torch.2_1_0.aten::detach(%108:<4608xf32>) 
58392:5:0 %238:<4608xf32> = torch.2_1_0.aten::zeros_like(%198:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58392:5:0 %239:<4608xf32> = torch.2_1_0.aten::detach(%108:<4608xf32>) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58392:5:0 %240:<4608xf32> = torch.2_1_0.aten::zeros_like(%239:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %257:<12288xf32> = torch.2_1_0.aten::zeros_like(%202:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %241:<4608xf32> = torch.2_1_0.aten::detach(%204:<4608xf32>) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58392:5:0 %198:<4608xf32> = torch.2_1_0.aten::detach(%108:<4608xf32>) 
58392:5:0 %242:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58392:5:0 %243:<12288xf32> = torch.2_1_0.aten::detach(%121:<12288xf32>) 
58391:4:0 %249:<12288xf32> = torch.2_1_0.aten::zeros_like(%202:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %255:<12288xf32> = torch.2_1_0.aten::detach(%243:<12288xf32>) 
58392:5:0 %244:<12288xf32> = torch.2_1_0.aten::zeros_like(%243:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %219:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58392:5:0 %239:<12288xf32> = torch.2_1_0.aten::detach(%121:<12288xf32>) 
58391:4:0 %269:<12288xf32> = torch.2_1_0.aten::detach(%262:<12288xf32>) 
58392:5:0 %245:<12288xf32> = torch.2_1_0.aten::zeros_like(%239:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%185:<12288xf32>) 
58392:5:0 %242:<12288xf32> = torch.2_1_0.aten::detach(%212:<12288xf32>) 
58392:5:0 %246:<12288xf32> = torch.2_1_0.aten::detach(%121:<12288xf32>) 
58391:4:0 %242:<12288xf32> = torch.2_1_0.aten::zeros_like(%202:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %247:<12288xf32> = torch.2_1_0.aten::detach(%232:<12288xf32>) 
58392:5:0 %239:<12288xf32> = torch.2_1_0.aten::detach(%122:<12288xf32>) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%185:<12288xf32>) 
58392:5:0 %248:<12288xf32> = torch.2_1_0.aten::zeros_like(%239:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %220:<12288xf32> = torch.2_1_0.aten::zeros_like(%202:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %243:<12288xf32> = torch.2_1_0.aten::detach(%122:<12288xf32>) 
58391:4:0 %202:<12288xf32> = torch.2_1_0.aten::detach(%262:<12288xf32>) 
58392:5:0 %249:<12288xf32> = torch.2_1_0.aten::zeros_like(%243:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %269:<12288xf32> = torch.2_1_0.aten::detach(%185:<12288xf32>) 
58392:5:0 %247:<12288xf32> = torch.2_1_0.aten::detach(%232:<12288xf32>) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%229:<12288xf32>) 
58392:5:0 %239:<12288xf32> = torch.2_1_0.aten::detach(%122:<12288xf32>) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58392:5:0 %250:<6144xf32> = torch.2_1_0.aten::detach(%166:<6144xf32>) 
58392:5:0 %243:<6144xf32> = torch.2_1_0.aten::detach(%114:<6144xf32>) 
58391:4:0 %252:<12288xf32> = torch.2_1_0.aten::zeros_like(%235:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %251:<6144xf32> = torch.2_1_0.aten::zeros_like(%243:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58392:5:0 %252:<6144xf32> = torch.2_1_0.aten::detach(%114:<6144xf32>) 
58391:4:0 %264:<12288xf32> = torch.2_1_0.aten::zeros_like(%235:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %253:<6144xf32> = torch.2_1_0.aten::zeros_like(%252:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %254:<6144xf32> = torch.2_1_0.aten::detach(%166:<6144xf32>) 
58391:4:0 %235:<12288xf32> = torch.2_1_0.aten::detach(%229:<12288xf32>) 
58392:5:0 %243:<6144xf32> = torch.2_1_0.aten::detach(%114:<6144xf32>) 
58391:4:0 %278:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58392:5:0 %255:<12288xf32> = torch.2_1_0.aten::detach(%214:<12288xf32>) 
58391:4:0 %233:<4608xf32> = torch.2_1_0.aten::detach(%254:<4608xf32>) 
58392:5:0 %252:<12288xf32> = torch.2_1_0.aten::detach(%124:<12288xf32>) 
58391:4:0 %233:<4608xf32> = torch.2_1_0.aten::detach(%183:<4608xf32>) 
58392:5:0 %256:<12288xf32> = torch.2_1_0.aten::zeros_like(%252:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %286:<4608xf32> = torch.2_1_0.aten::zeros_like(%233:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %153:<12288xf32> = torch.2_1_0.aten::detach(%124:<12288xf32>) 
58391:4:0 %271:<4608xf32> = torch.2_1_0.aten::detach(%183:<4608xf32>) 
58392:5:0 %257:<12288xf32> = torch.2_1_0.aten::zeros_like(%153:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %258:<12288xf32> = torch.2_1_0.aten::detach(%214:<12288xf32>) 
58392:5:0 %259:<12288xf32> = torch.2_1_0.aten::detach(%124:<12288xf32>) 
58391:4:0 %287:<4608xf32> = torch.2_1_0.aten::zeros_like(%271:<4608xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%218:<12288xf32>) 
58391:4:0 %271:<4608xf32> = torch.2_1_0.aten::detach(%254:<4608xf32>) 
58392:5:0 %255:<12288xf32> = torch.2_1_0.aten::detach(%125:<12288xf32>) 
58391:4:0 %288:<4608xf32> = torch.2_1_0.aten::detach(%183:<4608xf32>) 
58392:5:0 %261:<12288xf32> = torch.2_1_0.aten::zeros_like(%255:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %289:<12288xf32> = torch.2_1_0.aten::detach(%209:<12288xf32>) 
58392:5:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%125:<12288xf32>) 
58391:4:0 %233:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58392:5:0 %262:<12288xf32> = torch.2_1_0.aten::zeros_like(%260:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %290:<12288xf32> = torch.2_1_0.aten::zeros_like(%233:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %252:<12288xf32> = torch.2_1_0.aten::detach(%218:<12288xf32>) 
58391:4:0 %289:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58392:5:0 %263:<12288xf32> = torch.2_1_0.aten::detach(%125:<12288xf32>) 
58392:5:0 %264:<12288xf32> = torch.2_1_0.aten::detach(%213:<12288xf32>) 
58391:4:0 %291:<12288xf32> = torch.2_1_0.aten::zeros_like(%289:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %233:<12288xf32> = torch.2_1_0.aten::detach(%209:<12288xf32>) 
58391:4:0 %292:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58391:4:0 %293:<12288xf32> = torch.2_1_0.aten::detach(%267:<12288xf32>) 
58392:5:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%126:<12288xf32>) 
58391:4:0 %289:<12288xf32> = torch.2_1_0.aten::detach(%188:<12288xf32>) 
58392:5:0 %265:<12288xf32> = torch.2_1_0.aten::zeros_like(%260:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %294:<12288xf32> = torch.2_1_0.aten::zeros_like(%289:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %255:<12288xf32> = torch.2_1_0.aten::detach(%126:<12288xf32>) 
58391:4:0 %293:<12288xf32> = torch.2_1_0.aten::detach(%188:<12288xf32>) 
58392:5:0 %266:<12288xf32> = torch.2_1_0.aten::zeros_like(%255:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %264:<12288xf32> = torch.2_1_0.aten::detach(%213:<12288xf32>) 
58391:4:0 %295:<12288xf32> = torch.2_1_0.aten::zeros_like(%293:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:5:0 %267:<12288xf32> = torch.2_1_0.aten::detach(%126:<12288xf32>) 
58391:4:0 %289:<12288xf32> = torch.2_1_0.aten::detach(%267:<12288xf32>) 
58391:4:0 %296:<12288xf32> = torch.2_1_0.aten::detach(%188:<12288xf32>) 
58391:4:0 %297:<6144xf32> = torch.2_1_0.aten::detach(%181:<6144xf32>) 
58391:4:0 %293:<6144xf32> = torch.2_1_0.aten::detach(%184:<6144xf32>) 
58391:4:0 %83:<6144xf32> = torch.2_1_0.aten::zeros_like(%293:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58392:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58391:4:0 %293:<6144xf32> = torch.2_1_0.aten::detach(%184:<6144xf32>) 
58391:4:0 %298:<6144xf32> = torch.2_1_0.aten::zeros_like(%293:<6144xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %293:<6144xf32> = torch.2_1_0.aten::detach(%181:<6144xf32>) 
58391:4:0 %299:<6144xf32> = torch.2_1_0.aten::detach(%184:<6144xf32>) 
58391:4:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%268:<12288xf32>) 
58391:4:0 %301:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58391:4:0 %302:<12288xf32> = torch.2_1_0.aten::zeros_like(%301:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58391:4:0 %303:<12288xf32> = torch.2_1_0.aten::zeros_like(%300:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %301:<12288xf32> = torch.2_1_0.aten::detach(%268:<12288xf32>) 
58391:4:0 %304:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58391:4:0 %305:<12288xf32> = torch.2_1_0.aten::detach(%216:<12288xf32>) 
58391:4:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58391:4:0 %306:<12288xf32> = torch.2_1_0.aten::zeros_like(%300:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %305:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58391:4:0 %307:<12288xf32> = torch.2_1_0.aten::zeros_like(%305:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %300:<12288xf32> = torch.2_1_0.aten::detach(%216:<12288xf32>) 
58391:4:0 %308:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58391:4:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%227:<12288xf32>) 
58391:4:0 %305:<12288xf32> = torch.2_1_0.aten::detach(%191:<12288xf32>) 
58391:4:0 %310:<12288xf32> = torch.2_1_0.aten::zeros_like(%305:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %309:<12288xf32> = torch.2_1_0.aten::detach(%191:<12288xf32>) 
58391:4:0 %311:<12288xf32> = torch.2_1_0.aten::zeros_like(%309:<12288xf32>, dtype=None:NoneType, layout=None:NoneType, device=None:NoneType, pin_memory=False:pred, memory_format=None:NoneType) 
58391:4:0 %305:<12288xf32> = torch.2_1_0.aten::detach(%227:<12288xf32>) 
58391:4:0 %312:<12288xf32> = torch.2_1_0.aten::detach(%191:<12288xf32>) 
58391:None:0 None:NoneType: = torch.2_1_0.profiler::_record_function_exit(ScriptObject <__torch__.torch.classes.profiler._RecordFunction>:ScriptObject) 
58391:4:0 %219:<6400x12288xf16> = torch.2_1_0.aten::detach(%106:<6400x12288xf16>) 
58391:4:0 %292:<6400x12288xf32> = torch.2_1_0.aten::detach(%175:<6400x12288xf32>) 
58391:4:0 %269:<2048x12288xf16> = torch.2_1_0.aten::detach(%127:<2048x12288xf16>) 
58391:4:0 %278:<2048x12288xf32> = torch.2_1_0.aten::detach(%178:<2048x12288xf32>) 
58391:4:0 %296:<12288x1536xf16> = torch.2_1_0.aten::detach(%129:<12288x1536xf16>) 
58391:4:0 %288:<12288x1536xf32> = torch.2_1_0.aten::detach(%173:<12288x1536xf32>) 
58391:4:0 %304:<4608x12288xf16> = torch.2_1_0.aten::detach(%131:<4608x12288xf16>) 
58391:4:0 %308:<4608x12288xf32> = torch.2_1_0.aten::detach(%180:<4608x12288xf32>) 
58391:4:0 %312:<6144x12288xf16> = torch.2_1_0.aten::detach(%96:<6144x12288xf16>) 
58391:4:0 %255:<6144x12288xf32> = torch.2_1_0.aten::detach(%182:<6144x12288xf32>) 
58391:4:0 %202:<12288x6144xf16> = torch.2_1_0.aten::detach(%33:<12288x6144xf16>) 
58391:4:0 %235:<12288x6144xf32> = torch.2_1_0.aten::detach(%179:<12288x6144xf32>) 
58391:4:0 %271:<12288xf16> = torch.2_1_0.aten::detach(%128:<12288xf16>) 
58391:4:0 %299:<12288xf32> = torch.2_1_0.aten::detach(%177:<12288xf32>) 
58391:4:0 %313:<12288xf16> = torch.2_1_0.aten::detach(%37:<12288xf16>) 
58391:4:0 %289:<12288xf32> = torch.2_1_0.aten::detach(%185:<12288xf32>) 
58391:4:0 %293:<12288xf16> = torch.2_1_0.aten::detach(%111:<12288xf16>) 
58391:4:0 %301:<12288xf32> = torch.2_1_0.aten::detach(%186:<12288xf32>) 
58391:4:0 %233:<4608xf16> = torch.2_1_0.aten::detach(%115:<4608xf16>) 
58391:4:0 %300:<4608xf32> = torch.2_1_0.aten::detach(%183:<4608xf32>) 
58391:4:0 %309:<12288xf16> = torch.2_1_0.aten::detach(%132:<12288xf16>) 
58391:4:0 %314:<12288xf32> = torch.2_1_0.aten::detach(%174:<12288xf32>) 
58391:4:0 %315:<12288xf16> = torch.2_1_0.aten::detach(%105:<12288xf16>) 
58391:4:0 %316:<12288xf32> = torch.2_1_0.aten::detach(%188:<12288xf32>) 
58391:4:0 %317:<6144xf16> = torch.2_1_0.aten::detach(%104:<6144xf16>) 
58391:4:0 %318:<6144xf32> = torch.2_1_0.aten::detach(%184:<6144xf32>) 
58391:4:0 %319:<12288xf16> = torch.2_1_0.aten::detach(%88:<12288xf16>) 
58391:4:0 %320:<12288xf32> = torch.2_1_0.aten::detach(%187:<12288xf32>) 
58391:4:0 %321:<12288xf16> = torch.2_1_0.aten::detach(%134:<12288xf16>) 
58391:4:0 %322:<12288xf32> = torch.2_1_0.aten::detach(%192:<12288xf32>) 
58391:4:0 %323:<12288xf16> = torch.2_1_0.aten::detach(%102:<12288xf16>) 
58391:4:0 %324:<12288xf32> = torch.2_1_0.aten::detach(%191:<12288xf32>) 
58392:5:0 %206:<6400x12288xf16> = torch.2_1_0.aten::detach(%24:<6400x12288xf16>) 
58392:5:0 %197:<6400x12288xf32> = torch.2_1_0.aten::detach(%111:<6400x12288xf32>) 
58392:5:0 %198:<2048x12288xf16> = torch.2_1_0.aten::detach(%36:<2048x12288xf16>) 
58391:4:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%171:<1xi32>) 
58392:5:0 %237:<2048x12288xf32> = torch.2_1_0.aten::detach(%110:<2048x12288xf32>) 
58392:5:0 %246:<12288x1536xf16> = torch.2_1_0.aten::detach(%17:<12288x1536xf16>) 
58392:5:0 %239:<12288x1536xf32> = torch.2_1_0.aten::detach(%115:<12288x1536xf32>) 
58392:5:0 %243:<4608x12288xf16> = torch.2_1_0.aten::detach(%37:<4608x12288xf16>) 
58391:4:0 %219:<6400x12288xf16> = torch.2_1_0.aten::copy_(%219:<6400x12288xf16>, %292:<6400x12288xf32>, False:pred) 
58392:5:0 %259:<4608x12288xf32> = torch.2_1_0.aten::detach(%116:<4608x12288xf32>) 
58392:5:0 %263:<6144x12288xf16> = torch.2_1_0.aten::detach(%30:<6144x12288xf16>) 
58391:4:0 %269:<2048x12288xf16> = torch.2_1_0.aten::copy_(%269:<2048x12288xf16>, %278:<2048x12288xf32>, False:pred) 
58392:5:0 %267:<6144x12288xf32> = torch.2_1_0.aten::detach(%117:<6144x12288xf32>) 
58392:5:0 %191:<12288x6144xf16> = torch.2_1_0.aten::detach(%31:<12288x6144xf16>) 
58391:4:0 %296:<12288x1536xf16> = torch.2_1_0.aten::copy_(%296:<12288x1536xf16>, %288:<12288x1536xf32>, False:pred) 
58392:5:0 %205:<12288x6144xf32> = torch.2_1_0.aten::detach(%118:<12288x6144xf32>) 
58392:5:0 %187:<12288xf16> = torch.2_1_0.aten::detach(%7:<12288xf16>) 
58391:4:0 %304:<4608x12288xf16> = torch.2_1_0.aten::copy_(%304:<4608x12288xf16>, %308:<4608x12288xf32>, False:pred) 
58392:5:0 %192:<12288xf32> = torch.2_1_0.aten::detach(%112:<12288xf32>) 
58392:5:0 %241:<12288xf16> = torch.2_1_0.aten::detach(%11:<12288xf16>) 
58391:4:0 %312:<6144x12288xf16> = torch.2_1_0.aten::copy_(%312:<6144x12288xf16>, %255:<6144x12288xf32>, False:pred) 
58392:5:0 %254:<12288xf32> = torch.2_1_0.aten::detach(%109:<12288xf32>) 
58391:4:0 %202:<12288x6144xf16> = torch.2_1_0.aten::copy_(%202:<12288x6144xf16>, %235:<12288x6144xf32>, False:pred) 
58392:5:0 %247:<12288xf16> = torch.2_1_0.aten::detach(%13:<12288xf16>) 
58392:5:0 %252:<12288xf32> = torch.2_1_0.aten::detach(%119:<12288xf32>) 
58391:4:0 %271:<12288xf16> = torch.2_1_0.aten::copy_(%271:<12288xf16>, %299:<12288xf32>, False:pred) 
58392:5:0 %242:<4608xf16> = torch.2_1_0.aten::detach(%8:<4608xf16>) 
58392:5:0 %264:<4608xf32> = torch.2_1_0.aten::detach(%108:<4608xf32>) 
58391:4:0 %313:<12288xf16> = torch.2_1_0.aten::copy_(%313:<12288xf16>, %289:<12288xf32>, False:pred) 
58392:5:0 %255:<12288xf16> = torch.2_1_0.aten::detach(%21:<12288xf16>) 
58391:4:0 %293:<12288xf16> = torch.2_1_0.aten::copy_(%293:<12288xf16>, %301:<12288xf32>, False:pred) 
58392:5:0 %260:<12288xf32> = torch.2_1_0.aten::detach(%121:<12288xf32>) 
58392:5:0 %258:<12288xf16> = torch.2_1_0.aten::detach(%16:<12288xf16>) 
58391:4:0 %233:<4608xf16> = torch.2_1_0.aten::copy_(%233:<4608xf16>, %300:<4608xf32>, False:pred) 
58392:5:0 %268:<12288xf32> = torch.2_1_0.aten::detach(%122:<12288xf32>) 
58392:5:0 %269:<6144xf16> = torch.2_1_0.aten::detach(%6:<6144xf16>) 
58391:4:0 %309:<12288xf16> = torch.2_1_0.aten::copy_(%309:<12288xf16>, %314:<12288xf32>, False:pred) 
58392:5:0 %270:<6144xf32> = torch.2_1_0.aten::detach(%114:<6144xf32>) 
58391:4:0 %315:<12288xf16> = torch.2_1_0.aten::copy_(%315:<12288xf16>, %316:<12288xf32>, False:pred) 
58392:5:0 %271:<12288xf16> = torch.2_1_0.aten::detach(%33:<12288xf16>) 
58392:5:0 %272:<12288xf32> = torch.2_1_0.aten::detach(%124:<12288xf32>) 
58391:4:0 %317:<6144xf16> = torch.2_1_0.aten::copy_(%317:<6144xf16>, %318:<6144xf32>, False:pred) 
58392:5:0 %273:<12288xf16> = torch.2_1_0.aten::detach(%38:<12288xf16>) 
58392:5:0 %274:<12288xf32> = torch.2_1_0.aten::detach(%125:<12288xf32>) 
58391:4:0 %319:<12288xf16> = torch.2_1_0.aten::copy_(%319:<12288xf16>, %320:<12288xf32>, False:pred) 
58392:5:0 %146:<12288xf16> = torch.2_1_0.aten::detach(%34:<12288xf16>) 
58392:5:0 %275:<12288xf32> = torch.2_1_0.aten::detach(%126:<12288xf32>) 
58391:4:0 %321:<12288xf16> = torch.2_1_0.aten::copy_(%321:<12288xf16>, %322:<12288xf32>, False:pred) 
58391:4:0 %323:<12288xf16> = torch.2_1_0.aten::copy_(%323:<12288xf16>, %324:<12288xf32>, False:pred) 
58392:5:0 0:i32: = torch.2_1_0.aten::_local_scalar_dense(%107:<1xi32>) 
58392:5:0 %206:<6400x12288xf16> = torch.2_1_0.aten::copy_(%206:<6400x12288xf16>, %197:<6400x12288xf32>, False:pred) 
58392:5:0 %198:<2048x12288xf16> = torch.2_1_0.aten::copy_(%198:<2048x12288xf16>, %237:<2048x12288xf32>, False:pred) 
58392:5:0 %246:<12288x1536xf16> = torch.2_1_0.aten::copy_(%246:<12288x1536xf16>, %239:<12288x1536xf32>, False:pred) 
58392:5:0 %243:<4608x12288xf16> = torch.2_1_0.aten::copy_(%243:<4608x12288xf16>, %259:<4608x12288xf32>, False:pred) 
58392:5:0 %263:<6144x12288xf16> = torch.2_1_0.aten::copy_(%263:<6144x12288xf16>, %267:<6144x12288xf32>, False:pred) 
58391:4:0 %255:<f32> = torch.2_1_0.aten::add(%266:<f32>, 0:i32, alpha=1:i32) 
58392:5:0 %191:<12288x6144xf16> = torch.2_1_0.aten::copy_(%191:<12288x6144xf16>, %205:<12288x6144xf32>, False:pred) 
58392:5:0 %187:<12288xf16> = torch.2_1_0.aten::copy_(%187:<12288xf16>, %192:<12288xf32>, False:pred) 
58392:5:0 %241:<12288xf16> = torch.2_1_0.aten::copy_(%241:<12288xf16>, %254:<12288xf32>, False:pred) 
58392:5:0 %247:<12288xf16> = torch.2_1_0.aten::copy_(%247:<12288xf16>, %252:<12288xf32>, False:pred) 
58391:4:0 %299:<f32> = torch.2_1_0.aten::add(%263:<f32>+1, 0:i32, alpha=1:i32) 
58392:5:0 %242:<4608xf16> = torch.2_1_0.aten::copy_(%242:<4608xf16>, %264:<4608xf32>, False:pred) 
58392:5:0 %255:<12288xf16> = torch.2_1_0.aten::copy_(%255:<12288xf16>, %260:<12288xf32>, False:pred) 
58392:5:0 %258:<12288xf16> = torch.2_1_0.aten::copy_(%258:<12288xf16>, %268:<12288xf32>, False:pred) 
58392:5:0 %269:<6144xf16> = torch.2_1_0.aten::copy_(%269:<6144xf16>, %270:<6144xf32>, False:pred) 
58392:5:0 %271:<12288xf16> = torch.2_1_0.aten::copy_(%271:<12288xf16>, %272:<12288xf32>, False:pred) 
58392:5:0 %273:<12288xf16> = torch.2_1_0.aten::copy_(%273:<12288xf16>, %274:<12288xf32>, False:pred) 
58391:4:0 %289:<f32> = torch.2_1_0.aten::div(%255:<f32>, %299:<f32>) 
58392:5:0 %146:<12288xf16> = torch.2_1_0.aten::copy_(%146:<12288xf16>, %275:<12288xf32>, False:pred) 
58391:4:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%35:<1xf32>) 
58391:4:0 %258:<1xf32> = torch.2_1_0.aten::lift_fresh(%258:<1xf32>) 
58392:5:0 %259:<f32> = torch.2_1_0.aten::add(%202:<f32>, 0:i32, alpha=1:i32) 
58391:4:0 %235:<1xf32> = torch.2_1_0.aten::add(%258:<1xf32>, %289:<f32>, alpha=1:i32) 
58392:5:0 %205:<f32> = torch.2_1_0.aten::add(%210:<f32>+1, 0:i32, alpha=1:i32) 
58391:4:0 %280:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 %239:<f32> = torch.2_1_0.aten::div(%259:<f32>, %205:<f32>) 
58392:5:0 1024.0:f32: = torch.2_1_0.aten::_local_scalar_dense(%42:<1xf32>) 
58392:5:0 %202:<1xf32> = torch.2_1_0.aten::lift_fresh(%202:<1xf32>) 
58391:4:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%280:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
58392:5:0 %259:<1xf32> = torch.2_1_0.aten::add(%202:<1xf32>, %239:<f32>, alpha=1:i32) 
58392:5:0 %259:<1xu8> = torch.2_1_0.aten::empty(list{1:i32}, dtype=torch.uint8:dtype, layout=None:NoneType, device=cuda:device, pin_memory=None:NoneType, memory_format=None:NoneType) 
58392:5:0 ScriptObject <__torch__.torch.classes.c10d.Work>:ScriptObject: = torch.2_1_0.c10d::barrier(%259:<1xu8>, ScriptObject <__torch__.torch.classes.c10d.ProcessGroup>:ScriptObject, list{}, -1:i32) 
